{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04010a08",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8fa3d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "# set path\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../src/')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tokenizer import TokenizerBPE\n",
    "from data_handling import normalize_to_ascii\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pickle as pkl\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "\n",
    "# disable gpu for testing purposes\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ed7139",
   "metadata": {},
   "source": [
    "## Trivia Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a782972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_evidence(n, rank, evidence_article):\n",
    "    top_n = []\n",
    "    for i in range(n):\n",
    "        article = evidence_article[rank[i][0]]\n",
    "        top_n.append(article)\n",
    "    return top_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76f93d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c8d5c90126e481c84fb388da57dff0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76496 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open('../corpus/triviaqa-rc/qa/web-train.json', 'r', encoding='utf-8') as f:\n",
    "    squad = json.load(f)\n",
    "\n",
    "question_list = []\n",
    "answer_list = []\n",
    "evidence_list = []\n",
    "\n",
    "for qa in tqdm(squad['Data']):\n",
    "    question_list.append(qa['Question'])\n",
    "    answer_list.append(qa['Answer'][\"Value\"])\n",
    "    sr = qa['SearchResults']\n",
    "    if len(sr) > 0: \n",
    "        evidence_list.append(sr[0][\"Filename\"])\n",
    "    else:\n",
    "        evidence_list.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4090e18d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ac3ad275214acf844865a43bcb4b43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76496 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = \"../corpus/triviaqa-rc/evidence/web/\"\n",
    "\n",
    "evidence_article = {}\n",
    "seen = {}\n",
    "\n",
    "for i, evidence in tqdm(list(enumerate(evidence_list))):\n",
    "    if evidence is not None:\n",
    "        evidence = evidence.replace(\":\", \"_\").replace(\"?\", \"_\").replace(\"*\", \"_\").replace('\"', \"_\")\n",
    "        if not evidence in seen:\n",
    "            seen[evidence] = 1\n",
    "            try:\n",
    "                txt = open(path + evidence, 'r', encoding='utf-8').read()\n",
    "                evidence_article[evidence] = txt\n",
    "            except:\n",
    "                print(\"Error reading file: \", evidence)\n",
    "                continue\n",
    "        else:\n",
    "            seen[evidence] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98243205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "233689069\n",
      "64719\n"
     ]
    }
   ],
   "source": [
    "rank = [[evidence, freq] for freq, evidence in sorted(zip(seen.values(), seen.keys()), reverse=True)]\n",
    "\n",
    "article_list = get_top_n_evidence(15000, rank, evidence_article)\n",
    "\n",
    "print(len(\"\".join(article_list)))\n",
    "print(len(rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0780bbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../')\n",
    "tokenizer = pkl.load(open(\"../tokenizers/tokenizer_superQA_24k.pkl\", \"rb\"))\n",
    "tokenizer.create_hash()\n",
    "tokenizer.add_special_tokens([\"<s>\", \"</s>\", \"<q>\", \"<a>\", \"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6e1e7e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fused_article(corpus_list):\n",
    "    sos = \"<s>\"\n",
    "    eos = \"</s>\"\n",
    "\n",
    "    rcw = re.compile(r\"\\s+\")\n",
    "\n",
    "    corpus_padded_list = []\n",
    "    for line in corpus_list:\n",
    "        line = line.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "        line = rcw.sub(\" \", line).strip()\n",
    "        line = [sos, normalize_to_ascii(line).lower(), eos]\n",
    "        corpus_padded_list.extend(line)\n",
    "    \n",
    "    return \"\".join(corpus_padded_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ef90eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56bb5afdfe52448480e55221484a7c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fused = fused_article(article_list)\n",
    "corpus_encoded = tokenizer.encode(fused.lower(), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e65c4354",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(corpus_encoded, open('../corpus/corpus_clean/corpus_web_article_24k', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29376866",
   "metadata": {},
   "source": [
    "## QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d62eca6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fused_qa(question_list, answer_list):\n",
    "    q =\"<q>\"\n",
    "    a = \"<a>\"\n",
    "    sos = \"<s>\"\n",
    "    eos = \"</s>\"\n",
    "    rcw = re.compile(r\"\\s+\")\n",
    "\n",
    "    corpus_list = []\n",
    "    for question, answer in tqdm(list(zip(question_list, answer_list))):\n",
    "        question = question.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "        question = rcw.sub(\" \", question).strip()\n",
    "        answer = answer.replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "        answer = rcw.sub(\" \", answer).strip()\n",
    "        qa = [sos, q, normalize_to_ascii(question), a, normalize_to_ascii(answer), eos]\n",
    "        corpus_list.extend(qa)\n",
    "        \n",
    "    return \"\".join(corpus_list).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9406d4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a66cd293b7a4624a985029ed3cf9f1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76496 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed97770ec6b43d3ad4c34ed3e15adc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/24000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fused = fused_qa(question_list, answer_list)\n",
    "corpus_encoded = tokenizer.encode(fused, verbose=True)\n",
    "\n",
    "pkl.dump(corpus_encoded, open('../corpus/corpus_clean/corpus_web_qa_24k', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a5f20b",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
