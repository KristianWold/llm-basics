{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04010a08",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa3d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from src.tokenizer import TokenizerBPE, word_split, normalize_to_ascii\n",
    "\n",
    "import os\n",
    "import time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm.notebook import tqdm\n",
    "from src.transformer import *\n",
    "from src.data_handling import read_first_n, sample_batch\n",
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b59898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afd4e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = pkl.load(open(\"tokenizers/tokenizer_CNN16000_lowercase.pkl\", 'rb'))\n",
    "tokenizer.create_hash()\n",
    "\n",
    "random.seed(42)\n",
    "corpus_indicies = pkl.load(open('corpus/CNN_tokenized16000_lowercase.pkl', 'rb'))\n",
    "random.shuffle(corpus_indicies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f830d881",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a564402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmUpThenDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self,\n",
    "                 initial_learning_rate: float,\n",
    "                 warmup_steps: int,\n",
    "                 decay_schedule_fn: tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "        \"\"\"\n",
    "        initial_learning_rate: peak LR reached at end of warmup\n",
    "        warmup_steps:      # of steps to ramp from 0 â†’ initial_learning_rate\n",
    "        decay_schedule_fn: a tf.keras schedule to apply *after* warmup\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.initial_lr = initial_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.decay_schedule_fn = decay_schedule_fn\n",
    "\n",
    "    def __call__(self, step):\n",
    "        # Cast to float32 for safety in graph mode\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
    "\n",
    "        # compute linear warmup: lr = initial_lr * (step / warmup_steps)\n",
    "        warmup_lr = self.initial_lr * (step / warmup_steps)\n",
    "\n",
    "        # after warmup_steps, switch to decay schedule (shift step count)\n",
    "        decay_step = step - warmup_steps\n",
    "        decay_lr = self.decay_schedule_fn(decay_step)\n",
    "\n",
    "        # if step < warmup_steps, pick warmup_lr, else decay_lr\n",
    "        return tf.cond(step < warmup_steps,\n",
    "                       lambda: warmup_lr,\n",
    "                       lambda: decay_lr)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a5a33ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lr = 1e-3\n",
    "decay_steps = 50000\n",
    "decay_rate = 0.5\n",
    "decay_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_lr,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=False)\n",
    "\n",
    "warmup_steps = 10000\n",
    "lr_schedule = WarmUpThenDecay(\n",
    "    initial_learning_rate=initial_lr,\n",
    "    warmup_steps=warmup_steps,\n",
    "    decay_schedule_fn=decay_schedule)\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "max_seq_len = 200\n",
    "embed_dim = 1024\n",
    "tf_blocks = 8\n",
    "heads = 8\n",
    "ff_dim = 4*embed_dim\n",
    "\n",
    "unembed_dims = []\n",
    "\n",
    "model = Transformer(vocab_size=tokenizer.vocab_size,\n",
    "                    max_seq_len=max_seq_len,\n",
    "                    embed_dim=embed_dim,\n",
    "                    tf_blocks=tf_blocks,\n",
    "                    heads=heads,\n",
    "                    ff_dim = ff_dim,\n",
    "                    unembed_dims=unembed_dims,\n",
    "                    lr=lr_schedule,\n",
    "                    wd = 0.01,\n",
    "                    )\n",
    "\n",
    "losses_train = []\n",
    "losses_test = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7970a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"model_16k_tokens_lowercase_8blocks_wt_higherLR\"\n",
    "\n",
    "\n",
    "ckpt = tf.train.Checkpoint(\n",
    "    optimizer=model.optimizer,\n",
    "    model=model\n",
    ")\n",
    "ckpt_manager = tf.train.CheckpointManager(\n",
    "    ckpt, \n",
    "    directory=\"checkpoints/\" + name,      # folder where ckpts are saved\n",
    "    max_to_keep=5                         # only keep 5 latest checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "88b34765",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "losses_train, losses_test = pkl.load(open(\"checkpoints/losses_\" + name + \".pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6527620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 117413574\n"
     ]
    }
   ],
   "source": [
    "total_params = 0\n",
    "for var in model.parameter_list:\n",
    "    shape = var.get_shape()\n",
    "    num_params = 1\n",
    "    for dim in shape:\n",
    "        num_params *= dim\n",
    "    total_params += num_params\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabbfacb",
   "metadata": {},
   "source": [
    "## Text completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "58649737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(indices, merge_list):\n",
    "    indices = np.array(indices)\n",
    "    for pair, new_idx in merge_list:\n",
    "        slice = np.where(np.logical_and(indices[:-1] == pair[0],  indices[1:] == pair[1]))\n",
    "        if len(slice[0]) > 0:\n",
    "            indices[:-1][slice] = new_idx\n",
    "            indices = np.delete(indices, (slice[0]+1))\n",
    "\n",
    "    return tf.expand_dims(tf.convert_to_tensor(indices, dtype=tf.int32), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "e216ce01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[2655]], shape=(1, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "text = \"terrorist\"\n",
    "text = text.lower()\n",
    "\n",
    "indices = tf.cast(tokenizer.tokenizer.tokenize(text), tf.int32)\n",
    "indices = tokenize(indices, tokenizer.merge_list)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "74a017b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "terrorist group in the khy region of afghanistan in the khyber district of khyber, khyber, is a major attack for the khyber district of khyber pakkhtunkhwa pakkhwatunkhwa province in khyberkhwakhwa pakkhwatunkhwa, kwaakwa and kkhyber kwawa kwa khwawawa sew khy ktunkhwa kwa kwa kwawa, kakwawa kwa kkwawa kkk-k  kk kkk kkkkkkkkk kkkkkkkkkkkkkkkkkkkkkk\r"
     ]
    }
   ],
   "source": [
    "T = 0.5\n",
    "tf.random.set_seed(42)\n",
    "for i in range(200):\n",
    "    logits = model.call(indices)[0,-1:]\n",
    "    idx = tf.cast(tf.random.categorical(logits/T, num_samples=1), tf.int32)\n",
    "    indices = tf.concat([indices, idx], axis=1)\n",
    "    text_pred = tokenizer.detokenize(indices)\n",
    "    text_pred = text_pred.numpy()[0].decode('utf-8').replace(\"\\n\", \" \")\n",
    "    print(text_pred, end='\\r', flush=True)\n",
    "    #time.sleep(0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1788f0",
   "metadata": {},
   "source": [
    "## Embedding Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "e77fb01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def cosine_similarity(embed_a, embed_b):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two vectors.\n",
    "    \"\"\"\n",
    "    embed_b_T = tf.transpose(embed_b)\n",
    "    dot_product = embed_a@embed_b_T\n",
    "    \n",
    "    norm_a = tf.linalg.norm(embed_a, axis=1, keepdims=True)\n",
    "    norm_b = tf.linalg.norm(embed_b_T, axis=0, keepdims=True)\n",
    "\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "\n",
    "def cluster(X, n_clusters):\n",
    "    X = X/np.linalg.norm(X, axis=1, keepdims=True)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n",
    "    inertia = kmeans.inertia_\n",
    "    labels = kmeans.labels_\n",
    "    clusters = kmeans.cluster_centers_\n",
    "\n",
    "    return inertia, labels, clusters\n",
    "\n",
    "\n",
    "class EmbeddingClustering:\n",
    "    def __init__(self, tokenizer, n_clusters=10):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_clusters = n_clusters\n",
    "\n",
    "    def fit(self, word_embed):\n",
    "        inertia, labels, clusters = cluster(word_embed, self.n_clusters)\n",
    "        self.word_embed = word_embed\n",
    "        self.inertia = inertia\n",
    "        self.labels = labels\n",
    "        self.clusters = tf.convert_to_tensor(clusters, dtype=tf.float32)\n",
    "\n",
    "        cos_sim = cosine_similarity(self.clusters, word_embed)\n",
    "        self.idx_list =  tf.argsort(cos_sim, axis=-1, direction='DESCENDING', stable=False, name=None)\n",
    "\n",
    "    def print_clusters(self, n_words=10):\n",
    "        for idx in self.idx_list:\n",
    "            for i in idx[:n_words]:\n",
    "                word = self.tokenizer.detokenize(tf.expand_dims(tf.cast(i, tf.int32), axis=0))\n",
    "                word = word.numpy().decode('utf-8')\n",
    "                print(word)\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48583bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colors\n",
      "dresses\n",
      "lips\n",
      "makeup\n",
      "dress\n",
      "jeans\n",
      "pieces\n",
      "beauty\n",
      "buttons\n",
      "hair\n",
      "\n",
      "\n",
      "damasc\n",
      "0s\n",
      "hurrican\n",
      "mingham\n",
      "nairo\n",
      "sterdam\n",
      "propof\n",
      "barcelon\n",
      "traged\n",
      "catastro\n",
      "\n",
      "\n",
      "hurrican\n",
      "mingham\n",
      "raik\n",
      "prede\n",
      "environ\n",
      "recor\n",
      "agric\n",
      "theless\n",
      "uguay\n",
      "eleph\n",
      "\n",
      "\n",
      "acare\n",
      "assaul\n",
      "inev\n",
      "attem\n",
      "adjac\n",
      "ailand\n",
      "wawrink\n",
      "emerg\n",
      "controver\n",
      "strengthe\n",
      "\n",
      "\n",
      "citiz\n",
      "}\n",
      "invol\n",
      "dipl\n",
      "itored\n",
      "moil\n",
      "possib\n",
      "acare\n",
      "eleph\n",
      "environ\n",
      "\n",
      "\n",
      "insurgent\n",
      "haqqani\n",
      "insurgency\n",
      "airstrike\n",
      "infantry\n",
      "jihadist\n",
      "environ\n",
      "{\n",
      "streng\n",
      "abulary\n",
      "\n",
      "\n",
      "a\n",
      "the\n",
      "some\n",
      "an\n",
      "more\n",
      "many\n",
      "this\n",
      "one\n",
      "their\n",
      "his\n",
      "\n",
      "\n",
      "hezbol\n",
      "recor\n",
      "mingham\n",
      "environ\n",
      "raik\n",
      "hurrican\n",
      "uguay\n",
      "eleph\n",
      "adjac\n",
      "catastro\n",
      "\n",
      "\n",
      "citiz\n",
      "acp\n",
      "firs\n",
      "onathan\n",
      "idency\n",
      "employe\n",
      "hift\n",
      "streng\n",
      "abulary\n",
      "mogad\n",
      "\n",
      "\n",
      "0s\n",
      "injiang\n",
      "wawrink\n",
      "raik\n",
      "alep\n",
      "weren\n",
      "endum\n",
      "onathan\n",
      "inev\n",
      "portugu\n",
      "\n",
      "\n",
      "hezbol\n",
      "injiang\n",
      "sterdam\n",
      "mogad\n",
      "wawrink\n",
      "indust\n",
      "propof\n",
      "guate\n",
      "citiz\n",
      "assaul\n",
      "\n",
      "\n",
      "wondering\n",
      "wondered\n",
      "think\n",
      "believe\n",
      "know\n",
      "realize\n",
      "understand\n",
      "convinced\n",
      "skeptical\n",
      "worry\n",
      "\n",
      "\n",
      "citiz\n",
      "mogad\n",
      "signific\n",
      "injiang\n",
      "abulary\n",
      "luscon\n",
      "immen\n",
      "portra\n",
      "hurrican\n",
      "adjac\n",
      "\n",
      "\n",
      "chry\n",
      "discre\n",
      "condem\n",
      "nostal\n",
      "teles\n",
      "incar\n",
      "refr\n",
      "leng\n",
      "subur\n",
      "prelim\n",
      "\n",
      "\n",
      "pleaded\n",
      "appealed\n",
      "refused\n",
      "instructed\n",
      "indicted\n",
      "vowed\n",
      "advised\n",
      "immen\n",
      "subjected\n",
      "surrendered\n",
      "\n",
      "\n",
      "league's\n",
      "sport's\n",
      "department's\n",
      "gop's\n",
      "military's\n",
      "site's\n",
      "committee's\n",
      "army's\n",
      "fbi's\n",
      "organization's\n",
      "\n",
      "\n",
      "taking\n",
      "pulling\n",
      "bringing\n",
      "giving\n",
      "turning\n",
      "helping\n",
      "losing\n",
      "putting\n",
      "getting\n",
      "stepping\n",
      "\n",
      "\n",
      "hezbol\n",
      "shaba\n",
      "{\n",
      "eleph\n",
      "raik\n",
      "mingham\n",
      "recor\n",
      "weren\n",
      "territor\n",
      "guaran\n",
      "\n",
      "\n",
      "rhode\n",
      "kansas\n",
      "missouri\n",
      "nashville\n",
      "delaware\n",
      "milwaukee\n",
      "wyoming\n",
      "tampa\n",
      "louisville\n",
      "connecticut\n",
      "\n",
      "\n",
      "raik\n",
      "hezbol\n",
      "hurrican\n",
      "environ\n",
      "mingham\n",
      "auster\n",
      "icut\n",
      "onathan\n",
      "strengthe\n",
      "indust\n",
      "\n",
      "\n",
      "capt\n",
      "landsc\n",
      "chry\n",
      "rev\n",
      "sr\n",
      "sgt\n",
      "adm\n",
      "brig\n",
      "mrs\n",
      "scr\n",
      "\n",
      "\n",
      "greece's\n",
      "thailand's\n",
      "lebanon's\n",
      "argentina's\n",
      "somalia's\n",
      "indonesia's\n",
      "ukraine's\n",
      "libya's\n",
      "myanmar's\n",
      "kenya's\n",
      "\n",
      "\n",
      "economists\n",
      "commentators\n",
      "executives\n",
      "pundits\n",
      "regulators\n",
      "scholars\n",
      "legislators\n",
      "enthusiasts\n",
      "retailers\n",
      "editors\n",
      "\n",
      "\n",
      "adjac\n",
      "propof\n",
      "perpe\n",
      "environ\n",
      "raik\n",
      "recor\n",
      "infrast\n",
      "traff\n",
      "mingham\n",
      "ichet\n",
      "\n",
      "\n",
      "mingham\n",
      "abulary\n",
      "shaba\n",
      "hurrican\n",
      "wawrink\n",
      "assaul\n",
      "massachu\n",
      "essay\n",
      "mbley\n",
      "lades\n",
      "\n",
      "\n",
      "showdown\n",
      "tournament\n",
      "match\n",
      "clash\n",
      "title\n",
      "contest\n",
      "triumph\n",
      "victory\n",
      "championship\n",
      "ceremony\n",
      "\n",
      "\n",
      "emergency\n",
      "rescue\n",
      "relief\n",
      "disaster\n",
      "shelter\n",
      "aid\n",
      "recovery\n",
      "search\n",
      "passenger\n",
      "airport\n",
      "\n",
      "\n",
      "october\n",
      "june\n",
      "july\n",
      "november\n",
      "december\n",
      "tuesday\n",
      "january\n",
      "february\n",
      "monday\n",
      "wednesday\n",
      "\n",
      "\n",
      "in\n",
      "on\n",
      "to\n",
      "out\n",
      "back\n",
      "off\n",
      "up\n",
      "down\n",
      "over\n",
      "at\n",
      "\n",
      "\n",
      "authen\n",
      "hallow\n",
      "proble\n",
      "damasc\n",
      "tennes\n",
      "agric\n",
      "diox\n",
      "hurrican\n",
      "milwau\n",
      "mingham\n",
      "\n",
      "\n",
      "sterdam\n",
      "mogad\n",
      "setts\n",
      "massachu\n",
      "nairo\n",
      "0s\n",
      "avez\n",
      "acare\n",
      "assaul\n",
      "portugu\n",
      "\n",
      "\n",
      "mingham\n",
      "pean\n",
      "acare\n",
      "injiang\n",
      "shaba\n",
      "theless\n",
      "massachu\n",
      "uguay\n",
      "sterdam\n",
      "scenar\n",
      "\n",
      "\n",
      "initiatives\n",
      "sectors\n",
      "strategies\n",
      "protocols\n",
      "technologies\n",
      "platforms\n",
      "practices\n",
      "channels\n",
      "programs\n",
      "processes\n",
      "\n",
      "\n",
      "mogad\n",
      "mingham\n",
      "uguay\n",
      "shaba\n",
      "dipl\n",
      "wawrink\n",
      "sterdam\n",
      "grues\n",
      "citiz\n",
      "acare\n",
      "\n",
      "\n",
      "recor\n",
      "raik\n",
      "adjac\n",
      "uguay\n",
      "weren\n",
      "mingham\n",
      "gether\n",
      "citiz\n",
      "massachu\n",
      "controver\n",
      "\n",
      "\n",
      "remain\n",
      "look\n",
      "stay\n",
      "appear\n",
      "seem\n",
      "become\n",
      "feel\n",
      "continue\n",
      "keep\n",
      "speak\n",
      "\n",
      "\n",
      "probably\n",
      "certainly\n",
      "usually\n",
      "always\n",
      "obviously\n",
      "never\n",
      "actually\n",
      "indeed\n",
      "ultimately\n",
      "finally\n",
      "\n",
      "\n",
      "anthony's\n",
      "houston's\n",
      "baby's\n",
      "patient's\n",
      "daughter's\n",
      "girl's\n",
      "suspect's\n",
      "friend's\n",
      "victim's\n",
      "hernandez's\n",
      "\n",
      "\n",
      "damasc\n",
      "distin\n",
      "nairo\n",
      "refr\n",
      "0s\n",
      "authen\n",
      "cincin\n",
      "petrole\n",
      "theless\n",
      "portugu\n",
      "\n",
      "\n",
      "it's\n",
      "i've\n",
      "you've\n",
      "we're\n",
      "we've\n",
      "i'm\n",
      "there's\n",
      "they've\n",
      "you're\n",
      "they're\n",
      "\n",
      "\n",
      "massachu\n",
      "abulary\n",
      "raik\n",
      "adjac\n",
      "recor\n",
      "mingham\n",
      "citiz\n",
      "shaba\n",
      "environ\n",
      "includ\n",
      "\n",
      "\n",
      "during\n",
      "after\n",
      "at\n",
      "before\n",
      "when\n",
      "by\n",
      "for\n",
      "with\n",
      "from\n",
      "in\n",
      "\n",
      "\n",
      "santorum\n",
      "injiang\n",
      "raik\n",
      "wawrink\n",
      "adjac\n",
      "environ\n",
      "emerg\n",
      "streng\n",
      "eleph\n",
      "guate\n",
      "\n",
      "\n",
      "european\n",
      "british\n",
      "national\n",
      "western\n",
      "northern\n",
      "new\n",
      "southern\n",
      "former\n",
      "local\n",
      "french\n",
      "\n",
      "\n",
      "spokesman\n",
      "chief\n",
      "secretary\n",
      "deputy\n",
      "chairman\n",
      "attorney\n",
      "spokeswoman\n",
      "commissioner\n",
      "commander\n",
      "adviser\n",
      "\n",
      "\n",
      "citiz\n",
      "munic\n",
      "firs\n",
      "eleph\n",
      "guate\n",
      "adjac\n",
      "hawar\n",
      "acare\n",
      "moil\n",
      "idency\n",
      "\n",
      "\n",
      "severely\n",
      "unfairly\n",
      "poorly\n",
      "assaul\n",
      "swiftly\n",
      "endum\n",
      "grues\n",
      "iterran\n",
      "includ\n",
      "thoroughly\n",
      "\n",
      "\n",
      "beautiful\n",
      "tough\n",
      "nice\n",
      "exciting\n",
      "brilliant\n",
      "fascinating\n",
      "lovely\n",
      "tricky\n",
      "interesting\n",
      "gorgeous\n",
      "\n",
      "\n",
      "give\n",
      "bring\n",
      "take\n",
      "make\n",
      "get\n",
      "listen\n",
      "send\n",
      "learn\n",
      "see\n",
      "return\n",
      "\n",
      "\n",
      "portugu\n",
      "acare\n",
      "immen\n",
      "prede\n",
      "strengthe\n",
      "massachu\n",
      "hurrican\n",
      "nairo\n",
      "guate\n",
      "traff\n",
      "\n",
      "\n",
      "furthermore\n",
      "moreover\n",
      "thankfully\n",
      "immen\n",
      "propof\n",
      "yeah\n",
      "dipl\n",
      "mogad\n",
      "afterwards\n",
      "nairo\n",
      "\n",
      "\n",
      "threat\n",
      "problem\n",
      "solution\n",
      "situation\n",
      "signal\n",
      "danger\n",
      "decision\n",
      "conclusion\n",
      "relationship\n",
      "threshold\n",
      "\n",
      "\n",
      "frances\n",
      "gior\n",
      "christop\n",
      "gian\n",
      "ru\n",
      "achu\n",
      "mu\n",
      "hod\n",
      "tus\n",
      "hein\n",
      "\n",
      "\n",
      "mclaugh\n",
      "strugg\n",
      "raik\n",
      "liby\n",
      "uguay\n",
      "injiang\n",
      "}\n",
      "adjac\n",
      "eleph\n",
      "itored\n",
      "\n",
      "\n",
      "89\n",
      "84\n",
      "93\n",
      "56\n",
      "175\n",
      "58\n",
      "197\n",
      "73\n",
      "230\n",
      "62\n",
      "\n",
      "\n",
      "encouraged\n",
      "reluctant\n",
      "unwilling\n",
      "sought\n",
      "managed\n",
      "invited\n",
      "unable\n",
      "attempting\n",
      "wanted\n",
      "intended\n",
      "\n",
      "\n",
      "strengthe\n",
      "abulary\n",
      "citiz\n",
      "hezbol\n",
      "environ\n",
      "dipl\n",
      "immen\n",
      "adjac\n",
      "mingham\n",
      "traff\n",
      "\n",
      "\n",
      "citiz\n",
      "indust\n",
      "hift\n",
      "possib\n",
      "strengthe\n",
      "immen\n",
      "moil\n",
      "environ\n",
      "grues\n",
      "sterdam\n",
      "\n",
      "\n",
      "recor\n",
      "prede\n",
      "cigare\n",
      "massachu\n",
      "tunisia\n",
      "finland\n",
      "injiang\n",
      "shaba\n",
      "}\n",
      "experi\n",
      "\n",
      "\n",
      "characterized\n",
      "touted\n",
      "operated\n",
      "compiled\n",
      "regarded\n",
      "influenced\n",
      "funded\n",
      "referred\n",
      "starred\n",
      "billed\n",
      "\n",
      "\n",
      "daughters\n",
      "husbands\n",
      "siblings\n",
      "mothers\n",
      "fathers\n",
      "grandparents\n",
      "moms\n",
      "grandmother\n",
      "sisters\n",
      "dads\n",
      "\n",
      "\n",
      "comparing\n",
      "explaining\n",
      "urging\n",
      "acknowledging\n",
      "letting\n",
      "presenting\n",
      "confirming\n",
      "granting\n",
      "accusing\n",
      "highlighting\n",
      "\n",
      "\n",
      "territor\n",
      "assaul\n",
      "citiz\n",
      "alep\n",
      "raik\n",
      "auster\n",
      "sterdam\n",
      "prede\n",
      "cigare\n",
      "ichet\n",
      "\n",
      "\n",
      "streng\n",
      "environ\n",
      "attem\n",
      "assaul\n",
      "citiz\n",
      "includ\n",
      "\\\n",
      "dort\n",
      "emerg\n",
      "signific\n",
      "\n",
      "\n",
      "traged\n",
      "hallow\n",
      "uguay\n",
      "0s\n",
      "damasc\n",
      "eday\n",
      "recor\n",
      "auster\n",
      "catastro\n",
      "tennes\n",
      "\n",
      "\n",
      "mingham\n",
      "acare\n",
      "assaul\n",
      "recor\n",
      "strengthe\n",
      "mbley\n",
      "raik\n",
      "uguay\n",
      "shaba\n",
      "environ\n",
      "\n",
      "\n",
      "investigations\n",
      "inquiries\n",
      "arrests\n",
      "conclusions\n",
      "deliberations\n",
      "objections\n",
      "accusations\n",
      "suspicions\n",
      "allegations\n",
      "statements\n",
      "\n",
      "\n",
      "populous\n",
      "authoritarian\n",
      "predominantly\n",
      "shiite\n",
      "buddhist\n",
      "sunni\n",
      "tural\n",
      "sectarian\n",
      "separatist\n",
      "ethnic\n",
      "\n",
      "\n",
      "recor\n",
      "catastro\n",
      "eleph\n",
      "environ\n",
      "prede\n",
      "mingham\n",
      "hurrican\n",
      "portugu\n",
      "raik\n",
      "hallow\n",
      "\n",
      "\n",
      "10\n",
      "20\n",
      "15\n",
      "30\n",
      "25\n",
      "12\n",
      "14\n",
      "4\n",
      "40\n",
      "17\n",
      "\n",
      "\n",
      "took\n",
      "became\n",
      "went\n",
      "lasted\n",
      "gained\n",
      "remained\n",
      "looked\n",
      "came\n",
      "suffered\n",
      "arrived\n",
      "\n",
      "\n",
      "1981\n",
      "1964\n",
      "1988\n",
      "1992\n",
      "1990\n",
      "1991\n",
      "1963\n",
      "1969\n",
      "1978\n",
      "1985\n",
      "\n",
      "\n",
      "encourages\n",
      "attracts\n",
      "protects\n",
      "provides\n",
      "reminds\n",
      "prevents\n",
      "boasts\n",
      "eases\n",
      "gives\n",
      "citiz\n",
      "\n",
      "\n",
      "acare\n",
      "0s\n",
      "emerg\n",
      "injiang\n",
      "inev\n",
      "wawrink\n",
      "assaul\n",
      "streng\n",
      "signific\n",
      "weren\n",
      "\n",
      "\n",
      "doug\n",
      "jason\n",
      "johnny\n",
      "randy\n",
      "timothy\n",
      "geoff\n",
      "darren\n",
      "kevin\n",
      "bruce\n",
      "dave\n",
      "\n",
      "\n",
      "mogad\n",
      "immen\n",
      "hurrican\n",
      "assaul\n",
      "onathan\n",
      "acare\n",
      "strengthe\n",
      "citiz\n",
      "guantan\n",
      "nairo\n",
      "\n",
      "\n",
      "economist\n",
      "commentator\n",
      "columnist\n",
      "publisher\n",
      "consultant\n",
      "founder\n",
      "researcher\n",
      "coordinator\n",
      "editor\n",
      "analyst\n",
      "\n",
      "\n",
      "immen\n",
      "mogad\n",
      "portugu\n",
      "nairo\n",
      "hift\n",
      "cincin\n",
      "uguay\n",
      "assaul\n",
      "guate\n",
      "}\n",
      "\n",
      "\n",
      "undoubtedly\n",
      "endum\n",
      "mogad\n",
      "grues\n",
      "assaul\n",
      "cially\n",
      "hurrican\n",
      "fundamentally\n",
      "ricul\n",
      "attem\n",
      "\n",
      "\n",
      "streng\n",
      "adjac\n",
      "raik\n",
      "uguay\n",
      "cigare\n",
      "weren\n",
      "recor\n",
      "sched\n",
      "mingham\n",
      "munic\n",
      "\n",
      "\n",
      "obtaining\n",
      "citiz\n",
      "reducing\n",
      "abusing\n",
      "combating\n",
      "preserving\n",
      "hift\n",
      "employe\n",
      "invol\n",
      "moil\n",
      "\n",
      "\n",
      "assaul\n",
      "sterdam\n",
      "portugu\n",
      "signific\n",
      "immen\n",
      "shaba\n",
      "pean\n",
      "abulary\n",
      "mingham\n",
      "propof\n",
      "\n",
      "\n",
      "mingham\n",
      "moil\n",
      "weren\n",
      "guate\n",
      "raik\n",
      "mclaugh\n",
      "dipl\n",
      "emerg\n",
      "adjac\n",
      "abulary\n",
      "\n",
      "\n",
      "}\n",
      "dort\n",
      "prede\n",
      "shaba\n",
      "eleph\n",
      "indust\n",
      "mingham\n",
      "endum\n",
      "liby\n",
      "acare\n",
      "\n",
      "\n",
      "endum\n",
      "sterdam\n",
      "strengthe\n",
      "hurrican\n",
      "territor\n",
      "pean\n",
      "abulary\n",
      "magaz\n",
      "acare\n",
      "weren\n",
      "\n",
      "\n",
      "medical\n",
      "political\n",
      "social\n",
      "civil\n",
      "health\n",
      "criminal\n",
      "financial\n",
      "security\n",
      "justice\n",
      "legal\n",
      "\n",
      "\n",
      "'you\n",
      "'what\n",
      "'i'm\n",
      "you'd\n",
      "everybody's\n",
      "i'd\n",
      "'we\n",
      "we'd\n",
      "enegger\n",
      "includ\n",
      "\n",
      "\n",
      "indust\n",
      "tornadoes\n",
      "setts\n",
      "icut\n",
      "hurrican\n",
      "rainfall\n",
      "portra\n",
      "sterdam\n",
      "auster\n",
      "eleph\n",
      "\n",
      "\n",
      "ie's\n",
      "ner's\n",
      "as'\n",
      "berg's\n",
      "ard's\n",
      "ton's\n",
      "z's\n",
      "y's\n",
      "is'\n",
      "ley's\n",
      "\n",
      "\n",
      "cautioned\n",
      "insisted\n",
      "argued\n",
      "argues\n",
      "admits\n",
      "warned\n",
      "advises\n",
      "contends\n",
      "emphasized\n",
      "explains\n",
      "\n",
      "\n",
      "p\n",
      "l\n",
      "d\n",
      "c\n",
      "f\n",
      "g\n",
      "b\n",
      "j\n",
      "t\n",
      "st\n",
      "\n",
      "\n",
      "hezbol\n",
      "guantan\n",
      "uguay\n",
      "mingham\n",
      "weren\n",
      "hawar\n",
      "wawrink\n",
      "issip\n",
      "raik\n",
      "hurrican\n",
      "\n",
      "\n",
      "recor\n",
      "environ\n",
      "emerg\n",
      "mingham\n",
      "indust\n",
      "citiz\n",
      "massachu\n",
      "alep\n",
      "guate\n",
      "portra\n",
      "\n",
      "\n",
      "citiz\n",
      "attem\n",
      "environ\n",
      "massachu\n",
      "streng\n",
      "alep\n",
      "indust\n",
      "mogad\n",
      "emerg\n",
      "itored\n",
      "\n",
      "\n",
      "abulary\n",
      "employe\n",
      "archite\n",
      "citiz\n",
      "dipl\n",
      "assaul\n",
      "streng\n",
      "transparen\n",
      "moil\n",
      "tournam\n",
      "\n",
      "\n",
      "immen\n",
      "avez\n",
      "hurrican\n",
      "assaul\n",
      "{\n",
      "sterdam\n",
      "acare\n",
      "magaz\n",
      "mbley\n",
      "abulary\n",
      "\n",
      "\n",
      "lowest\n",
      "hottest\n",
      "smallest\n",
      "finest\n",
      "fastest\n",
      "toughest\n",
      "deadliest\n",
      "ricul\n",
      "streng\n",
      "oldest\n",
      "\n",
      "\n",
      "{\n",
      "avez\n",
      "portugu\n",
      "acare\n",
      "massachu\n",
      "sterdam\n",
      "0s\n",
      "abulary\n",
      "hurrican\n",
      "magaz\n",
      "\n",
      "\n",
      "sterdam\n",
      "uguay\n",
      "icut\n",
      "itored\n",
      "mingham\n",
      "strengthe\n",
      "}\n",
      "acare\n",
      "massachu\n",
      "grues\n",
      "\n",
      "\n",
      "wouldn't\n",
      "can't\n",
      "shouldn't\n",
      "it'll\n",
      "won't\n",
      "couldn't\n",
      "don't\n",
      "i'll\n",
      "we'll\n",
      "didn't\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_embed = model.word_embed\n",
    "embedding_clustering = EmbeddingClustering(tokenizer, n_clusters=100)\n",
    "embedding_clustering.fit(word_embed)\n",
    "embedding_clustering.print_clusters(n_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc968e2d",
   "metadata": {},
   "source": [
    "## Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de0e293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1583]], shape=(1, 1), dtype=int32)\n",
      "amput\n",
      "extin\n",
      "penetr\n",
      "dri\n",
      "accounted\n",
      "analy\n",
      "hydr\n",
      "abdu\n",
      "coron\n",
      "bag\n",
      "dehydr\n",
      "cru\n",
      "pray\n",
      "bodies\n",
      "extr\n",
      "injuries\n",
      "bottles\n",
      "explo\n",
      "elev\n",
      "distur\n",
      "pret\n",
      "deton\n",
      "excav\n",
      "sco\n",
      "-\n",
      "cl\n",
      "un\n",
      "bags\n",
      "cann\n",
      "shocks\n",
      "sne\n",
      "ww\n",
      "evacu\n",
      "nab\n",
      "there\n",
      "]\n",
      "fever\n",
      "susp\n",
      "scri\n",
      "orchestr\n",
      "it\n",
      "dis\n",
      "we'd\n",
      "sc\n",
      "tons\n",
      "bath\n",
      "incorpor\n",
      "circul\n",
      "hang\n",
      "belonged\n",
      "ext\n",
      "comb\n",
      "situ\n",
      "location\n",
      " \n",
      "sed\n",
      "satur\n",
      "searchers\n",
      "im\n",
      "supplies\n",
      "instructions\n",
      "blankets\n",
      "uses\n",
      "respir\n",
      "desper\n",
      "ref\n",
      "am\n",
      "erup\n",
      "@\n",
      "accum\n",
      "ju\n",
      "us\n",
      "photos\n",
      "dim\n",
      "sh\n",
      "exposure\n",
      "cr\n",
      "tam\n",
      "suc\n",
      "links\n",
      "\"\n",
      "escap\n",
      "intend\n",
      "foc\n",
      "have\n",
      "wels\n",
      "accommod\n",
      "ats\n",
      "earthquakes\n",
      "cups\n",
      "rocks\n",
      "orders\n",
      "returned\n",
      "sp\n",
      "evalu\n",
      "dha\n",
      "l\n",
      "origin\n",
      "als\n",
      "discovered\n"
     ]
    }
   ],
   "source": [
    "word_embed = model.word_embed\n",
    "\n",
    "text = \"republican\"\n",
    "text = text.lower()\n",
    "\n",
    "idx = tf.cast(tokenizer.tokenizer.tokenize(text), tf.int32)\n",
    "idx = tokenize(idx, tokenizer.merge_list)\n",
    "print(idx)\n",
    "embed = tf.expand_dims(word_embed[idx[0][0]], axis=0)\n",
    "\n",
    "cosine_sim = embed@tf.transpose(word_embed)\n",
    "idx = tf.argsort(cosine_sim, axis=-1, \n",
    "                 #direction='DESCENDING',\n",
    "                 direction='ASCENDING', \n",
    "                 stable=False, name=None)[0]\n",
    "\n",
    "for i in idx[:100]:\n",
    "    i = tf.expand_dims(i, axis=0)\n",
    "    print(tokenizer.detokenize(i).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a5f20b",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
