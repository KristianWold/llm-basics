{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04010a08",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa3d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from src.tokenizer import TokenizerChar, word_split, normalize_to_ascii\n",
    "\n",
    "import os\n",
    "import time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cde139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def read_first_n(directory_path, n):\n",
    "    # List all entries in the directory\n",
    "    filenames = os.listdir(directory_path)\n",
    "    # Filter to only .txt files\n",
    "    txt_files = [f for f in filenames if f.lower().endswith('.story')]\n",
    "    # Sort alphabetically (or by any other criteria you like)\n",
    "    #txt_files.sort()\n",
    "    # Take the first n\n",
    "    first_n = txt_files[:n]\n",
    "    \n",
    "    contents = []\n",
    "    for fname in first_n:\n",
    "        full_path = os.path.join(directory_path, fname)\n",
    "        with open(full_path, 'r', encoding='utf-8') as f:\n",
    "            contents.append(normalize_to_ascii(f.read()))\n",
    "    return contents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d4508a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = read_first_n('corpus/stories', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f2ee7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocabulary(corpus):\n",
    "    \"\"\"Return a dict mapping words to their counts.\"\"\"\n",
    "    vocab = {}\n",
    "    for line in corpus:\n",
    "        for word in line.strip().split():\n",
    "            vocab[word] = vocab.get(word, 0) + 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c28c6f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_freq(word_list):\n",
    "    \"\"\"Return a dict mapping pairs of words to their counts.\"\"\"\n",
    "    pairs = {}\n",
    "    for word in word_list:\n",
    "        for i in range(len(word) - 1):\n",
    "            pair = (word[i], word[i + 1])\n",
    "            pairs[pair] = pairs.get(pair, 0) + 1\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5613ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizerBPE:\n",
    "    def __init__(self, corpus, num_merges):\n",
    "        self.tokenizer = TokenizerChar(corpus)\n",
    "        self.token_to_idx = self.tokenizer.token_to_idx\n",
    "        self.vocab_size = self.tokenizer.vocab_size\n",
    "\n",
    "        self.word_list = []\n",
    "        for line in corpus:\n",
    "            self.word_list.extend(word_split(line))\n",
    "\n",
    "        self.merge_list = []\n",
    "        for i in tqdm(range(num_merges)):\n",
    "            self.merge()\n",
    "\n",
    "        vocab = list(self.token_to_idx.keys())\n",
    "        indicies = list(self.token_to_idx.values())\n",
    "\n",
    "        self.table_detokenize = tf.lookup.StaticHashTable(initializer=tf.lookup.KeyValueTensorInitializer(indicies, vocab), \n",
    "                                                          default_value=\"\")\n",
    "\n",
    "\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        indicies = np.array(self.tokenizer.tokenize(text))\n",
    "        for (idx1, idx2), new_idx in self.merge_list:\n",
    "            for i in reversed(range(len(indicies) - 1)):\n",
    "                pair = (indicies[i], indicies[i + 1])\n",
    "                if pair == (idx1, idx2):\n",
    "                    indicies[i] = new_idx\n",
    "                    indicies = np.delete(indicies, i + 1)\n",
    "        \n",
    "        return indicies\n",
    "\n",
    "    def detokenize(self, indices):\n",
    "        text = self.table_detokenize.lookup(indices)\n",
    "        text = tf.strings.reduce_join(text, axis=-1, separator=\"\")\n",
    "        return text\n",
    "\n",
    "    def merge(self):\n",
    "        pf = pair_freq(self.word_list)\n",
    "        key_max = max(pf, key=pf.get)\n",
    "        token1, token2 = key_max\n",
    "        new_token = token1 + token2\n",
    "        self.token_to_idx[new_token] = self.vocab_size\n",
    "\n",
    "        idx1, idx2 = self.token_to_idx[token1], self.token_to_idx[token2]\n",
    "        self.merge_list.append([(idx1, idx2), self.vocab_size])\n",
    "\n",
    "        self.vocab_size += 1\n",
    "\n",
    "        for word in self.word_list:\n",
    "            for i in reversed(range(len(word) - 1)):\n",
    "                pair = (word[i], word[i + 1])\n",
    "                if pair == key_max:\n",
    "                    word[i] = new_token\n",
    "                    word.pop(i + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e643c37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aab6d89d7734260a40b58b6057ead1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_bpe = TokenizerBPE(corpus[:10], num_merges=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "254dbc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(69, 57), 76], [(58, 63), 77], [(54, 67), 78], [(64, 63), 79], [(50, 63), 80], [(50, 69), 81], [(67, 54), 82], [(76, 54), 83], [(54, 63), 84], [(64, 67), 85], [(50, 61), 86], [(54, 53), 87], [(58, 68), 88], [(50, 68), 89], [(50, 67), 90], [(69, 64), 91], [(54, 68), 92], [(64, 70), 93], [(58, 69), 94], [(58, 52), 95], [(64, 55), 96], [(77, 56), 97], [(58, 79), 98], [(61, 54), 99], [(58, 53), 100], [(58, 56), 101], [(80, 53), 102], [(57, 54), 103], [(68, 69), 104], [(58, 61), 105], [(84, 69), 106], [(64, 62), 107], [(68, 50), 108], [(50, 62), 109], [(50, 52), 110], [(67, 58), 111], [(64, 61), 112], [(101, 57), 113], [(55, 85), 114], [(71, 54), 115], [(51, 54), 116], [(50, 53), 117], [(72, 57), 118], [(69, 78), 119], [(82, 68), 120], [(70, 67), 121], [(68, 54), 122], [(108, 100), 123], [(6, 68), 124], [(50, 74), 125], [(81, 98), 126], [(54, 62), 127], [(65, 54), 128], [(76, 81), 129], [(71, 78), 130], [(69, 67), 131], [(76, 78), 132], [(52, 79), 133], [(113, 69), 134], [(50, 56), 135], [(58, 62), 136], [(72, 89), 137], [(52, 57), 138], [(64, 69), 139], [(70, 63), 140], [(52, 54), 141], [(81, 54), 142], [(57, 88), 143], [(45, 103), 144], [(65, 67), 145], [(53, 54), 146], [(61, 64), 147], [(61, 74), 148], [(52, 69), 149], [(93, 67), 150], [(70, 69), 151], [(61, 53), 152], [(50, 65), 153], [(68, 70), 154], [(58, 67), 155], [(78, 68), 156], [(53, 125), 157], [(50, 82), 158], [(105, 61), 159], [(78, 54), 160], [(72, 58), 161], [(90, 69), 162], [(62, 106), 163], [(72, 54), 164], [(62, 80), 165], [(161, 76), 166], [(65, 64), 167], [(57, 89), 168], [(54, 65), 169], [(55, 67), 170], [(92, 69), 171], [(63, 54), 172], [(93, 63), 173], [(56, 54), 174], [(68, 11), 175], [(50, 115), 176], [(64, 82), 177], [(9, 2), 178], [(51, 74), 179], [(58, 86), 180], [(64, 130), 181], [(74, 54), 182], [(61, 50), 183], [(61, 58), 184], [(95, 80), 185], [(60, 54), 186], [(76, 88), 187], [(46, 11), 188], [(44, 74), 189], [(145, 64), 190], [(170, 107), 191], [(70, 68), 192], [(12, 12), 193], [(80, 69), 194], [(189, 111), 195], [(93, 69), 196], [(54, 73), 197], [(50, 77), 198], [(64, 132), 199], [(94, 74), 200], [(50, 51), 201], [(109, 50), 202], [(85, 69), 203], [(89, 69), 204], [(118, 64), 205], [(52, 107), 206], [(50, 55), 207], [(64, 72), 208], [(68, 9), 209], [(86, 61), 210], [(61, 134), 211], [(123, 11), 212], [(64, 65), 213], [(57, 113), 214], [(99, 56), 215], [(69, 54), 216], [(65, 203), 217], [(55, 95), 218], [(40, 51), 219], [(90, 74), 220], [(56, 67), 221], [(52, 67), 222], [(10, 10), 223], [(57, 176), 224], [(101, 63), 225]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_bpe.merge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1acbbb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicies = tf.cast(tokenizer_bpe.tokenize(corpus[0]), tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d8a4a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['3' '4' '5' '6' '7' '8' '9']\n",
      " ['a' 'b' 'c' 'd' 'e' 'f' 'g']]\n",
      "[['4' '5' '6' '7' '8' '9']\n",
      " ['b' 'c' 'd' 'e' 'f' 'g']]\n",
      "[['3' '4' '5' '6' '7' '8']\n",
      " ['a' 'b' 'c' 'd' 'e' 'f']]\n"
     ]
    },
    {
     "ename": "UFuncTypeError",
     "evalue": "ufunc 'add' did not contain a loop with signature matching types (dtype('<U1'), dtype('<U1')) -> None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUFuncTypeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(c[:,\u001b[38;5;241m1\u001b[39m:])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(c[:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m)\n",
      "\u001b[1;31mUFuncTypeError\u001b[0m: ufunc 'add' did not contain a loop with signature matching types (dtype('<U1'), dtype('<U1')) -> None"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([\"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"])\n",
    "b = np.array([\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"])\n",
    "\n",
    "\n",
    "c = np.array([a, b])\n",
    "print(c)\n",
    "print(c[:,1:])\n",
    "print(c[:,:-1])\n",
    "print(c + c)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
