{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04010a08",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fa3d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CPU dispatcher tracer already initlized",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: CPU dispatcher tracer already initlized"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CPU dispatcher tracer already initlized",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: CPU dispatcher tracer already initlized"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CPU dispatcher tracer already initlized",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: CPU dispatcher tracer already initlized"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CPU dispatcher tracer already initlized",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: CPU dispatcher tracer already initlized"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CPU dispatcher tracer already initlized",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: CPU dispatcher tracer already initlized"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CPU dispatcher tracer already initlized",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;31mRuntimeError\u001b[0m: CPU dispatcher tracer already initlized"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 4080, compute capability 8.9\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from src.tokenizer import TokenizerBPE, fuse_tokenized_corpus, chunk_corpus\n",
    "\n",
    "import os\n",
    "import time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm.notebook import tqdm\n",
    "from src.transformer import *\n",
    "from src.data_handling import split_on_value\n",
    "\n",
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8751dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = pkl.load(open(\"tokenizers/tokenizer_superQA_24k.pkl\", \"rb\"))\n",
    "tokenizer.create_hash()\n",
    "tokenizer.add_special_tokens([\"<s>\", \"</s>\", \"<q>\", \"<a>\",\"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73ef7665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(corpus, batch_size=32):\n",
    "    length = corpus.shape[1]\n",
    "\n",
    "    batches = length // batch_size\n",
    "\n",
    "    corpus = corpus[:, :batches * batch_size]\n",
    "\n",
    "    corpus = tf.reshape(corpus, [-1, batch_size])\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0868e180",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_coqa_sqa = pkl.load(open('corpus/corpus_clean/corpus_coqa_sqa_24k_padded', 'rb'))\n",
    "corpus_squad_sqa = pkl.load(open('corpus/corpus_clean/corpus_squad_sqa_24k_padded', 'rb'))\n",
    "corpus_quac_sqa = pkl.load(open('corpus/corpus_clean/corpus_quac_sqa_24k_padded', 'rb'))\n",
    "\n",
    "corpus_web_qa = pkl.load(open(\"corpus/corpus_clean/corpus_web_qa_24k\", 'rb'))\n",
    "corpus_web_qa = batch(corpus_web_qa, batch_size=768)\n",
    "\n",
    "corpus_wiki_qa = pkl.load(open(\"corpus/corpus_clean/corpus_wiki_qa_24k\", 'rb'))\n",
    "corpus_wiki_qa = batch(corpus_wiki_qa, batch_size=768)\n",
    "\n",
    "corpus_web_article = pkl.load(open(\"corpus/corpus_clean/corpus_web_article_24k\", 'rb'))\n",
    "corpus_web_article = batch(corpus_web_article, batch_size=768)\n",
    "\n",
    "corpus_wiki_article = pkl.load(open(\"corpus/corpus_clean/corpus_wiki_article_24k\", 'rb'))\n",
    "corpus_wiki_article = batch(corpus_wiki_article, batch_size=768)\n",
    "\n",
    "corpus_recipe = pkl.load(open(\"corpus/corpus_clean/corpus_recipe_24k\", 'rb'))\n",
    "corpus_recipe = batch(corpus_recipe, batch_size=768)\n",
    "\n",
    "corpus_recipe_qa = pkl.load(open('corpus/corpus_clean/corpus_recipe_qa_24k', 'rb'))\n",
    "corpus_recipe_qa = batch(corpus_recipe_qa, batch_size=768)\n",
    "\n",
    "corpus_recipe_qa_ingred = pkl.load(open('corpus/corpus_clean/corpus_recipe_qa_ingred_24k', 'rb'))\n",
    "corpus_recipe_qa_ingred = batch(corpus_recipe_qa_ingred, batch_size=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cedb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7199, 768)\n",
      "(19035, 768)\n",
      "(11567, 768)\n",
      "(2495, 768)\n",
      "(70687, 768)\n",
      "(2026, 768)\n",
      "(65387, 768)\n",
      "(7252, 768)\n",
      "(7594, 768)\n",
      "(8356, 768)\n"
     ]
    }
   ],
   "source": [
    "print(corpus_coqa_sqa.shape)\n",
    "print(corpus_squad_sqa.shape)\n",
    "print(corpus_quac_sqa.shape)\n",
    "\n",
    "print(corpus_web_qa.shape)\n",
    "print(corpus_web_article.shape)\n",
    "print(corpus_wiki_qa.shape)\n",
    "print(corpus_wiki_article.shape)\n",
    "\n",
    "print(corpus_recipe.shape)\n",
    "print(corpus_recipe_qa.shape)\n",
    "print(corpus_recipe_qa_ingred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9dbd56bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(corpus_coqa_sqa[:1000]==-1))\n",
    "print(np.sum(corpus_squad_sqa[:1000]==-1))\n",
    "print(np.sum(corpus_quac_sqa[:1000]==-1))\n",
    "print(np.sum(corpus_web_qa[:1000]==-1))\n",
    "print(np.sum(corpus_web_article[:1000]==-1))\n",
    "print(np.sum(corpus_wiki_qa[:1000]==-1))\n",
    "print(np.sum(corpus_wiki_article[:1000]==-1))\n",
    "print(np.sum(corpus_recipe[:1000]==-1))\n",
    "print(np.sum(corpus_recipe_qa[:1000]==-1))\n",
    "print(np.sum(corpus_recipe_qa_ingred[:1000]==-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a212b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154827264\n"
     ]
    }
   ],
   "source": [
    "corpus = tf.concat([corpus_coqa_sqa, \n",
    "                    corpus_squad_sqa, \n",
    "                    corpus_quac_sqa, \n",
    "                    corpus_web_qa, \n",
    "                    corpus_web_article, \n",
    "                    corpus_wiki_qa, \n",
    "                    corpus_wiki_article, \n",
    "                    corpus_recipe, \n",
    "                    corpus_recipe_qa,\n",
    "                    corpus_recipe_qa_ingred], axis=0)\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "corpus = tf.random.shuffle(corpus)\n",
    "length = corpus.shape[0]\n",
    "ratio = int(length * 0.95)\n",
    "\n",
    "corpus_train = corpus[:ratio] \n",
    "corpus_test = corpus[ratio:]\n",
    "\n",
    "print(length*768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8d39e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pipeline(corpus, batch_size=32):\n",
    "    samples = corpus.shape[0]\n",
    "\n",
    "    steps_per_epoch = samples // batch_size\n",
    "    \n",
    "    ds = tf.data.Dataset.from_tensor_slices(corpus)\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.shuffle(buffer_size=100*batch_size, reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return ds, steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37844241",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, steps_per_epoch = data_pipeline(corpus_train, batch_size=5)\n",
    "ds_test,_ = data_pipeline(corpus_test, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f830d881",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a5a33ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lr = 1e-4\n",
    "decay_steps = 60000\n",
    "decay_rate = 0.5\n",
    "decay_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_lr,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=False)\n",
    "\n",
    "warmup_steps = 1000\n",
    "lr_schedule = WarmUpThenDecay(\n",
    "    initial_learning_rate=initial_lr,\n",
    "    warmup_steps=warmup_steps,\n",
    "    decay_schedule_fn=decay_schedule)\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "max_seq_len = 768\n",
    "embed_dim = 896\n",
    "tf_blocks = 14\n",
    "heads = 14\n",
    "ff_dim = 4*embed_dim\n",
    "weight_decay = 0.025\n",
    "dropout = 0.1\n",
    "\n",
    "unembed_dims = []\n",
    "accum_steps = 10\n",
    "\n",
    "model = Transformer(vocab_size=tokenizer.vocab_size,\n",
    "                    max_seq_len=max_seq_len,\n",
    "                    embed_dim=embed_dim,\n",
    "                    tf_blocks=tf_blocks,\n",
    "                    heads=heads,\n",
    "                    ff_dim = ff_dim,\n",
    "                    unembed_dims=unembed_dims,\n",
    "                    tokenizer=tokenizer,\n",
    "                    lr=lr_schedule,\n",
    "                    wd = weight_decay,\n",
    "                    dropout=dropout,\n",
    "                    accum_steps=accum_steps,\n",
    "                    )\n",
    "\n",
    "losses_train = []\n",
    "losses_test = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7970a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#name = \"model_super_qa_larger\"\n",
    "name = \"model_super_qa_expand\"\n",
    "ckpt = tf.train.Checkpoint(\n",
    "    optimizer=model.opt,\n",
    "    model=model\n",
    ")\n",
    "ckpt_manager = tf.train.CheckpointManager(\n",
    "    ckpt, \n",
    "    directory=\"checkpoints/\" + name,      # folder where ckpts are saved\n",
    "    max_to_keep=5                         # only keep 5 latest checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "88b34765",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "losses_train, losses_test = pkl.load(open(\"checkpoints/losses_\" + name + \".pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a6527620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 157298091\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "total_params = 0\n",
    "for var in model.parameter_list:\n",
    "    shape = var.get_shape()\n",
    "    num_params = 1\n",
    "    for dim in shape:\n",
    "        num_params *= dim\n",
    "    total_params += num_params\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "print(model.accum_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "93977cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(iter_train, iter_test, steps_per_epoch, epochs):\n",
    "    for i in tqdm(range(steps_per_epoch//accum_steps*epochs)):\n",
    "        loss_train_temp = 0\n",
    "        loss_test_temp = 0\n",
    "        for _ in range(accum_steps):\n",
    "            batch_train = next(iter_train)\n",
    "            batch_test = next(iter_test)\n",
    "            \n",
    "            loss_train_temp += model.train_step(batch_train).numpy()\n",
    "            \n",
    "            if i % 25 == 0:\n",
    "                loss_test_temp += model.evaluate(batch_test).numpy()\n",
    "            \n",
    "        losses_train.append(loss_train_temp/accum_steps)\n",
    "\n",
    "        if i % 25 == 0:\n",
    "            losses_test.append(loss_test_temp/accum_steps)\n",
    "        else:\n",
    "            losses_test.append(losses_test[-1])\n",
    "\n",
    "        if (i+1) % 500 == 0:\n",
    "            ckpt_manager.save()\n",
    "            pkl.dump([losses_train, losses_test], open(\"checkpoints/losses_\" + name + \".pkl\", 'wb'))\n",
    "\n",
    "    \n",
    "        lr = model.opt.inner_optimizer._decayed_lr(tf.float32).numpy()\n",
    "        print(f\"Step {i+1}, Loss Train: {losses_train[-1]:.4f}, Loss Test: {losses_test[-1]:.4f}, LR: {lr:.6f}\")\n",
    "    ckpt_manager.save()\n",
    "    pkl.dump([losses_train, losses_test], open(\"checkpoints/losses_\" + name + \".pkl\", 'wb'))\n",
    "    return losses_train, losses_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c652c842",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_train = iter(ds_train)\n",
    "iter_test = iter(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28e04f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de9711b650cf44b99651120ede0789ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3830 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Loss Train: 3.0748, Loss Test: 2.5555, LR: 0.000051\n",
      "Step 2, Loss Train: 3.2989, Loss Test: 2.5555, LR: 0.000051\n",
      "Step 3, Loss Train: 3.1809, Loss Test: 2.5555, LR: 0.000051\n",
      "Step 4, Loss Train: 3.1435, Loss Test: 2.5555, LR: 0.000051\n",
      "Step 5, Loss Train: 3.1063, Loss Test: 2.5555, LR: 0.000051\n",
      "Step 6, Loss Train: 2.9866, Loss Test: 2.5555, LR: 0.000051\n",
      "Step 7, Loss Train: 3.0846, Loss Test: 2.5555, LR: 0.000051\n",
      "Step 8, Loss Train: 2.8301, Loss Test: 2.5555, LR: 0.000051\n",
      "Step 9, Loss Train: 3.3018, Loss Test: 2.5555, LR: 0.000051\n",
      "Step 10, Loss Train: 3.1562, Loss Test: 2.5555, LR: 0.000051\n",
      "Step 11, Loss Train: 3.2967, Loss Test: 2.5555, LR: 0.000051\n",
      "Step 12, Loss Train: 3.0919, Loss Test: 2.5555, LR: 0.000051\n",
      "Step 13, Loss Train: 3.1548, Loss Test: 2.5555, LR: 0.000051\n",
      "Step 14, Loss Train: 3.1811, Loss Test: 2.5555, LR: 0.000051\n",
      "Step 15, Loss Train: 3.0681, Loss Test: 2.5555, LR: 0.000051\n",
      "Step 16, Loss Train: 3.1393, Loss Test: 2.5555, LR: 0.000051\n",
      "Step 17, Loss Train: 3.1191, Loss Test: 2.5555, LR: 0.000051\n",
      "Step 18, Loss Train: 3.3501, Loss Test: 2.5555, LR: 0.000051\n",
      "Step 19, Loss Train: 2.9256, Loss Test: 2.5555, LR: 0.000051\n",
      "Step 20, Loss Train: 3.1424, Loss Test: 2.5555, LR: 0.000051\n",
      "Step 21, Loss Train: 3.0517, Loss Test: 2.5555, LR: 0.000051\n",
      "Step 22, Loss Train: 3.1991, Loss Test: 2.5555, LR: 0.000051\n",
      "Step 23, Loss Train: 3.2274, Loss Test: 2.5555, LR: 0.000051\n",
      "Step 24, Loss Train: 3.2655, Loss Test: 2.5555, LR: 0.000051\n",
      "Step 25, Loss Train: 3.0574, Loss Test: 2.5555, LR: 0.000051\n",
      "Step 26, Loss Train: 3.3176, Loss Test: 3.6216, LR: 0.000051\n",
      "Step 27, Loss Train: 3.0499, Loss Test: 3.6216, LR: 0.000051\n",
      "Step 28, Loss Train: 2.9543, Loss Test: 3.6216, LR: 0.000051\n",
      "Step 29, Loss Train: 2.9795, Loss Test: 3.6216, LR: 0.000051\n",
      "Step 30, Loss Train: 3.1439, Loss Test: 3.6216, LR: 0.000051\n",
      "Step 31, Loss Train: 3.1490, Loss Test: 3.6216, LR: 0.000051\n",
      "Step 32, Loss Train: 3.0233, Loss Test: 3.6216, LR: 0.000051\n",
      "Step 33, Loss Train: 3.1569, Loss Test: 3.6216, LR: 0.000051\n",
      "Step 34, Loss Train: 3.2245, Loss Test: 3.6216, LR: 0.000051\n",
      "Step 35, Loss Train: 3.0891, Loss Test: 3.6216, LR: 0.000051\n",
      "Step 36, Loss Train: 3.3597, Loss Test: 3.6216, LR: 0.000051\n",
      "Step 37, Loss Train: 3.1985, Loss Test: 3.6216, LR: 0.000051\n",
      "Step 38, Loss Train: 3.3415, Loss Test: 3.6216, LR: 0.000051\n",
      "Step 39, Loss Train: 3.1590, Loss Test: 3.6216, LR: 0.000051\n",
      "Step 40, Loss Train: 3.2855, Loss Test: 3.6216, LR: 0.000051\n",
      "Step 41, Loss Train: 3.2331, Loss Test: 3.6216, LR: 0.000051\n",
      "Step 42, Loss Train: 2.9920, Loss Test: 3.6216, LR: 0.000051\n",
      "Step 43, Loss Train: 2.9859, Loss Test: 3.6216, LR: 0.000051\n",
      "Step 44, Loss Train: 3.1655, Loss Test: 3.6216, LR: 0.000051\n",
      "Step 45, Loss Train: 3.1320, Loss Test: 3.6216, LR: 0.000051\n",
      "Step 46, Loss Train: 3.1599, Loss Test: 3.6216, LR: 0.000051\n",
      "Step 47, Loss Train: 2.9387, Loss Test: 3.6216, LR: 0.000051\n",
      "Step 48, Loss Train: 3.0536, Loss Test: 3.6216, LR: 0.000051\n",
      "Step 49, Loss Train: 2.9480, Loss Test: 3.6216, LR: 0.000051\n",
      "Step 50, Loss Train: 3.3166, Loss Test: 3.6216, LR: 0.000051\n",
      "Step 51, Loss Train: 3.1850, Loss Test: 2.7164, LR: 0.000051\n",
      "Step 52, Loss Train: 3.0825, Loss Test: 2.7164, LR: 0.000051\n",
      "Step 53, Loss Train: 3.3278, Loss Test: 2.7164, LR: 0.000051\n",
      "Step 54, Loss Train: 3.2187, Loss Test: 2.7164, LR: 0.000051\n",
      "Step 55, Loss Train: 3.2839, Loss Test: 2.7164, LR: 0.000051\n",
      "Step 56, Loss Train: 3.1876, Loss Test: 2.7164, LR: 0.000051\n",
      "Step 57, Loss Train: 3.1663, Loss Test: 2.7164, LR: 0.000051\n",
      "Step 58, Loss Train: 3.3223, Loss Test: 2.7164, LR: 0.000051\n",
      "Step 59, Loss Train: 3.1355, Loss Test: 2.7164, LR: 0.000051\n",
      "Step 60, Loss Train: 3.2295, Loss Test: 2.7164, LR: 0.000051\n",
      "Step 61, Loss Train: 3.1637, Loss Test: 2.7164, LR: 0.000051\n",
      "Step 62, Loss Train: 3.4069, Loss Test: 2.7164, LR: 0.000051\n",
      "Step 63, Loss Train: 3.2869, Loss Test: 2.7164, LR: 0.000051\n",
      "Step 64, Loss Train: 3.0129, Loss Test: 2.7164, LR: 0.000051\n",
      "Step 65, Loss Train: 3.2668, Loss Test: 2.7164, LR: 0.000051\n",
      "Step 66, Loss Train: 3.0390, Loss Test: 2.7164, LR: 0.000051\n",
      "Step 67, Loss Train: 3.4663, Loss Test: 2.7164, LR: 0.000051\n",
      "Step 68, Loss Train: 3.1033, Loss Test: 2.7164, LR: 0.000051\n",
      "Step 69, Loss Train: 3.1839, Loss Test: 2.7164, LR: 0.000051\n",
      "Step 70, Loss Train: 3.2428, Loss Test: 2.7164, LR: 0.000051\n",
      "Step 71, Loss Train: 3.0957, Loss Test: 2.7164, LR: 0.000051\n",
      "Step 72, Loss Train: 2.9767, Loss Test: 2.7164, LR: 0.000051\n",
      "Step 73, Loss Train: 3.4286, Loss Test: 2.7164, LR: 0.000051\n",
      "Step 74, Loss Train: 3.0925, Loss Test: 2.7164, LR: 0.000051\n",
      "Step 75, Loss Train: 2.7479, Loss Test: 2.7164, LR: 0.000051\n",
      "Step 76, Loss Train: 3.0503, Loss Test: 3.4310, LR: 0.000051\n",
      "Step 77, Loss Train: 3.1990, Loss Test: 3.4310, LR: 0.000051\n",
      "Step 78, Loss Train: 3.0753, Loss Test: 3.4310, LR: 0.000051\n",
      "Step 79, Loss Train: 3.1549, Loss Test: 3.4310, LR: 0.000051\n",
      "Step 80, Loss Train: 3.1399, Loss Test: 3.4310, LR: 0.000051\n",
      "Step 81, Loss Train: 2.9470, Loss Test: 3.4310, LR: 0.000051\n",
      "Step 82, Loss Train: 3.1794, Loss Test: 3.4310, LR: 0.000051\n",
      "Step 83, Loss Train: 3.1031, Loss Test: 3.4310, LR: 0.000051\n",
      "Step 84, Loss Train: 3.0419, Loss Test: 3.4310, LR: 0.000051\n",
      "Step 85, Loss Train: 3.1939, Loss Test: 3.4310, LR: 0.000051\n",
      "Step 86, Loss Train: 3.0367, Loss Test: 3.4310, LR: 0.000051\n",
      "Step 87, Loss Train: 3.3087, Loss Test: 3.4310, LR: 0.000051\n",
      "Step 88, Loss Train: 2.8961, Loss Test: 3.4310, LR: 0.000051\n",
      "Step 89, Loss Train: 3.0711, Loss Test: 3.4310, LR: 0.000051\n",
      "Step 90, Loss Train: 3.2693, Loss Test: 3.4310, LR: 0.000051\n",
      "Step 91, Loss Train: 3.2370, Loss Test: 3.4310, LR: 0.000051\n",
      "Step 92, Loss Train: 3.2754, Loss Test: 3.4310, LR: 0.000051\n",
      "Step 93, Loss Train: 3.1662, Loss Test: 3.4310, LR: 0.000051\n",
      "Step 94, Loss Train: 3.1600, Loss Test: 3.4310, LR: 0.000051\n",
      "Step 95, Loss Train: 3.4071, Loss Test: 3.4310, LR: 0.000051\n",
      "Step 96, Loss Train: 3.0628, Loss Test: 3.4310, LR: 0.000051\n",
      "Step 97, Loss Train: 3.2205, Loss Test: 3.4310, LR: 0.000051\n",
      "Step 98, Loss Train: 3.2035, Loss Test: 3.4310, LR: 0.000051\n",
      "Step 99, Loss Train: 3.2209, Loss Test: 3.4310, LR: 0.000051\n",
      "Step 100, Loss Train: 3.0484, Loss Test: 3.4310, LR: 0.000051\n",
      "Step 101, Loss Train: 3.0459, Loss Test: 2.5072, LR: 0.000051\n",
      "Step 102, Loss Train: 3.1960, Loss Test: 2.5072, LR: 0.000051\n",
      "Step 103, Loss Train: 3.2497, Loss Test: 2.5072, LR: 0.000051\n",
      "Step 104, Loss Train: 3.2521, Loss Test: 2.5072, LR: 0.000051\n",
      "Step 105, Loss Train: 3.0885, Loss Test: 2.5072, LR: 0.000051\n",
      "Step 106, Loss Train: 3.1349, Loss Test: 2.5072, LR: 0.000051\n",
      "Step 107, Loss Train: 3.2998, Loss Test: 2.5072, LR: 0.000051\n",
      "Step 108, Loss Train: 3.1736, Loss Test: 2.5072, LR: 0.000051\n",
      "Step 109, Loss Train: 3.3340, Loss Test: 2.5072, LR: 0.000051\n",
      "Step 110, Loss Train: 3.2714, Loss Test: 2.5072, LR: 0.000051\n",
      "Step 111, Loss Train: 3.2556, Loss Test: 2.5072, LR: 0.000051\n",
      "Step 112, Loss Train: 3.1212, Loss Test: 2.5072, LR: 0.000051\n",
      "Step 113, Loss Train: 2.9959, Loss Test: 2.5072, LR: 0.000051\n",
      "Step 114, Loss Train: 3.0335, Loss Test: 2.5072, LR: 0.000051\n",
      "Step 115, Loss Train: 3.1956, Loss Test: 2.5072, LR: 0.000051\n",
      "Step 116, Loss Train: 3.2807, Loss Test: 2.5072, LR: 0.000051\n",
      "Step 117, Loss Train: 3.0332, Loss Test: 2.5072, LR: 0.000051\n",
      "Step 118, Loss Train: 3.2702, Loss Test: 2.5072, LR: 0.000051\n",
      "Step 119, Loss Train: 2.9916, Loss Test: 2.5072, LR: 0.000051\n",
      "Step 120, Loss Train: 2.9756, Loss Test: 2.5072, LR: 0.000051\n",
      "Step 121, Loss Train: 3.0653, Loss Test: 2.5072, LR: 0.000051\n",
      "Step 122, Loss Train: 3.2054, Loss Test: 2.5072, LR: 0.000051\n",
      "Step 123, Loss Train: 3.0083, Loss Test: 2.5072, LR: 0.000051\n",
      "Step 124, Loss Train: 3.2939, Loss Test: 2.5072, LR: 0.000051\n",
      "Step 125, Loss Train: 3.1950, Loss Test: 2.5072, LR: 0.000051\n",
      "Step 126, Loss Train: 3.3444, Loss Test: 3.0200, LR: 0.000051\n",
      "Step 127, Loss Train: 3.2789, Loss Test: 3.0200, LR: 0.000051\n",
      "Step 128, Loss Train: 2.9843, Loss Test: 3.0200, LR: 0.000051\n",
      "Step 129, Loss Train: 3.1451, Loss Test: 3.0200, LR: 0.000051\n",
      "Step 130, Loss Train: 3.0934, Loss Test: 3.0200, LR: 0.000051\n",
      "Step 131, Loss Train: 3.1747, Loss Test: 3.0200, LR: 0.000051\n",
      "Step 132, Loss Train: 3.1415, Loss Test: 3.0200, LR: 0.000051\n",
      "Step 133, Loss Train: 3.3648, Loss Test: 3.0200, LR: 0.000051\n",
      "Step 134, Loss Train: 3.2540, Loss Test: 3.0200, LR: 0.000051\n",
      "Step 135, Loss Train: 3.1075, Loss Test: 3.0200, LR: 0.000051\n",
      "Step 136, Loss Train: 2.8907, Loss Test: 3.0200, LR: 0.000051\n",
      "Step 137, Loss Train: 3.1601, Loss Test: 3.0200, LR: 0.000051\n",
      "Step 138, Loss Train: 3.3786, Loss Test: 3.0200, LR: 0.000051\n",
      "Step 139, Loss Train: 2.9238, Loss Test: 3.0200, LR: 0.000051\n",
      "Step 140, Loss Train: 3.2912, Loss Test: 3.0200, LR: 0.000051\n",
      "Step 141, Loss Train: 2.9646, Loss Test: 3.0200, LR: 0.000051\n",
      "Step 142, Loss Train: 3.0767, Loss Test: 3.0200, LR: 0.000051\n",
      "Step 143, Loss Train: 3.1851, Loss Test: 3.0200, LR: 0.000051\n",
      "Step 144, Loss Train: 3.0194, Loss Test: 3.0200, LR: 0.000051\n",
      "Step 145, Loss Train: 3.2964, Loss Test: 3.0200, LR: 0.000051\n",
      "Step 146, Loss Train: 3.1575, Loss Test: 3.0200, LR: 0.000051\n",
      "Step 147, Loss Train: 2.9554, Loss Test: 3.0200, LR: 0.000051\n",
      "Step 148, Loss Train: 3.1191, Loss Test: 3.0200, LR: 0.000051\n",
      "Step 149, Loss Train: 3.1138, Loss Test: 3.0200, LR: 0.000051\n",
      "Step 150, Loss Train: 3.2365, Loss Test: 3.0200, LR: 0.000051\n",
      "Step 151, Loss Train: 2.9716, Loss Test: 2.8950, LR: 0.000051\n",
      "Step 152, Loss Train: 3.0311, Loss Test: 2.8950, LR: 0.000051\n",
      "Step 153, Loss Train: 3.3228, Loss Test: 2.8950, LR: 0.000051\n",
      "Step 154, Loss Train: 3.4258, Loss Test: 2.8950, LR: 0.000051\n",
      "Step 155, Loss Train: 3.1853, Loss Test: 2.8950, LR: 0.000051\n",
      "Step 156, Loss Train: 3.2020, Loss Test: 2.8950, LR: 0.000051\n",
      "Step 157, Loss Train: 3.0330, Loss Test: 2.8950, LR: 0.000051\n",
      "Step 158, Loss Train: 2.9333, Loss Test: 2.8950, LR: 0.000051\n",
      "Step 159, Loss Train: 2.9282, Loss Test: 2.8950, LR: 0.000051\n",
      "Step 160, Loss Train: 3.0910, Loss Test: 2.8950, LR: 0.000051\n",
      "Step 161, Loss Train: 3.1378, Loss Test: 2.8950, LR: 0.000051\n",
      "Step 162, Loss Train: 3.1456, Loss Test: 2.8950, LR: 0.000051\n",
      "Step 163, Loss Train: 3.1995, Loss Test: 2.8950, LR: 0.000051\n",
      "Step 164, Loss Train: 3.2414, Loss Test: 2.8950, LR: 0.000051\n",
      "Step 165, Loss Train: 3.1136, Loss Test: 2.8950, LR: 0.000051\n",
      "Step 166, Loss Train: 3.2601, Loss Test: 2.8950, LR: 0.000051\n",
      "Step 167, Loss Train: 3.1489, Loss Test: 2.8950, LR: 0.000051\n",
      "Step 168, Loss Train: 3.0765, Loss Test: 2.8950, LR: 0.000051\n",
      "Step 169, Loss Train: 3.2345, Loss Test: 2.8950, LR: 0.000051\n",
      "Step 170, Loss Train: 3.0528, Loss Test: 2.8950, LR: 0.000051\n",
      "Step 171, Loss Train: 3.2678, Loss Test: 2.8950, LR: 0.000051\n",
      "Step 172, Loss Train: 3.2475, Loss Test: 2.8950, LR: 0.000051\n",
      "Step 173, Loss Train: 3.1132, Loss Test: 2.8950, LR: 0.000051\n",
      "Step 174, Loss Train: 3.3240, Loss Test: 2.8950, LR: 0.000051\n",
      "Step 175, Loss Train: 3.4101, Loss Test: 2.8950, LR: 0.000051\n",
      "Step 176, Loss Train: 3.1266, Loss Test: 3.0904, LR: 0.000051\n",
      "Step 177, Loss Train: 3.2548, Loss Test: 3.0904, LR: 0.000051\n",
      "Step 178, Loss Train: 2.9732, Loss Test: 3.0904, LR: 0.000051\n",
      "Step 179, Loss Train: 3.1244, Loss Test: 3.0904, LR: 0.000051\n",
      "Step 180, Loss Train: 3.0854, Loss Test: 3.0904, LR: 0.000051\n",
      "Step 181, Loss Train: 3.1011, Loss Test: 3.0904, LR: 0.000051\n",
      "Step 182, Loss Train: 3.0720, Loss Test: 3.0904, LR: 0.000051\n",
      "Step 183, Loss Train: 3.2734, Loss Test: 3.0904, LR: 0.000051\n",
      "Step 184, Loss Train: 3.4102, Loss Test: 3.0904, LR: 0.000051\n",
      "Step 185, Loss Train: 3.0614, Loss Test: 3.0904, LR: 0.000051\n",
      "Step 186, Loss Train: 3.1571, Loss Test: 3.0904, LR: 0.000051\n",
      "Step 187, Loss Train: 2.9956, Loss Test: 3.0904, LR: 0.000051\n",
      "Step 188, Loss Train: 3.0803, Loss Test: 3.0904, LR: 0.000051\n",
      "Step 189, Loss Train: 2.9479, Loss Test: 3.0904, LR: 0.000051\n",
      "Step 190, Loss Train: 3.1020, Loss Test: 3.0904, LR: 0.000051\n",
      "Step 191, Loss Train: 3.0679, Loss Test: 3.0904, LR: 0.000051\n",
      "Step 192, Loss Train: 3.1561, Loss Test: 3.0904, LR: 0.000051\n",
      "Step 193, Loss Train: 3.1722, Loss Test: 3.0904, LR: 0.000051\n",
      "Step 194, Loss Train: 3.0982, Loss Test: 3.0904, LR: 0.000051\n",
      "Step 195, Loss Train: 2.9886, Loss Test: 3.0904, LR: 0.000051\n",
      "Step 196, Loss Train: 3.0013, Loss Test: 3.0904, LR: 0.000051\n",
      "Step 197, Loss Train: 3.0911, Loss Test: 3.0904, LR: 0.000051\n",
      "Step 198, Loss Train: 3.1285, Loss Test: 3.0904, LR: 0.000051\n",
      "Step 199, Loss Train: 2.9692, Loss Test: 3.0904, LR: 0.000051\n",
      "Step 200, Loss Train: 3.1710, Loss Test: 3.0904, LR: 0.000051\n",
      "Step 201, Loss Train: 3.1567, Loss Test: 3.3841, LR: 0.000051\n",
      "Step 202, Loss Train: 3.2107, Loss Test: 3.3841, LR: 0.000051\n",
      "Step 203, Loss Train: 3.1143, Loss Test: 3.3841, LR: 0.000051\n",
      "Step 204, Loss Train: 3.1506, Loss Test: 3.3841, LR: 0.000051\n",
      "Step 205, Loss Train: 3.0770, Loss Test: 3.3841, LR: 0.000051\n",
      "Step 206, Loss Train: 3.0282, Loss Test: 3.3841, LR: 0.000051\n",
      "Step 207, Loss Train: 3.1356, Loss Test: 3.3841, LR: 0.000051\n",
      "Step 208, Loss Train: 3.6137, Loss Test: 3.3841, LR: 0.000051\n",
      "Step 209, Loss Train: 3.1736, Loss Test: 3.3841, LR: 0.000051\n",
      "Step 210, Loss Train: 3.2315, Loss Test: 3.3841, LR: 0.000051\n",
      "Step 211, Loss Train: 3.2704, Loss Test: 3.3841, LR: 0.000051\n",
      "Step 212, Loss Train: 3.3213, Loss Test: 3.3841, LR: 0.000051\n",
      "Step 213, Loss Train: 3.1507, Loss Test: 3.3841, LR: 0.000051\n",
      "Step 214, Loss Train: 3.1051, Loss Test: 3.3841, LR: 0.000051\n",
      "Step 215, Loss Train: 3.1375, Loss Test: 3.3841, LR: 0.000051\n",
      "Step 216, Loss Train: 3.0348, Loss Test: 3.3841, LR: 0.000051\n",
      "Step 217, Loss Train: 3.1635, Loss Test: 3.3841, LR: 0.000051\n",
      "Step 218, Loss Train: 3.4232, Loss Test: 3.3841, LR: 0.000051\n",
      "Step 219, Loss Train: 3.0420, Loss Test: 3.3841, LR: 0.000051\n",
      "Step 220, Loss Train: 2.9879, Loss Test: 3.3841, LR: 0.000051\n",
      "Step 221, Loss Train: 3.1487, Loss Test: 3.3841, LR: 0.000051\n",
      "Step 222, Loss Train: 2.9729, Loss Test: 3.3841, LR: 0.000051\n",
      "Step 223, Loss Train: 3.0549, Loss Test: 3.3841, LR: 0.000051\n",
      "Step 224, Loss Train: 3.2058, Loss Test: 3.3841, LR: 0.000051\n",
      "Step 225, Loss Train: 3.2531, Loss Test: 3.3841, LR: 0.000051\n",
      "Step 226, Loss Train: 3.1719, Loss Test: 2.7514, LR: 0.000051\n",
      "Step 227, Loss Train: 3.1638, Loss Test: 2.7514, LR: 0.000051\n",
      "Step 228, Loss Train: 3.1213, Loss Test: 2.7514, LR: 0.000051\n",
      "Step 229, Loss Train: 3.1404, Loss Test: 2.7514, LR: 0.000051\n",
      "Step 230, Loss Train: 3.1735, Loss Test: 2.7514, LR: 0.000051\n",
      "Step 231, Loss Train: 3.0864, Loss Test: 2.7514, LR: 0.000051\n",
      "Step 232, Loss Train: 2.9854, Loss Test: 2.7514, LR: 0.000051\n",
      "Step 233, Loss Train: 3.1219, Loss Test: 2.7514, LR: 0.000051\n",
      "Step 234, Loss Train: 3.2119, Loss Test: 2.7514, LR: 0.000051\n",
      "Step 235, Loss Train: 3.2705, Loss Test: 2.7514, LR: 0.000051\n",
      "Step 236, Loss Train: 2.9780, Loss Test: 2.7514, LR: 0.000051\n",
      "Step 237, Loss Train: 3.3576, Loss Test: 2.7514, LR: 0.000051\n",
      "Step 238, Loss Train: 3.1331, Loss Test: 2.7514, LR: 0.000051\n",
      "Step 239, Loss Train: 2.9425, Loss Test: 2.7514, LR: 0.000051\n",
      "Step 240, Loss Train: 3.1246, Loss Test: 2.7514, LR: 0.000051\n",
      "Step 241, Loss Train: 3.2319, Loss Test: 2.7514, LR: 0.000051\n",
      "Step 242, Loss Train: 3.2871, Loss Test: 2.7514, LR: 0.000051\n",
      "Step 243, Loss Train: 3.2416, Loss Test: 2.7514, LR: 0.000051\n",
      "Step 244, Loss Train: 3.2690, Loss Test: 2.7514, LR: 0.000051\n",
      "Step 245, Loss Train: 3.1651, Loss Test: 2.7514, LR: 0.000051\n",
      "Step 246, Loss Train: 2.9529, Loss Test: 2.7514, LR: 0.000051\n",
      "Step 247, Loss Train: 3.2532, Loss Test: 2.7514, LR: 0.000051\n",
      "Step 248, Loss Train: 3.2597, Loss Test: 2.7514, LR: 0.000051\n",
      "Step 249, Loss Train: 3.1649, Loss Test: 2.7514, LR: 0.000051\n",
      "Step 250, Loss Train: 3.2594, Loss Test: 2.7514, LR: 0.000051\n",
      "Step 251, Loss Train: 3.1494, Loss Test: 3.3248, LR: 0.000051\n",
      "Step 252, Loss Train: 3.0502, Loss Test: 3.3248, LR: 0.000051\n",
      "Step 253, Loss Train: 3.2177, Loss Test: 3.3248, LR: 0.000051\n",
      "Step 254, Loss Train: 2.9999, Loss Test: 3.3248, LR: 0.000051\n",
      "Step 255, Loss Train: 2.9426, Loss Test: 3.3248, LR: 0.000051\n",
      "Step 256, Loss Train: 3.3849, Loss Test: 3.3248, LR: 0.000051\n",
      "Step 257, Loss Train: 3.2286, Loss Test: 3.3248, LR: 0.000051\n",
      "Step 258, Loss Train: 3.2518, Loss Test: 3.3248, LR: 0.000051\n",
      "Step 259, Loss Train: 3.3992, Loss Test: 3.3248, LR: 0.000051\n",
      "Step 260, Loss Train: 3.3728, Loss Test: 3.3248, LR: 0.000051\n",
      "Step 261, Loss Train: 3.0238, Loss Test: 3.3248, LR: 0.000051\n",
      "Step 262, Loss Train: 3.0435, Loss Test: 3.3248, LR: 0.000051\n",
      "Step 263, Loss Train: 3.1467, Loss Test: 3.3248, LR: 0.000051\n",
      "Step 264, Loss Train: 3.1855, Loss Test: 3.3248, LR: 0.000051\n",
      "Step 265, Loss Train: 3.1733, Loss Test: 3.3248, LR: 0.000051\n",
      "Step 266, Loss Train: 3.1343, Loss Test: 3.3248, LR: 0.000051\n",
      "Step 267, Loss Train: 3.2326, Loss Test: 3.3248, LR: 0.000051\n",
      "Step 268, Loss Train: 3.1939, Loss Test: 3.3248, LR: 0.000051\n",
      "Step 269, Loss Train: 3.2029, Loss Test: 3.3248, LR: 0.000051\n",
      "Step 270, Loss Train: 3.0793, Loss Test: 3.3248, LR: 0.000051\n",
      "Step 271, Loss Train: 3.1301, Loss Test: 3.3248, LR: 0.000051\n",
      "Step 272, Loss Train: 3.3630, Loss Test: 3.3248, LR: 0.000051\n",
      "Step 273, Loss Train: 3.0776, Loss Test: 3.3248, LR: 0.000051\n",
      "Step 274, Loss Train: 3.1298, Loss Test: 3.3248, LR: 0.000051\n",
      "Step 275, Loss Train: 3.0484, Loss Test: 3.3248, LR: 0.000051\n",
      "Step 276, Loss Train: 3.2865, Loss Test: 3.2501, LR: 0.000051\n",
      "Step 277, Loss Train: 3.2035, Loss Test: 3.2501, LR: 0.000051\n",
      "Step 278, Loss Train: 3.1251, Loss Test: 3.2501, LR: 0.000051\n",
      "Step 279, Loss Train: 3.3362, Loss Test: 3.2501, LR: 0.000051\n",
      "Step 280, Loss Train: 3.2128, Loss Test: 3.2501, LR: 0.000051\n",
      "Step 281, Loss Train: 3.1090, Loss Test: 3.2501, LR: 0.000051\n",
      "Step 282, Loss Train: 3.2923, Loss Test: 3.2501, LR: 0.000051\n",
      "Step 283, Loss Train: 3.1566, Loss Test: 3.2501, LR: 0.000051\n",
      "Step 284, Loss Train: 3.3520, Loss Test: 3.2501, LR: 0.000051\n",
      "Step 285, Loss Train: 3.0783, Loss Test: 3.2501, LR: 0.000051\n",
      "Step 286, Loss Train: 3.2571, Loss Test: 3.2501, LR: 0.000051\n",
      "Step 287, Loss Train: 3.4634, Loss Test: 3.2501, LR: 0.000051\n",
      "Step 288, Loss Train: 3.1143, Loss Test: 3.2501, LR: 0.000051\n",
      "Step 289, Loss Train: 3.1626, Loss Test: 3.2501, LR: 0.000051\n",
      "Step 290, Loss Train: 3.0133, Loss Test: 3.2501, LR: 0.000051\n",
      "Step 291, Loss Train: 3.0448, Loss Test: 3.2501, LR: 0.000051\n",
      "Step 292, Loss Train: 3.0457, Loss Test: 3.2501, LR: 0.000051\n",
      "Step 293, Loss Train: 3.1748, Loss Test: 3.2501, LR: 0.000051\n",
      "Step 294, Loss Train: 3.1321, Loss Test: 3.2501, LR: 0.000051\n",
      "Step 295, Loss Train: 3.1319, Loss Test: 3.2501, LR: 0.000051\n",
      "Step 296, Loss Train: 3.0271, Loss Test: 3.2501, LR: 0.000051\n",
      "Step 297, Loss Train: 3.1425, Loss Test: 3.2501, LR: 0.000051\n",
      "Step 298, Loss Train: 3.4088, Loss Test: 3.2501, LR: 0.000051\n",
      "Step 299, Loss Train: 3.0329, Loss Test: 3.2501, LR: 0.000051\n",
      "Step 300, Loss Train: 3.1837, Loss Test: 3.2501, LR: 0.000051\n",
      "Step 301, Loss Train: 2.9147, Loss Test: 3.1361, LR: 0.000051\n",
      "Step 302, Loss Train: 3.4583, Loss Test: 3.1361, LR: 0.000051\n",
      "Step 303, Loss Train: 3.0400, Loss Test: 3.1361, LR: 0.000051\n",
      "Step 304, Loss Train: 3.4205, Loss Test: 3.1361, LR: 0.000051\n",
      "Step 305, Loss Train: 3.1989, Loss Test: 3.1361, LR: 0.000051\n",
      "Step 306, Loss Train: 3.1810, Loss Test: 3.1361, LR: 0.000051\n",
      "Step 307, Loss Train: 3.1659, Loss Test: 3.1361, LR: 0.000051\n",
      "Step 308, Loss Train: 2.9285, Loss Test: 3.1361, LR: 0.000051\n",
      "Step 309, Loss Train: 3.1625, Loss Test: 3.1361, LR: 0.000051\n",
      "Step 310, Loss Train: 3.0365, Loss Test: 3.1361, LR: 0.000051\n",
      "Step 311, Loss Train: 3.3509, Loss Test: 3.1361, LR: 0.000051\n",
      "Step 312, Loss Train: 3.2386, Loss Test: 3.1361, LR: 0.000051\n",
      "Step 313, Loss Train: 3.0464, Loss Test: 3.1361, LR: 0.000051\n",
      "Step 314, Loss Train: 3.2282, Loss Test: 3.1361, LR: 0.000051\n",
      "Step 315, Loss Train: 3.0161, Loss Test: 3.1361, LR: 0.000051\n",
      "Step 316, Loss Train: 2.9416, Loss Test: 3.1361, LR: 0.000051\n",
      "Step 317, Loss Train: 2.9794, Loss Test: 3.1361, LR: 0.000051\n",
      "Step 318, Loss Train: 3.0568, Loss Test: 3.1361, LR: 0.000051\n",
      "Step 319, Loss Train: 3.2968, Loss Test: 3.1361, LR: 0.000051\n",
      "Step 320, Loss Train: 3.2658, Loss Test: 3.1361, LR: 0.000051\n",
      "Step 321, Loss Train: 3.3402, Loss Test: 3.1361, LR: 0.000051\n",
      "Step 322, Loss Train: 3.0280, Loss Test: 3.1361, LR: 0.000051\n",
      "Step 323, Loss Train: 3.4511, Loss Test: 3.1361, LR: 0.000051\n",
      "Step 324, Loss Train: 3.0488, Loss Test: 3.1361, LR: 0.000051\n",
      "Step 325, Loss Train: 3.0194, Loss Test: 3.1361, LR: 0.000051\n",
      "Step 326, Loss Train: 2.9958, Loss Test: 3.1713, LR: 0.000051\n",
      "Step 327, Loss Train: 3.0123, Loss Test: 3.1713, LR: 0.000051\n",
      "Step 328, Loss Train: 3.1055, Loss Test: 3.1713, LR: 0.000051\n",
      "Step 329, Loss Train: 3.2367, Loss Test: 3.1713, LR: 0.000051\n",
      "Step 330, Loss Train: 3.3984, Loss Test: 3.1713, LR: 0.000051\n",
      "Step 331, Loss Train: 3.2844, Loss Test: 3.1713, LR: 0.000051\n",
      "Step 332, Loss Train: 3.1959, Loss Test: 3.1713, LR: 0.000051\n",
      "Step 333, Loss Train: 3.2666, Loss Test: 3.1713, LR: 0.000051\n",
      "Step 334, Loss Train: 3.1350, Loss Test: 3.1713, LR: 0.000051\n",
      "Step 335, Loss Train: 3.2347, Loss Test: 3.1713, LR: 0.000051\n",
      "Step 336, Loss Train: 3.0620, Loss Test: 3.1713, LR: 0.000051\n",
      "Step 337, Loss Train: 3.0758, Loss Test: 3.1713, LR: 0.000051\n",
      "Step 338, Loss Train: 3.0663, Loss Test: 3.1713, LR: 0.000051\n",
      "Step 339, Loss Train: 3.2616, Loss Test: 3.1713, LR: 0.000051\n",
      "Step 340, Loss Train: 3.0772, Loss Test: 3.1713, LR: 0.000051\n",
      "Step 341, Loss Train: 3.1479, Loss Test: 3.1713, LR: 0.000051\n",
      "Step 342, Loss Train: 3.2063, Loss Test: 3.1713, LR: 0.000051\n",
      "Step 343, Loss Train: 3.1557, Loss Test: 3.1713, LR: 0.000051\n",
      "Step 344, Loss Train: 3.0443, Loss Test: 3.1713, LR: 0.000051\n",
      "Step 345, Loss Train: 3.2749, Loss Test: 3.1713, LR: 0.000051\n",
      "Step 346, Loss Train: 3.2827, Loss Test: 3.1713, LR: 0.000051\n",
      "Step 347, Loss Train: 3.2707, Loss Test: 3.1713, LR: 0.000051\n",
      "Step 348, Loss Train: 3.3888, Loss Test: 3.1713, LR: 0.000051\n",
      "Step 349, Loss Train: 3.2952, Loss Test: 3.1713, LR: 0.000051\n",
      "Step 350, Loss Train: 3.0477, Loss Test: 3.1713, LR: 0.000051\n",
      "Step 351, Loss Train: 3.2548, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 352, Loss Train: 3.1838, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 353, Loss Train: 3.1406, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 354, Loss Train: 3.2467, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 355, Loss Train: 3.1385, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 356, Loss Train: 3.0614, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 357, Loss Train: 3.1375, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 358, Loss Train: 3.3254, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 359, Loss Train: 3.2166, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 360, Loss Train: 3.1050, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 361, Loss Train: 3.0673, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 362, Loss Train: 3.1743, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 363, Loss Train: 3.0438, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 364, Loss Train: 2.9536, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 365, Loss Train: 2.9543, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 366, Loss Train: 3.0312, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 367, Loss Train: 3.1063, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 368, Loss Train: 3.1682, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 369, Loss Train: 3.2403, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 370, Loss Train: 3.2132, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 371, Loss Train: 3.1570, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 372, Loss Train: 3.0967, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 373, Loss Train: 3.1784, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 374, Loss Train: 3.0230, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 375, Loss Train: 3.2153, Loss Test: 3.2560, LR: 0.000051\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x000001C91FE18B20>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2762, in while_loop\n",
      "    loop_vars = body(*loop_vars)  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2753, in <lambda>\n",
      "    body = lambda i, lv: (i + 1, orig_body(*lv))  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\", line 655, in compute\n",
      "    return (next_i, flat_a_out, tas)  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\", line 650, in <listcomp>\n",
      "    tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_a_out)]  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 243, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x000001C92372D210>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2761, in while_loop\n",
      "    while cond(*loop_vars):  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2753, in <lambda>\n",
      "    body = lambda i, lv: (i + 1, orig_body(*lv))  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\", line 655, in compute\n",
      "    return (next_i, flat_a_out, tas)  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\", line 650, in <listcomp>\n",
      "    tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_a_out)]  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 243, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n",
      "Step 376, Loss Train: 3.0738, Loss Test: 3.1182, LR: 0.000051\n",
      "Step 377, Loss Train: 3.2683, Loss Test: 3.1182, LR: 0.000051\n",
      "Step 378, Loss Train: 3.2803, Loss Test: 3.1182, LR: 0.000051\n",
      "Step 379, Loss Train: 3.1771, Loss Test: 3.1182, LR: 0.000051\n",
      "Step 380, Loss Train: 3.2116, Loss Test: 3.1182, LR: 0.000051\n",
      "Step 381, Loss Train: 3.2504, Loss Test: 3.1182, LR: 0.000051\n",
      "Step 382, Loss Train: 3.0288, Loss Test: 3.1182, LR: 0.000051\n",
      "Step 383, Loss Train: 3.3398, Loss Test: 3.1182, LR: 0.000051\n",
      "Step 384, Loss Train: 3.1176, Loss Test: 3.1182, LR: 0.000051\n",
      "Step 385, Loss Train: 3.0286, Loss Test: 3.1182, LR: 0.000051\n",
      "Step 386, Loss Train: 3.3409, Loss Test: 3.1182, LR: 0.000051\n",
      "Step 387, Loss Train: 2.9671, Loss Test: 3.1182, LR: 0.000051\n",
      "Step 388, Loss Train: 3.0357, Loss Test: 3.1182, LR: 0.000051\n",
      "Step 389, Loss Train: 3.0160, Loss Test: 3.1182, LR: 0.000051\n",
      "Step 390, Loss Train: 3.2223, Loss Test: 3.1182, LR: 0.000051\n",
      "Step 391, Loss Train: 3.0419, Loss Test: 3.1182, LR: 0.000051\n",
      "Step 392, Loss Train: 3.1163, Loss Test: 3.1182, LR: 0.000051\n",
      "Step 393, Loss Train: 2.9864, Loss Test: 3.1182, LR: 0.000051\n",
      "Step 394, Loss Train: 3.0453, Loss Test: 3.1182, LR: 0.000051\n",
      "Step 395, Loss Train: 3.2959, Loss Test: 3.1182, LR: 0.000051\n",
      "Step 396, Loss Train: 2.9880, Loss Test: 3.1182, LR: 0.000051\n",
      "Step 397, Loss Train: 3.2980, Loss Test: 3.1182, LR: 0.000051\n",
      "Step 398, Loss Train: 2.9830, Loss Test: 3.1182, LR: 0.000051\n",
      "Step 399, Loss Train: 2.9286, Loss Test: 3.1182, LR: 0.000051\n",
      "Step 400, Loss Train: 3.1318, Loss Test: 3.1182, LR: 0.000051\n",
      "Step 401, Loss Train: 3.2327, Loss Test: 2.8407, LR: 0.000051\n",
      "Step 402, Loss Train: 3.3466, Loss Test: 2.8407, LR: 0.000051\n",
      "Step 403, Loss Train: 3.0334, Loss Test: 2.8407, LR: 0.000051\n",
      "Step 404, Loss Train: 3.2108, Loss Test: 2.8407, LR: 0.000051\n",
      "Step 405, Loss Train: 3.3628, Loss Test: 2.8407, LR: 0.000051\n",
      "Step 406, Loss Train: 3.0985, Loss Test: 2.8407, LR: 0.000051\n",
      "Step 407, Loss Train: 2.9130, Loss Test: 2.8407, LR: 0.000051\n",
      "Step 408, Loss Train: 3.0898, Loss Test: 2.8407, LR: 0.000051\n",
      "Step 409, Loss Train: 3.2360, Loss Test: 2.8407, LR: 0.000051\n",
      "Step 410, Loss Train: 3.0075, Loss Test: 2.8407, LR: 0.000051\n",
      "Step 411, Loss Train: 3.2082, Loss Test: 2.8407, LR: 0.000051\n",
      "Step 412, Loss Train: 3.0508, Loss Test: 2.8407, LR: 0.000051\n",
      "Step 413, Loss Train: 3.2638, Loss Test: 2.8407, LR: 0.000051\n",
      "Step 414, Loss Train: 3.0873, Loss Test: 2.8407, LR: 0.000051\n",
      "Step 415, Loss Train: 3.2413, Loss Test: 2.8407, LR: 0.000051\n",
      "Step 416, Loss Train: 3.1513, Loss Test: 2.8407, LR: 0.000051\n",
      "Step 417, Loss Train: 3.2262, Loss Test: 2.8407, LR: 0.000051\n",
      "Step 418, Loss Train: 3.2126, Loss Test: 2.8407, LR: 0.000051\n",
      "Step 419, Loss Train: 2.9307, Loss Test: 2.8407, LR: 0.000051\n",
      "Step 420, Loss Train: 2.9624, Loss Test: 2.8407, LR: 0.000051\n",
      "Step 421, Loss Train: 3.2285, Loss Test: 2.8407, LR: 0.000051\n",
      "Step 422, Loss Train: 3.1777, Loss Test: 2.8407, LR: 0.000051\n",
      "Step 423, Loss Train: 3.0254, Loss Test: 2.8407, LR: 0.000051\n",
      "Step 424, Loss Train: 3.2157, Loss Test: 2.8407, LR: 0.000051\n",
      "Step 425, Loss Train: 3.1116, Loss Test: 2.8407, LR: 0.000051\n",
      "Step 426, Loss Train: 3.0328, Loss Test: 2.9563, LR: 0.000051\n",
      "Step 427, Loss Train: 3.0014, Loss Test: 2.9563, LR: 0.000051\n",
      "Step 428, Loss Train: 3.3818, Loss Test: 2.9563, LR: 0.000051\n",
      "Step 429, Loss Train: 3.0628, Loss Test: 2.9563, LR: 0.000051\n",
      "Step 430, Loss Train: 3.2216, Loss Test: 2.9563, LR: 0.000051\n",
      "Step 431, Loss Train: 3.1494, Loss Test: 2.9563, LR: 0.000051\n",
      "Step 432, Loss Train: 2.9810, Loss Test: 2.9563, LR: 0.000051\n",
      "Step 433, Loss Train: 3.0473, Loss Test: 2.9563, LR: 0.000051\n",
      "Step 434, Loss Train: 3.2061, Loss Test: 2.9563, LR: 0.000051\n",
      "Step 435, Loss Train: 3.1973, Loss Test: 2.9563, LR: 0.000051\n",
      "Step 436, Loss Train: 2.9835, Loss Test: 2.9563, LR: 0.000051\n",
      "Step 437, Loss Train: 3.1528, Loss Test: 2.9563, LR: 0.000051\n",
      "Step 438, Loss Train: 3.4390, Loss Test: 2.9563, LR: 0.000051\n",
      "Step 439, Loss Train: 3.1663, Loss Test: 2.9563, LR: 0.000051\n",
      "Step 440, Loss Train: 3.2187, Loss Test: 2.9563, LR: 0.000051\n",
      "Step 441, Loss Train: 2.8772, Loss Test: 2.9563, LR: 0.000051\n",
      "Step 442, Loss Train: 3.0761, Loss Test: 2.9563, LR: 0.000051\n",
      "Step 443, Loss Train: 3.1339, Loss Test: 2.9563, LR: 0.000051\n",
      "Step 444, Loss Train: 3.1021, Loss Test: 2.9563, LR: 0.000051\n",
      "Step 445, Loss Train: 3.2149, Loss Test: 2.9563, LR: 0.000051\n",
      "Step 446, Loss Train: 3.0419, Loss Test: 2.9563, LR: 0.000051\n",
      "Step 447, Loss Train: 3.1319, Loss Test: 2.9563, LR: 0.000051\n",
      "Step 448, Loss Train: 3.1385, Loss Test: 2.9563, LR: 0.000051\n",
      "Step 449, Loss Train: 3.2327, Loss Test: 2.9563, LR: 0.000051\n",
      "Step 450, Loss Train: 3.0468, Loss Test: 2.9563, LR: 0.000051\n",
      "Step 451, Loss Train: 2.9459, Loss Test: 2.7068, LR: 0.000051\n",
      "Step 452, Loss Train: 2.9804, Loss Test: 2.7068, LR: 0.000051\n",
      "Step 453, Loss Train: 3.2086, Loss Test: 2.7068, LR: 0.000051\n",
      "Step 454, Loss Train: 3.1126, Loss Test: 2.7068, LR: 0.000051\n",
      "Step 455, Loss Train: 2.8558, Loss Test: 2.7068, LR: 0.000051\n",
      "Step 456, Loss Train: 3.1685, Loss Test: 2.7068, LR: 0.000051\n",
      "Step 457, Loss Train: 3.3541, Loss Test: 2.7068, LR: 0.000051\n",
      "Step 458, Loss Train: 2.8684, Loss Test: 2.7068, LR: 0.000051\n",
      "Step 459, Loss Train: 3.0587, Loss Test: 2.7068, LR: 0.000051\n",
      "Step 460, Loss Train: 2.8412, Loss Test: 2.7068, LR: 0.000051\n",
      "Step 461, Loss Train: 3.3849, Loss Test: 2.7068, LR: 0.000051\n",
      "Step 462, Loss Train: 3.3283, Loss Test: 2.7068, LR: 0.000051\n",
      "Step 463, Loss Train: 3.0210, Loss Test: 2.7068, LR: 0.000051\n",
      "Step 464, Loss Train: 3.1430, Loss Test: 2.7068, LR: 0.000051\n",
      "Step 465, Loss Train: 2.9975, Loss Test: 2.7068, LR: 0.000051\n",
      "Step 466, Loss Train: 3.0329, Loss Test: 2.7068, LR: 0.000051\n",
      "Step 467, Loss Train: 3.0710, Loss Test: 2.7068, LR: 0.000051\n",
      "Step 468, Loss Train: 3.3044, Loss Test: 2.7068, LR: 0.000051\n",
      "Step 469, Loss Train: 3.1222, Loss Test: 2.7068, LR: 0.000051\n",
      "Step 470, Loss Train: 2.7704, Loss Test: 2.7068, LR: 0.000051\n",
      "Step 471, Loss Train: 3.4112, Loss Test: 2.7068, LR: 0.000051\n",
      "Step 472, Loss Train: 3.0751, Loss Test: 2.7068, LR: 0.000051\n",
      "Step 473, Loss Train: 3.0426, Loss Test: 2.7068, LR: 0.000051\n",
      "Step 474, Loss Train: 3.1902, Loss Test: 2.7068, LR: 0.000051\n",
      "Step 475, Loss Train: 3.1561, Loss Test: 2.7068, LR: 0.000051\n",
      "Step 476, Loss Train: 3.4442, Loss Test: 3.3358, LR: 0.000051\n",
      "Step 477, Loss Train: 3.2271, Loss Test: 3.3358, LR: 0.000051\n",
      "Step 478, Loss Train: 3.1826, Loss Test: 3.3358, LR: 0.000051\n",
      "Step 479, Loss Train: 3.0439, Loss Test: 3.3358, LR: 0.000051\n",
      "Step 480, Loss Train: 3.0258, Loss Test: 3.3358, LR: 0.000051\n",
      "Step 481, Loss Train: 3.2309, Loss Test: 3.3358, LR: 0.000051\n",
      "Step 482, Loss Train: 3.0322, Loss Test: 3.3358, LR: 0.000051\n",
      "Step 483, Loss Train: 3.0399, Loss Test: 3.3358, LR: 0.000051\n",
      "Step 484, Loss Train: 3.1100, Loss Test: 3.3358, LR: 0.000051\n",
      "Step 485, Loss Train: 3.2902, Loss Test: 3.3358, LR: 0.000051\n",
      "Step 486, Loss Train: 3.0763, Loss Test: 3.3358, LR: 0.000051\n",
      "Step 487, Loss Train: 3.0112, Loss Test: 3.3358, LR: 0.000051\n",
      "Step 488, Loss Train: 3.1104, Loss Test: 3.3358, LR: 0.000051\n",
      "Step 489, Loss Train: 3.1700, Loss Test: 3.3358, LR: 0.000051\n",
      "Step 490, Loss Train: 3.0007, Loss Test: 3.3358, LR: 0.000051\n",
      "Step 491, Loss Train: 2.9777, Loss Test: 3.3358, LR: 0.000051\n",
      "Step 492, Loss Train: 3.1348, Loss Test: 3.3358, LR: 0.000051\n",
      "Step 493, Loss Train: 2.9886, Loss Test: 3.3358, LR: 0.000051\n",
      "Step 494, Loss Train: 3.3125, Loss Test: 3.3358, LR: 0.000051\n",
      "Step 495, Loss Train: 3.2763, Loss Test: 3.3358, LR: 0.000051\n",
      "Step 496, Loss Train: 3.4463, Loss Test: 3.3358, LR: 0.000051\n",
      "Step 497, Loss Train: 3.2853, Loss Test: 3.3358, LR: 0.000051\n",
      "Step 498, Loss Train: 2.9805, Loss Test: 3.3358, LR: 0.000051\n",
      "Step 499, Loss Train: 3.2989, Loss Test: 3.3358, LR: 0.000051\n",
      "Step 500, Loss Train: 3.0645, Loss Test: 3.3358, LR: 0.000051\n",
      "Step 501, Loss Train: 2.8658, Loss Test: 3.5388, LR: 0.000051\n",
      "Step 502, Loss Train: 3.3419, Loss Test: 3.5388, LR: 0.000051\n",
      "Step 503, Loss Train: 3.1511, Loss Test: 3.5388, LR: 0.000051\n",
      "Step 504, Loss Train: 3.2883, Loss Test: 3.5388, LR: 0.000051\n",
      "Step 505, Loss Train: 3.1002, Loss Test: 3.5388, LR: 0.000051\n",
      "Step 506, Loss Train: 3.2844, Loss Test: 3.5388, LR: 0.000051\n",
      "Step 507, Loss Train: 3.3793, Loss Test: 3.5388, LR: 0.000051\n",
      "Step 508, Loss Train: 3.0718, Loss Test: 3.5388, LR: 0.000051\n",
      "Step 509, Loss Train: 3.0856, Loss Test: 3.5388, LR: 0.000051\n",
      "Step 510, Loss Train: 3.0234, Loss Test: 3.5388, LR: 0.000051\n",
      "Step 511, Loss Train: 3.1564, Loss Test: 3.5388, LR: 0.000051\n",
      "Step 512, Loss Train: 3.1328, Loss Test: 3.5388, LR: 0.000051\n",
      "Step 513, Loss Train: 3.3139, Loss Test: 3.5388, LR: 0.000051\n",
      "Step 514, Loss Train: 3.2285, Loss Test: 3.5388, LR: 0.000051\n",
      "Step 515, Loss Train: 2.9476, Loss Test: 3.5388, LR: 0.000051\n",
      "Step 516, Loss Train: 2.9495, Loss Test: 3.5388, LR: 0.000051\n",
      "Step 517, Loss Train: 3.0367, Loss Test: 3.5388, LR: 0.000051\n",
      "Step 518, Loss Train: 3.1279, Loss Test: 3.5388, LR: 0.000051\n",
      "Step 519, Loss Train: 3.1734, Loss Test: 3.5388, LR: 0.000051\n",
      "Step 520, Loss Train: 3.2707, Loss Test: 3.5388, LR: 0.000051\n",
      "Step 521, Loss Train: 3.1488, Loss Test: 3.5388, LR: 0.000051\n",
      "Step 522, Loss Train: 3.3261, Loss Test: 3.5388, LR: 0.000051\n",
      "Step 523, Loss Train: 3.0197, Loss Test: 3.5388, LR: 0.000051\n",
      "Step 524, Loss Train: 3.1880, Loss Test: 3.5388, LR: 0.000051\n",
      "Step 525, Loss Train: 3.2265, Loss Test: 3.5388, LR: 0.000051\n",
      "Step 526, Loss Train: 3.1285, Loss Test: 3.2514, LR: 0.000051\n",
      "Step 527, Loss Train: 2.8892, Loss Test: 3.2514, LR: 0.000051\n",
      "Step 528, Loss Train: 3.1642, Loss Test: 3.2514, LR: 0.000051\n",
      "Step 529, Loss Train: 3.3439, Loss Test: 3.2514, LR: 0.000051\n",
      "Step 530, Loss Train: 3.3063, Loss Test: 3.2514, LR: 0.000051\n",
      "Step 531, Loss Train: 3.0415, Loss Test: 3.2514, LR: 0.000051\n",
      "Step 532, Loss Train: 3.2025, Loss Test: 3.2514, LR: 0.000051\n",
      "Step 533, Loss Train: 3.0221, Loss Test: 3.2514, LR: 0.000051\n",
      "Step 534, Loss Train: 2.9664, Loss Test: 3.2514, LR: 0.000051\n",
      "Step 535, Loss Train: 3.0871, Loss Test: 3.2514, LR: 0.000051\n",
      "Step 536, Loss Train: 3.1686, Loss Test: 3.2514, LR: 0.000051\n",
      "Step 537, Loss Train: 3.1570, Loss Test: 3.2514, LR: 0.000051\n",
      "Step 538, Loss Train: 3.0289, Loss Test: 3.2514, LR: 0.000051\n",
      "Step 539, Loss Train: 3.2213, Loss Test: 3.2514, LR: 0.000051\n",
      "Step 540, Loss Train: 3.1654, Loss Test: 3.2514, LR: 0.000051\n",
      "Step 541, Loss Train: 3.0666, Loss Test: 3.2514, LR: 0.000051\n",
      "Step 542, Loss Train: 3.2525, Loss Test: 3.2514, LR: 0.000051\n",
      "Step 543, Loss Train: 3.0595, Loss Test: 3.2514, LR: 0.000051\n",
      "Step 544, Loss Train: 3.0157, Loss Test: 3.2514, LR: 0.000051\n",
      "Step 545, Loss Train: 3.3432, Loss Test: 3.2514, LR: 0.000051\n",
      "Step 546, Loss Train: 3.2161, Loss Test: 3.2514, LR: 0.000051\n",
      "Step 547, Loss Train: 3.0916, Loss Test: 3.2514, LR: 0.000051\n",
      "Step 548, Loss Train: 3.3292, Loss Test: 3.2514, LR: 0.000051\n",
      "Step 549, Loss Train: 3.2348, Loss Test: 3.2514, LR: 0.000051\n",
      "Step 550, Loss Train: 3.1736, Loss Test: 3.2514, LR: 0.000051\n",
      "Step 551, Loss Train: 3.1456, Loss Test: 2.9974, LR: 0.000051\n",
      "Step 552, Loss Train: 2.9856, Loss Test: 2.9974, LR: 0.000051\n",
      "Step 553, Loss Train: 3.2162, Loss Test: 2.9974, LR: 0.000051\n",
      "Step 554, Loss Train: 3.2590, Loss Test: 2.9974, LR: 0.000051\n",
      "Step 555, Loss Train: 3.1882, Loss Test: 2.9974, LR: 0.000051\n",
      "Step 556, Loss Train: 3.1229, Loss Test: 2.9974, LR: 0.000051\n",
      "Step 557, Loss Train: 3.1487, Loss Test: 2.9974, LR: 0.000051\n",
      "Step 558, Loss Train: 3.2575, Loss Test: 2.9974, LR: 0.000051\n",
      "Step 559, Loss Train: 3.0315, Loss Test: 2.9974, LR: 0.000051\n",
      "Step 560, Loss Train: 3.2775, Loss Test: 2.9974, LR: 0.000051\n",
      "Step 561, Loss Train: 2.9508, Loss Test: 2.9974, LR: 0.000051\n",
      "Step 562, Loss Train: 3.1626, Loss Test: 2.9974, LR: 0.000051\n",
      "Step 563, Loss Train: 3.3280, Loss Test: 2.9974, LR: 0.000051\n",
      "Step 564, Loss Train: 3.0581, Loss Test: 2.9974, LR: 0.000051\n",
      "Step 565, Loss Train: 2.9773, Loss Test: 2.9974, LR: 0.000051\n",
      "Step 566, Loss Train: 3.3770, Loss Test: 2.9974, LR: 0.000051\n",
      "Step 567, Loss Train: 3.1758, Loss Test: 2.9974, LR: 0.000051\n",
      "Step 568, Loss Train: 3.0179, Loss Test: 2.9974, LR: 0.000051\n",
      "Step 569, Loss Train: 3.3050, Loss Test: 2.9974, LR: 0.000051\n",
      "Step 570, Loss Train: 3.1486, Loss Test: 2.9974, LR: 0.000051\n",
      "Step 571, Loss Train: 2.7833, Loss Test: 2.9974, LR: 0.000051\n",
      "Step 572, Loss Train: 3.2885, Loss Test: 2.9974, LR: 0.000051\n",
      "Step 573, Loss Train: 2.8708, Loss Test: 2.9974, LR: 0.000051\n",
      "Step 574, Loss Train: 3.1291, Loss Test: 2.9974, LR: 0.000051\n",
      "Step 575, Loss Train: 3.1107, Loss Test: 2.9974, LR: 0.000051\n",
      "Step 576, Loss Train: 3.1765, Loss Test: 2.9811, LR: 0.000051\n",
      "Step 577, Loss Train: 3.2821, Loss Test: 2.9811, LR: 0.000051\n",
      "Step 578, Loss Train: 3.2681, Loss Test: 2.9811, LR: 0.000051\n",
      "Step 579, Loss Train: 3.0873, Loss Test: 2.9811, LR: 0.000051\n",
      "Step 580, Loss Train: 3.3101, Loss Test: 2.9811, LR: 0.000051\n",
      "Step 581, Loss Train: 3.0919, Loss Test: 2.9811, LR: 0.000051\n",
      "Step 582, Loss Train: 3.0783, Loss Test: 2.9811, LR: 0.000051\n",
      "Step 583, Loss Train: 3.0070, Loss Test: 2.9811, LR: 0.000051\n",
      "Step 584, Loss Train: 3.0312, Loss Test: 2.9811, LR: 0.000051\n",
      "Step 585, Loss Train: 3.1247, Loss Test: 2.9811, LR: 0.000051\n",
      "Step 586, Loss Train: 3.2815, Loss Test: 2.9811, LR: 0.000051\n",
      "Step 587, Loss Train: 3.1301, Loss Test: 2.9811, LR: 0.000051\n",
      "Step 588, Loss Train: 3.0696, Loss Test: 2.9811, LR: 0.000051\n",
      "Step 589, Loss Train: 3.0598, Loss Test: 2.9811, LR: 0.000051\n",
      "Step 590, Loss Train: 3.2574, Loss Test: 2.9811, LR: 0.000051\n",
      "Step 591, Loss Train: 2.9410, Loss Test: 2.9811, LR: 0.000051\n",
      "Step 592, Loss Train: 3.0004, Loss Test: 2.9811, LR: 0.000051\n",
      "Step 593, Loss Train: 3.2806, Loss Test: 2.9811, LR: 0.000051\n",
      "Step 594, Loss Train: 3.2442, Loss Test: 2.9811, LR: 0.000051\n",
      "Step 595, Loss Train: 3.1339, Loss Test: 2.9811, LR: 0.000051\n",
      "Step 596, Loss Train: 3.0898, Loss Test: 2.9811, LR: 0.000051\n",
      "Step 597, Loss Train: 3.2510, Loss Test: 2.9811, LR: 0.000051\n",
      "Step 598, Loss Train: 3.0040, Loss Test: 2.9811, LR: 0.000051\n",
      "Step 599, Loss Train: 3.2344, Loss Test: 2.9811, LR: 0.000051\n",
      "Step 600, Loss Train: 3.0234, Loss Test: 2.9811, LR: 0.000051\n",
      "Step 601, Loss Train: 3.1705, Loss Test: 2.9519, LR: 0.000051\n",
      "Step 602, Loss Train: 3.3771, Loss Test: 2.9519, LR: 0.000051\n",
      "Step 603, Loss Train: 3.1437, Loss Test: 2.9519, LR: 0.000051\n",
      "Step 604, Loss Train: 3.0149, Loss Test: 2.9519, LR: 0.000051\n",
      "Step 605, Loss Train: 3.3139, Loss Test: 2.9519, LR: 0.000051\n",
      "Step 606, Loss Train: 3.1440, Loss Test: 2.9519, LR: 0.000051\n",
      "Step 607, Loss Train: 3.1008, Loss Test: 2.9519, LR: 0.000051\n",
      "Step 608, Loss Train: 3.2097, Loss Test: 2.9519, LR: 0.000051\n",
      "Step 609, Loss Train: 3.1326, Loss Test: 2.9519, LR: 0.000051\n",
      "Step 610, Loss Train: 3.1753, Loss Test: 2.9519, LR: 0.000051\n",
      "Step 611, Loss Train: 3.1092, Loss Test: 2.9519, LR: 0.000051\n",
      "Step 612, Loss Train: 3.1657, Loss Test: 2.9519, LR: 0.000051\n",
      "Step 613, Loss Train: 3.2581, Loss Test: 2.9519, LR: 0.000051\n",
      "Step 614, Loss Train: 3.0469, Loss Test: 2.9519, LR: 0.000051\n",
      "Step 615, Loss Train: 3.3104, Loss Test: 2.9519, LR: 0.000051\n",
      "Step 616, Loss Train: 3.2681, Loss Test: 2.9519, LR: 0.000051\n",
      "Step 617, Loss Train: 3.2392, Loss Test: 2.9519, LR: 0.000051\n",
      "Step 618, Loss Train: 3.2634, Loss Test: 2.9519, LR: 0.000051\n",
      "Step 619, Loss Train: 3.2313, Loss Test: 2.9519, LR: 0.000051\n",
      "Step 620, Loss Train: 3.1333, Loss Test: 2.9519, LR: 0.000051\n",
      "Step 621, Loss Train: 3.1588, Loss Test: 2.9519, LR: 0.000051\n",
      "Step 622, Loss Train: 2.8372, Loss Test: 2.9519, LR: 0.000051\n",
      "Step 623, Loss Train: 3.2894, Loss Test: 2.9519, LR: 0.000051\n",
      "Step 624, Loss Train: 3.1654, Loss Test: 2.9519, LR: 0.000051\n",
      "Step 625, Loss Train: 3.3223, Loss Test: 2.9519, LR: 0.000051\n",
      "Step 626, Loss Train: 3.3000, Loss Test: 2.9775, LR: 0.000051\n",
      "Step 627, Loss Train: 3.1606, Loss Test: 2.9775, LR: 0.000051\n",
      "Step 628, Loss Train: 3.1055, Loss Test: 2.9775, LR: 0.000051\n",
      "Step 629, Loss Train: 3.1917, Loss Test: 2.9775, LR: 0.000051\n",
      "Step 630, Loss Train: 3.2591, Loss Test: 2.9775, LR: 0.000051\n",
      "Step 631, Loss Train: 3.0945, Loss Test: 2.9775, LR: 0.000051\n",
      "Step 632, Loss Train: 3.1457, Loss Test: 2.9775, LR: 0.000051\n",
      "Step 633, Loss Train: 3.1589, Loss Test: 2.9775, LR: 0.000051\n",
      "Step 634, Loss Train: 3.2355, Loss Test: 2.9775, LR: 0.000051\n",
      "Step 635, Loss Train: 2.8284, Loss Test: 2.9775, LR: 0.000051\n",
      "Step 636, Loss Train: 3.1096, Loss Test: 2.9775, LR: 0.000051\n",
      "Step 637, Loss Train: 3.3751, Loss Test: 2.9775, LR: 0.000051\n",
      "Step 638, Loss Train: 3.4111, Loss Test: 2.9775, LR: 0.000051\n",
      "Step 639, Loss Train: 3.2692, Loss Test: 2.9775, LR: 0.000051\n",
      "Step 640, Loss Train: 3.2010, Loss Test: 2.9775, LR: 0.000051\n",
      "Step 641, Loss Train: 3.1546, Loss Test: 2.9775, LR: 0.000051\n",
      "Step 642, Loss Train: 3.2117, Loss Test: 2.9775, LR: 0.000051\n",
      "Step 643, Loss Train: 2.9185, Loss Test: 2.9775, LR: 0.000051\n",
      "Step 644, Loss Train: 3.0449, Loss Test: 2.9775, LR: 0.000051\n",
      "Step 645, Loss Train: 3.2470, Loss Test: 2.9775, LR: 0.000051\n",
      "Step 646, Loss Train: 3.1896, Loss Test: 2.9775, LR: 0.000051\n",
      "Step 647, Loss Train: 3.2286, Loss Test: 2.9775, LR: 0.000051\n",
      "Step 648, Loss Train: 3.0924, Loss Test: 2.9775, LR: 0.000051\n",
      "Step 649, Loss Train: 3.4164, Loss Test: 2.9775, LR: 0.000051\n",
      "Step 650, Loss Train: 3.2055, Loss Test: 2.9775, LR: 0.000051\n",
      "Step 651, Loss Train: 3.1125, Loss Test: 3.1987, LR: 0.000051\n",
      "Step 652, Loss Train: 3.1070, Loss Test: 3.1987, LR: 0.000051\n",
      "Step 653, Loss Train: 3.3438, Loss Test: 3.1987, LR: 0.000051\n",
      "Step 654, Loss Train: 3.1614, Loss Test: 3.1987, LR: 0.000051\n",
      "Step 655, Loss Train: 3.3429, Loss Test: 3.1987, LR: 0.000051\n",
      "Step 656, Loss Train: 3.0289, Loss Test: 3.1987, LR: 0.000051\n",
      "Step 657, Loss Train: 2.9538, Loss Test: 3.1987, LR: 0.000051\n",
      "Step 658, Loss Train: 3.1357, Loss Test: 3.1987, LR: 0.000051\n",
      "Step 659, Loss Train: 3.1267, Loss Test: 3.1987, LR: 0.000051\n",
      "Step 660, Loss Train: 3.2334, Loss Test: 3.1987, LR: 0.000051\n",
      "Step 661, Loss Train: 3.1524, Loss Test: 3.1987, LR: 0.000051\n",
      "Step 662, Loss Train: 3.0716, Loss Test: 3.1987, LR: 0.000051\n",
      "Step 663, Loss Train: 3.0793, Loss Test: 3.1987, LR: 0.000051\n",
      "Step 664, Loss Train: 3.1372, Loss Test: 3.1987, LR: 0.000051\n",
      "Step 665, Loss Train: 3.3018, Loss Test: 3.1987, LR: 0.000051\n",
      "Step 666, Loss Train: 3.2738, Loss Test: 3.1987, LR: 0.000051\n",
      "Step 667, Loss Train: 3.2689, Loss Test: 3.1987, LR: 0.000051\n",
      "Step 668, Loss Train: 3.1612, Loss Test: 3.1987, LR: 0.000051\n",
      "Step 669, Loss Train: 3.1275, Loss Test: 3.1987, LR: 0.000051\n",
      "Step 670, Loss Train: 3.0673, Loss Test: 3.1987, LR: 0.000051\n",
      "Step 671, Loss Train: 3.2059, Loss Test: 3.1987, LR: 0.000051\n",
      "Step 672, Loss Train: 3.1895, Loss Test: 3.1987, LR: 0.000051\n",
      "Step 673, Loss Train: 3.2919, Loss Test: 3.1987, LR: 0.000051\n",
      "Step 674, Loss Train: 3.0078, Loss Test: 3.1987, LR: 0.000051\n",
      "Step 675, Loss Train: 3.1528, Loss Test: 3.1987, LR: 0.000051\n",
      "Step 676, Loss Train: 3.3492, Loss Test: 3.3478, LR: 0.000051\n",
      "Step 677, Loss Train: 3.1271, Loss Test: 3.3478, LR: 0.000051\n",
      "Step 678, Loss Train: 3.2275, Loss Test: 3.3478, LR: 0.000051\n",
      "Step 679, Loss Train: 3.1302, Loss Test: 3.3478, LR: 0.000051\n",
      "Step 680, Loss Train: 3.0988, Loss Test: 3.3478, LR: 0.000051\n",
      "Step 681, Loss Train: 3.0006, Loss Test: 3.3478, LR: 0.000051\n",
      "Step 682, Loss Train: 3.1179, Loss Test: 3.3478, LR: 0.000051\n",
      "Step 683, Loss Train: 3.2444, Loss Test: 3.3478, LR: 0.000051\n",
      "Step 684, Loss Train: 3.2335, Loss Test: 3.3478, LR: 0.000051\n",
      "Step 685, Loss Train: 3.2759, Loss Test: 3.3478, LR: 0.000051\n",
      "Step 686, Loss Train: 3.1181, Loss Test: 3.3478, LR: 0.000051\n",
      "Step 687, Loss Train: 2.9927, Loss Test: 3.3478, LR: 0.000051\n",
      "Step 688, Loss Train: 3.1457, Loss Test: 3.3478, LR: 0.000051\n",
      "Step 689, Loss Train: 3.0992, Loss Test: 3.3478, LR: 0.000051\n",
      "Step 690, Loss Train: 3.0739, Loss Test: 3.3478, LR: 0.000051\n",
      "Step 691, Loss Train: 3.0671, Loss Test: 3.3478, LR: 0.000051\n",
      "Step 692, Loss Train: 3.2129, Loss Test: 3.3478, LR: 0.000051\n",
      "Step 693, Loss Train: 3.2579, Loss Test: 3.3478, LR: 0.000051\n",
      "Step 694, Loss Train: 3.2981, Loss Test: 3.3478, LR: 0.000051\n",
      "Step 695, Loss Train: 3.2050, Loss Test: 3.3478, LR: 0.000051\n",
      "Step 696, Loss Train: 3.1270, Loss Test: 3.3478, LR: 0.000051\n",
      "Step 697, Loss Train: 2.9469, Loss Test: 3.3478, LR: 0.000051\n",
      "Step 698, Loss Train: 3.3200, Loss Test: 3.3478, LR: 0.000051\n",
      "Step 699, Loss Train: 3.1711, Loss Test: 3.3478, LR: 0.000051\n",
      "Step 700, Loss Train: 3.4246, Loss Test: 3.3478, LR: 0.000051\n",
      "Step 701, Loss Train: 3.1190, Loss Test: 3.0411, LR: 0.000051\n",
      "Step 702, Loss Train: 3.1181, Loss Test: 3.0411, LR: 0.000051\n",
      "Step 703, Loss Train: 3.3167, Loss Test: 3.0411, LR: 0.000051\n",
      "Step 704, Loss Train: 3.1121, Loss Test: 3.0411, LR: 0.000051\n",
      "Step 705, Loss Train: 3.0146, Loss Test: 3.0411, LR: 0.000051\n",
      "Step 706, Loss Train: 3.2195, Loss Test: 3.0411, LR: 0.000051\n",
      "Step 707, Loss Train: 3.0352, Loss Test: 3.0411, LR: 0.000051\n",
      "Step 708, Loss Train: 3.1944, Loss Test: 3.0411, LR: 0.000051\n",
      "Step 709, Loss Train: 3.1330, Loss Test: 3.0411, LR: 0.000051\n",
      "Step 710, Loss Train: 2.9967, Loss Test: 3.0411, LR: 0.000051\n",
      "Step 711, Loss Train: 3.2410, Loss Test: 3.0411, LR: 0.000051\n",
      "Step 712, Loss Train: 3.2168, Loss Test: 3.0411, LR: 0.000051\n",
      "Step 713, Loss Train: 3.1187, Loss Test: 3.0411, LR: 0.000051\n",
      "Step 714, Loss Train: 3.0254, Loss Test: 3.0411, LR: 0.000051\n",
      "Step 715, Loss Train: 3.2259, Loss Test: 3.0411, LR: 0.000051\n",
      "Step 716, Loss Train: 3.2500, Loss Test: 3.0411, LR: 0.000051\n",
      "Step 717, Loss Train: 3.2446, Loss Test: 3.0411, LR: 0.000051\n",
      "Step 718, Loss Train: 3.0057, Loss Test: 3.0411, LR: 0.000051\n",
      "Step 719, Loss Train: 3.3615, Loss Test: 3.0411, LR: 0.000051\n",
      "Step 720, Loss Train: 3.1456, Loss Test: 3.0411, LR: 0.000051\n",
      "Step 721, Loss Train: 3.1029, Loss Test: 3.0411, LR: 0.000051\n",
      "Step 722, Loss Train: 3.1101, Loss Test: 3.0411, LR: 0.000051\n",
      "Step 723, Loss Train: 3.4215, Loss Test: 3.0411, LR: 0.000051\n",
      "Step 724, Loss Train: 3.3281, Loss Test: 3.0411, LR: 0.000051\n",
      "Step 725, Loss Train: 3.0745, Loss Test: 3.0411, LR: 0.000051\n",
      "Step 726, Loss Train: 3.2751, Loss Test: 3.6243, LR: 0.000051\n",
      "Step 727, Loss Train: 3.1607, Loss Test: 3.6243, LR: 0.000051\n",
      "Step 728, Loss Train: 3.1053, Loss Test: 3.6243, LR: 0.000051\n",
      "Step 729, Loss Train: 3.0390, Loss Test: 3.6243, LR: 0.000051\n",
      "Step 730, Loss Train: 3.2816, Loss Test: 3.6243, LR: 0.000051\n",
      "Step 731, Loss Train: 3.1690, Loss Test: 3.6243, LR: 0.000051\n",
      "Step 732, Loss Train: 3.0960, Loss Test: 3.6243, LR: 0.000051\n",
      "Step 733, Loss Train: 3.0290, Loss Test: 3.6243, LR: 0.000051\n",
      "Step 734, Loss Train: 3.2982, Loss Test: 3.6243, LR: 0.000051\n",
      "Step 735, Loss Train: 2.9089, Loss Test: 3.6243, LR: 0.000051\n",
      "Step 736, Loss Train: 3.1745, Loss Test: 3.6243, LR: 0.000051\n",
      "Step 737, Loss Train: 2.9257, Loss Test: 3.6243, LR: 0.000051\n",
      "Step 738, Loss Train: 2.9673, Loss Test: 3.6243, LR: 0.000051\n",
      "Step 739, Loss Train: 2.9862, Loss Test: 3.6243, LR: 0.000051\n",
      "Step 740, Loss Train: 3.1674, Loss Test: 3.6243, LR: 0.000051\n",
      "Step 741, Loss Train: 2.9824, Loss Test: 3.6243, LR: 0.000051\n",
      "Step 742, Loss Train: 3.2675, Loss Test: 3.6243, LR: 0.000051\n",
      "Step 743, Loss Train: 3.0800, Loss Test: 3.6243, LR: 0.000051\n",
      "Step 744, Loss Train: 2.9634, Loss Test: 3.6243, LR: 0.000051\n",
      "Step 745, Loss Train: 3.2469, Loss Test: 3.6243, LR: 0.000051\n",
      "Step 746, Loss Train: 3.2546, Loss Test: 3.6243, LR: 0.000051\n",
      "Step 747, Loss Train: 3.0723, Loss Test: 3.6243, LR: 0.000051\n",
      "Step 748, Loss Train: 3.1974, Loss Test: 3.6243, LR: 0.000051\n",
      "Step 749, Loss Train: 3.3858, Loss Test: 3.6243, LR: 0.000051\n",
      "Step 750, Loss Train: 3.3066, Loss Test: 3.6243, LR: 0.000051\n",
      "Step 751, Loss Train: 3.0710, Loss Test: 3.3430, LR: 0.000051\n",
      "Step 752, Loss Train: 3.1345, Loss Test: 3.3430, LR: 0.000051\n",
      "Step 753, Loss Train: 2.9957, Loss Test: 3.3430, LR: 0.000051\n",
      "Step 754, Loss Train: 2.9514, Loss Test: 3.3430, LR: 0.000051\n",
      "Step 755, Loss Train: 3.1944, Loss Test: 3.3430, LR: 0.000051\n",
      "Step 756, Loss Train: 3.1315, Loss Test: 3.3430, LR: 0.000051\n",
      "Step 757, Loss Train: 3.0622, Loss Test: 3.3430, LR: 0.000051\n",
      "Step 758, Loss Train: 3.3134, Loss Test: 3.3430, LR: 0.000051\n",
      "Step 759, Loss Train: 3.2436, Loss Test: 3.3430, LR: 0.000051\n",
      "Step 760, Loss Train: 2.9393, Loss Test: 3.3430, LR: 0.000051\n",
      "Step 761, Loss Train: 3.0256, Loss Test: 3.3430, LR: 0.000051\n",
      "Step 762, Loss Train: 3.0841, Loss Test: 3.3430, LR: 0.000051\n",
      "Step 763, Loss Train: 3.1967, Loss Test: 3.3430, LR: 0.000051\n",
      "Step 764, Loss Train: 3.2856, Loss Test: 3.3430, LR: 0.000051\n",
      "Step 765, Loss Train: 3.2978, Loss Test: 3.3430, LR: 0.000051\n",
      "Step 766, Loss Train: 2.8770, Loss Test: 3.3430, LR: 0.000051\n",
      "Step 767, Loss Train: 3.0963, Loss Test: 3.3430, LR: 0.000051\n",
      "Step 768, Loss Train: 3.0810, Loss Test: 3.3430, LR: 0.000051\n",
      "Step 769, Loss Train: 2.7589, Loss Test: 3.3430, LR: 0.000051\n",
      "Step 770, Loss Train: 3.1798, Loss Test: 3.3430, LR: 0.000051\n",
      "Step 771, Loss Train: 3.2854, Loss Test: 3.3430, LR: 0.000051\n",
      "Step 772, Loss Train: 3.3855, Loss Test: 3.3430, LR: 0.000051\n",
      "Step 773, Loss Train: 3.2504, Loss Test: 3.3430, LR: 0.000051\n",
      "Step 774, Loss Train: 3.1513, Loss Test: 3.3430, LR: 0.000051\n",
      "Step 775, Loss Train: 3.2101, Loss Test: 3.3430, LR: 0.000051\n",
      "Step 776, Loss Train: 3.3337, Loss Test: 2.7700, LR: 0.000051\n",
      "Step 777, Loss Train: 3.0501, Loss Test: 2.7700, LR: 0.000051\n",
      "Step 778, Loss Train: 3.5069, Loss Test: 2.7700, LR: 0.000051\n",
      "Step 779, Loss Train: 3.2710, Loss Test: 2.7700, LR: 0.000051\n",
      "Step 780, Loss Train: 3.1546, Loss Test: 2.7700, LR: 0.000051\n",
      "Step 781, Loss Train: 3.2640, Loss Test: 2.7700, LR: 0.000051\n",
      "Step 782, Loss Train: 3.0591, Loss Test: 2.7700, LR: 0.000051\n",
      "Step 783, Loss Train: 3.2733, Loss Test: 2.7700, LR: 0.000051\n",
      "Step 784, Loss Train: 3.1887, Loss Test: 2.7700, LR: 0.000051\n",
      "Step 785, Loss Train: 3.1405, Loss Test: 2.7700, LR: 0.000051\n",
      "Step 786, Loss Train: 3.2393, Loss Test: 2.7700, LR: 0.000051\n",
      "Step 787, Loss Train: 3.1482, Loss Test: 2.7700, LR: 0.000051\n",
      "Step 788, Loss Train: 2.9541, Loss Test: 2.7700, LR: 0.000051\n",
      "Step 789, Loss Train: 3.2023, Loss Test: 2.7700, LR: 0.000051\n",
      "Step 790, Loss Train: 3.1881, Loss Test: 2.7700, LR: 0.000051\n",
      "Step 791, Loss Train: 3.1818, Loss Test: 2.7700, LR: 0.000051\n",
      "Step 792, Loss Train: 3.2007, Loss Test: 2.7700, LR: 0.000051\n",
      "Step 793, Loss Train: 3.2690, Loss Test: 2.7700, LR: 0.000051\n",
      "Step 794, Loss Train: 3.0089, Loss Test: 2.7700, LR: 0.000051\n",
      "Step 795, Loss Train: 3.2260, Loss Test: 2.7700, LR: 0.000051\n",
      "Step 796, Loss Train: 3.0810, Loss Test: 2.7700, LR: 0.000051\n",
      "Step 797, Loss Train: 3.4335, Loss Test: 2.7700, LR: 0.000051\n",
      "Step 798, Loss Train: 3.1442, Loss Test: 2.7700, LR: 0.000051\n",
      "Step 799, Loss Train: 3.2377, Loss Test: 2.7700, LR: 0.000051\n",
      "Step 800, Loss Train: 3.1048, Loss Test: 2.7700, LR: 0.000051\n",
      "Step 801, Loss Train: 3.0303, Loss Test: 3.0365, LR: 0.000051\n",
      "Step 802, Loss Train: 3.0681, Loss Test: 3.0365, LR: 0.000051\n",
      "Step 803, Loss Train: 3.1624, Loss Test: 3.0365, LR: 0.000051\n",
      "Step 804, Loss Train: 3.4501, Loss Test: 3.0365, LR: 0.000051\n",
      "Step 805, Loss Train: 2.9631, Loss Test: 3.0365, LR: 0.000051\n",
      "Step 806, Loss Train: 3.0759, Loss Test: 3.0365, LR: 0.000051\n",
      "Step 807, Loss Train: 3.1973, Loss Test: 3.0365, LR: 0.000051\n",
      "Step 808, Loss Train: 2.9939, Loss Test: 3.0365, LR: 0.000051\n",
      "Step 809, Loss Train: 3.1499, Loss Test: 3.0365, LR: 0.000051\n",
      "Step 810, Loss Train: 3.4035, Loss Test: 3.0365, LR: 0.000051\n",
      "Step 811, Loss Train: 3.1761, Loss Test: 3.0365, LR: 0.000051\n",
      "Step 812, Loss Train: 3.3668, Loss Test: 3.0365, LR: 0.000051\n",
      "Step 813, Loss Train: 2.8698, Loss Test: 3.0365, LR: 0.000051\n",
      "Step 814, Loss Train: 3.1515, Loss Test: 3.0365, LR: 0.000051\n",
      "Step 815, Loss Train: 3.2276, Loss Test: 3.0365, LR: 0.000051\n",
      "Step 816, Loss Train: 3.1907, Loss Test: 3.0365, LR: 0.000051\n",
      "Step 817, Loss Train: 2.9847, Loss Test: 3.0365, LR: 0.000051\n",
      "Step 818, Loss Train: 3.1734, Loss Test: 3.0365, LR: 0.000051\n",
      "Step 819, Loss Train: 3.3652, Loss Test: 3.0365, LR: 0.000051\n",
      "Step 820, Loss Train: 3.2416, Loss Test: 3.0365, LR: 0.000051\n",
      "Step 821, Loss Train: 3.1602, Loss Test: 3.0365, LR: 0.000051\n",
      "Step 822, Loss Train: 3.2455, Loss Test: 3.0365, LR: 0.000051\n",
      "Step 823, Loss Train: 3.2090, Loss Test: 3.0365, LR: 0.000051\n",
      "Step 824, Loss Train: 3.0494, Loss Test: 3.0365, LR: 0.000051\n",
      "Step 825, Loss Train: 3.2099, Loss Test: 3.0365, LR: 0.000051\n",
      "Step 826, Loss Train: 3.2474, Loss Test: 3.0876, LR: 0.000051\n",
      "Step 827, Loss Train: 3.2832, Loss Test: 3.0876, LR: 0.000051\n",
      "Step 828, Loss Train: 2.8199, Loss Test: 3.0876, LR: 0.000051\n",
      "Step 829, Loss Train: 3.2635, Loss Test: 3.0876, LR: 0.000051\n",
      "Step 830, Loss Train: 3.1727, Loss Test: 3.0876, LR: 0.000051\n",
      "Step 831, Loss Train: 3.0736, Loss Test: 3.0876, LR: 0.000051\n",
      "Step 832, Loss Train: 3.1602, Loss Test: 3.0876, LR: 0.000051\n",
      "Step 833, Loss Train: 2.9517, Loss Test: 3.0876, LR: 0.000051\n",
      "Step 834, Loss Train: 3.0622, Loss Test: 3.0876, LR: 0.000051\n",
      "Step 835, Loss Train: 3.0120, Loss Test: 3.0876, LR: 0.000051\n",
      "Step 836, Loss Train: 3.3325, Loss Test: 3.0876, LR: 0.000051\n",
      "Step 837, Loss Train: 3.2342, Loss Test: 3.0876, LR: 0.000051\n",
      "Step 838, Loss Train: 3.1861, Loss Test: 3.0876, LR: 0.000051\n",
      "Step 839, Loss Train: 3.0205, Loss Test: 3.0876, LR: 0.000051\n",
      "Step 840, Loss Train: 2.9559, Loss Test: 3.0876, LR: 0.000051\n",
      "Step 841, Loss Train: 3.0830, Loss Test: 3.0876, LR: 0.000051\n",
      "Step 842, Loss Train: 3.2415, Loss Test: 3.0876, LR: 0.000051\n",
      "Step 843, Loss Train: 2.9834, Loss Test: 3.0876, LR: 0.000051\n",
      "Step 844, Loss Train: 2.9515, Loss Test: 3.0876, LR: 0.000051\n",
      "Step 845, Loss Train: 3.1794, Loss Test: 3.0876, LR: 0.000051\n",
      "Step 846, Loss Train: 2.8919, Loss Test: 3.0876, LR: 0.000051\n",
      "Step 847, Loss Train: 3.1085, Loss Test: 3.0876, LR: 0.000051\n",
      "Step 848, Loss Train: 3.0956, Loss Test: 3.0876, LR: 0.000051\n",
      "Step 849, Loss Train: 2.8664, Loss Test: 3.0876, LR: 0.000051\n",
      "Step 850, Loss Train: 3.1974, Loss Test: 3.0876, LR: 0.000051\n",
      "Step 851, Loss Train: 3.0220, Loss Test: 3.1029, LR: 0.000051\n",
      "Step 852, Loss Train: 2.7727, Loss Test: 3.1029, LR: 0.000051\n",
      "Step 853, Loss Train: 3.3020, Loss Test: 3.1029, LR: 0.000051\n",
      "Step 854, Loss Train: 3.2260, Loss Test: 3.1029, LR: 0.000051\n",
      "Step 855, Loss Train: 3.0978, Loss Test: 3.1029, LR: 0.000051\n",
      "Step 856, Loss Train: 3.3431, Loss Test: 3.1029, LR: 0.000051\n",
      "Step 857, Loss Train: 3.3038, Loss Test: 3.1029, LR: 0.000051\n",
      "Step 858, Loss Train: 3.1904, Loss Test: 3.1029, LR: 0.000051\n",
      "Step 859, Loss Train: 3.0425, Loss Test: 3.1029, LR: 0.000051\n",
      "Step 860, Loss Train: 3.4096, Loss Test: 3.1029, LR: 0.000051\n",
      "Step 861, Loss Train: 3.2108, Loss Test: 3.1029, LR: 0.000051\n",
      "Step 862, Loss Train: 2.9529, Loss Test: 3.1029, LR: 0.000051\n",
      "Step 863, Loss Train: 3.1559, Loss Test: 3.1029, LR: 0.000051\n",
      "Step 864, Loss Train: 2.9947, Loss Test: 3.1029, LR: 0.000051\n",
      "Step 865, Loss Train: 3.5040, Loss Test: 3.1029, LR: 0.000051\n",
      "Step 866, Loss Train: 3.1192, Loss Test: 3.1029, LR: 0.000051\n",
      "Step 867, Loss Train: 3.0063, Loss Test: 3.1029, LR: 0.000051\n",
      "Step 868, Loss Train: 3.1850, Loss Test: 3.1029, LR: 0.000051\n",
      "Step 869, Loss Train: 3.2276, Loss Test: 3.1029, LR: 0.000051\n",
      "Step 870, Loss Train: 3.1393, Loss Test: 3.1029, LR: 0.000051\n",
      "Step 871, Loss Train: 3.0014, Loss Test: 3.1029, LR: 0.000051\n",
      "Step 872, Loss Train: 3.1197, Loss Test: 3.1029, LR: 0.000051\n",
      "Step 873, Loss Train: 3.1715, Loss Test: 3.1029, LR: 0.000051\n",
      "Step 874, Loss Train: 3.3183, Loss Test: 3.1029, LR: 0.000051\n",
      "Step 875, Loss Train: 3.0746, Loss Test: 3.1029, LR: 0.000051\n",
      "Step 876, Loss Train: 3.2616, Loss Test: 2.5006, LR: 0.000051\n",
      "Step 877, Loss Train: 3.0238, Loss Test: 2.5006, LR: 0.000051\n",
      "Step 878, Loss Train: 3.2854, Loss Test: 2.5006, LR: 0.000051\n",
      "Step 879, Loss Train: 3.1003, Loss Test: 2.5006, LR: 0.000051\n",
      "Step 880, Loss Train: 3.2824, Loss Test: 2.5006, LR: 0.000051\n",
      "Step 881, Loss Train: 3.2329, Loss Test: 2.5006, LR: 0.000051\n",
      "Step 882, Loss Train: 3.1299, Loss Test: 2.5006, LR: 0.000051\n",
      "Step 883, Loss Train: 3.2456, Loss Test: 2.5006, LR: 0.000051\n",
      "Step 884, Loss Train: 3.1563, Loss Test: 2.5006, LR: 0.000051\n",
      "Step 885, Loss Train: 3.2141, Loss Test: 2.5006, LR: 0.000051\n",
      "Step 886, Loss Train: 3.1655, Loss Test: 2.5006, LR: 0.000051\n",
      "Step 887, Loss Train: 3.0320, Loss Test: 2.5006, LR: 0.000051\n",
      "Step 888, Loss Train: 2.9393, Loss Test: 2.5006, LR: 0.000051\n",
      "Step 889, Loss Train: 3.0338, Loss Test: 2.5006, LR: 0.000051\n",
      "Step 890, Loss Train: 3.3042, Loss Test: 2.5006, LR: 0.000051\n",
      "Step 891, Loss Train: 3.0711, Loss Test: 2.5006, LR: 0.000051\n",
      "Step 892, Loss Train: 3.1571, Loss Test: 2.5006, LR: 0.000051\n",
      "Step 893, Loss Train: 3.0108, Loss Test: 2.5006, LR: 0.000051\n",
      "Step 894, Loss Train: 3.0935, Loss Test: 2.5006, LR: 0.000051\n",
      "Step 895, Loss Train: 3.2511, Loss Test: 2.5006, LR: 0.000051\n",
      "Step 896, Loss Train: 3.1800, Loss Test: 2.5006, LR: 0.000051\n",
      "Step 897, Loss Train: 3.3041, Loss Test: 2.5006, LR: 0.000051\n",
      "Step 898, Loss Train: 3.0099, Loss Test: 2.5006, LR: 0.000051\n",
      "Step 899, Loss Train: 3.3315, Loss Test: 2.5006, LR: 0.000051\n",
      "Step 900, Loss Train: 3.3568, Loss Test: 2.5006, LR: 0.000051\n",
      "Step 901, Loss Train: 2.8951, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 902, Loss Train: 3.1216, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 903, Loss Train: 3.2268, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 904, Loss Train: 2.9842, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 905, Loss Train: 3.2012, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 906, Loss Train: 3.2680, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 907, Loss Train: 3.0418, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 908, Loss Train: 3.1476, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 909, Loss Train: 3.1860, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 910, Loss Train: 3.0714, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 911, Loss Train: 3.1913, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 912, Loss Train: 3.0336, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 913, Loss Train: 3.1557, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 914, Loss Train: 3.2085, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 915, Loss Train: 3.2858, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 916, Loss Train: 3.0868, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 917, Loss Train: 3.1524, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 918, Loss Train: 3.0802, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 919, Loss Train: 3.0470, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 920, Loss Train: 3.1931, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 921, Loss Train: 3.0325, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 922, Loss Train: 3.0878, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 923, Loss Train: 2.9282, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 924, Loss Train: 3.2302, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 925, Loss Train: 3.0899, Loss Test: 3.2560, LR: 0.000051\n",
      "Step 926, Loss Train: 3.0756, Loss Test: 2.8450, LR: 0.000051\n",
      "Step 927, Loss Train: 3.2205, Loss Test: 2.8450, LR: 0.000051\n",
      "Step 928, Loss Train: 3.2775, Loss Test: 2.8450, LR: 0.000051\n",
      "Step 929, Loss Train: 3.2705, Loss Test: 2.8450, LR: 0.000051\n",
      "Step 930, Loss Train: 3.3294, Loss Test: 2.8450, LR: 0.000051\n",
      "Step 931, Loss Train: 3.1165, Loss Test: 2.8450, LR: 0.000051\n",
      "Step 932, Loss Train: 3.1878, Loss Test: 2.8450, LR: 0.000051\n",
      "Step 933, Loss Train: 3.1828, Loss Test: 2.8450, LR: 0.000051\n",
      "Step 934, Loss Train: 3.2403, Loss Test: 2.8450, LR: 0.000051\n",
      "Step 935, Loss Train: 3.2789, Loss Test: 2.8450, LR: 0.000051\n",
      "Step 936, Loss Train: 2.9081, Loss Test: 2.8450, LR: 0.000051\n",
      "Step 937, Loss Train: 3.0926, Loss Test: 2.8450, LR: 0.000051\n",
      "Step 938, Loss Train: 3.1342, Loss Test: 2.8450, LR: 0.000051\n",
      "Step 939, Loss Train: 3.0316, Loss Test: 2.8450, LR: 0.000051\n",
      "Step 940, Loss Train: 3.1972, Loss Test: 2.8450, LR: 0.000051\n",
      "Step 941, Loss Train: 3.1779, Loss Test: 2.8450, LR: 0.000051\n",
      "Step 942, Loss Train: 3.0538, Loss Test: 2.8450, LR: 0.000051\n",
      "Step 943, Loss Train: 3.2651, Loss Test: 2.8450, LR: 0.000051\n",
      "Step 944, Loss Train: 3.2219, Loss Test: 2.8450, LR: 0.000051\n",
      "Step 945, Loss Train: 2.9668, Loss Test: 2.8450, LR: 0.000051\n",
      "Step 946, Loss Train: 3.1037, Loss Test: 2.8450, LR: 0.000051\n",
      "Step 947, Loss Train: 3.3360, Loss Test: 2.8450, LR: 0.000051\n",
      "Step 948, Loss Train: 3.0173, Loss Test: 2.8450, LR: 0.000051\n",
      "Step 949, Loss Train: 3.1131, Loss Test: 2.8450, LR: 0.000051\n",
      "Step 950, Loss Train: 2.8559, Loss Test: 2.8450, LR: 0.000051\n",
      "Step 951, Loss Train: 2.9425, Loss Test: 2.8453, LR: 0.000051\n",
      "Step 952, Loss Train: 3.0887, Loss Test: 2.8453, LR: 0.000051\n",
      "Step 953, Loss Train: 3.1385, Loss Test: 2.8453, LR: 0.000051\n",
      "Step 954, Loss Train: 3.0857, Loss Test: 2.8453, LR: 0.000051\n",
      "Step 955, Loss Train: 3.4016, Loss Test: 2.8453, LR: 0.000051\n",
      "Step 956, Loss Train: 3.0449, Loss Test: 2.8453, LR: 0.000051\n",
      "Step 957, Loss Train: 2.9212, Loss Test: 2.8453, LR: 0.000051\n",
      "Step 958, Loss Train: 3.1661, Loss Test: 2.8453, LR: 0.000051\n",
      "Step 959, Loss Train: 3.1589, Loss Test: 2.8453, LR: 0.000051\n",
      "Step 960, Loss Train: 3.1380, Loss Test: 2.8453, LR: 0.000051\n",
      "Step 961, Loss Train: 3.3428, Loss Test: 2.8453, LR: 0.000051\n",
      "Step 962, Loss Train: 2.9711, Loss Test: 2.8453, LR: 0.000051\n",
      "Step 963, Loss Train: 3.0148, Loss Test: 2.8453, LR: 0.000051\n",
      "Step 964, Loss Train: 3.0663, Loss Test: 2.8453, LR: 0.000051\n",
      "Step 965, Loss Train: 3.0854, Loss Test: 2.8453, LR: 0.000051\n",
      "Step 966, Loss Train: 3.1157, Loss Test: 2.8453, LR: 0.000051\n",
      "Step 967, Loss Train: 3.1350, Loss Test: 2.8453, LR: 0.000051\n",
      "Step 968, Loss Train: 3.0449, Loss Test: 2.8453, LR: 0.000051\n",
      "Step 969, Loss Train: 3.3218, Loss Test: 2.8453, LR: 0.000051\n",
      "Step 970, Loss Train: 3.0457, Loss Test: 2.8453, LR: 0.000051\n",
      "Step 971, Loss Train: 3.1567, Loss Test: 2.8453, LR: 0.000051\n",
      "Step 972, Loss Train: 3.0983, Loss Test: 2.8453, LR: 0.000051\n",
      "Step 973, Loss Train: 3.2180, Loss Test: 2.8453, LR: 0.000051\n",
      "Step 974, Loss Train: 3.1044, Loss Test: 2.8453, LR: 0.000051\n",
      "Step 975, Loss Train: 3.0511, Loss Test: 2.8453, LR: 0.000051\n",
      "Step 976, Loss Train: 3.1219, Loss Test: 2.9823, LR: 0.000051\n",
      "Step 977, Loss Train: 3.1630, Loss Test: 2.9823, LR: 0.000051\n",
      "Step 978, Loss Train: 2.9508, Loss Test: 2.9823, LR: 0.000051\n",
      "Step 979, Loss Train: 3.0118, Loss Test: 2.9823, LR: 0.000051\n",
      "Step 980, Loss Train: 3.1539, Loss Test: 2.9823, LR: 0.000051\n",
      "Step 981, Loss Train: 3.1034, Loss Test: 2.9823, LR: 0.000051\n",
      "Step 982, Loss Train: 3.0879, Loss Test: 2.9823, LR: 0.000051\n",
      "Step 983, Loss Train: 3.0388, Loss Test: 2.9823, LR: 0.000051\n",
      "Step 984, Loss Train: 3.3287, Loss Test: 2.9823, LR: 0.000051\n",
      "Step 985, Loss Train: 3.1637, Loss Test: 2.9823, LR: 0.000051\n",
      "Step 986, Loss Train: 3.1944, Loss Test: 2.9823, LR: 0.000051\n",
      "Step 987, Loss Train: 3.2862, Loss Test: 2.9823, LR: 0.000051\n",
      "Step 988, Loss Train: 3.2569, Loss Test: 2.9823, LR: 0.000051\n",
      "Step 989, Loss Train: 3.2259, Loss Test: 2.9823, LR: 0.000051\n",
      "Step 990, Loss Train: 3.0623, Loss Test: 2.9823, LR: 0.000051\n",
      "Step 991, Loss Train: 2.8777, Loss Test: 2.9823, LR: 0.000051\n",
      "Step 992, Loss Train: 3.2808, Loss Test: 2.9823, LR: 0.000051\n",
      "Step 993, Loss Train: 3.2695, Loss Test: 2.9823, LR: 0.000051\n",
      "Step 994, Loss Train: 3.1781, Loss Test: 2.9823, LR: 0.000051\n",
      "Step 995, Loss Train: 3.0063, Loss Test: 2.9823, LR: 0.000051\n",
      "Step 996, Loss Train: 3.1946, Loss Test: 2.9823, LR: 0.000051\n",
      "Step 997, Loss Train: 3.3818, Loss Test: 2.9823, LR: 0.000051\n",
      "Step 998, Loss Train: 2.9027, Loss Test: 2.9823, LR: 0.000051\n",
      "Step 999, Loss Train: 3.2161, Loss Test: 2.9823, LR: 0.000051\n",
      "Step 1000, Loss Train: 3.0196, Loss Test: 2.9823, LR: 0.000051\n",
      "Step 1001, Loss Train: 3.1034, Loss Test: 2.8719, LR: 0.000051\n",
      "Step 1002, Loss Train: 3.2579, Loss Test: 2.8719, LR: 0.000051\n",
      "Step 1003, Loss Train: 2.9846, Loss Test: 2.8719, LR: 0.000051\n",
      "Step 1004, Loss Train: 2.9320, Loss Test: 2.8719, LR: 0.000051\n",
      "Step 1005, Loss Train: 3.0528, Loss Test: 2.8719, LR: 0.000051\n",
      "Step 1006, Loss Train: 3.2955, Loss Test: 2.8719, LR: 0.000051\n",
      "Step 1007, Loss Train: 3.1241, Loss Test: 2.8719, LR: 0.000051\n",
      "Step 1008, Loss Train: 3.0046, Loss Test: 2.8719, LR: 0.000051\n",
      "Step 1009, Loss Train: 3.0601, Loss Test: 2.8719, LR: 0.000051\n",
      "Step 1010, Loss Train: 2.9225, Loss Test: 2.8719, LR: 0.000051\n",
      "Step 1011, Loss Train: 3.1935, Loss Test: 2.8719, LR: 0.000051\n",
      "Step 1012, Loss Train: 3.1183, Loss Test: 2.8719, LR: 0.000051\n",
      "Step 1013, Loss Train: 3.2047, Loss Test: 2.8719, LR: 0.000051\n",
      "Step 1014, Loss Train: 3.1543, Loss Test: 2.8719, LR: 0.000051\n",
      "Step 1015, Loss Train: 3.2991, Loss Test: 2.8719, LR: 0.000051\n",
      "Step 1016, Loss Train: 2.8951, Loss Test: 2.8719, LR: 0.000051\n",
      "Step 1017, Loss Train: 2.9348, Loss Test: 2.8719, LR: 0.000051\n",
      "Step 1018, Loss Train: 3.1381, Loss Test: 2.8719, LR: 0.000051\n",
      "Step 1019, Loss Train: 3.4288, Loss Test: 2.8719, LR: 0.000051\n",
      "Step 1020, Loss Train: 3.2713, Loss Test: 2.8719, LR: 0.000051\n",
      "Step 1021, Loss Train: 2.8625, Loss Test: 2.8719, LR: 0.000051\n",
      "Step 1022, Loss Train: 2.8811, Loss Test: 2.8719, LR: 0.000051\n",
      "Step 1023, Loss Train: 3.3330, Loss Test: 2.8719, LR: 0.000051\n",
      "Step 1024, Loss Train: 3.2052, Loss Test: 2.8719, LR: 0.000051\n",
      "Step 1025, Loss Train: 3.1577, Loss Test: 2.8719, LR: 0.000051\n",
      "Step 1026, Loss Train: 3.1555, Loss Test: 3.2196, LR: 0.000051\n",
      "Step 1027, Loss Train: 3.4069, Loss Test: 3.2196, LR: 0.000051\n",
      "Step 1028, Loss Train: 3.2394, Loss Test: 3.2196, LR: 0.000051\n",
      "Step 1029, Loss Train: 3.1419, Loss Test: 3.2196, LR: 0.000051\n",
      "Step 1030, Loss Train: 3.0194, Loss Test: 3.2196, LR: 0.000051\n",
      "Step 1031, Loss Train: 3.2423, Loss Test: 3.2196, LR: 0.000051\n",
      "Step 1032, Loss Train: 3.4834, Loss Test: 3.2196, LR: 0.000051\n",
      "Step 1033, Loss Train: 3.1767, Loss Test: 3.2196, LR: 0.000051\n",
      "Step 1034, Loss Train: 3.2602, Loss Test: 3.2196, LR: 0.000051\n",
      "Step 1035, Loss Train: 3.1594, Loss Test: 3.2196, LR: 0.000051\n",
      "Step 1036, Loss Train: 3.0134, Loss Test: 3.2196, LR: 0.000051\n",
      "Step 1037, Loss Train: 2.9800, Loss Test: 3.2196, LR: 0.000051\n",
      "Step 1038, Loss Train: 3.1875, Loss Test: 3.2196, LR: 0.000051\n",
      "Step 1039, Loss Train: 3.3246, Loss Test: 3.2196, LR: 0.000051\n",
      "Step 1040, Loss Train: 3.0939, Loss Test: 3.2196, LR: 0.000051\n",
      "Step 1041, Loss Train: 2.8371, Loss Test: 3.2196, LR: 0.000051\n",
      "Step 1042, Loss Train: 3.2014, Loss Test: 3.2196, LR: 0.000051\n",
      "Step 1043, Loss Train: 3.2295, Loss Test: 3.2196, LR: 0.000051\n",
      "Step 1044, Loss Train: 3.2167, Loss Test: 3.2196, LR: 0.000051\n",
      "Step 1045, Loss Train: 2.9696, Loss Test: 3.2196, LR: 0.000051\n",
      "Step 1046, Loss Train: 2.9858, Loss Test: 3.2196, LR: 0.000051\n",
      "Step 1047, Loss Train: 2.9924, Loss Test: 3.2196, LR: 0.000051\n",
      "Step 1048, Loss Train: 3.0880, Loss Test: 3.2196, LR: 0.000051\n",
      "Step 1049, Loss Train: 3.0379, Loss Test: 3.2196, LR: 0.000051\n",
      "Step 1050, Loss Train: 3.1645, Loss Test: 3.2196, LR: 0.000051\n",
      "Step 1051, Loss Train: 3.1176, Loss Test: 3.6204, LR: 0.000051\n",
      "Step 1052, Loss Train: 3.1777, Loss Test: 3.6204, LR: 0.000051\n",
      "Step 1053, Loss Train: 3.0678, Loss Test: 3.6204, LR: 0.000051\n",
      "Step 1054, Loss Train: 3.2691, Loss Test: 3.6204, LR: 0.000051\n",
      "Step 1055, Loss Train: 3.1612, Loss Test: 3.6204, LR: 0.000051\n",
      "Step 1056, Loss Train: 3.2501, Loss Test: 3.6204, LR: 0.000051\n",
      "Step 1057, Loss Train: 3.0921, Loss Test: 3.6204, LR: 0.000051\n",
      "Step 1058, Loss Train: 3.0523, Loss Test: 3.6204, LR: 0.000051\n",
      "Step 1059, Loss Train: 3.1407, Loss Test: 3.6204, LR: 0.000051\n",
      "Step 1060, Loss Train: 3.4395, Loss Test: 3.6204, LR: 0.000051\n",
      "Step 1061, Loss Train: 2.9172, Loss Test: 3.6204, LR: 0.000051\n",
      "Step 1062, Loss Train: 3.1960, Loss Test: 3.6204, LR: 0.000051\n",
      "Step 1063, Loss Train: 3.2928, Loss Test: 3.6204, LR: 0.000051\n",
      "Step 1064, Loss Train: 2.9431, Loss Test: 3.6204, LR: 0.000051\n",
      "Step 1065, Loss Train: 3.1530, Loss Test: 3.6204, LR: 0.000051\n",
      "Step 1066, Loss Train: 3.0384, Loss Test: 3.6204, LR: 0.000051\n",
      "Step 1067, Loss Train: 3.1004, Loss Test: 3.6204, LR: 0.000051\n",
      "Step 1068, Loss Train: 3.2166, Loss Test: 3.6204, LR: 0.000051\n",
      "Step 1069, Loss Train: 3.4596, Loss Test: 3.6204, LR: 0.000051\n",
      "Step 1070, Loss Train: 3.3242, Loss Test: 3.6204, LR: 0.000051\n",
      "Step 1071, Loss Train: 3.1736, Loss Test: 3.6204, LR: 0.000051\n",
      "Step 1072, Loss Train: 3.2694, Loss Test: 3.6204, LR: 0.000051\n",
      "Step 1073, Loss Train: 3.2560, Loss Test: 3.6204, LR: 0.000051\n",
      "Step 1074, Loss Train: 3.2719, Loss Test: 3.6204, LR: 0.000051\n",
      "Step 1075, Loss Train: 3.1701, Loss Test: 3.6204, LR: 0.000051\n",
      "Step 1076, Loss Train: 2.9317, Loss Test: 2.6257, LR: 0.000051\n",
      "Step 1077, Loss Train: 3.0441, Loss Test: 2.6257, LR: 0.000051\n",
      "Step 1078, Loss Train: 3.3490, Loss Test: 2.6257, LR: 0.000051\n",
      "Step 1079, Loss Train: 3.0693, Loss Test: 2.6257, LR: 0.000051\n",
      "Step 1080, Loss Train: 3.1686, Loss Test: 2.6257, LR: 0.000051\n",
      "Step 1081, Loss Train: 3.4131, Loss Test: 2.6257, LR: 0.000051\n",
      "Step 1082, Loss Train: 3.0818, Loss Test: 2.6257, LR: 0.000051\n",
      "Step 1083, Loss Train: 3.1997, Loss Test: 2.6257, LR: 0.000051\n",
      "Step 1084, Loss Train: 2.9344, Loss Test: 2.6257, LR: 0.000051\n",
      "Step 1085, Loss Train: 3.1685, Loss Test: 2.6257, LR: 0.000051\n",
      "Step 1086, Loss Train: 3.1622, Loss Test: 2.6257, LR: 0.000051\n",
      "Step 1087, Loss Train: 2.9920, Loss Test: 2.6257, LR: 0.000051\n",
      "Step 1088, Loss Train: 3.1493, Loss Test: 2.6257, LR: 0.000051\n",
      "Step 1089, Loss Train: 3.1214, Loss Test: 2.6257, LR: 0.000051\n",
      "Step 1090, Loss Train: 3.4658, Loss Test: 2.6257, LR: 0.000051\n",
      "Step 1091, Loss Train: 2.9557, Loss Test: 2.6257, LR: 0.000051\n",
      "Step 1092, Loss Train: 3.0046, Loss Test: 2.6257, LR: 0.000051\n",
      "Step 1093, Loss Train: 3.1478, Loss Test: 2.6257, LR: 0.000051\n",
      "Step 1094, Loss Train: 3.1011, Loss Test: 2.6257, LR: 0.000051\n",
      "Step 1095, Loss Train: 2.8984, Loss Test: 2.6257, LR: 0.000051\n",
      "Step 1096, Loss Train: 3.1262, Loss Test: 2.6257, LR: 0.000051\n",
      "Step 1097, Loss Train: 2.9951, Loss Test: 2.6257, LR: 0.000051\n",
      "Step 1098, Loss Train: 3.2200, Loss Test: 2.6257, LR: 0.000051\n",
      "Step 1099, Loss Train: 3.0644, Loss Test: 2.6257, LR: 0.000051\n",
      "Step 1100, Loss Train: 3.2688, Loss Test: 2.6257, LR: 0.000051\n",
      "Step 1101, Loss Train: 3.1540, Loss Test: 2.9242, LR: 0.000051\n",
      "Step 1102, Loss Train: 3.1433, Loss Test: 2.9242, LR: 0.000051\n",
      "Step 1103, Loss Train: 3.1876, Loss Test: 2.9242, LR: 0.000051\n",
      "Step 1104, Loss Train: 2.9978, Loss Test: 2.9242, LR: 0.000051\n",
      "Step 1105, Loss Train: 3.3234, Loss Test: 2.9242, LR: 0.000051\n",
      "Step 1106, Loss Train: 3.0243, Loss Test: 2.9242, LR: 0.000051\n",
      "Step 1107, Loss Train: 2.9628, Loss Test: 2.9242, LR: 0.000051\n",
      "Step 1108, Loss Train: 2.9985, Loss Test: 2.9242, LR: 0.000051\n",
      "Step 1109, Loss Train: 3.2560, Loss Test: 2.9242, LR: 0.000051\n",
      "Step 1110, Loss Train: 3.3076, Loss Test: 2.9242, LR: 0.000051\n",
      "Step 1111, Loss Train: 3.2915, Loss Test: 2.9242, LR: 0.000051\n",
      "Step 1112, Loss Train: 3.0471, Loss Test: 2.9242, LR: 0.000051\n",
      "Step 1113, Loss Train: 2.9653, Loss Test: 2.9242, LR: 0.000051\n",
      "Step 1114, Loss Train: 3.1648, Loss Test: 2.9242, LR: 0.000051\n",
      "Step 1115, Loss Train: 3.1284, Loss Test: 2.9242, LR: 0.000051\n",
      "Step 1116, Loss Train: 3.0978, Loss Test: 2.9242, LR: 0.000051\n",
      "Step 1117, Loss Train: 3.2793, Loss Test: 2.9242, LR: 0.000051\n",
      "Step 1118, Loss Train: 3.1068, Loss Test: 2.9242, LR: 0.000051\n",
      "Step 1119, Loss Train: 3.2730, Loss Test: 2.9242, LR: 0.000051\n",
      "Step 1120, Loss Train: 3.0510, Loss Test: 2.9242, LR: 0.000051\n",
      "Step 1121, Loss Train: 3.1106, Loss Test: 2.9242, LR: 0.000051\n",
      "Step 1122, Loss Train: 3.1154, Loss Test: 2.9242, LR: 0.000051\n",
      "Step 1123, Loss Train: 3.3155, Loss Test: 2.9242, LR: 0.000051\n",
      "Step 1124, Loss Train: 3.2465, Loss Test: 2.9242, LR: 0.000051\n",
      "Step 1125, Loss Train: 2.9881, Loss Test: 2.9242, LR: 0.000051\n",
      "Step 1126, Loss Train: 3.2292, Loss Test: 3.0968, LR: 0.000051\n",
      "Step 1127, Loss Train: 3.1414, Loss Test: 3.0968, LR: 0.000051\n",
      "Step 1128, Loss Train: 3.1854, Loss Test: 3.0968, LR: 0.000051\n",
      "Step 1129, Loss Train: 3.0969, Loss Test: 3.0968, LR: 0.000051\n",
      "Step 1130, Loss Train: 3.1225, Loss Test: 3.0968, LR: 0.000051\n",
      "Step 1131, Loss Train: 3.1851, Loss Test: 3.0968, LR: 0.000051\n",
      "Step 1132, Loss Train: 3.1010, Loss Test: 3.0968, LR: 0.000051\n",
      "Step 1133, Loss Train: 3.1897, Loss Test: 3.0968, LR: 0.000051\n",
      "Step 1134, Loss Train: 3.3336, Loss Test: 3.0968, LR: 0.000051\n",
      "Step 1135, Loss Train: 3.1451, Loss Test: 3.0968, LR: 0.000051\n",
      "Step 1136, Loss Train: 3.3264, Loss Test: 3.0968, LR: 0.000051\n",
      "Step 1137, Loss Train: 3.0310, Loss Test: 3.0968, LR: 0.000051\n",
      "Step 1138, Loss Train: 3.1497, Loss Test: 3.0968, LR: 0.000051\n",
      "Step 1139, Loss Train: 3.1285, Loss Test: 3.0968, LR: 0.000051\n",
      "Step 1140, Loss Train: 3.3203, Loss Test: 3.0968, LR: 0.000051\n",
      "Step 1141, Loss Train: 3.3582, Loss Test: 3.0968, LR: 0.000051\n",
      "Step 1142, Loss Train: 3.1748, Loss Test: 3.0968, LR: 0.000051\n",
      "Step 1143, Loss Train: 2.8514, Loss Test: 3.0968, LR: 0.000051\n",
      "Step 1144, Loss Train: 3.1862, Loss Test: 3.0968, LR: 0.000051\n",
      "Step 1145, Loss Train: 3.0212, Loss Test: 3.0968, LR: 0.000051\n",
      "Step 1146, Loss Train: 3.2189, Loss Test: 3.0968, LR: 0.000051\n",
      "Step 1147, Loss Train: 2.9576, Loss Test: 3.0968, LR: 0.000051\n",
      "Step 1148, Loss Train: 3.3701, Loss Test: 3.0968, LR: 0.000051\n",
      "Step 1149, Loss Train: 3.3387, Loss Test: 3.0968, LR: 0.000051\n",
      "Step 1150, Loss Train: 3.0479, Loss Test: 3.0968, LR: 0.000051\n",
      "Step 1151, Loss Train: 3.4800, Loss Test: 3.1545, LR: 0.000051\n",
      "Step 1152, Loss Train: 3.2900, Loss Test: 3.1545, LR: 0.000051\n",
      "Step 1153, Loss Train: 3.1341, Loss Test: 3.1545, LR: 0.000051\n",
      "Step 1154, Loss Train: 3.0052, Loss Test: 3.1545, LR: 0.000051\n",
      "Step 1155, Loss Train: 2.8761, Loss Test: 3.1545, LR: 0.000051\n",
      "Step 1156, Loss Train: 3.1820, Loss Test: 3.1545, LR: 0.000051\n",
      "Step 1157, Loss Train: 3.1519, Loss Test: 3.1545, LR: 0.000051\n",
      "Step 1158, Loss Train: 3.3620, Loss Test: 3.1545, LR: 0.000051\n",
      "Step 1159, Loss Train: 2.8783, Loss Test: 3.1545, LR: 0.000051\n",
      "Step 1160, Loss Train: 3.1355, Loss Test: 3.1545, LR: 0.000051\n",
      "Step 1161, Loss Train: 2.8613, Loss Test: 3.1545, LR: 0.000051\n",
      "Step 1162, Loss Train: 3.3572, Loss Test: 3.1545, LR: 0.000051\n",
      "Step 1163, Loss Train: 2.7508, Loss Test: 3.1545, LR: 0.000051\n",
      "Step 1164, Loss Train: 3.1639, Loss Test: 3.1545, LR: 0.000051\n",
      "Step 1165, Loss Train: 3.2008, Loss Test: 3.1545, LR: 0.000051\n",
      "Step 1166, Loss Train: 3.0557, Loss Test: 3.1545, LR: 0.000051\n",
      "Step 1167, Loss Train: 3.0481, Loss Test: 3.1545, LR: 0.000051\n",
      "Step 1168, Loss Train: 3.0896, Loss Test: 3.1545, LR: 0.000051\n",
      "Step 1169, Loss Train: 2.9707, Loss Test: 3.1545, LR: 0.000051\n",
      "Step 1170, Loss Train: 3.0249, Loss Test: 3.1545, LR: 0.000051\n",
      "Step 1171, Loss Train: 3.1161, Loss Test: 3.1545, LR: 0.000051\n",
      "Step 1172, Loss Train: 3.1075, Loss Test: 3.1545, LR: 0.000051\n",
      "Step 1173, Loss Train: 3.1390, Loss Test: 3.1545, LR: 0.000051\n",
      "Step 1174, Loss Train: 3.0525, Loss Test: 3.1545, LR: 0.000051\n",
      "Step 1175, Loss Train: 3.0710, Loss Test: 3.1545, LR: 0.000051\n",
      "Step 1176, Loss Train: 3.0808, Loss Test: 3.6206, LR: 0.000051\n",
      "Step 1177, Loss Train: 3.2890, Loss Test: 3.6206, LR: 0.000051\n",
      "Step 1178, Loss Train: 3.1548, Loss Test: 3.6206, LR: 0.000051\n",
      "Step 1179, Loss Train: 3.2781, Loss Test: 3.6206, LR: 0.000051\n",
      "Step 1180, Loss Train: 3.1557, Loss Test: 3.6206, LR: 0.000051\n",
      "Step 1181, Loss Train: 3.3271, Loss Test: 3.6206, LR: 0.000051\n",
      "Step 1182, Loss Train: 3.2802, Loss Test: 3.6206, LR: 0.000051\n",
      "Step 1183, Loss Train: 3.3188, Loss Test: 3.6206, LR: 0.000051\n",
      "Step 1184, Loss Train: 3.0913, Loss Test: 3.6206, LR: 0.000051\n",
      "Step 1185, Loss Train: 3.1525, Loss Test: 3.6206, LR: 0.000051\n",
      "Step 1186, Loss Train: 3.1037, Loss Test: 3.6206, LR: 0.000051\n",
      "Step 1187, Loss Train: 3.2170, Loss Test: 3.6206, LR: 0.000051\n",
      "Step 1188, Loss Train: 3.0589, Loss Test: 3.6206, LR: 0.000051\n",
      "Step 1189, Loss Train: 3.0271, Loss Test: 3.6206, LR: 0.000051\n",
      "Step 1190, Loss Train: 3.2820, Loss Test: 3.6206, LR: 0.000051\n",
      "Step 1191, Loss Train: 3.0889, Loss Test: 3.6206, LR: 0.000051\n",
      "Step 1192, Loss Train: 3.1318, Loss Test: 3.6206, LR: 0.000051\n",
      "Step 1193, Loss Train: 3.0192, Loss Test: 3.6206, LR: 0.000051\n",
      "Step 1194, Loss Train: 2.9445, Loss Test: 3.6206, LR: 0.000051\n",
      "Step 1195, Loss Train: 3.0208, Loss Test: 3.6206, LR: 0.000051\n",
      "Step 1196, Loss Train: 3.1458, Loss Test: 3.6206, LR: 0.000051\n",
      "Step 1197, Loss Train: 2.9848, Loss Test: 3.6206, LR: 0.000051\n",
      "Step 1198, Loss Train: 3.0969, Loss Test: 3.6206, LR: 0.000051\n",
      "Step 1199, Loss Train: 3.1597, Loss Test: 3.6206, LR: 0.000051\n",
      "Step 1200, Loss Train: 3.2993, Loss Test: 3.6206, LR: 0.000051\n",
      "Step 1201, Loss Train: 3.3623, Loss Test: 3.2685, LR: 0.000051\n",
      "Step 1202, Loss Train: 3.0817, Loss Test: 3.2685, LR: 0.000051\n",
      "Step 1203, Loss Train: 3.4396, Loss Test: 3.2685, LR: 0.000051\n",
      "Step 1204, Loss Train: 3.0527, Loss Test: 3.2685, LR: 0.000051\n",
      "Step 1205, Loss Train: 3.2241, Loss Test: 3.2685, LR: 0.000051\n",
      "Step 1206, Loss Train: 2.9472, Loss Test: 3.2685, LR: 0.000051\n",
      "Step 1207, Loss Train: 2.9555, Loss Test: 3.2685, LR: 0.000051\n",
      "Step 1208, Loss Train: 3.3155, Loss Test: 3.2685, LR: 0.000051\n",
      "Step 1209, Loss Train: 3.0826, Loss Test: 3.2685, LR: 0.000051\n",
      "Step 1210, Loss Train: 3.2369, Loss Test: 3.2685, LR: 0.000051\n",
      "Step 1211, Loss Train: 3.0624, Loss Test: 3.2685, LR: 0.000051\n",
      "Step 1212, Loss Train: 3.0386, Loss Test: 3.2685, LR: 0.000051\n",
      "Step 1213, Loss Train: 3.1373, Loss Test: 3.2685, LR: 0.000051\n",
      "Step 1214, Loss Train: 3.3400, Loss Test: 3.2685, LR: 0.000051\n",
      "Step 1215, Loss Train: 3.1261, Loss Test: 3.2685, LR: 0.000051\n",
      "Step 1216, Loss Train: 3.1999, Loss Test: 3.2685, LR: 0.000051\n",
      "Step 1217, Loss Train: 3.3168, Loss Test: 3.2685, LR: 0.000051\n",
      "Step 1218, Loss Train: 3.1534, Loss Test: 3.2685, LR: 0.000051\n",
      "Step 1219, Loss Train: 3.0862, Loss Test: 3.2685, LR: 0.000051\n",
      "Step 1220, Loss Train: 3.0941, Loss Test: 3.2685, LR: 0.000051\n",
      "Step 1221, Loss Train: 3.1991, Loss Test: 3.2685, LR: 0.000051\n",
      "Step 1222, Loss Train: 3.1568, Loss Test: 3.2685, LR: 0.000051\n",
      "Step 1223, Loss Train: 3.2950, Loss Test: 3.2685, LR: 0.000051\n",
      "Step 1224, Loss Train: 3.2432, Loss Test: 3.2685, LR: 0.000051\n",
      "Step 1225, Loss Train: 3.0443, Loss Test: 3.2685, LR: 0.000051\n",
      "Step 1226, Loss Train: 3.2385, Loss Test: 2.8991, LR: 0.000051\n",
      "Step 1227, Loss Train: 3.0591, Loss Test: 2.8991, LR: 0.000051\n",
      "Step 1228, Loss Train: 3.1009, Loss Test: 2.8991, LR: 0.000051\n",
      "Step 1229, Loss Train: 2.9787, Loss Test: 2.8991, LR: 0.000051\n",
      "Step 1230, Loss Train: 2.9775, Loss Test: 2.8991, LR: 0.000051\n",
      "Step 1231, Loss Train: 3.0925, Loss Test: 2.8991, LR: 0.000051\n",
      "Step 1232, Loss Train: 3.0583, Loss Test: 2.8991, LR: 0.000051\n",
      "Step 1233, Loss Train: 3.1342, Loss Test: 2.8991, LR: 0.000051\n",
      "Step 1234, Loss Train: 3.2505, Loss Test: 2.8991, LR: 0.000051\n",
      "Step 1235, Loss Train: 3.1510, Loss Test: 2.8991, LR: 0.000051\n",
      "Step 1236, Loss Train: 3.3272, Loss Test: 2.8991, LR: 0.000051\n",
      "Step 1237, Loss Train: 3.1269, Loss Test: 2.8991, LR: 0.000051\n",
      "Step 1238, Loss Train: 3.2240, Loss Test: 2.8991, LR: 0.000051\n",
      "Step 1239, Loss Train: 3.1010, Loss Test: 2.8991, LR: 0.000051\n",
      "Step 1240, Loss Train: 3.2439, Loss Test: 2.8991, LR: 0.000051\n",
      "Step 1241, Loss Train: 3.2260, Loss Test: 2.8991, LR: 0.000051\n",
      "Step 1242, Loss Train: 3.1216, Loss Test: 2.8991, LR: 0.000051\n",
      "Step 1243, Loss Train: 3.2204, Loss Test: 2.8991, LR: 0.000051\n",
      "Step 1244, Loss Train: 3.0385, Loss Test: 2.8991, LR: 0.000051\n",
      "Step 1245, Loss Train: 3.3507, Loss Test: 2.8991, LR: 0.000051\n",
      "Step 1246, Loss Train: 3.0955, Loss Test: 2.8991, LR: 0.000051\n",
      "Step 1247, Loss Train: 3.2817, Loss Test: 2.8991, LR: 0.000051\n",
      "Step 1248, Loss Train: 3.0010, Loss Test: 2.8991, LR: 0.000051\n",
      "Step 1249, Loss Train: 3.2934, Loss Test: 2.8991, LR: 0.000051\n",
      "Step 1250, Loss Train: 3.0559, Loss Test: 2.8991, LR: 0.000051\n",
      "Step 1251, Loss Train: 3.2764, Loss Test: 3.1474, LR: 0.000051\n",
      "Step 1252, Loss Train: 3.3493, Loss Test: 3.1474, LR: 0.000051\n",
      "Step 1253, Loss Train: 3.3303, Loss Test: 3.1474, LR: 0.000051\n",
      "Step 1254, Loss Train: 3.3233, Loss Test: 3.1474, LR: 0.000051\n",
      "Step 1255, Loss Train: 3.1122, Loss Test: 3.1474, LR: 0.000051\n",
      "Step 1256, Loss Train: 3.2957, Loss Test: 3.1474, LR: 0.000051\n",
      "Step 1257, Loss Train: 2.9302, Loss Test: 3.1474, LR: 0.000051\n",
      "Step 1258, Loss Train: 3.1431, Loss Test: 3.1474, LR: 0.000051\n",
      "Step 1259, Loss Train: 3.2391, Loss Test: 3.1474, LR: 0.000051\n",
      "Step 1260, Loss Train: 3.0590, Loss Test: 3.1474, LR: 0.000051\n",
      "Step 1261, Loss Train: 3.3334, Loss Test: 3.1474, LR: 0.000051\n",
      "Step 1262, Loss Train: 3.4046, Loss Test: 3.1474, LR: 0.000051\n",
      "Step 1263, Loss Train: 3.3262, Loss Test: 3.1474, LR: 0.000051\n",
      "Step 1264, Loss Train: 3.1469, Loss Test: 3.1474, LR: 0.000051\n",
      "Step 1265, Loss Train: 3.0565, Loss Test: 3.1474, LR: 0.000051\n",
      "Step 1266, Loss Train: 2.9947, Loss Test: 3.1474, LR: 0.000051\n",
      "Step 1267, Loss Train: 3.2439, Loss Test: 3.1474, LR: 0.000051\n",
      "Step 1268, Loss Train: 3.2457, Loss Test: 3.1474, LR: 0.000051\n",
      "Step 1269, Loss Train: 2.9942, Loss Test: 3.1474, LR: 0.000051\n",
      "Step 1270, Loss Train: 3.1105, Loss Test: 3.1474, LR: 0.000051\n",
      "Step 1271, Loss Train: 3.0429, Loss Test: 3.1474, LR: 0.000051\n",
      "Step 1272, Loss Train: 3.2294, Loss Test: 3.1474, LR: 0.000051\n",
      "Step 1273, Loss Train: 3.1222, Loss Test: 3.1474, LR: 0.000051\n",
      "Step 1274, Loss Train: 3.4758, Loss Test: 3.1474, LR: 0.000051\n",
      "Step 1275, Loss Train: 3.3502, Loss Test: 3.1474, LR: 0.000051\n",
      "Step 1276, Loss Train: 3.2369, Loss Test: 2.7867, LR: 0.000051\n",
      "Step 1277, Loss Train: 3.4484, Loss Test: 2.7867, LR: 0.000051\n",
      "Step 1278, Loss Train: 3.3116, Loss Test: 2.7867, LR: 0.000051\n",
      "Step 1279, Loss Train: 2.9451, Loss Test: 2.7867, LR: 0.000051\n",
      "Step 1280, Loss Train: 3.2417, Loss Test: 2.7867, LR: 0.000051\n",
      "Step 1281, Loss Train: 3.2569, Loss Test: 2.7867, LR: 0.000051\n",
      "Step 1282, Loss Train: 3.0518, Loss Test: 2.7867, LR: 0.000051\n",
      "Step 1283, Loss Train: 3.0493, Loss Test: 2.7867, LR: 0.000051\n",
      "Step 1284, Loss Train: 3.1430, Loss Test: 2.7867, LR: 0.000051\n",
      "Step 1285, Loss Train: 3.2739, Loss Test: 2.7867, LR: 0.000051\n",
      "Step 1286, Loss Train: 3.1969, Loss Test: 2.7867, LR: 0.000051\n",
      "Step 1287, Loss Train: 2.8151, Loss Test: 2.7867, LR: 0.000051\n",
      "Step 1288, Loss Train: 3.0587, Loss Test: 2.7867, LR: 0.000051\n",
      "Step 1289, Loss Train: 2.8273, Loss Test: 2.7867, LR: 0.000051\n",
      "Step 1290, Loss Train: 3.2652, Loss Test: 2.7867, LR: 0.000051\n",
      "Step 1291, Loss Train: 3.0721, Loss Test: 2.7867, LR: 0.000051\n",
      "Step 1292, Loss Train: 2.9371, Loss Test: 2.7867, LR: 0.000051\n",
      "Step 1293, Loss Train: 2.9811, Loss Test: 2.7867, LR: 0.000051\n",
      "Step 1294, Loss Train: 3.1735, Loss Test: 2.7867, LR: 0.000051\n",
      "Step 1295, Loss Train: 2.9495, Loss Test: 2.7867, LR: 0.000051\n",
      "Step 1296, Loss Train: 3.1826, Loss Test: 2.7867, LR: 0.000051\n",
      "Step 1297, Loss Train: 2.9709, Loss Test: 2.7867, LR: 0.000051\n",
      "Step 1298, Loss Train: 3.0607, Loss Test: 2.7867, LR: 0.000051\n",
      "Step 1299, Loss Train: 2.9961, Loss Test: 2.7867, LR: 0.000051\n",
      "Step 1300, Loss Train: 3.1775, Loss Test: 2.7867, LR: 0.000051\n",
      "Step 1301, Loss Train: 3.2311, Loss Test: 3.5987, LR: 0.000051\n",
      "Step 1302, Loss Train: 3.1017, Loss Test: 3.5987, LR: 0.000051\n",
      "Step 1303, Loss Train: 3.2327, Loss Test: 3.5987, LR: 0.000051\n",
      "Step 1304, Loss Train: 3.0339, Loss Test: 3.5987, LR: 0.000051\n",
      "Step 1305, Loss Train: 3.2833, Loss Test: 3.5987, LR: 0.000051\n",
      "Step 1306, Loss Train: 3.0742, Loss Test: 3.5987, LR: 0.000051\n",
      "Step 1307, Loss Train: 3.0286, Loss Test: 3.5987, LR: 0.000051\n",
      "Step 1308, Loss Train: 3.2451, Loss Test: 3.5987, LR: 0.000051\n",
      "Step 1309, Loss Train: 3.1647, Loss Test: 3.5987, LR: 0.000051\n",
      "Step 1310, Loss Train: 3.1082, Loss Test: 3.5987, LR: 0.000051\n",
      "Step 1311, Loss Train: 3.1537, Loss Test: 3.5987, LR: 0.000051\n",
      "Step 1312, Loss Train: 3.1737, Loss Test: 3.5987, LR: 0.000051\n",
      "Step 1313, Loss Train: 3.0716, Loss Test: 3.5987, LR: 0.000051\n",
      "Step 1314, Loss Train: 3.0011, Loss Test: 3.5987, LR: 0.000051\n",
      "Step 1315, Loss Train: 3.2731, Loss Test: 3.5987, LR: 0.000051\n",
      "Step 1316, Loss Train: 3.2792, Loss Test: 3.5987, LR: 0.000051\n",
      "Step 1317, Loss Train: 3.2508, Loss Test: 3.5987, LR: 0.000051\n",
      "Step 1318, Loss Train: 3.0351, Loss Test: 3.5987, LR: 0.000051\n",
      "Step 1319, Loss Train: 3.0788, Loss Test: 3.5987, LR: 0.000051\n",
      "Step 1320, Loss Train: 3.1671, Loss Test: 3.5987, LR: 0.000051\n",
      "Step 1321, Loss Train: 3.1697, Loss Test: 3.5987, LR: 0.000051\n",
      "Step 1322, Loss Train: 3.2496, Loss Test: 3.5987, LR: 0.000051\n",
      "Step 1323, Loss Train: 3.2522, Loss Test: 3.5987, LR: 0.000051\n",
      "Step 1324, Loss Train: 3.0109, Loss Test: 3.5987, LR: 0.000051\n",
      "Step 1325, Loss Train: 3.2502, Loss Test: 3.5987, LR: 0.000051\n",
      "Step 1326, Loss Train: 3.2779, Loss Test: 2.7829, LR: 0.000051\n",
      "Step 1327, Loss Train: 3.2590, Loss Test: 2.7829, LR: 0.000051\n",
      "Step 1328, Loss Train: 2.7356, Loss Test: 2.7829, LR: 0.000051\n",
      "Step 1329, Loss Train: 3.3246, Loss Test: 2.7829, LR: 0.000051\n",
      "Step 1330, Loss Train: 3.0528, Loss Test: 2.7829, LR: 0.000051\n",
      "Step 1331, Loss Train: 3.1284, Loss Test: 2.7829, LR: 0.000051\n",
      "Step 1332, Loss Train: 3.0634, Loss Test: 2.7829, LR: 0.000051\n",
      "Step 1333, Loss Train: 3.0560, Loss Test: 2.7829, LR: 0.000051\n",
      "Step 1334, Loss Train: 3.0558, Loss Test: 2.7829, LR: 0.000051\n",
      "Step 1335, Loss Train: 3.0258, Loss Test: 2.7829, LR: 0.000051\n",
      "Step 1336, Loss Train: 3.1286, Loss Test: 2.7829, LR: 0.000051\n",
      "Step 1337, Loss Train: 3.1347, Loss Test: 2.7829, LR: 0.000051\n",
      "Step 1338, Loss Train: 3.2091, Loss Test: 2.7829, LR: 0.000051\n",
      "Step 1339, Loss Train: 3.2582, Loss Test: 2.7829, LR: 0.000051\n",
      "Step 1340, Loss Train: 3.1419, Loss Test: 2.7829, LR: 0.000051\n",
      "Step 1341, Loss Train: 3.1226, Loss Test: 2.7829, LR: 0.000051\n",
      "Step 1342, Loss Train: 3.1113, Loss Test: 2.7829, LR: 0.000051\n",
      "Step 1343, Loss Train: 3.0939, Loss Test: 2.7829, LR: 0.000051\n",
      "Step 1344, Loss Train: 3.3067, Loss Test: 2.7829, LR: 0.000051\n",
      "Step 1345, Loss Train: 3.1444, Loss Test: 2.7829, LR: 0.000051\n",
      "Step 1346, Loss Train: 3.2250, Loss Test: 2.7829, LR: 0.000051\n",
      "Step 1347, Loss Train: 3.0321, Loss Test: 2.7829, LR: 0.000051\n",
      "Step 1348, Loss Train: 3.1484, Loss Test: 2.7829, LR: 0.000051\n",
      "Step 1349, Loss Train: 2.8776, Loss Test: 2.7829, LR: 0.000051\n",
      "Step 1350, Loss Train: 3.3621, Loss Test: 2.7829, LR: 0.000051\n",
      "Step 1351, Loss Train: 3.0408, Loss Test: 2.7806, LR: 0.000051\n",
      "Step 1352, Loss Train: 3.1879, Loss Test: 2.7806, LR: 0.000051\n",
      "Step 1353, Loss Train: 3.2003, Loss Test: 2.7806, LR: 0.000051\n",
      "Step 1354, Loss Train: 2.9662, Loss Test: 2.7806, LR: 0.000051\n",
      "Step 1355, Loss Train: 2.9989, Loss Test: 2.7806, LR: 0.000051\n",
      "Step 1356, Loss Train: 3.1837, Loss Test: 2.7806, LR: 0.000051\n",
      "Step 1357, Loss Train: 3.1060, Loss Test: 2.7806, LR: 0.000051\n",
      "Step 1358, Loss Train: 3.1605, Loss Test: 2.7806, LR: 0.000051\n",
      "Step 1359, Loss Train: 2.9262, Loss Test: 2.7806, LR: 0.000051\n",
      "Step 1360, Loss Train: 3.0259, Loss Test: 2.7806, LR: 0.000051\n",
      "Step 1361, Loss Train: 2.9915, Loss Test: 2.7806, LR: 0.000051\n",
      "Step 1362, Loss Train: 2.7525, Loss Test: 2.7806, LR: 0.000051\n",
      "Step 1363, Loss Train: 3.5468, Loss Test: 2.7806, LR: 0.000051\n",
      "Step 1364, Loss Train: 3.1319, Loss Test: 2.7806, LR: 0.000051\n",
      "Step 1365, Loss Train: 2.9723, Loss Test: 2.7806, LR: 0.000051\n",
      "Step 1366, Loss Train: 2.8897, Loss Test: 2.7806, LR: 0.000051\n",
      "Step 1367, Loss Train: 3.2322, Loss Test: 2.7806, LR: 0.000051\n",
      "Step 1368, Loss Train: 2.9744, Loss Test: 2.7806, LR: 0.000051\n",
      "Step 1369, Loss Train: 3.0100, Loss Test: 2.7806, LR: 0.000051\n",
      "Step 1370, Loss Train: 3.1645, Loss Test: 2.7806, LR: 0.000051\n",
      "Step 1371, Loss Train: 3.2291, Loss Test: 2.7806, LR: 0.000051\n",
      "Step 1372, Loss Train: 3.2378, Loss Test: 2.7806, LR: 0.000051\n",
      "Step 1373, Loss Train: 3.1374, Loss Test: 2.7806, LR: 0.000051\n",
      "Step 1374, Loss Train: 3.3693, Loss Test: 2.7806, LR: 0.000051\n",
      "Step 1375, Loss Train: 3.3990, Loss Test: 2.7806, LR: 0.000051\n",
      "Step 1376, Loss Train: 3.0020, Loss Test: 2.7743, LR: 0.000051\n",
      "Step 1377, Loss Train: 3.1994, Loss Test: 2.7743, LR: 0.000051\n",
      "Step 1378, Loss Train: 2.9366, Loss Test: 2.7743, LR: 0.000051\n",
      "Step 1379, Loss Train: 3.1982, Loss Test: 2.7743, LR: 0.000051\n",
      "Step 1380, Loss Train: 3.1238, Loss Test: 2.7743, LR: 0.000051\n",
      "Step 1381, Loss Train: 3.3268, Loss Test: 2.7743, LR: 0.000051\n",
      "Step 1382, Loss Train: 2.8661, Loss Test: 2.7743, LR: 0.000051\n",
      "Step 1383, Loss Train: 3.1959, Loss Test: 2.7743, LR: 0.000051\n",
      "Step 1384, Loss Train: 3.0734, Loss Test: 2.7743, LR: 0.000051\n",
      "Step 1385, Loss Train: 3.0140, Loss Test: 2.7743, LR: 0.000051\n",
      "Step 1386, Loss Train: 3.3082, Loss Test: 2.7743, LR: 0.000051\n",
      "Step 1387, Loss Train: 3.0451, Loss Test: 2.7743, LR: 0.000051\n",
      "Step 1388, Loss Train: 3.1544, Loss Test: 2.7743, LR: 0.000051\n",
      "Step 1389, Loss Train: 3.0824, Loss Test: 2.7743, LR: 0.000051\n",
      "Step 1390, Loss Train: 2.8261, Loss Test: 2.7743, LR: 0.000051\n",
      "Step 1391, Loss Train: 3.1943, Loss Test: 2.7743, LR: 0.000051\n",
      "Step 1392, Loss Train: 3.0473, Loss Test: 2.7743, LR: 0.000051\n",
      "Step 1393, Loss Train: 3.1377, Loss Test: 2.7743, LR: 0.000051\n",
      "Step 1394, Loss Train: 3.1117, Loss Test: 2.7743, LR: 0.000051\n",
      "Step 1395, Loss Train: 3.0994, Loss Test: 2.7743, LR: 0.000051\n",
      "Step 1396, Loss Train: 3.0943, Loss Test: 2.7743, LR: 0.000051\n",
      "Step 1397, Loss Train: 3.3565, Loss Test: 2.7743, LR: 0.000051\n",
      "Step 1398, Loss Train: 3.2198, Loss Test: 2.7743, LR: 0.000051\n",
      "Step 1399, Loss Train: 3.0497, Loss Test: 2.7743, LR: 0.000051\n",
      "Step 1400, Loss Train: 3.2130, Loss Test: 2.7743, LR: 0.000051\n",
      "Step 1401, Loss Train: 3.1821, Loss Test: 2.8388, LR: 0.000051\n",
      "Step 1402, Loss Train: 2.8967, Loss Test: 2.8388, LR: 0.000051\n",
      "Step 1403, Loss Train: 2.9959, Loss Test: 2.8388, LR: 0.000051\n",
      "Step 1404, Loss Train: 3.1270, Loss Test: 2.8388, LR: 0.000051\n",
      "Step 1405, Loss Train: 2.7982, Loss Test: 2.8388, LR: 0.000051\n",
      "Step 1406, Loss Train: 3.0350, Loss Test: 2.8388, LR: 0.000051\n",
      "Step 1407, Loss Train: 3.3482, Loss Test: 2.8388, LR: 0.000051\n",
      "Step 1408, Loss Train: 3.0985, Loss Test: 2.8388, LR: 0.000051\n",
      "Step 1409, Loss Train: 3.2662, Loss Test: 2.8388, LR: 0.000051\n",
      "Step 1410, Loss Train: 3.3084, Loss Test: 2.8388, LR: 0.000051\n",
      "Step 1411, Loss Train: 3.2839, Loss Test: 2.8388, LR: 0.000051\n",
      "Step 1412, Loss Train: 2.9074, Loss Test: 2.8388, LR: 0.000051\n",
      "Step 1413, Loss Train: 3.4361, Loss Test: 2.8388, LR: 0.000051\n",
      "Step 1414, Loss Train: 3.2353, Loss Test: 2.8388, LR: 0.000051\n",
      "Step 1415, Loss Train: 2.8885, Loss Test: 2.8388, LR: 0.000051\n",
      "Step 1416, Loss Train: 3.0626, Loss Test: 2.8388, LR: 0.000051\n",
      "Step 1417, Loss Train: 3.3194, Loss Test: 2.8388, LR: 0.000051\n",
      "Step 1418, Loss Train: 3.2964, Loss Test: 2.8388, LR: 0.000051\n",
      "Step 1419, Loss Train: 3.0608, Loss Test: 2.8388, LR: 0.000051\n",
      "Step 1420, Loss Train: 3.2938, Loss Test: 2.8388, LR: 0.000051\n",
      "Step 1421, Loss Train: 3.1153, Loss Test: 2.8388, LR: 0.000051\n",
      "Step 1422, Loss Train: 3.0098, Loss Test: 2.8388, LR: 0.000051\n",
      "Step 1423, Loss Train: 3.3725, Loss Test: 2.8388, LR: 0.000051\n",
      "Step 1424, Loss Train: 3.2820, Loss Test: 2.8388, LR: 0.000051\n",
      "Step 1425, Loss Train: 3.0438, Loss Test: 2.8388, LR: 0.000051\n",
      "Step 1426, Loss Train: 3.1403, Loss Test: 3.2009, LR: 0.000051\n",
      "Step 1427, Loss Train: 3.0090, Loss Test: 3.2009, LR: 0.000051\n",
      "Step 1428, Loss Train: 3.1598, Loss Test: 3.2009, LR: 0.000051\n",
      "Step 1429, Loss Train: 3.0553, Loss Test: 3.2009, LR: 0.000051\n",
      "Step 1430, Loss Train: 3.1736, Loss Test: 3.2009, LR: 0.000051\n",
      "Step 1431, Loss Train: 2.9520, Loss Test: 3.2009, LR: 0.000051\n",
      "Step 1432, Loss Train: 2.9112, Loss Test: 3.2009, LR: 0.000051\n",
      "Step 1433, Loss Train: 3.1162, Loss Test: 3.2009, LR: 0.000051\n",
      "Step 1434, Loss Train: 2.9537, Loss Test: 3.2009, LR: 0.000051\n",
      "Step 1435, Loss Train: 3.1755, Loss Test: 3.2009, LR: 0.000051\n",
      "Step 1436, Loss Train: 3.2878, Loss Test: 3.2009, LR: 0.000051\n",
      "Step 1437, Loss Train: 2.9307, Loss Test: 3.2009, LR: 0.000051\n",
      "Step 1438, Loss Train: 3.1703, Loss Test: 3.2009, LR: 0.000051\n",
      "Step 1439, Loss Train: 3.3141, Loss Test: 3.2009, LR: 0.000051\n",
      "Step 1440, Loss Train: 2.9162, Loss Test: 3.2009, LR: 0.000051\n",
      "Step 1441, Loss Train: 3.0595, Loss Test: 3.2009, LR: 0.000051\n",
      "Step 1442, Loss Train: 3.2849, Loss Test: 3.2009, LR: 0.000051\n",
      "Step 1443, Loss Train: 3.1482, Loss Test: 3.2009, LR: 0.000051\n",
      "Step 1444, Loss Train: 3.2741, Loss Test: 3.2009, LR: 0.000051\n",
      "Step 1445, Loss Train: 3.0281, Loss Test: 3.2009, LR: 0.000051\n",
      "Step 1446, Loss Train: 2.9815, Loss Test: 3.2009, LR: 0.000051\n",
      "Step 1447, Loss Train: 3.4012, Loss Test: 3.2009, LR: 0.000051\n",
      "Step 1448, Loss Train: 3.2583, Loss Test: 3.2009, LR: 0.000051\n",
      "Step 1449, Loss Train: 3.0686, Loss Test: 3.2009, LR: 0.000051\n",
      "Step 1450, Loss Train: 3.0852, Loss Test: 3.2009, LR: 0.000051\n",
      "Step 1451, Loss Train: 3.1053, Loss Test: 3.0938, LR: 0.000051\n",
      "Step 1452, Loss Train: 3.1418, Loss Test: 3.0938, LR: 0.000051\n",
      "Step 1453, Loss Train: 2.9492, Loss Test: 3.0938, LR: 0.000051\n",
      "Step 1454, Loss Train: 3.2362, Loss Test: 3.0938, LR: 0.000051\n",
      "Step 1455, Loss Train: 3.1250, Loss Test: 3.0938, LR: 0.000051\n",
      "Step 1456, Loss Train: 3.2894, Loss Test: 3.0938, LR: 0.000051\n",
      "Step 1457, Loss Train: 3.0665, Loss Test: 3.0938, LR: 0.000051\n",
      "Step 1458, Loss Train: 3.4317, Loss Test: 3.0938, LR: 0.000051\n",
      "Step 1459, Loss Train: 3.2907, Loss Test: 3.0938, LR: 0.000051\n",
      "Step 1460, Loss Train: 3.2456, Loss Test: 3.0938, LR: 0.000051\n",
      "Step 1461, Loss Train: 3.0776, Loss Test: 3.0938, LR: 0.000051\n",
      "Step 1462, Loss Train: 3.1298, Loss Test: 3.0938, LR: 0.000051\n",
      "Step 1463, Loss Train: 3.2195, Loss Test: 3.0938, LR: 0.000051\n",
      "Step 1464, Loss Train: 3.1089, Loss Test: 3.0938, LR: 0.000051\n",
      "Step 1465, Loss Train: 3.1097, Loss Test: 3.0938, LR: 0.000051\n",
      "Step 1466, Loss Train: 3.4133, Loss Test: 3.0938, LR: 0.000051\n",
      "Step 1467, Loss Train: 3.1089, Loss Test: 3.0938, LR: 0.000051\n",
      "Step 1468, Loss Train: 2.9454, Loss Test: 3.0938, LR: 0.000051\n",
      "Step 1469, Loss Train: 3.2008, Loss Test: 3.0938, LR: 0.000051\n",
      "Step 1470, Loss Train: 3.0482, Loss Test: 3.0938, LR: 0.000051\n",
      "Step 1471, Loss Train: 3.2540, Loss Test: 3.0938, LR: 0.000051\n",
      "Step 1472, Loss Train: 3.0932, Loss Test: 3.0938, LR: 0.000051\n",
      "Step 1473, Loss Train: 2.9726, Loss Test: 3.0938, LR: 0.000051\n",
      "Step 1474, Loss Train: 3.2708, Loss Test: 3.0938, LR: 0.000051\n",
      "Step 1475, Loss Train: 3.3370, Loss Test: 3.0938, LR: 0.000051\n",
      "Step 1476, Loss Train: 2.7555, Loss Test: 3.0304, LR: 0.000051\n",
      "Step 1477, Loss Train: 3.2359, Loss Test: 3.0304, LR: 0.000051\n",
      "Step 1478, Loss Train: 3.0990, Loss Test: 3.0304, LR: 0.000051\n",
      "Step 1479, Loss Train: 3.0181, Loss Test: 3.0304, LR: 0.000051\n",
      "Step 1480, Loss Train: 3.0867, Loss Test: 3.0304, LR: 0.000051\n",
      "Step 1481, Loss Train: 3.2199, Loss Test: 3.0304, LR: 0.000051\n",
      "Step 1482, Loss Train: 3.1838, Loss Test: 3.0304, LR: 0.000051\n",
      "Step 1483, Loss Train: 3.1533, Loss Test: 3.0304, LR: 0.000051\n",
      "Step 1484, Loss Train: 3.1585, Loss Test: 3.0304, LR: 0.000051\n",
      "Step 1485, Loss Train: 3.0979, Loss Test: 3.0304, LR: 0.000051\n",
      "Step 1486, Loss Train: 3.1060, Loss Test: 3.0304, LR: 0.000051\n",
      "Step 1487, Loss Train: 3.0473, Loss Test: 3.0304, LR: 0.000051\n",
      "Step 1488, Loss Train: 3.1855, Loss Test: 3.0304, LR: 0.000051\n",
      "Step 1489, Loss Train: 3.1000, Loss Test: 3.0304, LR: 0.000051\n",
      "Step 1490, Loss Train: 3.3382, Loss Test: 3.0304, LR: 0.000051\n",
      "Step 1491, Loss Train: 3.3114, Loss Test: 3.0304, LR: 0.000051\n",
      "Step 1492, Loss Train: 3.3052, Loss Test: 3.0304, LR: 0.000051\n",
      "Step 1493, Loss Train: 3.0550, Loss Test: 3.0304, LR: 0.000051\n",
      "Step 1494, Loss Train: 3.1457, Loss Test: 3.0304, LR: 0.000051\n",
      "Step 1495, Loss Train: 3.3688, Loss Test: 3.0304, LR: 0.000051\n",
      "Step 1496, Loss Train: 3.0253, Loss Test: 3.0304, LR: 0.000051\n",
      "Step 1497, Loss Train: 3.1561, Loss Test: 3.0304, LR: 0.000051\n",
      "Step 1498, Loss Train: 3.1024, Loss Test: 3.0304, LR: 0.000051\n",
      "Step 1499, Loss Train: 3.2101, Loss Test: 3.0304, LR: 0.000051\n",
      "Step 1500, Loss Train: 2.9978, Loss Test: 3.0304, LR: 0.000051\n",
      "Step 1501, Loss Train: 3.0370, Loss Test: 3.3429, LR: 0.000051\n",
      "Step 1502, Loss Train: 3.0933, Loss Test: 3.3429, LR: 0.000051\n",
      "Step 1503, Loss Train: 3.1518, Loss Test: 3.3429, LR: 0.000051\n",
      "Step 1504, Loss Train: 3.1932, Loss Test: 3.3429, LR: 0.000051\n",
      "Step 1505, Loss Train: 3.1149, Loss Test: 3.3429, LR: 0.000051\n",
      "Step 1506, Loss Train: 3.0810, Loss Test: 3.3429, LR: 0.000051\n",
      "Step 1507, Loss Train: 3.0733, Loss Test: 3.3429, LR: 0.000051\n",
      "Step 1508, Loss Train: 2.9880, Loss Test: 3.3429, LR: 0.000051\n",
      "Step 1509, Loss Train: 3.1054, Loss Test: 3.3429, LR: 0.000051\n",
      "Step 1510, Loss Train: 3.0985, Loss Test: 3.3429, LR: 0.000051\n",
      "Step 1511, Loss Train: 3.2154, Loss Test: 3.3429, LR: 0.000051\n",
      "Step 1512, Loss Train: 3.2980, Loss Test: 3.3429, LR: 0.000051\n",
      "Step 1513, Loss Train: 3.3698, Loss Test: 3.3429, LR: 0.000051\n",
      "Step 1514, Loss Train: 2.7733, Loss Test: 3.3429, LR: 0.000051\n",
      "Step 1515, Loss Train: 3.3160, Loss Test: 3.3429, LR: 0.000051\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43miter_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miter_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[30], line 9\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(iter_train, iter_test, steps_per_epoch, epochs)\u001b[0m\n\u001b[0;32m      6\u001b[0m batch_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iter_train)\n\u001b[0;32m      7\u001b[0m batch_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iter_test)\n\u001b[1;32m----> 9\u001b[0m loss_train_temp \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m25\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     12\u001b[0m     loss_test_temp \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(batch_test)\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loop(iter_train, iter_test, steps_per_epoch, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b9fcb0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_manager.save()\n",
    "pkl.dump([losses_train, losses_test], open(\"checkpoints/losses_\" + name + \".pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7ea6bb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0EAAAHWCAYAAACxAYILAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAqe1JREFUeJzs3Qd4U1UbB/B/ki5mgbL33nvKkD1FEVBURFmKCigiisqnoqIIiHuADAVFUBEFUdl7yt57773K6kryPe8pSZM0SZM2zfz/nidtcnNzc5rbJPe955z31RiNRiOIiIiIiIhChNbXDSAiIiIiIvImBkFERERERBRSGAQREREREVFIYRBEREREREQhhUEQERERERGFFAZBREREREQUUhgEERERERFRSGEQREREREREIYVBEBERERERhRQGQURE5BW9e/dGyZIlA2a7vrZixQpoNBr1213Hjx9Xj506dWqmtI2IKNAxCCIiykRyECoHo5s3b/Z1UygTjBs3joEGEVEACvN1A4iIiDJi0qRJMBgMPguC8ubNq3qjPK1p06a4e/cuIiIi3H5siRIl1GPDw8M93i4iomDAniAiIgpIt2/fVr/lQD8yMhKB0l5XabVaREVFqd/ukt5HeaxOp3P7sUREoYBBEBGRH9i2bRs6dOiAnDlzInv27GjVqhX+++8/q3USExPx/vvvo1y5cuoANyYmBk2aNMHixYvN65w/fx59+vRB0aJFVWBQqFAhPPzww2qOiKX58+fj/vvvR7Zs2ZAjRw507NgRe/bssVrH1W3ZM2fOHFStWlW1U37Pnj3b5Tkv9uazSE+LvC5HjhzBAw88oNrco0cPu3OCTI//5JNPMHHiRJQpU0a1v169eti0aVOqdvz++++oXLmyVVtdmWck98trtnLlSvV8cmnevLnVMEi5b8CAAcifP796HcWJEyfUsgoVKiBLlixqP3br1i3V62rv9ZHtSxv37t2LFi1aIGvWrChSpAg+/vhjl1/DM2fOoHPnzup6vnz58Nprr0Gv11s9/sqVK3j66afV/2OuXLnQq1cv7Nixg/OMiChocDgcEZGPyYG0BCRywPn666+rno0JEyaoA145iG7QoIFa77333sOoUaPw7LPPon79+oiNjVVzjbZu3Yo2bdqodR555BG1vZdeekkdpF+8eFEFSSdPnjQf1E+bNk0d1LZr1w5jxozBnTt3MH78eBVQSTBmWs+VbdmzaNEi9VgJLKS9ckBtCqYyIikpSbVZ2ikBjgQAzsyYMQM3b97E888/rw7eJVDo2rUrjh49ah4m9u+//+Lxxx9HtWrVVFuvXbuGZ555RgUWafniiy/UayPBxFtvvaWWFShQwGodCXYk0Bg+fLi5J0gCsXXr1uGJJ55Qr4kELPL6y/6W4Catv0va2L59e/W3PPbYY5g1axbeeOMN9TdIIO2MBDvyGsr/lLyGS5YswaeffqoCxf79+6t1ZGjhQw89hI0bN6plFStWxF9//aX+Z4iIgoaRiIgyzZQpU4zyUbtp0yaH63Tu3NkYERFhPHLkiHnZ2bNnjTly5DA2bdrUvKxGjRrGjh07OtzOtWvX1HONHTvW4To3b9405sqVy9ivXz+r5efPnzdGR0ebl7uyLUdq1qxpLFSokPH69evmZYsWLVLbK1GihHnZ8uXL1TL5benYsWNqubx2Jr169VLL3nzzzVTPJ/dZbtf0+JiYGOPVq1fNy//66y+1/O+//zYvq1atmrFo0aLqdTFZsWJFqrY6UqVKFWOzZs0c7vcmTZoYk5KSrO67c+dOqvXXr1+v1v/pp5+cvj7yXLbrxcfHGwsWLGh85JFHXHoNR4wYYfXctWrVMtapU8d8+48//lDrffHFF+Zler3e2LJly1TbJCIKVBwOR0TkQ3JmXnpOZHhS6dKlzctl6NmTTz6JNWvWqB4fIcOSpGfm0KFDdrclQ6tkEr0Mn5LeAnukJ+f69evo3r07Ll++bL7I3BHpHVi+fLnL27Ln3Llz2L59u+o1iI6ONi+XnirpGcooU2+FK6SHJ3fu3Obb0tsmpCdInD17Frt27ULPnj1Vb45Js2bNVK+KJ/Tr1y/VvBx5bS2HOEpPWdmyZdX+lV69tEhbn3rqKfNt2U/SM2j6u9LywgsvWN2W18XysQsWLFA9ZdJ2E5mXNHDgQJe2T0QUCBgEERH50KVLl9RwNJkfYqtSpUpqaNKpU6fU7REjRqgApnz58uogfejQodi5c6d5fZn3IsPbZL6PDMuS7GIyBEzm9piYAqiWLVuqYVqWFwnGZMibq9uyR+a7CJm3ZMve3+iOsLAwt4bUFS9e3Oq2KSAyBXWmtkoAYsvesvQoVapUqmWStU2GxxUrVky9zpJdTl5/2bc3btxIc5vyGsjwPtu/zZVgVeY9yXM5e6y8LhKE2w7L89RrQkTkDxgEEREFCAlEJDHADz/8oCbHT548GbVr11a/TQYPHoyDBw+q+S1ywPvOO++oYErm+ghTKmmZFyS9QrYXmfvh6rYyyvZA3sR2kr6JBAzuZEpzlBnNaJRRXd5h2etjIvOIRo4cqebzzJw5UwWf8tpLggRXUn1n5O9itjgiomRMjEBE5ENyVl7OuB84cCDVffv371cH/dJjYJInTx6VZEAut27dUoGRJEyQZAkmMsn91VdfVRfp+alZs6aa/P7zzz+r+4RkK2vdunWa7XO2LUf1aYS9IXu2f6OpZ0Z6QCyZemgym6mthw8fTnWfvWXuBHLOSCIDGS4or6NJXFxcqtfBV+R1kWGR0kNp2Rvk6mtCRBQI2BNERORDcma+bdu2qgfGMkXyhQsXVHYzyYQmWeOEzB2xnRsiQ5Ti4+PVbTlolYNp2yBG0kmb1pHMYLK9jz76SM1HsTc8z9Vt2SPDqCRQ+vHHH62GdklPh2Q+sz3Ylr9/1apVqQqQekPhwoVVj9pPP/2kAkoTycgnc4VcISnG3Q1e5G+27bX5+uuvHfaAeZv8j8j/hhShNZEeqm+//dan7SIi8iT2BBEReYEMYZMJ57ZefvllfPjhhypIkIBHUirL3BdJkS3BhmX9F0ksIGmU69Spo3qEJD229Cq8+OKL6n4Zuib1hWSYlawr25GaNxJQSTpmIQGQpGOWGjAylE6WS2+UpL2WdNGNGzfGN99849K2HJHhc1J3SP6evn374urVq+ogv0qVKlbBhiROkPo4cp/0qEiQ9c8//5jnJXmDBINS+0j+buldk7kx8vdLcGTZVkdkX8jrKftQAlLpYZP5Vs48+OCDajii/P3y2q5fv16lqpbhcP5AknRIogXp/ZPeH0mRPXfuXLUf09v7RUTkbxgEERF5gRwo2yMFLCU4WL16NYYNG6YCCDnrLpnaZMiZqUaQGDRokDoYlTkkEiBJT4ocfEuCBCHD5iTr29KlS9VBtgQucgAr806kbo+JZJ2TXpDRo0dj7NixaltSF0eyhEkg4M627JEaNlKA9O2331Z/kwQ3U6ZMUb1dtoVRJQCSXofvvvtOzfmRoEvaJEGIN0g9nF9++UUNKXzzzTdVQgcpBio9WbbFY+2RBAcyfE+CValJJJnl0gqCvvzyS9UbNH36dNXbJgGYBEHSA+MPpG0SEEuALq+DDMns0qUL3n33XdVWmR9GRBToNJIn29eNICIi8icypE96yKSHjpLNmTNHBUOStl2CISKiQMY5QUREFLKkFyopKclqmfRW7dixQw09DFWSxtuSzFeSXjsZTinDKImIAh2HwxERUcg6c+aMypInxUdliKBk5JOheQULFkxVVDSUSBpvCYQaNmyohkv++eefWLdunZpDZS/tNxFRoOFwOCIiClmSwe65557D2rVrVWY8yfYmCSFkvpQpnXgoksyEksJbEiPIvCVJ+tC/f39zEg4iokDHIIiIiIiIiEIK5wQREREREVFIYRBEREREREQhJaATI0gtjbNnz6oK5izeRkREREQUuoxGo6rZJolupMZZ0AZBEgBJQT8iIiIiIiJx6tQpFC1aFEEbBEkPkOkPldoFREREREQUmmJjY1UHiSlGCNogyDQETgIgBkFERERERKRxYZoMEyMQEREREVFIYRBEREREREQhhUEQERERERGFlICeE0RERERElFba5KSkJOj1el83hTJIp9MhLCzMI6VxGAQRERERUVBKSEjAuXPncOfOHV83hTwka9asKFSoECIiIjK0HQZBRERERBR0DAYDjh07pnoPpHimHDR7ogeBfNejJ0HtpUuX1H4tV65cmgVRnWEQRERERERBRw6YJRCSujHSe0CBL0uWLAgPD8eJEyfU/o2Kikr3tpgYgYiIiIiCVkZ6Cyh49yf/K4iIiIiIKKQwCCIiIiIiopDCIIiIiIiIiEIKgyAiIiIiIj8hGeycXd57770MbXvOnDkeWy+QMTscEREREZGfkLpGJr/99huGDx+OAwcOmJdlz57dRy0LLuwJ8pSjK4GfHwGunfB1S4iIiIjIQa2ZOwlJXr/I87qqYMGC5kt0dLTqlbFc9uuvv6JSpUoqPXTFihUxbtw482MlbfSLL76oionK/SVKlMCoUaPUfSVLllS/u3TporZpuu0uSTs+YsQIFC1aFJGRkahZsyYWLFjgUhvkdZCerOLFi6vHSv2mQYMGwRfYE+QpP3VK/j37eaBvyj8CEREREfmHu4l6VB6+0OvPu3dEO2SNyPhh9/Tp01XP0DfffINatWph27Zt6NevH7Jly4ZevXrhq6++wty5czFz5kwVaJw6dUpdxKZNm5A/f35MmTIF7du3V0Vk0+PLL7/Ep59+igkTJqg2/PDDD+jUqRP27NmjCpg6a8Mff/yBzz//XAVyVapUwfnz57Fjxw74AoMgT7uZ0oVJREREROQp7777rgpAunbtqm6XKlUKe/fuVQGJBEEnT55UgUiTJk1Ub4/0wpjky5dP/c6VK5fqUUqvTz75BG+88QaeeOIJdXvMmDFYvnw5vvjiC3z77bdO2yD3yXO3bt1aFT2VIKl+/frwBQZBRERERBQSsoTrVK+ML543o27fvo0jR47gmWeeUb0/JklJSWrYnOjduzfatGmDChUqqN6eBx98EG3btoWnxMbG4uzZs2jcuLHVcrlt6tFx1oZu3bqpYKl06dLqvgceeAAPPfQQwsK8H5IwCPI4ja8bQERERER2SM+EJ4al+cKtW7fU70mTJqFBgwZW95mGttWuXRvHjh3D/PnzsWTJEjz22GOq12XWrFlea2dtJ20oVqyYSvIgyxcvXowBAwZg7NixWLlypeoZ8iYmRiAiIiIi8nMFChRQiQSOHj2KsmXLWl1kWJxJzpw58fjjj6tgSbLLyTycq1evqvsk0NDr9elug2xb2rB27Vqr5XK7cuXKLrUhS5YsqvdH5g6tWLEC69evx65du+BtgRkKExERERGFmPfff19lU5PhbzKcLD4+Hps3b8a1a9cwZMgQfPbZZyormyQs0Gq1+P3339UcHJkHJCQj3NKlS9XwNcnOljt3bofPJb0527dvt1omc32GDh2q5iaVKVNGZYaTRAuyniRtEM7aMHXqVBWESU9W1qxZ8fPPP6ugyHLekLcwCPI0DYfDEREREZHnPfvssyp4kCFkEoxIVrhq1aph8ODB6v4cOXLg448/xqFDh9QQuXr16mHevHkqGBGSVEGCpUmTJqFIkSI4fvy4w+eS9WytXr1aBWE3btzAq6++iosXL6oeIMkGJwFSWm2QQGj06NFq2xIMSdv//vtvxMTEwNs0RncSl/sZmZwlkbDsCOl286n3kiekoUhdoN9S37aFiIiIKMTFxcWp3gwZKib1aij492usG7EB5wR5SELlR9TvuIqdfd0UIiIiIiJygkGQhxj3/at+H1n7p6+bQkRERERETjAI8pBIY5z6XSVuq6+bQkRERERETjAIIiIiIiKikMIgiIiIiIiIQgqDICIiIiIiCikMgoiIiIiIKKQwCCIiIiIiopDCIIiIiIiIiEIKgyAiIiIioiBXsmRJfPHFF75uht9gEERERERE5Cc0Go3Ty3vvvZeu7W7atAnPPfdchtrWvHlzDB48GMEgzNcNICIiIiKiZOfOnTNf/+233zB8+HAcOHDAvCx79uzm60ajEXq9HmFhaR/S58uXLxNaG7jYE0REREREocFoBBJue/8iz+uiggULmi/R0dGq98d0e//+/ciRIwfmz5+POnXqIDIyEmvWrMGRI0fw8MMPo0CBAipIqlevHpYsWeJ0OJxGo8HkyZPRpUsXZM2aFeXKlcPcuXMz9PL+8ccfqFKlimqXPN+nn35qdf+4cePU80RFRam2Pvroo+b7Zs2ahWrVqiFLliyIiYlB69atcfv2bQRlT5BErtKl9/PPP+P8+fMoXLgwevfujbffflvtGCIiIiIij0m8A3xU2PvP+7+zQEQ2j23uzTffxCeffILSpUsjd+7cOHXqFB544AGMHDlSBSA//fQTHnroIdWDVLx4cYfbef/99/Hxxx9j7Nix+Prrr9GjRw+cOHECefLkcbtNW7ZswWOPPaaO7R9//HGsW7cOAwYMUAGNHN9v3rwZgwYNwrRp09CoUSNcvXoVq1evNvd+de/eXbVFgrKbN2+q+6SnKyiDoDFjxmD8+PH48ccfVdQoL06fPn1U1CsvEhERERERWRsxYgTatGljvi1BS40aNcy3P/jgA8yePVv17Lz44osOt9O7d28VfIiPPvoIX331FTZu3Ij27du73abPPvsMrVq1wjvvvKNuly9fHnv37lUBljzPyZMnkS1bNjz44IOqN6tEiRKoVauWOQhKSkpC165d1XIhvUKZyadBkESI0nXXsWNHdVu6zX755Rf14hMREREReVR41uReGV88rwfVrVvX6vatW7dUD8y///5rDiju3r2rAg9nqlevbr4uAUrOnDlx8eLFdLVp37596rjeUuPGjdUQPBn9JUGbBDjSeyVBllxMQ/EkgJMASgKfdu3aoW3btmqonPRyBeWcIOkKW7p0KQ4ePKhu79ixQ41r7NChg9314+PjERsba3UhIiIiInKJTLeQYWnevnh4mocELJZee+011fMjvTkyjGz79u0qoEhISHC6nfDwcJuXRwODwYDMIL0/W7duVR0ehQoVUgkfJPi5fv06dDodFi9erOY6Va5cWQ3Nq1ChAo4dO4agDIJkPOMTTzyBihUrqp0gXWKSdk/GI9ozatQoNVTOdClWrJjX20xERERE5E/Wrl2rhpxJz4oEP5JE4fjx415tQ6VKlVQ7bNslw+IkyBGSxU4SHsjcn507d6o2Llu2zByASc+RzFPatm0bIiIiVGAXlMPhZs6cienTp2PGjBlqTpBErRIESYKEXr16pVp/2LBhGDJkiPm29AQxECIiIiKiUCYZ1/7880+VDEGCCZmXk1k9OpcuXVLH7JakZ+fVV19VWelkPpIkRli/fj2++eYblRFO/PPPPzh69CiaNm2qhrnNmzdPtVF6fDZs2KBGh8kwuPz586vb8jwSWAVlEDR06FBzb5CQyFUyUkiPj70gSLJdyMXv3b0OZMnl61YQERERUQiQpAR9+/ZVU03y5s2LN954I9OmjcyYMUNdLEngI9mdpYNDhrnJbQmMJIGD9FCJXLlyqUBN5i7FxcWpwE2GxklHiMwnWrVqlZo/JO2WuUOSXtvRFBlP0BgzM/dcGiRl3ocffoj+/fubl0kANGXKFPM8IWfkRZJhcTdu3FATuXzqveiU60OPANny+rI1RERERCFNDrRlTkmpUqVUXRoK/v0a60Zs4NOeIOmyk3zmkr9cokAZ/2eKZAOaQe/rFhARERERkT8GQZL5QcYsSiElSccnc4Gef/551Y0W0IwMgoiIiIiI/JVPgyBJlSdj/+QS6Fbrq+J+3e7kG8bMmYhGREREREQBniI7mIxNejzlhj7Rl00hIiIiIiInGAR5yG1YTMw6vMSXTSEiIiKie3yYA4z8eH8yCMoM+/72dQuIiIiIQlp4eLj6fefOHV83hTzItD9N+zcg5wQFEyM0KTeOrfRlU4iIiIhCnk6nU7VpJPmWyJo1qyokSoFJeoAkAJL9KftV9m9GMAjKjCCIiIiIiHyuYMGC6rcpEKLAlytXLvN+zQgGQR6SanTijt+AGhbJEoiIiIjIq6Tnp1ChQsifPz8SE5m4KtCFh4dnuAfIhEFQZvUEzX6OQRARERGRH5ADZ08dPFNwYGKETBwOZ/ylO2Bg4VQiIiIiIn/CIMhD7CXr0xyYB/zYyQetISIiIiIiRxgEech5Y4z9O06sQcLl495uDhEREREROcAgyEMSnUyvivimhlfbQkREREREjjEI8pIRs7dBb2DFYiIiIiIiX2MQ5CXDdzTH7G1nfN0MIiIiIqKQxyDIiy7ejPN1E4iIiIiIQh6DIC8ycjQcEREREZHPMQjyovzXd/i6CUREREREIY9BkBd129HX100gIiIiIgp5DIK87PrteF83gYiIiIgopDEI8rKErTN83QQiIiIiopDGIMhDSsRkRbP4z9JcL//SwV5pDxERERER2ccgyEOK5MqCE8aCvm4GERERERGlgUEQERERERGFFAZBvqgBpE/MxJYQEREREZEzDIJ84PilG75uAhERERFRyGIQ5CEajevrxl2/mJlNISIiIiIiJxgE+UDFXxv6uglERERERCGLQRAREREREYUUBkFERERERBRSGAR5WPv40ThiKOTrZhARERERkQMMgjxsv7E4WiV8iqGJz/m6KUREREREZAeDoEzyu765r5tARERERER2MAjKRElGxy/v3QS9V9tCRERERER+EASVLFkSGo0m1WXgwIEIBv9Lesbhfe/M3unVthARERERkR8EQZs2bcK5c+fMl8WLF6vl3bp1QzDYYKjk8L7onZO92hYiIiIiIvKDIChfvnwoWLCg+fLPP/+gTJkyaNasGQJNz4YlUy07YSyIZvGfYZa+aar7/hc23UstIyIiIiIiv5wTlJCQgJ9//hl9+/ZVQ+LsiY+PR2xsrNXFX7SvWhCP1ilqNxDabUgdIOk0Ri+1jIiIiIiI/DIImjNnDq5fv47evXs7XGfUqFGIjo42X4oVKwZ/UiJPVl83gYiIiIiIAiUI+v7779GhQwcULlzY4TrDhg3DjRs3zJdTp055tY1ERERERBT4wuAHTpw4gSVLluDPP/90ul5kZKS6+CtHA9yMsD+8j4iIiIiIQrQnaMqUKcifPz86duyIYHTRmMvu8oQkg9fbQkREREQU6nweBBkMBhUE9erVC2FhftExlW6O+nsWGurZXf79mmOZ2h4iIiIiIvLDIEiGwZ08eVJlhQt0jobDGaBFvbhvUy3fcuJqpreJiIiIiIis+bzrpW3btjAagyNdtLM/w14v0e4DBwHY7yUiIiIiIqIg7QkKJkaHfUFAPMJTLYswxmdyi4iIiIiIyBaDIA8yOOkJuoHseDuxj9WyIWG/Z36jiIiIiIjICoMgT0pjWN/P+tZWtzvr1mVyg4iIiIiIyBaDIK9ivSAiIiIiIl9jEORBkeE6XzeBiIiIiIjSwCDIg4Ilyx0RERERUTBjEERERERERCGFQZCvsfeIiIiIiMirGAR5UIdqhdx+zO45n2VKW4iIiIiIyD4GQR5UJl92tx9TdccIHL54K1PaQ0REREREqTEI8gM7T1/3dROIiIiIiEIGg6BMki3Cfrrs08a8Xm8LERERERGlYBDkYTvebYsVrzVH8Zhsdu+PN4Z7vU1ERERERJQizOI6eUB0lnB10bjxGI07KxMRERERUYawJyiTfNC5it3lBjsvue7mWS+0iIiIiIiIBIOgTFKnRB4cGtkBu95ri+yRKR1u9qoC/bdynlfbRkREREQUyhgEZaJwnRY5osLxTJNSTnuCPtJ/7uWWERERERGFLgZBXrbMUMvXTSAiIiIiCmkMgrzsy6Suvm4CEREREVFIYxDkZfGI8HUTiIiIiIhCGoMgIiIiIiIKKQyCvMBeRjhbF2LjvNASIiIiIiJiEOQDN41ZUi0bMH0rLt2M90l7iIiIiIhCCYMgH2gXPybVsohTazBwxlaftIeIiIiIKJQwCPKBROhSLfslYiQ2Hrvqk/YQEREREYUSBkE+cBnRvm4CEREREVHIYhDkA0YHL3tdzX4Yja6kUSAiIiIiovRiEORHZkWOwPQNJ33dDCIiIiKioMYgyM98t/KIr5tARERERBTUGAT5mdPX7uLHdcd93QwiIiIioqDFIMgLosJTv8x9EobaXbeTdi3enbvHC60iIiIiIgpNDIK8oGfDkiiVN5vVsi2G8nbX/SriWy+1ioiIiIgoNDEI8oLskWH4o38jq2VGaHzWHiIiIiKiUObzIOjMmTN46qmnEBMTgyxZsqBatWrYvHkzgo1t6msNnKXCNmLYnzuRpDdkeruIiIiIiEJNmC+f/Nq1a2jcuDFatGiB+fPnI1++fDh06BBy586NUHY8qgdKbpyBormzYmCLsr5uDhERERFRUPFpEDRmzBgUK1YMU6ZMMS8rVaoUgpFGYzv8zXlRVA0MGLvwAIMgIiIiIqJgGg43d+5c1K1bF926dUP+/PlRq1YtTJo0yeH68fHxiI2NtboE6nC4tAwPm6YCpcMXb2Vam4iIiIiIQpFPg6CjR49i/PjxKFeuHBYuXIj+/ftj0KBB+PHHH+2uP2rUKERHR5sv0osUqJLS6ITrE7YQ8yPeROvPVnqtTUREREREoUBjdLeLwoMiIiJUT9C6devMyyQI2rRpE9avX2+3J0guJtITJIHQjRs3kDNnTvizK7fiUefDJVbLntHNwzvhPzt9XMm4GTg+umMmt46IiIiIKLBJbCAdJa7EBj7tCSpUqBAqV65staxSpUo4efKk3fUjIyPVH2R5CWTf6x9wab25O87it032XxMiIiIiIgqgxAiSGe7AgQNWyw4ePIgSJUog2OTJFoH7SueB9LttOHbVrccO+mWb+t2iYn7kzxGVSS0kIiIiIgoNPu0JeuWVV/Dff//ho48+wuHDhzFjxgxMnDgRAwcORDBmh/ul33349bn7rJafMuRz+rh8uGa+vmjPBVyIjcu0NhIRERERhQKfzgkS//zzD4YNG6bqA0l67CFDhqBfv34eH/fnT0q++a/5enbcwe6oZ52vHzfD6vb4HrXRoVqhTGsfEREREVGgCZg5QeLBBx/Erl27EBcXh3379rkcAAWLW8ia5jrP6FKCJtF/+tZMbBERERERUXDzeRBEaXsnfDomhX+KGNwwL7sVn+TTNhERERERBSoGQQGijW4LtkT1N9/u+NVqn7aHiIiIiChQMQjyA2v0Vdx+zIkrdzKlLUREREREwY5BkA9IYgNLQxIHuPzYapqj5ut/7zjr0XYREREREYUCBkE+IJndjo/uaL59Eblx2ehadru/I982X3/pXv0gIiIiIiJyHYMgP7HFUD5dvUE/rT+eSS0iIiIiIgpODIL8xBuJ/bDHUMLl3qAcSJ4TNPyvPVb3SdmnG3cTM6WNRERERETBgEGQn7iOHBib9LjL65fTnLa7/K05u1Hj/UVYfeiSB1tHRERERBQ8GAT5ESM0Lq/7Z+R7yI3YVMtnbDipfn+++KBH20ZERBTMTl65oxIOGQxGXzeFiLyAQZAPNSiVx+r2dWM2tx7/T+Rb6ve7f+3GX9vP4OilWx5tHxERUahoOna5Sjj0144zvm4KEXkBgyAfmtyrrtXtHcYybj2+iOaK+v3j+hN4+dftaPnpSvN9Go3rvUpERESUbOOxa75uAhF5AYMgH8oRFW6zxP3A5XjUk3gr7OdUy7WMgYiIiNKBw+GIQgGDoCDQL2xeqmWadARUREREoc7IGIgoJDAIChIdtBvU76baHRgX/gVyGq77uklERERERH6JQVCQ+Db8K5TVnMZPEWPwgG4jHjr3FZL0Bl83i4iIyGvm7zqH9+buydD3XyD0BE377wTmbGMCB6KMYBDkZ04b86brcVqNEUsiXzffLqS5grJvzVfFU4mIiEJB/+lbMXXdcczZfhbB6uz1u3hnzm4M/m27r5tCFNAYBPmZDvGj8a++foa3E4Ek9bvUsHn4eukhD7SMiIgoMFy6GZ/uxxr9PDFCbFyir5tAFBQYBPmZm8iKgYmDMSGpo0eCIPHpvcKpcYn6DLePiIgomHEABVFoYBDkp0Yndc/Q4zU2Z7Jen7UDFd9ZgFdn7sB3K49ksHVERERERIGLQZCfMkKLcnE/pfvxlbQnUVRz0Xx75ubT6vcfW09j9Pz9uBWf3FOUyOQJREREZuwIIgoNDIL8WCLCMvT4vyLecfiR/tmig3hswnpUfXch1h+5gmd/3IQdp5hWm4iIiIiCX8aOssmvxWhu4njUk+r6m4nP4ld9S/N9P6w9Zr7efdJ/6veSfRdxfHTG5iIREREFMn+fE8Ri6ESewZ4gP/FMk1J2l7+S0N8j2x8dPhnlNac8si0iIiJfO3P9rsp+evV2gkczvPl7djgi8gwGQT42Z2BjjOxSFY/ULmr3/tmG+zM0N8jSosg3kBuxHtkWERGRLz0+Yb3Kfhpq9XICPUiT+oUGQ2D/DRQcGAT5WM1iudCjQYlMnRtkqYjmskvrfb74IF75bTuLrRIRkV86fe2u+r32sGvfay7j116men7aFrT7YhUTM5HPMQgKEHsMzgMlV/0T+TYaaXenud6XSw9h9rYz2G4nWYIERgyOiIgoEMzdcRYj/93L7617bscn4cilWx7bnt5gxNnryQGpKxbtvYBDF29h8/FrHmsDUXowCAoQPRPe9Ni2ZkR8hD66+Qg3F1Q14gXdXLTUbk21bnyS9Zka6cJ+ZPw69Jm6yWPtISIiyqzkAYN+2YZJq4+p5D+uCKRQKT2BXbOxK9Dq05XYdtIzQYhkl200ehmW7b/gke0ReQuDoABxBdHYYKjose29Gz4Nz+jmqesNtXvxZviv+CHiE4yatw+Xbsab13vjj534ZOEB8+1jV25j68nrWHHgEpLYlU1ERAHi6u2U7zZn/L3HKKPZ4S7fSn4dluzzTNCy/MAl9XvK2uOp7rtxJxGrDl5SvUVE/oYpsgNIGPQe3Z4EPpcRjQe1ySmyxYRVR9XF5MSVO/hm+WG0qJgfdUrktnq8RsM0nURE5Fv+HrSEss7j1uLY5dt496HK6NPYfhZcIl9hT5CfyBap83oQJD4Jn4Dmuh1prufOeF8iIiJvkU6GfeesM59eiI3Dxdi4dG0vkEKqszfisOHoFa89X3ySHgt2n8eNu4kurS8BkPh7x9mgy3JHgY9BkJ8oEZMN9Upa97TYCs+EIMhV9j6qkgwcDkdE6ScHqTM3nUJcou8+2yg4dPhytdXtqeuOo/5HS4M+A1nj0cvw+MT/sOWEd5IMjJl/AC/8vAW9ftjolecjykwMgvzI7y80cnr/HH3y/fHGcPyhbwJvkpTZ645YpyH9ffNpr7aBiIJLl3Hr8PofOzF6/n7zMtYQIU+6m44A2x9H10lR2N83Oy54vuXEVa+0Y/a25O99+5ljvdIEouAIgt577z01r8TyUrGi5yb/B5vv9Q+gZ8IbqBc/Dq8mDsiU5yiukYmSRrtd2k9O2mD1IXfq6p1MaQMRhYYz94bZLrXIKjVg+la0+XylGnZDvrX/fCx6TP4PWz2URSxQuHIsv/vMDZy/kb7hdu6SoX5SFHborJ0IKgyaKNR7gqpUqYJz586ZL2vWrEEoy5s90uF9euiwylADscimbvdPeNnjz78q8hW8HzbV4f2tP1uZcuNeXoTBv27DwBlbOTmVKADrhfjD+9ayCfN3n8eRS7ex/kjG5jlcvZ2Ajceu+sXfF6iemrwBaw9fQddx6xDsDl646fR+yXJ25V5WtRkbTuLBr9fgvlFLvdK263dcm3/ja5zjQ4HG50FQWFgYChYsaL7kzZsXoWzdmy1dXneBoV6mtKFX2GIcj3pSXXroljhdNzYuEXO2n8W/O8/hokVqbSLybyeu3EaVdxfi2R83wx9lNPtk04+X47EJ67HUxdowlNrlWwkIdK7GwG0/X2XxGOsHye0aIxahzodL1Hfd/2bvcmmbCUkGdPpmDYb9GWQ9OOlgbzcwZCKEehB06NAhFC5cGKVLl0aPHj1w8uRJh+vGx8cjNjbW6hJsIsJc3yVGaNE94S28n/h0prVnZPgPKAD7Y403yVlWizmnPOFKlHErDlz0ylDTGRuTP2uX7vd9kGDvsyOjPTi34pP84u9jPTXXTVh5BENmbve7OWFJFu2RUQ+2/tl51m6h0NWHLmHn6Rv4ZaPjuTye6mFJT+2gjNYbIgp0Pg2CGjRogKlTp2LBggUYP348jh07hvvvvx83b9rvlh41ahSio6PNl2LFiiHUrTdUwRR9BxUMScKEzFBfmzJp2ZIUTSUiz1lz6DJ6T9mE+z9e7uumkAf8seU0KryzAEs9VJQyvY5cuoXrd3zTq+NqKmUxav5+/Ln1DNZlcChkRhndzHD44oxt6Dt1c6rA3d8LhHp6+Fp6z1tIkgVTwgWikAmCOnTogG7duqF69epo164d5s2bh+vXr2PmzJl21x82bBhu3Lhhvpw6lbGzK8EWDFWMn5Ip2/464hvU1+zDk7qlmBj+KXIiOe+/uHAzzioLD8ffE6XfZi9leApUcrBkqjsSCF79fYc6EH7Gh0MOj1++jVafrkTNEYu9/tyztpxGjfcX4Ztlh9x63J2E5F48n3Hja+y6i0GeZHb75V7vayDLjCLpnb9di1d+24FNx/n5RyE2HM5Srly5UL58eRw+fNju/ZGRkciZM6fVJRhN6ZO+uT4yPC6zzIz8AB+Ff4+2ui3YGdUPOXELuRGLA988iiba5PHRLT5Z4dMveyJ/cfTSLUzfcCLoa5Rkpl2nb6TKJCcHS/I54+kjWglUpq49hr1ng2+I9UYHB5aSCGBbOrK+yVC1uwmuZe57fVZyIe5PFh2EJ92MS3S9lyWd5+Uk5fR/6SxCKv+ri/act3pqyew27M9duHY7HT1yNn9DJsQhfuHYpcA5wUHBwa+CoFu3buHIkSMoVKgQQlmLCvnN13NGheH46I7wN2PCJ+Ht8Ol4SPcffo4YZV6+zA/mFxDZC0pe/nUbDpx3ngHKU1p+uhJvzd6NH9cdRzCS7FjLPfhet9eDLCmBTXaevq6yUHqaBKkyVG3y6qN47++9eOAr64Kbrsz3eWv2LjVZPhDIHBXTXClJBCB1mi45SGjjKNDpPuk/VBq+wOHjMtu5G3dR7b1F6DJubaY9h/xfPDJ+PZ6Y+J/qSfvM4n/R1QKmz03bgnm7znmmbhEyx7fLj8CXOHCEQjoIeu2117By5UocP34c69atQ5cuXaDT6dC9e3dfNotc0EG3CY/oVjv8kpJx0kT+QA76JCj5a/tZPDLeu6l+twXhvDkJSCQ7Vp+pmzy2zbSOhTp9sxabjl9Ls4finTm71TwcV41bfkT1XstcFFdJ2m3JOic1dGS41/QNJ+1OlvdHT3+/ET2/32C17Oy9Wk22pv1nP4DfcCy5Z2nBnvPILEYnQfL8XcnPKwkHMotlD27zT1Zg/Ir0BQsZTfPuTvDwx1bH//cS6DsrtOqz5CfMD0c+FubLJz99+rQKeK5cuYJ8+fKhSZMm+O+//9R1SpY/Z5Rb609Jaoc+YQvhbc20O7DNUAaxyI6Go5apZaO6VlMHDCMeroJu361HozJ5Mfyhyl5vG4U2GUJlYjoLHgikWKjBAGSJ0MGf2iQBiUlcoh5R4TqPHCB9+M9eHLp4K93bWLT3PKb9d0JdHqlT1KXH/LX9jIvtM0JGX+m0GhUAifZfuNdr5ClbTlxDTLYIlMybXC8uLZo0Eto4Ogy9Fa9PV/Bw4sodlMmXLcNzR6Qn6vS1O7hyOwH9f96CEQ9XxUM1CmP32cwLfgK5N2O/k15u0zD1BqViUDwma9C/FkQBEQT9+uuvvnz6gDCsQ0W31n8/qSfiEIEk6FBBc0rN4fGGHyPGqN/N4j/DCWNBdV3GP4sz1+6qD2i5MAgibzucgQNrX6r74RLcjEvC/g/aq0BD5qxIqt1pz9ZH/hzunRzxlP3nrA+0Ony5Gstfa+6RbU9ecyxDj796O9HtAzNXj9n6/bQZu8/EYsVQz/yt6XXyyh1zb6Y/DpN+ftoWNST6k2418KiLgagjD369WhXNNXnpl20okDNKZY9zR3p6GzLzWD49saGngosrt+MdBkES6EvSkZIx2aDVpi+Adfe1loLGVoJ0rhP5L7+aE0Sp5c4W4eYjNBiT1B2fJj2G5xKH4PXEfuif8DK8ZWXkEPTVzUcJzXm7E3PlzLFciCjF4Ys3U/VSSQAkTDWDZM7KgQs38fm9+QnOki7I3CdXJ6/bs+fsDTQbu1zVP3HGUaY2mUdxw40q974aFiPFLF2xZN9FnI+NUynMHen41Wq89ntyIoDMIskM3JVWj4ynMnrKdkxzQn/IYEArLAMgk+UHOOc0s/yw9rgaNmw6eelIeuMUe/9lb8/Znc6tEXkGgyA/Z/rAmfl8w3Q9eqa+BeYbGsCbhodPU8GQCEcSsiFlzHnFdxaoy7rDjg8miHw95Csz2ab/lQxdrT9bhaYOagPZHjzEJxlUYFTh7fl484/UleglYUG7L1apM+npJXVPZFiT/HbXiSu31TyKGiMWIaNSnSl2YMHu82qeUnqyeHnKnrPJc4QykzdDRXcPdh+f+J/V7UMXbqohhP5OTsr5Q0Fb+V+X4PHKrXg7QzGtX0hHca3MQUpvRkrTyZXf7s0dkueV94enguQdp66nSiBiOtFjz+0AGrpMIRYESX0emc9jsnHjRgwePBgTJ070ZNsIQI6o5BGL9UvlwcEPOyDQrI58GXuinkF2JJ/NNnn/770+axORIzLZvcLbCzD8L+dnKKesPabO/MsBixwkyAG4ZBhLxeZgZeXBS6g8fCE+XpAyEX/JvUKajg74F+4+j802aY6/X3NMHWD+uin1ZOfZ2844PJPuKnd6a0fN32d14CXzAG2HcMnBnbOeKUfHWbU/WOxSmuIXfnY87Dc9x3ASSLb+bKVKAuHp7GxDf9+hUjzb65UaMnM75tzbf7bk/8PZAakE11KHxjZrW1rBjL0tzt1xVv2PW5KeStu05ZYs9/vec7Fo8/kquMOyJ1QCSk9xtv/lf7L6e4tStdVTw8/sBStH770vpVdx8d6UIroDp2/FiH/2qqxylvpO3YSeP2x06fkkc9+oeftTJQxxhW1P9Hcrj6osd2MXHnD4GNsESKevOT+pkFYCEY3F+6TKuwvx0bx9ababyOtB0JNPPonly5PPWp4/fx5t2rRRgdBbb72FESNGZKhBlOyjLtUwuHU5lM2fA4HqOd3fKKhJzui0MPIN3K9NOWstw3pi4xLVhNe/d5y1+iB+9sdNDg8EiDLTV0uTizr+tP6E0/UkiJcDta+XHVYT1SVl9If/pv2F/f7cPer3ODeyTUmq6Ee/S56M70rwIgew3jRh5VH8/N8JhwewrT5boQ7uPl3k+GDKmUmr7ASXaVi2P+XgMj0k853MJeszxXMZ8EzZ2X7fchpfLkldPHTm5lNqvsvg37anum/FgYsqILQdbie1cqSAbLfv1qHruHVqKNPjE9arYMleoGWPvQP+Qb9sQ6zNWfqq7y7EQ9+ssUo77ykyMkC2b/s+dKWtGbH33A0k6A1eLcD78cIDav889f0GNdfs8r2en/X3gn35PLG0/MAlq9tpvY9+sAhe5aROtffSTpRkL5vrmHsnapx9VtX/aGmqIEj+Nqm3lZFh7yPvfZZOTMd7nyjTg6Ddu3ejfv366vrMmTNRtWpVleJ6+vTpmDp1ano2STaebFAcg1uXRyD7X/gv5utFNFcwLWK01f1yBm7+7vNqwqvJdyuOqPH3ciDgqW54IntcPUNqSXo8LP8vZWjabZseDlczjtmjcWUQkpNmuxNcOQs6zt1wL8X9yat31OvSY/J/mGpTGylRn9zg/4457tG56OGaM32neqZos+3ZcU99Itkbhuds6N+X94ICy8Bk5qZTql6PZD+U9OGm7GBHL9/Ge3P3qFo6rgw7Tm8tqylr3XvcpuNXVW0f2941GYr25GTrtN2+ZszA8FaXtm/xBNfdmDsn5MSLq+T/wPbzKbPffzIkVOptudp7RRRwQVBiYiIiIyPV9SVLlqBTp07qesWKFXHuXGAUjSP/ItW1X5yxFd8sP2x1lklSpBJ5kpxplQOXxmOWWQXgwlncLZXea7y/yGqezI27qQ9cX/51u1vzKlwKfNI40DGd9ZcEC646bxHomOZByQHpyHQMQZG/Qeq2rD3s+boo9naJDB0zBbEy98STnJ3BPuJi78eC3efw/t/JvX6W9dNM3D2/Y+8/5PU/djpM7PDjvZ7MsYsO2B2SZfl3eKvnUMokSN2sxydYzx36x4OFZuV7xNWTGVtOXDUH6Bnx838nHd53ITZ1cOHKuz2jJwBl2K30EqaX5WeDaUje2IWu1dKSuln2hsU6YlnAOqNp1Ym8EgRVqVIF3333HVavXo3Fixejffv2avnZs2cRExOTnk2SlzIordFXwYsJL8FXfo94D121qceKyzho2y9DGdtuW6lbJpGvdXB2UzIHSVE70xeIq5OqKbRIimFJNS29HTIU09UDDplrcydBj38tqsDPu1e40XJcvztkSGhGU3hLz8uniw46fE+M+HuvuQdLhlWZeiHiE1MOoE0vQUY+Yf50UqzRlrw3B//qftIFk5ojFuGR75JTRa9LoyDl1TvufQ6Y5lSZevos53qNdrGo6gs/b03VU9Lz+/SfGff0wWGrT1faXS6B5e4z7tXhkffQ0zYFWJ25m6hXWe5M//c37rreE5JWL6vtfBpHJEB9ZPx6dQIho0GIuz1Brmy53sglGepR7vXDRkxbb7+nTupxpUWCHkuS9e/b5cm9zGn9K+5y8/9HkrgQBVSdoDFjxqBLly4YO3YsevXqhRo1aqjlc+fONQ+TI8/zxOgwIzT4x9AQ/8Q1xPGoJ+Ft9bQHUS/iIP6Ma4pcuIkKmtPYYJRaSBqXzpJ3GZd84LPprdbIlyO5N9LENH6/UqEcKrPVu3P3YGi7ChjYomym/T0UmD6wOBCQYUSVCuVM93vMck6bZYAk0srUdP+Y5VYHgZ8vsR/M2LIdcmY6s2/7fjG9J0rlzYqiebKab3u6xsy8XedUCml7qbZNpM6OVK3vVreYmt+SERKMSq+CSCuNt+Xkc+nl2Xz8GuqWzO1w/XibniBX5npZGvmv/YNMy0KwC1zssZCharmyhmeor9DV+EmSBMjwOnfZ9qa6ou29RASHRnZwq26Ou8M07ZEgz9R746jAqDtBp7tDyc7YjG6w15t3+VaC6lF+uGYRpJd8Fklx3/TU45LEFumVkR4oooAIgpo3b47Lly8jNjYWuXOnfJk899xzyJo186oRU4pBrco5nDzqzG1kMV+vFjcZxTUXcM2YA+uiBsGb+un+wVvhM9T1AQmDMM9wn931/th6Gk/UL4byBXIgOku4VXYsySDz3VN1VDAkB1eWX5QSAAnJbFO9aLRK/zn6kepqO0S2gYocHEtl+vRwlhZ54R7HE/Tlf9ids+AZcfZGHE57MB20LXsBkGkIlKWhs3a6nfo2rbPyMh/GVZI4wLKnx+7zIWMmrT7mcm+YTqPBT+uPq4NdjZ1U46bPsRi368XB7aDebnbDTCY9bZk9AMr2ff3g1ynJHdxJFOCIbSKDtEiAY6nXlI3qpJ47Vh20TpYQLDgYjgJiONzdu3cRHx9vDoBOnDiBL774AgcOHED+/Pk93Uay84XWzc1q3K8mvIDdhpIYkfi0edlNZMUeYykkICW48BZTACTa6JwPYZADKZmLMd/iLPt3K4+oLFRvzdmthsy989ceh1/8kpFp68nreO4nz0yWpuDT/otVKh3svvOxDg/EPV2o0bI3yh3Hr6TOZLX60GWcvX43VU+UM5a9VM5OfEsacJk/kd4eG1tS9NVT3D0YTCsA8iYJBv83Z5fK/td0bOoaUZYBcnoDdOlleOW3HRkKZAOdpDp3l6MeIk+x/IqyTWnuCsveTXd4uzSFnKh053PP9DmU2a8/UYZ6gh5++GF07doVL7zwAq5fv44GDRogPDxc9Q599tln6N+/f3o2S5k4J+gPQ1P8kdDU7n1XkQOXjNHIp3FvLK+n5MFNvKT7E3/om+Is8jpc7z2bicZiy/GrqQ6EHL1OV2zOwBEJOdt+/EryEBUZRmk7R0ICI0n9KoGGP5CA3p5HxycPFXXkks0k7dUWc+vMc4LsvHUajlqm0ghPf7YBskem6ysjQ2xTBFvKjAxU3kxKaVs80kRqIz1hU3w0PdwZ1qRNx7wjmYCfETKUVHqDyD5PF3GVdOoZIXNu3SEnIO31gBMFdE/Q1q1bcf/996vrs2bNQoECBVRv0E8//YSvvvrK022keyLDdObrtvNhMsIALRrGf41ycT9hYpJn5wq4opluJ14Nn4XfIj5Qt5tod6nhcrYDU+xl2nEl/afJTVagJjtMc0tsSa2LL5YcRMev1uB5FydcOyO9SZK+ODOHvDnzp0UviGRdtBy7bxpOespONkYJgDxxwOtrjiaK2xq3wvUUxBlhO/nckicCIHdcvBkHO9NHXJqAnxEyPNFUjyYYrD9yRdXmcZejIZ/V31+U7ra4M0zU1fhX9pXtcD7pgSYKVOk6rXfnzh3kyJE8t2LRokWqV0ir1eK+++5TwRBlDpnkuH14G3XwEhWeEhB5QtK9f4WPknpglaE6fo4YBW8rpr2E78I/R3vdJnMSh8l694MylhciT5BaF56047RvelrtaTJmudWkaUmT/Hi94g6zhgW6d+bsxjSLgq7O2B7kZRbLRAlChsX5igT5NYrm8slzeyJNtb9Ib70jR5lM7Q0ndYe7NYjSo9HoZR7blgRja/ykx51CQ7p6gsqWLYs5c+bg1KlTWLhwIdq2bauWX7x4ETlzJmdZosyRK2sEYrIn9wKVzpst1f01i2X8i2yNoRp8xRQAibfDpyMM0ntjREvtVpTXnIIGBkQgEb11C9BJa3/4j7M0tlIPZdS8fWq4CZE3eSsJgqv1Uyx7guSssas1cAKRqwFQKPeGhkKJFn85QWabNXKFkyGf9hy55FqPsitza5buu5DhYXKeImnTn3Ij3TqRT3qChg8fjieffBKvvPIKWrZsiYYNG5p7hWrVqpXhRpFrZvVvhHVHLpuLN/ZuVBINSuVBfzdrldhTPW4idkY9B187HNXT6f1XE3KkCtpsq7xbktodE1YdVZelrzZDmXzZPdZWIn/lykHTkjQmW1+/k4BXf3dtkj0FHtu6RsHIT2IgVHzHOhW5u++ry7fcT6bgyDM/+k/CIFMtIiK/7gl69NFHcfLkSWzevFn1BJm0atUKn3/+uSfbR07kyRaBB6sXNt/uVreox4rqxSIwggN3h+1Z9hIF69Af8j9y0NLPjw427JFsi87M3Hw6w4VdiYiIAjoIEgULFlS9PmfPnsXp08l1MqRQasWKUviSfMdfznV5T1YkTwjvqVuIpRGvohA41I38x7kbd1H3wyXmBAP+ytUCnkRERCEbBBkMBowYMQLR0dEoUaKEuuTKlQsffPCBuo+Ca8zzGWMMysf9iK7x78Ef7Y3qi0G6PzEi/EeU0Z7DMIsaRK5UZCfKTJJimoiIiIIgCHrrrbfwzTffYPTo0di2bZu6fPTRR/j666/xzjvveL6VlK4A6PjolMxqzSvkc3t73yU9iK2GsmgR/5kqqHrK6L+FcIeEzzJfj0ASSmvO4sOw71FUcynNGjHBWn2biIisU3ITEWUoMcKPP/6IyZMno1OnTuZl1atXR5EiRTBgwACMHDkyPZulTNS/WRm3M9CMTnrS6vZtRCEQSIa5etr9iNHcRD3tAbRL+DjVOg00+3DcWAAXkEcVXJQkCdIr9EKzMiicK4tP2k1EREREftwTdPXqVbtzf2SZ3Ee+UTR3FoczgixrgqTXHUThsfjA6OmTAEhU0J5WabbbaDfj+/CxKIzLKgD6LfIDbIh60SpJwk/rT+CFnzNeFJOIiIiIgrAnqEaNGmo43FdffWW1XJZJjxB519o3WyIuUa9qCGW2jcZKGJP4BHqGLcKj8e9ibdTL8HeWabZb6ZLTiTuy00FBywuxcSiQMzB6woiIiIgoE4Kgjz/+GB07dsSSJUvMNYLWr1+viqfOmzcvPZukDCji5eFb4/WdMF7/kNR3RpP4L7AmcjCCzb5zsXj5121oWbEAvluZXLtgaLsKGNiirLq+5cRVDPplO97vVAWtKuVXlb2zRabr7UREREREgTAcrlmzZjh48CC6dOmC69evq0vXrl2xZ88eTJs2zfOtJJfFZMv83qBkycPrThvzY1DCQAST+bvOocOXq3Hwwi1zACTGLjyA3zadxNnrd/HI+PU4c/0unv1pM56ftgVV3l2II5eSa6isOHARJ664VtE7EFy7nYD35u7B7jP2e8n8pQ6PsyK5RERERJY0RqPnkirv2LEDtWvXhl6vhzfExsaqNN03btxAzpw5vfKc/k5251dLD6Ns/uzoWL0QSr75r1r++wsN0e279ZnynOFIwvSIkaivPYBA0z5+NPYbi7v1mByRYbhp54C7T+OSeKBaIfPr/Eu/+zB+5RF88HAVlIjJ5pH26g1GPD9tMyoXyokhbSvAG176ZRv+3nE2VcZBf3H9TgJqjljssH2m9wAREQUWf/zOIf/mTmyQ7mKp5J80Gg1ebl1OBUCidaUCKF8gO2oWy4Vfn7svU54zEWF4LOFdv60j5MyCyDcRgUS3HmMvABIaaDBrc3LhYNF90n8q/fagX5LnIR29dAsTVh5B49HLMOLvvUh0sXjm3rOx+GdnchCy6tAlLNl3EV8tO5xqPQ+ez7By4Hws/Nmes/7dPiIiIvI/nMQQ5Cb1rKPqB2m1GtxXOiZTn2ursTzui/sa/0W9hEDSQbsBfxmaOF2nvOYUrhhz4gqiHa5zMy4Rv29JCYJMzsfGqd8tP11pXvbD2mMI02nwUPXCalhd+6oFVbB06todzHqhkVU2vwe+Wq1+588RhfjElMDp0fHrVHD79oOVse3kNfSdugn/e6ASutUthvQyBVISTFsGd/4sk2I/IiIiCmLsCQpycjArAZC3nEeMeY7Q24l9UCFuKvxdM91Op/eX0pzDosg3sCWqv9P17AVAwmAEvlp6KNXyiauO4qFv1qi03At2n8fcHWex7eR1bDlxDVdvJ6Ra/8AFSfudcsS/+cQ1TF5zDLtO30CXcetw7U4ihs6y/lsSklzrbVLtNBjVdqRukmWvkkU85JeMDhPDExEREXmgJ0iSHzgjCRLIv7WokA/L0yiaWrVITuw+k/4hRnMNjbEwrh7i4a0kDRnTVbdGHUa/nvg89NCpw+qSmvM4biyIj8K+R21t6gDGHZduxuOzxQedrmNZn+ixCclziv73QEXEZIs0L4+9m4h35uxO9VgJpGzdSUhCg4+W4mZcEgY0L4PWlQugQoEcTjPYnbh6B9tPJb+HkwxGhOtSRz8ylO+JesXwUqty8BfsCSIiIqJMDYJkolFa9/fsmVKThfzPxJ518cnCA5iw6qjDdXTajHcQBkoAZPKIbo26lIybgeNRPdSy08a8KKq57LM2fTRvv9Xt71akZKpzJklvUIkCTL1A41YcURdx9KMHUvUMSja7f3eeQ8+GJc3LLNewHBonQ/c+XXzQ5SBIepe2nbqGyoWikSVCAswUxy/fxqbjV9G1dlGnxXylV0racOrqHYz8dx/6NS2FOiXyuPT8RERERBkOgqZMmeLO6uSHwnVaDHugEh6vV8xqjkrurOFqOFWoOxCZEsTbC4DqaA6goXYvxukfhsHLo0kdJWSwVWn4AiTq7XePJBoMiNRaByO9p2xSv2PjEq2G8J2/EYeC0VF2ZwStO3IZjcrkTbMtP64/jvf/3ov6pfJg5vPJNcVMmn+yQv3eduq6qrck/5uWluy9gPf/2YNTV++ie/3i2HsuFjtOXceCPeetMgaxI4iIiIjcxTlBIap0vuz4a2BjNK+QDx8/Wh2TetbNtPFF05JaY5m+Jr5K6gx/F6lxHmj8Efk+Xgv/HY/pkg/g/ZGjACgtC/dcMF8v//Z83DdqKdYevgx7HYOj5+9X6bqXH7iIdYft95ZJr5EEQGLjsasOn3fGhpOqMK30Gn27/LB6TiE1mCQAEr9sPImTFrWXpG7RjXtBu9QxciSzMuYRERFRYGN2uBBWo1guTO1T3zxvJbO8k9TXfP2IoTAe1P2H+fr6uIYcaK3dih5hSxFoymiSU1YHmsmrj6F/szJqrlB01nDcTXBe02vwb9tRMGeU3fvenrMLv2w8ZbXMcridzB9y5Nhl62Ky83adx/xq51VBWrF3RLtUj7HsqZy67ri6DGpVDpNXH7U7JHDgjK24dpu9m0RERJTJxVIzYvTo0Rg2bBhefvllfPHFFy49hsVSPUvO1meL1OF/s3erYUfeYTTPwQk0/+jvw6DEF70+LM5TWlfKr2oOpaV03mw4ahO0FM+TFSev3rEbWMv/TmSYFvE2mekOfthBzf2RGKnUsHmpHtuwdAzWH72CjJCkHhLg1SuZB7McZOsjIqLAwGKp5C53YgO/6AnatGkTJkyYgOrVq/u6KSFN5m2Q66RHa5a+KbYZyuIGsql0AvlwHU20u/Cv4T4kIBz+zJUASNgGQMJeACRMwbNtACQOXripaiHlyWY/aUZGAyBhymp44or99hFR4HtIu06dfJLPWSKi9PL5Kexbt26hR48emDRpEnLnzu3r5pATpjlElGJqxMfYEfUc3g37CcU0F7ApagA+jxiPg1G9sC+yNzpp1/q6iX7jwa/XqIBK6hsREaVHDtzB1xHf4NuIrxCFzBvGTUTBz+dB0MCBA9GxY0e0bt06zXXj4+NVN5flhbxHhjrlzpq5qa8lLXUg6hO2EKsjX7FalkWTgK8ivnX6uAK4qnqPiIgobdGaW+brkXBvzp8GBlX82lFOSbmvh24JwuBaJk4iCmw+DYJ+/fVXbN26FaNGjXJpfVlPxvmZLsWKFcv0NlJm06B3wlDzrVbxn6B1/McIJlkRZ75eRnMGT+kWQwc9siAOG6JeVL1HWqQePkZERClKa85iTeRg820NjCiquYT3wqZiYvinyA97vcxGNNduU/eNDPseyyNfxTO61HMShdw3MvwH9NItysS/goj8hc/mBJ06dUolQVi8eDGiouxnn7IliROGDBlivi09QQyEvCsz8mhsNaQU3kyCDoeNRbFQXxftdJtTrdsp/gPMjXwHgWRvVF8MThiAz8LHQ6tJfv0Ghv2FQpqUtNENtXuw1lDNh60kIvJv/XVzrW5vj3re6rYRGjyfOAT9dP/giLEwcuMWKmpPol+YddDzStgf+F6fPOE+G+7iNrJY3d9MuwOlNecwR98Ym4wVHbTGiOd1/2CnsTTWG6p45O8johAJgrZs2YKLFy+idu3a5mV6vR6rVq3CN998o4a+6XTWRR0jIyPVhTJXvuzefY1jkR2PxL+LRIRBj+R9vshBECRfcoHoi4hxVrctAyAxPWIUOsZ/hD3GknYfnx13cAtZM7WNRET+rFvYKqf3y3dGW/0mvBU+w6XtddT+p+YWTUzqiI+SUrKUNtXtUr+lfEPJOPvbaqfdjGHhv6jrpeJ+htGNgTWRSEAd7UFsMlRU33tEFGLD4Vq1aoVdu3Zh+/bt5kvdunVVkgS5bhsAkfd82LkqmpXPhym966W6z51+oOEPVnZ53S3GCthpLGO+/aehCXokDMPMpGYIFf9G/s/uK3y/did2Rz2L4WE/+aRdRESBYmLE52muk10ThxKa8yoAEs+F/etw3cba5IBIyBBm+TyWOUPFNCnZNR93UDw73MHcok/Cv8OMiI9c+kyXeUwDdXOs2kFEAR4E5ciRA1WrVrW6ZMuWDTExMeo6+U7B6Cj82Lc+WlTMjyZlkxMVVCyYw+3t9G1SKt1tkLNqMjzs9STb4Q72vZ3YB8HgcOTT6ouzhXYbZkW8p+YPTYsYre7rG7YAMyPeV+Pa3cmk5F7oSkQU/FLP+7H/OSm99C/r/sCjupXYFvm8+jw+HNUTlbQnzOuMDp+Mh7VrrB5XXXMEh6J64tWwmam2+ZDuP/X76bAl5mUyL1QCneTP7BTttZswNHymageRI3N3nMX3a45lypSFYMZ+WHLq6+618PuWU+hcs4i67ev3l6PhcKuDZD5NmMagvjhN6moPWt1fX3tAXd5K6ovKmhO4bIzGRdhPLV9JcwLzI4dhsb42thnKYYOhoupxIyIKdXJSydK/EdITb98r4X+kWvaIzjro+TJiHJ40LMOYxCew1Vgeb4VPV8tfCpuj5hYdMSZ/h9rLSHfMWAhfhH+LTrr1OGOMQeP4r833F7focSKyJy5Rr2rwiftK50GVwtG+blLA8KsgaMUK+13K5Du5s0XguaYpw9T8iaPkCaHgeFTK+HXTmHUZNmE5Lr2XbqH63Ua3VV0s1yUiohRVLHp20quBdj/+jHwPoxK7Iy9umJcvjRzq8LNXMtK9mvCCCoBEEc0VdNauwW1EYbGhrsqAZyI9RWkl0Hlct1z1Wr2a2B8njQUy/DeR/0vQp2SXvRXH9O4BVSeIAou9jqBFrzTF7AGN0LZyAWjuddRkCc+cOV03LZIDDEp80Xxdb/OvXD1ukvn6IYP9M3CWPk58DIGqtuYgVke8jL2RfdFGuxnttBvVmPWsGhYSJCLyNkmYUEYr9YhSPKD9D5+Gj0NNzeFU638a8V2qRDqTIj7Dc7q/0V630bxchsTlxG1siByAsWHfIQKJViUYWmq3Ykz4JNTTHsSqyFdUHTqT3IjFh2Hfq1TiBXFFLSuquYiymtN2/gIj3gj7Bd0czHXKTHIyL5/dVOdEQd4TRIGpfIHk+UITe9bF8cu38fmSg+jf3PO9R58mPqrObEnAch05EI8I/JLUAtk0cThtzIeV+upoptuJFfoaiEU28+O+SHrEPAHW1m1jJKrGf696UAaH/YEIjR6BRs48msgXpzPyhRmHCJTVnMFBY1H1lSO1i57QLcc0fRueOSQiygTj7n0H2Q6jc+Z/97LPWdoZ1c+cKc+ULa9h3NdorNuNT8InWK0rdei+TuqMT5Mew5SIsaipPaKW58BddE9821xzqUbcRNxAdvPj6mkOoH/Y3+r67/rm8KZx4V+ig24TeiW8gZWGGl597kDl62kKgYxBELmlcLTzmk4l82bDl0/UypTnvozkca7j9J3Ny4YlJX8hiJcSX0J7w0Ys0Ne3etx5o/WcmWOGAiilvaCunzLmNw8hKx8/DcejnkQwk5pFOw2lUF17TA3ZuGjMhc8jxqv7OunWoUG8dSpvIiLyb+ujXnJ4n8xJek73LyI1ieZlDXV7gZSbaKPbgiqa46q20u/6ZsiluWU1t3SfsUSmtT0GN9R3z2x9E3VyUwIg8ZzuH7tB0Kh5+3Diyh181b0WIsI4mEmxCII0puE45BIGQeSWl1qVw6Wb8fhz2xmX1v+jfyMcuXQLr8/ameHn3mOwX0PHRHp/ZupbmG/3SRiq6vF4IhnAXWMEsmgSEAwkABKmGhcmBTTX1fAKGa6RgHBsN5b1UQuJiMhTLAMgk07atebrlj1IhTVXrAqYS3IdGX1hefLRkyZHfIpa2sN4N3waWsZ/Yl5eUHNVDY2TSf9R94bXxyfpMWHVUXV984mraFQmOXttsLgVn4RfNpxE0/L5UCEdGXnJfQyjyS3ZI8Pw2eM1XV6/TonceLhm4Qw9p3ww9k543aqOkCuWG2phhr6Vui6TUjcakoMhy0DJctKprbPGPJiU9IB6bJuEsQgFB6N6YWbkB5gTOVwV9Gut3YJG2t3Ih+vQIfCGChIRUWpfRXxrd3kz7Y5U34qvh6dO8+0pEgCZLIt8zXxd5lTNjhiOJENKawwp8/+RqA++MWCTVh3FyHn70O8n9xI+GVkGI93YE0Tp0qFqQczffd6ldTV20lpXKZwTe87GuvT4o8bC6pJRMsa4uuYYNhkr4I3wX52uK0Pm2iaMNVfzljlHnyU+iiHhs7BKX81cUTyYHYjqnWrZbH1jlfXohcRXVK+RFBmcpW/qMP0rEREFVpa87yM+hT+oqT2KWxk92I+/CcTFAjkLA2e3AXnLAZH+2cuy8VhyIouTV+9g4qojeLZJaWi1aQ9v45yg9GMQRD4xpE15rDp4CT+uz3haUlfdRRQ2GCs5rTvUJP5LVNMcxXyDzCuyvu9rfWcsNtRRCQXq6Q/g14gPEWq66JKHUOzRPWNeJhNoy8X9ZA4YiYgouEhWubyaG3g8YbhXP+s1GT3YH1MSMCQB7UYBC4cBMeWAl/yztIbldJ6P5u1HxYI51dA4yjwcDkfpkpEzD7WL51Jv7Pw5nSdZyExvJj6Ly8acqpaCJenxmW9okCoAEpJAQSaI6qHDf4bKXmyt/2uo3aOGz2W/V+1cqp9bklSvA3RzXNqWjANvot2lUroSEZHvtdVtQW3tYbTQJhflTE1OKRqQCzeR4973gKel67BDAiAhAZC4csgDDTHizundePP3bfhjy2kXVjfim2WH8P2a5Pm49py5fhfrjiSnLjeR+dcuNQeZS28wqr8hGPHULXn17EblQjnx54DGqdbZ+FYr/LHlDMYs2O+VNv2qb4lf1dyg9GdSeTexF94P/9FqWaf4DzA38h2Emp8ixpivbzBUVEUDRf24b1Xl9Id169TtX/QtcQ051fyi8eFfYKehNL7Rd7Ha1qO6VRgbPlFl9bsv3v649cwgQVw+zQ0VCBMRUWp9dAtRTHNR1exLmV9rxMbIgcivuW5er2Tc9Ax9v5o5OPg2HZRfv5OgssRljfDC4aw856K3gf3/IOu146ivb4IhWwbgkTpSbsKxU1fv4pNFB9X1rrWKqCL0tt6anf4h9pYBirvJ4bacuIrXft+JlxvmQWf9QqBGdyA65e85fyMOHb9ajVrFc2Fyr3o4eOEmXvt9Bw5duIXqRaPxQ+96yBYZuKEEe4LIq0rmTSl2WiZfSl2C/Dmi8HRD6zScbSoXwNJXm2ViazL2AX3VaD2uWGo1SPKG1vEfI5SZAiCxMWqgOQAS0Zrb6vcrYbPUmcXXwn9HZc1xq3NZ7bXJKVILapIL5knhV89L/cW6NPI1rIl8WaWKJSKi1CS99jvh0/Fx+CR8Hp58kkrqzlkGQGklHXKHRmNMmdtz+5J5efjdy7h2cB0afLQUbT5bpXorXLZqLLD+2+Shcn8+Z38dycIw4wlg3tCUZWe2Auu/Aa4lf0d01a1RvV+Iu5EcIK0fB2yT4M9aXFJKUqFEvcF+kw5cQGFctvnbka7kCkNmbleZ9MykbXet98/aw5fxwg9rcPaHnqh0dSmiF7wILPsQmNLBar1l+y/iyu0ELNl3Ud3+YslB7Dx9A3cT9dhw7CqqvLsQv206iUDFIIgyneX7eEiblHTV7aoUwHsPVVZptE2Z5/aOaGf1OAmU3mhf0SPtkEx1njTP0EAlChie2AvV4ibjHGLU8sPGohiQMAjfJnVSvSKUYkXkq6oK+Ythf5mXzYv8H45H9UBj7S7U0RxAK13KcIt3wqbhQGQvVNckF/mzlvKlJ0PnslhUTrenpOYcntQtVXUv5PmWRLymvsQ7aDeoXqCimuQvoHa6jain2a+qrxMRkeM5olJbb0nk66nui/DQySvd/n+ACU2BUUWR/auKyHkvVULjOQ2Re0YHVNHvx/XrV5Gwaw6QeNe1jcrB/sL/AXevATt/s7/Oue3AwfnAxonA7/eSBMWnHqL9W8QHwOjiwIYJyUPu/hoA7PvH4VM7CtUmhH+OdVGD0Ebr/nwlzc2zKKq5hK7aVVi69yz+3HoGKw+kBIz4vRcwpgSwYjQQn/z69f95C0oemYaHNKtVId/7tfd6oq5bBzS2ySikZ8jWG38EbqKowO3DIp9qXDYGC/a4lh0uTKfFsA4VcTs+CWXzZ7cq6tW7cSmrde11aWcJ90ys3rhMDLacSO5dSMu8Qfdj9rbTamLiq7/vsLuOzA16JXGg/ccb7lMXk2AvwuoOGepmz/SIUamWPRM2X/2WIYYGowal45PPskkgZdpOu/jRWBj5JhKMOlXw1lkAZqms9izK4qw52YPJA9qNGBSWPH+pQdw3uIA8drcXhXi8HPYn/tY3xF5jSRVgSSA3OPFFJokgopDWRbdGDTvPqMjZfaxuN9LuxWpDNfPtPyPfS74yWzL2tAee+AU4uQ7IUQiY3s21Jzm9BYgpDdy5CsSUAU6sB7Zb9OjsmQ10m2q3a6a+9kDylQVvpCzcPgOo9KD5prPpNElJehy+GKsK1opnw+ZhcUJduGzLj8jz9yCsiUy+mTvxJr7Xd4Q+IQ5Y8wVQvh2w995JxxWjgFMbgKdnIzYuCXnDbjjsubt4Mw5nrt21m93XUnvtRlTQnAKMD6Sv68rH+E1N6fJkgxLImSXc5d6V55u5V+NHeHwanotv0MMjO6jArXLhykjSGxwGQe6YmNRRpZMWPye1UhW5H9RtULe3GsqqCafknFZjREFcwXnEWAVSEgCJCI1n6hhJcGSST3Mdl4y5EAa9KiAbjVvIhjg8qFuP/90rNivZ8arGTcZH4d+r2+sMKzBd31pdL6a5gCRjmLmX0NRr9UbYr5ipb46txvIO2yGpyKM08ThtzK9uy7ALyUh4zZhDpSgnIvJXo8Mnq8/Nn/VtPLrd7yK+cHznwQXAiHSM+JhsEay1HQkseiv1Ou9FA2WS6w66y2iUIXBGVJRgIUGGhEcB++epHqmwi3tgOV4kl1VScCeS4oE/ngX2zbVaLEMVXwqbg1x/JQ89x5J3rR93ZBnwr9Rjam6VHdf26KjruHU4fe0uutQqYjX3yOhofxzrCZRujkDDIIjSRafV4OGagVUbppTFfCRnpIfK3vWM+CipBxbo66Gu9gAm6Tuqyf+mIKhrwvuorjmKatpjGBn+g0eeL1j9F/WS0/slUCivOa3SmMfgJjZEDsCQxP74x9Aw3c/5T8RbqKw9gcpxP2BHlP3x44sshoPIPpReot/0LbA6MjlY6ZcwBG20WzDX0Ag/3+vxeiJsBebqG6KRdg8+THwKcwxNrLa5OSo5c2GNuIm4gewoqzmbMt8qdQF4M0k68VX419hqKI/v9Q+k++8mIsqID8OnOAmCjIh09kHmK/YCIJMjS13bRsKt5O6f2LNA9gIoOasDjkftUXcl/TwRGLwN+LW73YdaBSYaB3OV7lxO7t2xCYBMct2be+vQpkmoqSlm9VxyktGSBEBix6nr6KObj3fDp8H4Q0NE6YfY3+Yti+F3AYRBEPktT3esPlyjCF75bYebdQk81x8lZ/236pPP/EuB0ZvGrNhhkB4yjUqosFNfBocNRVBJewLvhf/ksecNJceinkq17MuIcXjd6GDcdxr+iXzbfH1t5CCH6xXWJBe5szwbJ4GuyaSIz9Tvx7DSar1OuvXq9xcR4zAnzjoIMimpOY8dxrLqrKolmQMlta9EOJJQR3sQ54x5cJ92HzrqNqpLchAk/8OBN0yByF4R61LaC75uBrlhoG4OjhkLYpexFE4ZC6hlWRGHvVF9EbSOrQQWvAls+A6o0hVRV5IDIBF2/SiOHj2E0g4eWlF7Sg2fH5QwEMVPHgPidUCppkCBKsnJDWRujwfIaJTn7o1OcUYCMQmA1PVT6zENT+BaZHYcMBTFLqPFXxGAQ+EEgyDyewUs6gm1rpTfnKUkLfVK5sam4ylzgCwrL0tqx3cfqoy528+ifdVCKBQdhQ//3YcnGxRzqUJzRknNoQWqIKs1Kea6QV8Jf+rvx86ofpnejlBRRGNdfyE9cmtcHKZwTyXtKbfWl4x5vyS1UD2CMi7cxPTfqLUYiNBdtxSjwr/HG4n98Ju+OQ5F9bS7TTnYWBDxBtYbquCNJAdZkCxIfaeKmpPYbywOA/PmkJ9ZY6jGICjADA2fab5eMW4K4hCJN8OShxIHNQmAxJ4/U91V+qe05/x8FfEt4KgkkwdMjXCSxXZCM9TUdMUZYwx02pR53EJOxkkpiXy6G2iClOAuUDEIIr/XrkpBvNCsjMpTL9elLsDk1cfwzXL782j+GthYFR0rkDPSKgiy1K1uMdQpkUddTCb3cm0yYu9GJTF13XHULZEbox+phtnbzuDb5fayl6W/mOyOkyln/W8YsyJakznF58h/SJIFudgaGDYHyw21zHOOhARAYkz4JJTSOE5Q0lH3H4prL6G4dkWqIEhSj8vQwb3GEuZQ639h0/Fs2Hz8kNQeI5LsB1auMSIH7qpaIkSeYjl8hwLP/qg+amhwz7DFvm4KOXNuO+ZEbldXu8LFURR/PANU6gSEpa6B5M94qo/8Vpgu+QtPembe7FBRBUAiV9YIvNYuJdW2rRrFcqF/8zJq3pKnrX2zpWrLd0/VwQ996qFs/hxWEwc9YdgDlVTmuRcSBuO4oQC6J7yNIQkvqPsOZ6lmHhZia5+huEfbQf6hjW6rVQBk64Wwvx3e11u30Oq2VHOXFOHis/DxKj3587qUdK4SAIm+YQvU76qao7hfu9NqG/lwXQ3XWBXxssqGZ1JDcxifhH+HfLiGL8K/xa6oZ1FL44Hq7G6QFLFNLNpEwcVTtWfId0xDgykw/HnjcddXXjwcgYZBEPmd4Q9WRrE8WTCsQyX4k2WvNkORXFkQFa5D+6oFkTMqXC2XQGjj/1ph6zsZz4Lz5RM1Ua9kHtXLJMPlmid8rtIv/2loiqYRv+HP6hPRPeEtPJzwgdXjnk94Bc8mWKeAJqqiTQ54TDZH9sf8yGF4VveveT6SKYh6WfeH3TlR0yJGY0hYypCWpfdSjUsPk6Q1l+Fz4q/I4Woe1Kaogeh8r0CuswDN08ppTuOziO/MiSeIiMiLNoxHoGEQRH6nb5NSWP16SxTL4/pQmmbl86Va5iynQeVCOeFJ+XNGIU+2CESGpbyliubOYnfdLOE6q9uLXmlqvl4yJpv6LUViba36X3vkzBqp5nfEIjt2GUqa71toqIczSP0aCKmvY3LFmMOtv4uCxwDdXwjTJFcrfzt8utVcp6ORPfBKuHUQ9G/EMPN1qZv0ffhYNNDsQ06NdUHCBfdSlPv6zH1hD8z7sqe1dguWRryqesXIt2z/n75JethnbSGiwMcgiILCZ4/VwIPVC+HnZxo4XW/JkGb4oXddl+sbuXsIN/P5hmru0h/9G+L7XvVUcgaZQ2Ryf7m8+P2FhlYBUfkCOdCuSgHUKBqNqkWiXU60csCYevjbo/Gpu6ObJnxuvn7MWMjNv4iCxevhjsd226ZHtdeL1Eq3Db9FWvdAmrTUbrW7XHMv2YJkspO04UKCCelZaqHdpobVyW/1fJpjyIE75mx3P4WPwqHIp1Fbc9Clvy+tgKuRdjdKOJk/5cjkiE9RRntO9YrJNsh/LNXX9nUTiCiAMTECBYWY7JH45knrL0TLsl6l8ib3sJTNn11d0sOVmkEyH2n2gMbm27+/0EgVXD1+5TZqFcuNl1uXs/u4CU/XVem4Tc/hqBfJsndrVGJ3PFpOiwH7q5uXPfHo41i56waaHf/SvKxYqUrocGwUrhpzIEKTiJkRH6DQvZTOLNRKnvBDxCd2l0sV9KO6lLTlryU+j0/CJ1itMyVibKqey38NDdBUt8tcEb5M3DQ1T07e1VkRjzv3UoNbcxwESYA1I+Ijdb1k3AwHaxnxStgfOGwojL8NjeyuIdt4OH6ESlnuGqPKppTEr1qPezrhTWwzlsOHiT1w0phfpZI3FTC2pTdqoLMT6KelVtx3eD/8R/PQUSIKLuwJooAl82fEqK7JyQKcWTD4fvhKmE6LqX3qWwVAnWoUVr+fb1babpD17P2l0aNBcXPwZs8VRAM9/8IyXcoB26N1iqLZ0+9YrSdJHPYZS+AC8qg6DQ3jv8F9cV/j08RH8VzCqygb9xN+TGqDXglvOP07lumTX2+i9LINgBz1Sj2k+89q2Yu6OSrhwoTwz1V9ESkG2023Al+Gf4Pm2m0oprlgDuxNRXMlACmrOa2ut9VtTvN562v2q+x8X0d8o24X1VxUGfRsNdbuxtO6RciLG+p2Gc0ZlYxBerX2RPZRBXobapNTx34T/jV2RPZDbsS68OpQWkyfkJ8lPorVhuSTP5P1HbHIUA+XjLkcPq58/E+oGzce8cbkeZziT739ulyWYpEN45M6WS3rGD/S6rZkUnwg/iP0SRiKg4bAKiBOFOp4eooC1sM1i6iMcZKoIK1ek8gw++v4yqeP1UC/+0ujSmH7c5PkbxrZpRre/Ws3jl1Oqf7cvEI+jFmw32pdCQKlCKx5XpQuHKjZA9ieMu/D1nnE4Gt9V3X98brFcDrLCGzZeEp9kduemRez9Y3xSuJAHNc9md4/mSjdZL6S5ZwlOTNvOjv/8L0kDLZFc68Zs9ut7bQsYgieSRyKJ3TL0FS7C5W0J9EjYRgqalLqOrXVbsLEiM+xVl8l1eNfv1f35AndcoxM6mHuYRLZNPHIhnj8EjFS9Tg9eC+Y66xbiyn6Dhl+HcixxYY6VrerxU3G3Ii3VM0r6UW8jGhcRzYUwHV1vyv9Qnpo1QkkS3uMpczXX04YgL8MycGUJLAZipQEIt70dmIffBg+xSfPTRTIGARRQHMUAPm7cJ0W1Yomz/9x5uFaRfDj+hMod28IX6VCOVWGujPXUyand6lVFNWK5EKJGItEEg98AhiSgIoPptrmgQ/b49NFBzFxVfJE7zGPJp9RlbTjFd62OPPd/Tfg5jnsnPslRiUy+KHA4qi4bWnteSy/l+HORLLcWZIASDTWOS4GKHOmHtGtdnh/BBLN1x0NpC2tOYsrxpy4gewWlXBkIK/9QRrSqzUp/FO01iXPo+qf8DIWGOqpHqxthnLYYyiJCtqTWGuoioK4iiyaBKfzAKWn6z7tPmwxlMNdu0MMvU1CE02a877sBTC3kBXN4z9FL90ifK9/QNWoapnwqVVtIfcLACc/dpOhPOppD6rAWoxPegjFNBfxlyFl6LMYmvicCoBzZkJdN+nFitSk/E9Z+lnfhkEQUTowCKKgJcPQAl3t4rmx+vUWyJ8z0rysQsEcVkGQSDXPKSIr0HVi8tWElMKrfRuXctgrprn3hd834TV80jQMecq3UxkaHv+rEO7i3jZ6/wv82gN46g+gSB3g70EwntoEzaV9HvubiQKFHAg7cjCql/n68PBp+EHfHh+FfQ8dDDhrjLHq2WoS/wVOG/NhYvhnam5Lx4RRqqZTHe0hrDZUQzbEIUYTq9KbWxof8SUGJAzCYJsiu1Jj7LuIL8zzWq4huce5lOYczhnzIA6RiESCqutkGnrYMO5rnEMMfKWDdoOafyN/z2ZjxXRt47ixEN5PSnndbYPJAQkvY1LEpxiZ2ANN7s05s/R+4tNYbqiJWprDuIWUeZkDE15Gv7B/MV3fSt0ek9Td7vNLL1GN+ImqJ9LT+iQOtep1NJE5bKY5UuPCv8RXSV3wVrijeW9EZIlBEAWt9lUKqno7dUq6lgnOHstzklHhvgmqbFOFv/tQZRw4fxP97k8ZluFMloiUoCdnluS3vKlnyZ5lhtq4XqcZ8tybo2RVc7ZkE+BNi6xhnb5W4w6rDpuFRIRhb9Qz0EnAVLAarjX7ELl/YwpbCl71tQdcXvcF3d94MmyZ3fvWRA62un0oqqfL2y2muZRqmSkAEtuiXkCn+A8wNzJ5ruAJQ340S/gC8yKGqax3JuujXsKwxGewWF8Xl1XQ5Hqxaen1ikGs3SBKPg/yIBaX4PhzuKl2hwrohAzHrRbvuDhwRkgihbrxUstEg2raY6nun6pvpwInCaYsXURujExyLbBx1ItnsttQElW1x9X1dfrKmKZvY/7bHTltzIt1hqp27+uR8D/1W+ZImV43BkFErmEQREErIkyLWf3tZ3lypzdJirfeSUhCoWj7Gdu8rURMNqx9s2W6s+iJrrWL4vqdRNS1CBAts+m5mxVPhqKII13no/zRH4FmbyB37hJAr7+B6Y9hTfk3UO/wl4hMuJbqsY9HfYff4l5Q1/cYSqRKzUwUDN4M/zVTtjvMQUY0S6YASJTQXsTk8LFWAZDJqPDv1WWuviEGJb6EmprDyK25ieWGWiiMyyrBxCJ9XZxFXrwaNhMvhc3BtKTWeDpsiXr8In0dLNDXU8WdxaO6leZkGF8kdcXXSV3uZflLIenTf4oYY74tKdVFLc0hnDHmVQFICtNwONcDtNSSH/tlUldVYHe2vomaLxRnjEgzgPGEJxLeVr16d4yR6nnlOevFjUMZ7VlsMFS06kUam/gYvtV3dro9SXjjDpmrNipsErqHLTcHWJ3iP8TWqOTPYKJQwiCIyIXirYHu6+61sPrQJZUEQei0GvRrmpKZzlngk9bhhuW6hvyVgOrjUu4s1RT4n2TP0gHXugK7/wDObAH2/2Ne5cdXHwOMnVB/+Gx1wCNZtogo85jmFDkiSSeuG7OjZ9jiVPe9F/6TGjYmAZAwBUCirW6LuvwZ1xSVNCessgHKkL0kow4/6duorGtNtLvxs81cLFNyiVfCZql5TqZSAGU0Z7HSUAMP6DbCU2Qe1tOJyb0onjYoYSB6hS3CeWNudLRps5wwumW07t2/hFy4ZEjObvdY/DtorNuNKUntcR3Wxa0lcHtGNw9/6xuqIGabwX6qdimkXU17HJ8kdkNhzWU8eS/gaRKf3OP0YdJTFkFQPlxFTvyrr5+qrWKVvpo5XT1RsGEQRBQCHqpRWF3SxY2TrpYZ+cwkABLSM3T/EODOVVyKvYt8Z5eqVLdDVHKLbBjf/wF8ufQwcDJ9zSQiz7EXAJm8Gz4tzcc31e5Mtey18N/V5ePEx50W7zUFQJY9XY9hpXlZxnqCMt9cQ2PMTWis5l0t0NdHEnRpDnkz2WishI1Jleze93nSoyoQEnP0TbDLIlOdpYcSRqrhiVJG4THdcjyJlIBH3LZIgjEwYVDy78TBGJoYh4nhn2KhoR52GkqjZ9gijEnsjo26gfCGKnHf44vwb9FGZ7/4MpGnMQgiIqc8friRNQ8uPzQV9b5Mzqw15N7iOiXy4Ke+9YH3XNtM2/gxaKHdjicfaIkSi/t5upVElE5fh3+VqtaTJWcBkCskuUQgiEfEvcK7RoxL6oQDhqIZ3qYpw90Go/1AKZkmuY6cw89vDUrF/azus8yYJ0WIn0p8y3z71URXiwJ7xm1kweDEgXjT+ItVDyNRZmEQRESKzmJYW3SWcKvhgF8sOYQWFe7VIbKjdL5suHo7AWXyOU64YElSfX/2WI0051kZO38HTdJdICoXMKuP1X0HjcVwUF8MD5VsgpcSXsQNZLOaW+CIwahRBTmJKHM4C4A8IUqTgMCiwcdJT/jkmW8Z7X/GylwkVz8FJZmGzCXzViD0TlJfBkEU/EHQ+PHj1eX48eRMKVWqVMHw4cPRoQOLyhF5mySBmP5sA8Qn6ZEnW4R5+Usty6FJ2byoWsRxXaPFrzSDwWhU9Y9cJckZnKr/HDQ1LVLRFqoBfF3b7qrJZ1uBg6WeRvlj94bqPDED+DX1/KJ3kvpgmb6WyoZFRIHHlDyB0iZ1pOboG2GLoXy6t9E24WMciOrt0XYR+QOfFlIpWrQoRo8ejS1btmDz5s1o2bIlHn74YezZ47hAHRFlnsZl86JlxQJWyySJQt2SeZwWppV13AmAnOozH6j7DNDybevlMWWAty8CT88B3rlsdZepUGyhohZj5Ct2TN6WnYKLksp3SAKzIREFokAZDucPJBvf4MQXMU3fNkPD+iSLnK2XEwbAU7rFD/fYtogCoifooYcesro9cuRI1TP033//qV4hIgpBJRolX+wJiwTKtFBXC0VH4dLNeFUodsmQZrgTr0eO7Ta1PxxtB1BpfCWLlWBGOqLAEWYq3kw+USNuIqI1t1VacUuxxizIqbEu5O2qTekskEsUsD1BlvR6PX799Vfcvn0bDRs2tLtOfHw8YmNjrS5EFJpWvd4Cu99vp3qopBcqOms4ECVFHj3nu6QHPbo9Isq4PmELfd2EkPNRYvKJoilJ7VR68ZPGAriIXKrgq6lX6P74L/F7UvKJJaJA4PPECLt27VJBT1xcHLJnz47Zs2ejcuXkN5WtUaNG4f333/d6G4nI/0jgk2qEXvUngH1/J9cnckCGw9l6N7EXuuhW46ChGB4LW4k/9U3wh/5+rDVUw15DCXwV8a3V+r0ThqKg5hpGh0/O8N9RKe4H7Ivqm+HtEBFllnmG+1ArrjKuWdUu0uDJxLeBxJQl7yf1RLewVb5oIlHg9QRVqFAB27dvx4YNG9C/f3/06tULe/futbvusGHDcOPGDfPl1KlTXm8vEfmxsAigx+9AI8dJD6QKva0f9e3QOeFDvJHUD63ix2JIYn8VAIm5hkYqIDL5JulhrDDUwq/6Fipr0hljjNMm7TGUcHjfDkNp3LWo2eGKTvEfuLU+EZEnXEPONIsmSDFYT3g90brswX5DcqFvoqAKgiIiIlC2bFnUqVNH9fTUqFEDX35pv6hYZGQkcubMaXUhInKVVJ9fZqjl8H5JG3vEWMTmi16DTYYK5lufJD1uXt4s4XM0jv/KYSD0bMKr6JzwAQbcK0ho6aIxF7omJPdsb7y3/fX3hpY4s9NYJtU4fCKiYDJTnzz306R3wus+awsFL58HQbYMBoOa+0NE5AnDEp9Rv/snvIwJ+ofSVf5VqqfbJ9vS4OH4D3HHGGnVY/RBYg8sMdRBIsLUUBIJiCzHy581xqjMTaJ3whvonvAWXk1MyVj3W1Jzh+1pGv85usS/j3bxo1E7foLbfw8RUWZ5JaG/R7cnJ5nOIwYl42bgiiZPunrd3bVYX8dj2yL/5dMgSIa3rVq1StUJkrlBcnvFihXo0aOHL5tFREHkF30rlIv7CfMNDdK9jb3Gkng8/h3cH/+53fsvIxrV4ifj1cT+eCj+Q3yc+Dh+0rezWkcCoqFJz5tvGy2CManUvt5QBWeRF+3jR6Nx3Jd4I+k5zNXbTxIjk5K3GcvhgLE4khDmVgIHTw6nswz6hASCIt7o8+mmROQjsw3WnwsmpiQK7rL8rBxR4HP8nv0pvJXYFzfv9YJL8LNMXxMDEl+2epzM7RQbDMmZ515NeEENZzbZV9q6ALelL5O6qBNZ7ngtMeXzPVAdMshIiNDh02+qixcvomfPnjh37hyio6NRvXp1LFy4EG3atPFls4goyEhvTEZtMFZyer+pV2eXsTR26Z31HDm331jcfP2VxAH4NulhNNXuxFvhM/BlUle7j/kmqTMKaq5irr6RSthQX7sPXyV1xTfhX6sAaaWhBh7XLcdZY141nO6DxKfwTvjP6rGTkh5Av7B5Kc9vKIaK2lMuf+k/olttvl05for6HYkEFlckImw1lEVt7WF1/bCxCBohZc73OWMeFNJcTXMb0mtucjmsIKo8OQrzFu5HtQOtoYPe/NlrylL3ZcQ4lcVuRNLT+CzpUZw25lefSVLv6BXtLPO6F/M1RKWjyZ9ZYqW+Oprpdqrrss3v9R1V+54NS643Jz3vA8P+Up+zJ4wFsDjSeojeKUP+VG3fZSiJatrjCASfJj6Kr/VdQ6pkhE+DoO+//96XT09E5DMJLnz8yhex9PYc0BdXX7wXkNvhZGQpiGgyQ99K/X4gYZR52UQ1FDDZbkNKUdmRST1Ukocndcuw3FATawzVzF+C2w1lUFN7JNXzyRlY6flyFNTJwQYRha5H4t/Fk2HLMCrxSUQgETeRFdW1R9ATi9X99eLG4Say4Nvwr7DEUNvuNh6Lf0edoHkvsad5mdEIVC6cE1P61EfJN/+1CoDEX4YmKFSlA77bIiVUNCoAsvxMOmAoal73esHGGJTwIp4OW4R62oN4M7Ef1utesvp8tuyFks/iQYkpSXeeTngT0yJGm2/fQDYcNBRBee2ZlPamY/i1LRlGbcq4t73Gu8mfyduST2JZkiHf4yOs59RPTOqIJ3TLXKrf9KPN6IVQ4HdzgoiIgplkPTppyIf/3Zur5KoLkLHwGf9CFefUtkw0KhnEB0lPqwDIksHB8z2XOMTcluX6Guq39C5ZkjlOt4xReCeRPUJEoWaLsYIaHixDhWWYrwRBknGzY/xIVI+bhEvIhThE4pnEoWrIsj0bjZXQL/FVnEE+qyAoLXGR0nNk/7NrnqEB3k7so4Yt58sZpbJ/dkt4D6XifsY5xKjhcr8ktcARY+E0n2e1oToaxn2Nu/eKxh4zFkS7hDHoHD/C3MueplwlgLcvOi1MOzQpZa6o+vNbvYukIg3wUWJ3/JDU3nzfQWNRVIybYvXdIqMEPkvqZrXNGVUzXtohWDAIIiLyctajpglf3stC5xsyp+j5hFfwaPxwp+sdNxY0X68QN1VNTJaDBZm/ZPJ84hA1z+gHfcqXsZB1qsb/gGn6tlbLrxuz4fmEwZilb4rpSa3UPCIZ37/FUA6+dMRQCEMTn/NpG4iC3R5jKcQim1uP+fZJ+z1FjmjsxD9D25kyfGrws74NBj39GBqUikGebMkBzH2l85mzfw5LkvTcyRuZmPQgLhmjcafuADzZoDgKRaeUNPihd13kKlQKNeMnonLcD6q3STKMbjeWVZ+VI5OewiFjUas5lC8m2CnfEBaJ61lL2v1bpDCtpYgwHZA9P+J7zlO9+5a9aPKdIoGljASQ5y8ZN1291j/q2+KZhFdhiCmnAqhStVrafS7jvb95nyFlSLa9pBeL9e7tD3/G2atERCFooaGe096qltrtGJb4LA4bCuO0MZ95OIl8yVtKQHiqtN22pLhsDc1RzDfUV9uSRBALDfXVfe8k9YEBWkzXt8Z7YVPRO2yROTtTG90W82TdchZDTGyHzHweMQ7ZcRfRmjtIr44JH6kDiA2GSmio3YuXwmajqOZyurdHRJ6RLdK2Kra1XFnD0aJCfszelvwZobUTBck6JhUL5kCbygXU9SVDmuFOQpIKhmZtOY3Fey+o+4b/tUfdLz1W9eO/xbEHH8JHAJbtv4C+UzejboncaFmxgLrIsDxHRiQ+rU70yEmfHcayatk3+DplhZrJyReyFSoPHEmeOzQy8Uk1B3RGknWacFGhQHJpGJ02+W+Uz6vthtI4bBFspdCYP7OXGuog7vn/IWtEGO4zGnGswjModeB77DMUQyUX54CKeIRjn7E42mArggGDICIiStVbZarTMU7fOcPbk+KyK2C/PpMEQPbGzydajPXvljAcOhhUgovh4dNQQXNSTTZ+IWGwGjJzf/yXuF+7Cz9GjElX+0YnPqECIFMv2Ul9Afymb4FuuhUYGz7Ral3pLbJdRt614t4QTAoNGntdOyogyIEDF26i3/2lUTBnlDkIMgUIVtuw+GyxHFInwY+pN6hnw5LqIj6atw9xiYbk9S0+oyToWTi4KYrmdq0+m/TkvJPU1/EK97+qfoV3/BiYdgiGhi9h0p8FMUlvP+NnmC757wjXJbdJ5kRJoW+TvNkjcfmW/TIzptdAXs9Sj43Bk+8UwHljHiyLfM1qPUmOME77RfKNHn8A0x8x37fKUB2VtScQLDgcjoiI/MLseyltdxtK4qOkJ3HamFedSb2OHLiCaDW047XEF/BQwkeoGjcZC+71JkkgpUkeLW/X4IQBdpdXivsBD8ePwAQHBxy/65vbXSZDX2ReQf24b9WwE8lEZSmYhov4o6FBkIqYXFMiJitqF89l974JT9fBmEeqoW/jlEQvon3Vgoi4FyTYY3TyWWHSsLT9AtiiQsEcyBaZvj6ELOE6zIxIDiqM9Z8HdPe2k6cU8PJ2aOs/g9falk/1uJVhjYCoaKBy8kkpO3Ge4iBeTH2fLhzrDFVx8l7iCHEbUaheNBpbst2PeuqzbTpQrjUMj/yACUkdUT7uR5WEZ2pSe1w1Wg/TQ5E6wAtrEGgYBBFRUHugWsq8FvJvMqzuvriv0SVhhMrq1CT+K/yg72B3XfkytpZyYDMk4QWricVzDE1UgVkJXCzdRZQaomI7xC8tMpxP5hVcvJetb7LFBOhRid3RL/E1NIr7yuXtyQRnKQjpqlbxY3HZmDwsJhTJECUKDSuHtkCOqJShbJZK5s2Gx+sVR5YI6+Fy5QvkwOZ3WjsMAFxJrvBep5R5j+4qlsdxL1H+nJF47M3JQP910LRPySxn6cWW5TClj/Vw5RFRrwNDjwJZcjntHXMUHDmShDB1UkcSKsjJpL8GNla9apfUZ1vyxrTVHsGUrM+ooc9Ckl3Ujf8OZwYcAer0AZ76E+i3DChonVgnEDAIIqKg9vGjHDoTSKQyfHrqOmkcXDdNLJZhbhK4mJIf2PbepEXShb+UkJKG3JJkr6oTN16dKZ1wLxW5ZMRqGf+J021KEd8ycdPUBOfDNkUKm8R/qWqe2PoqqbOaAF03fjxKx6VOk0sUzFzpxZH3f86ocEy1CSRMDC5EQSVisqF7/WIutWl012ro3SglscET9Yrjg4ftB1EyHwlaLVCgSvJvB7Lb9DTlyBKR0mt0z9JXm2Fw63LY+L9WVkPeJIhxp5foDqLMw4EluHqzQyXkzxGJEQ7+BnPve0Q24KEvgLL2s/sFAgZBRBTUbL9MKHTrLpmGs9WNG4/3k1JqjzjTLX44nkt4BZ0TPsDfhkYO15PheqYzpSZx91Ln2vpL30gFMBLs2dY5ETIMRRJISM2Tf/XJQ/5MJFtVMo3VfCp7/tE3cHifPEf1uIlqyKHJG4mSFSu1qUltzWmAiXwpJlvywbozpoP95hVShnpZHv+70BHklifqF0/Vc/T0vblFonHZlF5ee0kb7ElMSp6PZPJaW1N2uxRl8mXH4Nblkd8i6JGeoMm96qJAzkin86KcaVgmBhvfam2eH+WIvblXgYZBEBERBbx1hiqYr6+HL5K6prmuDOdwtebSJmNFLHKSSc8Z6Q0an/QQPk58DI3jUooYJkGbKoCxntOU0rYhiQPQI2GYmr8kw1ZshwFKgGZLskVJ/ZIXEwdhmEXNkC7x72ODoeK910iDWGRHp/jkSdUyxl+SQdjzXlJvq3Tpjsi8AUvfJaUU6BUPxEt+LSL3Sc+K9KIMamU/lb7lu8deoOHucLiMMNo8gc6ix8fFGAhVCkcjX45I1CyWC0c+egBNyqWcrHBGenKqFonGs01K27nP+nalQq4PqbXXblf/Fn/GU6RERBTwZF5P/8TkgKCuZj/8xZik7naWun70IKnJpcikIxKgSaCVFXGYom+Pa8YciEVW8zwnKUS501AGN5FFDQl8PMG6NtRV5FTzpuLupUC31dFO4HLNmB25NbfU9V4Jb2Cb2n7ycz4flpIueFxSJ7wQ9rf59l6j/TPLEtzJkJzjUU+m+XpQaJKeFcvelfSoUjgn9pyNRbPyKcVXnfFUsGQZFLnaGxOdNRzr32ypelsczf+xx7Sq3k7jbbcy8ek6uP/j5UgvSbcd6AL/LyAiIrKw2VhRDe06aigEf2SZCtxEKtc31e3CUUNBDwVaKfY4CD4cFWQ0OWAoavexlr1WKw01HAZIrgxRPGYooAIgW3P0jdBZty7V8gX69PXKUeiwFzNEhGnxdfdaWLrvIh6r69pcn/RyFjy503sS5iTDXVrb1xvsBEE2T14sj21yGdd882QtZI3QBcVQ88D/C4iIiGw4GtrlD+wdJEkxxROGAthvdFyt3RtMhRqF1iLYOWIsjEo4qa4bnJzNlur0g8P+VNdNk63tWaWvprb5vUX2Pyl8OzPyA3X99cTnMThRElEYcTwquaCkyOSRTBQELHtb+t1fCttPXUerSgVUsoTS+ewH/J7k7H/UnV6d9DANBUzSp90TlF4PVi+MYMEgiIiIyAumJ7VCj7Cl+O5eBjlrGjX/yNekSONLYbORU3MXaw0pk73fSeyNm8YsmKlvrmY0TYz4TNVwsqU3Wp+9TjDqEKHRp+op+kHfXhXRtSSFbyVjnnV2QOtDtzn3akkROcwOaXHjrY6V073N9A6Hc9oTBHglCLI7HM7Ok+fLEYlLN+0XVw0FDIKIiIi84K2kZ1RWOtsscv6mQ/xotNZtxUx9M/Oya8iJYUkp2ePqxH/n0iGd7dC/5vGfoYzmLLYa7U9wd5Ye/b3EnlhoqOviX0Ghytfz9YvHOK4TlNnJBEzbt5sdzs6T/9CrHh76JvCKnHoKs8MRERF5iSYs7RS/viZ1j37Ut1PFZB1z7WjuCnKmmn+01Vg+XYeqB4zF/OAQl/ydp4acVS2S/L+bO6trJy1mPNsAb3aoiI7VHA8XczVFdnqZtt6lVhH0aVwSTdNIAlGtaDSqF5VsmUC9ksnFn+2xTMMdTBgEEREReYllUcVQ0DfhdWw1lMWTCf/L8LZOGV3L6kWhzVNhRre6xVTB0GnPOK63ZalR2bx4oVkZlYQhs9uWVpAlmdvefagKOlRNO9HK2EdroEeD4hjV1XEWyv91qIgiubJgZJeqCCYcDkdEROQFEenI9hRo7tgkQ5BED10TRmRomw/Gf4gYzU2cNqYUvyRyxFOdLVHhujQLhrorsxIjtKiQD8sPXMKjdYq6/dgKBXNgZBfHAZBoUDoGa99siWDDIIiIiMgLahSLDvrsZtP1rdFSux1LDbU9ts3dxtJMC0cuy+wMbBmRWU374vFa2HbqGhqXda2oKiVjEERERBTiB2eeImmxeyS+5etmEPklbSZ9BEhx1eYV2FPqruDvmyciIiKioFUwOirgahiR77EniIiIyEuM6S0+QkQONSwdgw86V0WpmGzwZ2E67wZBzcrnQ47IMJUFjlJjEEREREREAUur1eDp+0rAn1QokAMHLtxEk7J5kS97JObtPocn6hX3ahsK58qCTW+3RqSTjHWhjEEQEREREZEHTepZFxuPX8WD1QupIOSjrtVUxjlv88VzBgoGQURERB5Wo2g0dpy+4etmEJGPFI/Jqi4mDEb8D/vHiIiIPKxkXvtzEwJ9SlCDUnl83QQiIo9gEERERF6XN3uEr5tAREQhjEEQEYWcJ+oVy/A2skdyNDG5KcB7gUS9kuwJIqLgwCCIiCgdSufz71SsvvJc09IurTewRVlEZwlHqAn0OOjFlmV93QQiIo9gEERE5IJnmpTy6fNXLpQTgWBIm/JprrNkSDP0blQSEQ7StnoywGxRIZ/HtkXJk7uHtqvg62YQEWUYgyAiIhc8bjOEztt1v2f0a4BgUTZ/dmg0jl/ByT3romvtIh55Lp3WN19zdhMgBEmx+KK5s7i87rgetTO1LURE6cUgiIhCTsHoKJfX/bRbDewY3hblC+Tw2bAmqfidK2vwJRJw1KNQOl92FI52/UDbGSexFnmhV7JAzkiPPW9hN963RERpYRBERCHn+aZlXF73kTpFEZ019OaueMNjdR0nqDAG+OwZR60P9BTZolyBHC4Ne/S0GsVyIVB80Lmqy+vK0FAtg3Uir2MQREQh5YOHqyBLhGtF62Ky+UfvS+/GJX3dhIAVqseWmT2HzNUscZ4M+gKpV8+dIYPC2fBQIgrCIGjUqFGoV68ecuTIgfz586Nz5844cOCAL5tEREGum5PeB1tZIz1f4TtvdteHB+XKGo6fn2mAwa29c9a9Z8MSXnmekOXFXqDRj1Tz3pNRakHQ40cU7HwaBK1cuRIDBw7Ef//9h8WLFyMxMRFt27bF7du3fdksIgry7FauKpjT83MQ1g9r6dJ6/w5qgm3vtEGTcnmh89JYmfqlQqsGjC+GVxXO5dr/VLE8GZsTFQzD7kIJ+4GIQiwIWrBgAXr37o0qVaqgRo0amDp1Kk6ePIktW7b4sllEFML6N3d9vlB6hOtc+9iN0GlDcohMlcLeSwVe0IOT9l3Vs2FJtK5UAFnTGJI5uJX359yklRZ+4tN1fNaWgKPx/zlL3/eq6+tmEPmUX80JunHjhvqdJ4/9s5Hx8fGIjY21uhARkf+wjNt+6O3+QdaMZ+/zWSZA4am40+igK0ZqI03uVRd7R7R3+vhwBzWUvOmtBypZ9Q62rVIQoaZN5QIIRi0r5kfR3Fl93Qwin/L9p+w9BoMBgwcPRuPGjVG1alWHc4iio6PNl2LFXB/bT0Tkis41PVOfZud7beHvutfP3M/QMvmyu/0YR5n4nAUnMm/K1sa3WmHdmy1dToJBqWm1mnTtw2AiPbLpUa1ItMfbQkRBGgTJ3KDdu3fj119/dbjOsGHDVG+R6XLq1CmvtpGIglve7BGoUNC6HpC3D57cNSEDQ5S61y8eMPNI5gxo7PA+mTdlK3+OKBTOlQUafx+X5EXhOvdfi/tK+888sUDal+4kQBG+GPka6GnoiYIiCHrxxRfxzz//YPny5ShatKjD9SIjI5EzZ06rCxGRp4RpXf9I1PjgQNHe0JzoLGnXMCqb37Wz+baHRA9Uc3/4k9biaC4yTOe1JAaTe9Z1++y7vaAvrWPRxmVjEKjSE+R2qlEY3z5ZG6tfb+H7A2gvBwoydJGIgpdP3+EyZloCoNmzZ2PZsmUoVcp6EiYRkb8y+iDLWOm82dL1uEIO5sU4Oyh+tU15DGheNl2JH95oXxGDWpZ1ez5ORrSuXACP1S3q1hl2owd6zwLpeD89f68k5+hYvRCK5fH9/JHMioEcvX/l/zg98vhJfbG0+HPPL1HQB0EyBO7nn3/GjBkzVK2g8+fPq8vdu3d92SwiCgEDPJkFzl6Pgh+N3BnzSHW3H9O9QXGXM9nZy7A3pG2FVMslK9qXT9REIHP1wNHXx5e+fn6T6kW9n4bcXX8NdDzU0luiPNhrSkQBEASNHz9eze1p3rw5ChUqZL789ttvvmwWEYWA1+2c5bUNXAJpDoKzBA0yN8ZVswc0woxnG7g9p8EVkhXtYQ8lnvB3T9/nf4VnffHfLEPKXB2O6Zc03ilCPLVvfXgbe4Io1IX58skdpRAlIiLPHJDljEp7zpClWsVzu/8kQapVxfy4fjcR567fxdkbcW49NndW/xsS5atvXJ0/dYv66VzDOiX4viPyNs76IyIK0exJtsemtiemLO+XNNRTetcLuLPUGTn8/r53Pcx6oSFWv9HS7v1R4Vq//d/xp7Bj9CPVPLIdbxcP9tSz+WsM6Ov/USJfYxBERJQJgq2jW9JQeyp9uDfZHoCWcjO5hBx467T2j2K71naczTTY2Wbi+7Cz/fp+tr2Ljl5LfxVYrSUidzAIIiLK4AGP1+IdHwdW3jyjnVnP5cr8jIzsgsgwLb54PLCTP7giR1Q4RnZJCXyecnEOVOtK+UMuINGEwImaEjGuZQ8c0qY8mpbP5/B+ScdO5C0MgoiIKKAO3iz981ITv+qh2zuiPTrXcj/5w74R7eGPxVKdyR7p/rRibyQbqV7UvXpRnvZMk1JOA3rbDIlZIgI3M1z3+sWw/LXmeNVONkh7utQqgp/61nd4MkLSsUugROQNDIKIiPxQkI2myzRVLYZlufuaZTQgsnc472y4l7M5GOk5EJbEDY6fy1qOqDC0r+J68dsCOSMzpXcg0OahuDsPSdJt929Wxq/mNjniSqFlV8gQU3f/okqFWOyefI9BEBFROvjHYUzG2AYBaQUFgTjPyXnPQwD+QRbcKUY749n73Kr7NLCF+4Vy82ZPyYi3cmhzLBzc1KXHtalcAJ5ku8cHtSqXvu1o0ld4VZvGvCdNOt5XmRE3SQFcmcv1dfdaVsuL5MqCxmVjPP+ERH6GQRARUTqkddziJyd77fqkWw2XD1ApNKUnxffPzzbA/eXyYs7AxigRkw1R4TqPpnF39T1VIKd1cOhseFVavWOdaxZGRhWKdr1OlyMPVS9sN5h7vb1rw9AckblcD9Ww/hslAcr0Z+9zazsNSudxab082fwvdTyFLgZBRER+Nkwls/stHq1TNCAzvfmDLC4e2HuDu/+uzv5/Xmtb3iow1qbjvVCxYE5Me6YBahbLBV+Y3LMuHqldFM+nMRzN0qiuztN3t6yYH1WLpG/o1h/9G+K7p+qgZF7nSQNceallDk2RXKl7/gY0T7vHbsLTdTKtTmPOe0Pq8udIadtbD1TCiIer4L9hrXB8dEer9bO5MI9Mhm4SeQP/04iIMnhgwMLPGde9fnH0aFDc49tN7wGsrbc7VsKO0zfUQbE9WSN0GNahos+TArijYsEc2H/+prr+Ysv0DRnzprRevdaVC6jLvnOxLm8zrYNyOTEiwd3uM65v06ROieTekeX7L2YoocSmt1ojX45IHLl0C+nhao+cMzP6NcArv23Hhdj4NIdNVimSE43K5E13L5B8Fqw7cgUtKqQ/kyCRKxgEEREFyEGrt8Vkj/RaoCdZoyyTHHhKOzeSATjz7P2lnd7/63P3oXpR7/aA2O6SmGwRuHI7weXHB1LNnpxRYSiTL7vHt+vtRA0SzDQrn09lQft35zmXHyM8+RZ0t9dbgpp5g+7HO3/txrxd590eyuhu0DapZ12Pb5fIFofDERH5kC+HftgeB9keEMqZ25+faYBZLzTMtKGCfRuXQosK+VC3REpBTU+ybXdmHfJ6qzPQdEBsa2CLMvjbz9KFe9Lmt9uYh16lxZujWmsXdy/w/bFPfZU4IRDr4chJkXE96qjkDxkVFc7DT/I9/hcSEd3jyQN9eweY3eoUTbVsy9ttfF7XxJkm5fKibknXJj3b06hMcpapyg5S4g5/qDKm3DswpLTlsBhKZfkv1qthSRTOlSUoh2jOfL4hIsK0eLxeMTQolf7/RXeZ/iMdvZSFo6PwY9/6XmtPeveovXeWL/4/LNvRsVphNCmblzWByKcYBBERuTEuXuqnfN8rfUM17A0/koM7V4clBeKB7TdP1sb/HqiIqX3reWybElB90Llqhg9AM1wnyO3EBJm4/4I4hqx/L/CRYVK/Pd8Q9Up6vtdQ/p/c1bxifuRwYThYITsJDSz529s6o58zeS2G0Toin3uSTTC96cuJPIFBEBGFjPwOhhK5My5+w/9ao1Ul67omrh4zmNbrWquI+t3LQdV0b3H3WCetSeT2zurKkLrnmpaxyh7libY9fV8JNbcilLkT9wRakVJnnmlSyuV1f+hd16X/M/l/ckX/5q5nnjORxAqhYOLTdVQwWb6A/cyTwfMfSMGCiRGIKGR4c66As+ca/Uh1PFG/OGq5OZ/A1ym8pcJ8n8YlMWXtcbv386xuikI5M1YbpmmIB3jOtK9ayOn9ERZFYVtW9GwhVl/yZWIEV7T1UBISIm9hEERE5GUyFMQ0xMdfSIDT2qaHyx5Zx1EQRCmis4Zj8StNERmmQ9Oxy916rCSJkP2Rlsw8s+7nJbNSsUxdXipvNjxRrxhyB2hhTgmAo8K0qtepYHTaPajTn22AY5dv4+05u73SPqJgweFwRESZ5NNuNVA2v+fT+qZHsTzOeya2vN3apfkNwcQ2iCjn5r5KK4V6uQI5UDzGeaFMR0Gyo/3XpoqDQDWAxxplNOAa16M2vupey2J7GtXb+kb7ihlsl/OGpafZWSLSrtnzSO0imNizLmoVz41C0RbJLhzs5MZl8+IpF4fz+UtiBCJ/wCCIiCiTPFKnKJYMaeax7WXkwGXsozWc3h9mMYQomDl6DZ+6rzh+6O255A2ucOcg/c/+jfHF4zWt5l0F0kFlmXzZ1O9ONZLnw3nSA9UKuV2AVLSrUsAjhURdLbYracyll8rbQ9UCrVePyFs4HI6IKIMyek714RqFse3kdXWgeOTSbQ+1KvQ4SsPtig87V/Prg0upD9T5XkINV+TOmvlDwYpYpOROy9wXm+DIpVuolgkFcdPj0TpF8Uk35ycGPMlZsd1gSlpBFEgYBBFRyJK6PZduxWPFgUs+PWP6dMOSauhU1cLRqDFiEYI10EgvScYgmlfIp/ZV73u3bUk9owlP13HpbHtGetYkAcTMTafwYouy+Gn9CZcfVzudBWHdKST73VN1cO1OAkrYDMPLjNFP8nq/36mKS6+3ZBasXjTjRTYzU9vKBbBo7wWnQYovO1X8LZU2UaBjEEREIWvsvTPBJd/81/4KXjrikTpBMq4/EOoAyWTzTW+1VnMbqr670CvP+e5DVdTvST3r4sSV2yiTz/HcnXZeyFAlQ9JeaV3O5WFLy15thqX7LuLpNFKi1yyWC9tPXVdFQd2tu2LSvmry3++t/6VejewHpIFIAui7iXpUHu6d/2tX55b5iqSgX3kw+QSRJzh6u8j76bPFBz32PESuCo1B4EREXqYNgIH46W2iDM1KzxyMjArXaVE2fw6fpwoXtm0I0zluU+l82dGvaek055/8+tx9mDfofnSqURiZbcwj7g//8yfPulEryJ19mjUi5f/aD/7NrNiGtc7qZMkJg/TqeS+w9VYGS1/0LhMJ9gQREWXSAbunx/7bO7lfNHf669EEQMeT33uuaWmcvxGXrgM5238RCZIqF3Z9O+7uPinSO2vLafNzkX9yJfaa0qce6pe0H6R0r18cbSqnrz5Siwr50KJCSrpxomDGIIiIKINnfn0ZTBTNnVXVCZG6Mg9+vcZ3DQlR/3ugUrofKwVzf914Cq3TecDqrkZl83rleQKBL3oyPSmzApXCFskunmpQAjM2nERbR2nZiQJcYH8KEBGRmk9EgSdnVDiWvdbcK8+V1U59mhxRoXkIID0oL7cql67H+rLzNL3DxmTOUaMyMVh35IrbBX/XvNEiw8NPs4Tr1Fyr2sXtJ/ioWzJ5eYGcrs99I/KE0PwEJCIKgjPZnz9eE/7ildYp9Wv8WT0HQ4iCWcuK+VX2QVsfP1odA2dsRf9mZVPd52fTYTxq5gsNPbq97vWL4ZeNp9L9eFeHydYpkVvN9SmZjgK86Y1j7AVAubK4V1R53sv348+tp9G3sf15XLmyRmDne20RFcYhmuRdDIKIiDKpMGRm2vluW2i1mkyp6ZIehXNFIRDkyureAZw/cHQAa7n7nZ2td1SUtURMNvzz0v0IRO7MnUov0yuax6LmksbHQ+vSO9fHk0Z2qYqXf93ucnIKSaH+atsKafaKEnkbgyAiIg+mqh3dtRpaVbI+UJFhHhdi483pi9OrYZkYTF5zTF13JwASxfJkxeSedVWK61BlOySsbP7sWH6vRlQgknlgj9QuCr3BoDL2Bar0vOs61yyC2wkyxCrzaw+91Kocjl+5jYdrFsG+c7Gp7vd0tsLMyErnyXmLMg/xj/6NPLdBIh9hEERE5EEy2d3W4iHNcOzSbVQvGp2hORwyrGlqn3oob2dokysycwJ+aS/0fnnaiy3KQafVBtzEb6kbdPlWPNpULqhqC4UiOQnw9H3O6y55Mtic3Kueum4vCAomTBhJoYRBEBFRBj1YoxB+WHsMJRyM1ZehHjVcOFiVM8ofdq6Km3FJ6myrvfubu5EV6q0MZC5zV50SgTfXJipCizc72B8q5s9WDm2O87FxTovGWvK3ejfkXVXsDB3MbTHEjyhUMQgiIrqnWhHrnhpXjx0l65EcmBbImfF5MU956Oz2/g/ae60WzH2lAy8AclbfKT7JAH+WLTLM5QCIkrWtXACL9l7w6Da71CoCf7b7/Xa4k5CUagjs4leaqv8holCXupofEVGQ6Vo7+WBlkIO0uIteaYrnm5ZWvTDpJZPM/akApSttKZk38IawZVRM9pS5M2Ha1F+B055pgII5ozDh6Tpebhn5XRr5NLrQ6vp5pkFJ2JA/R+oTM/YyBZqw05BCiU9PBaxatQpjx47Fli1bcO7cOcyePRudO3f2ZZOIKAh98mgNVRdEAhV7ZI7NMC8OHfMXMtdh/bCWiAyh1LRyYLhkSDOEaTXQ2UkuUb9UHvz3v1Y+aVsokl5ESVCR2Z5sUBxnrt9F03L5PL7t5uXzYeKqo4gMc++8sqS83nLimkpuQUQhFgTdvn0bNWrUQN++fdG1a1dfNoWIgnwStaMAKNQVis7c1Nn+yBsH3Z6UGXN6pDaLr8mJiVfalMekVUe9Mszxfx440WFvXzQqmxd/DmiEkm5+xsx8viFu3E1EHpvhap7IUpmZ2eGIgoVPg6AOHTqoCxERBS4eYAWOsY9Wx95zsWhaLh3DwzKJpI4fOW8fApnMC3SX9ETaBkBE5D0BNTMuPj5eXUxiY4M7VSURkTtaVcyPpfsvupWKO5SE6nwHy7+7W91iLj3G2bwRT5MaVtuHt8F3K4/iu5VHvPa8RBTaAioIGjVqFN5//31fN4OIyC999nhN/LX9DDpWK+TrplCAkvlSl27Ge33IoAzPk3TltYrnwvPTtsCXCkdH4eyNODXXh4iCV0Blhxs2bBhu3Lhhvpw6dcrXTSIi8qtEBz0blrTKgEbkDgl+GpaJ8Wkqa5kr82yTUj5rw/KhzbH57daqhyrUcHgehZKA6gmKjIxUFyIiIgo+UhBYMvRtOn7VZ22QbImR2X2XMdEXxW2/6l4L/+48i+eblfH+kxP5SEAFQURERKHmvtK+65kJtoP9jLJXdycQGOE8e0mnGoXVhSiU+DQIunXrFg4fPmy+fezYMWzfvh158uRB8eLFfdk0IiLyISacA1a/3gKbT1xFpxrJxX5DSc1iueCPutcvjv3nJbse5wsRBTqfBkGbN29GixYtzLeHDBmifvfq1QtTp071YcuIiALzTDUFD5mTEorzUkTjsnkxqWddv6vpFBGmxaiu1X3dDCIK9CCoefPmMLLABBERUaYJ1GC+TeUCCEWFogNzyB1RoOGcICIiIiIfm9qnHk5dvYNa6Si8SkTuYxBERETpUjR3Fpy+dhcdqhb0dVOIAl7zCvl93QSikMIgiIiI0uXvF5tg++nrnCRO5EeyhKdO782ZB0QBXiyViMhfjOpaDTkiw/DOg5URqnJni0CLCvmh0wbopBOiIPJB56oqq95LLcv6uilEAYE9QURE6VClcDR2vNsWWgYAfu/hmoXx1/azGNCCB4f+IG/2SFy+FY/WlUIz8UFmefq+EupCRK5hEERElE4MgALDp91q4IVmZVCxYA6EogI5/Svb2MqhzXHxZjxK5c3m66YQUQhjEEREREEtTKdFpUI5EUo0Gg3WvtkSCUkG5IgKhz/JFhmGUpE8/CAi3+KnEBERURAqkiuLr5tAROS3mBiBiMiB+qVifN0EIqIMY3I4otTYE0REZGPV0BZYeegSHqtb1NdNISIiokzAIIiIyEbxmKx4OoZZlnydQYyIiCizMAgiIiK/89R9xbHn7A20rJjf100hIqIgxCCIiIj8TmSYDp89VtPXzSAioiDFxAhERERERBRSGAQRERERBbFeDUuq3/eVzuPrphD5DQ6HIyIiIgpiHasXQsVCzVA8T1ZfN4XIbzAIIiIiIgpyZfJl93UTiPwKh8MREREREVFIYRBEREREREQhhUEQERERERGFFAZBREREREQUUhgEERERERFRSGEQREREREREIYVBEBERERERhRQGQUREREREFFIYBBERERERUUhhEERERERERCGFQRAREREREYUUBkFERERERBRSGAQREREREVFIYRBEREREREQhJQwBzGg0qt+xsbG+bgoREREREfmQKSYwxQhBGwTdvHlT/S5WrJivm0JERERERH4SI0RHRztdR2N0JVTyUwaDAWfPnkWOHDmg0Wh8HnlKMHbq1CnkzJnTp20hz+F+DU7cr8GH+zQ4cb8GH+7T4BTrJ/tVwhoJgAoXLgytVhu8PUHyxxUtWhT+RHY839TBh/s1OHG/Bh/u0+DE/Rp8uE+DU04/2K9p9QCZMDECERERERGFFAZBREREREQUUhgEeUhkZCTeffdd9ZuCB/drcOJ+DT7cp8GJ+zX4cJ8Gp8gA3K8BnRiBiIiIiIjIXewJIiIiIiKikMIgiIiIiIiIQgqDICIiIiIiCikMgoiIiIiIKKQwCPKQb7/9FiVLlkRUVBQaNGiAjRs3+rpJIWvVqlV46KGHVLVgjUaDOXPmWN0vuUCGDx+OQoUKIUuWLGjdujUOHTpktc7Vq1fRo0cPVfArV65ceOaZZ3Dr1i2rdXbu3In7779f7XOpkvzxxx+nasvvv/+OihUrqnWqVauGefPmZdJfHdxGjRqFevXqIUeOHMifPz86d+6MAwcOWK0TFxeHgQMHIiYmBtmzZ8cjjzyCCxcuWK1z8uRJdOzYEVmzZlXbGTp0KJKSkqzWWbFiBWrXrq0y3JQtWxZTp05N1R6+3zNu/PjxqF69urmwXsOGDTF//nzz/dyfwWH06NHqc3jw4MHmZdy3gee9995T+9HyIt9tJtyngenMmTN46qmn1H6T4yE5Ttm8eXPoHC9JdjjKmF9//dUYERFh/OGHH4x79uwx9uvXz5grVy7jhQsXfN20kDRv3jzjW2+9Zfzzzz8l86Fx9uzZVvePHj3aGB0dbZwzZ45xx44dxk6dOhlLlSplvHv3rnmd9u3bG2vUqGH877//jKtXrzaWLVvW2L17d/P9N27cMBYoUMDYo0cP4+7du42//PKLMUuWLMYJEyaY11m7dq1Rp9MZP/74Y+PevXuNb7/9tjE8PNy4a9cuL70SwaNdu3bGKVOmqNd6+/btxgceeMBYvHhx461bt8zrvPDCC8ZixYoZly5daty8ebPxvvvuMzZq1Mh8f1JSkrFq1arG1q1bG7dt26b+T/LmzWscNmyYeZ2jR48as2bNahwyZIjaZ19//bXahwsWLDCvw/e7Z8ydO9f477//Gg8ePGg8cOCA8X//+596f8g+FtyfgW/jxo3GkiVLGqtXr258+eWXzcu5bwPPu+++a6xSpYrx3Llz5sulS5fM93OfBp6rV68aS5QoYezdu7dxw4YN6vVfuHCh8fDhwyFzvMQgyAPq169vHDhwoPm2Xq83Fi5c2Dhq1CiftouMqYIgg8FgLFiwoHHs2LHmZdevXzdGRkaqN6aQN6A8btOmTeZ15s+fb9RoNMYzZ86o2+PGjTPmzp3bGB8fb17njTfeMFaoUMF8+7HHHjN27NjRqj0NGjQwPv/885n014aOixcvqn20cuVK8z6UD8zff//dvM6+ffvUOuvXr1e35UtXq9Uaz58/b15n/Pjxxpw5c5r34+uvv66+6C09/vjjKggz4fs988h7avLkydyfQeDmzZvGcuXKGRcvXmxs1qyZOQjivg3cIEgOdO3hPg1Mb7zxhrFJkyYO7w+F4yUOh8ughIQEbNmyRXURmmi1WnV7/fr1Pm0bpXbs2DGcP3/ean9FR0erLnXT/pLf0qVbt25d8zqyvuzXDRs2mNdp2rQpIiIizOu0a9dODdG6du2aeR3L5zGtw/+LjLtx44b6nSdPHvVb3oOJiYlWr7d0qxcvXtxqv0oXe4ECBaz2R2xsLPbs2ePSPuP7PXPo9Xr8+uuvuH37thoWx/0Z+GRolAx9sn39uW8DlwyDkmHmpUuXVsOfZHib4D4NTHPnzlXHOd26dVPDE2vVqoVJkyaF1PESg6AMunz5svoCt3xjC7kt/zzkX0z7xNn+kt/ygWApLCxMHXBbrmNvG5bP4Wgd/l9kjMFgUPMLGjdujKpVq6pl8prKB6x8GDvbr+ndZ/JFfffuXb7fPWzXrl1q/oCM/3/hhRcwe/ZsVK5cmfszwElAu3XrVjWXzxb3bWCSA1+Zn7NgwQI1n08OkGWOx82bN7lPA9TRo0fVvixXrhwWLlyI/v37Y9CgQfjxxx9D5ngpLFO3TkSUCWeYd+/ejTVr1vi6KZRBFSpUwPbt21XP3qxZs9CrVy+sXLnS182iDDh16hRefvllLF68WE1wpuDQoUMH83VJaCJBUYkSJTBz5kw1YZ4C84Ri3bp18dFHH6nb0hMk363fffed+iwOBewJyqC8efNCp9OlyoIitwsWLOizdpF9pn3ibH/J74sXL1rdLxlsJAOK5Tr2tmH5HI7W4f9F+r344ov4559/sHz5chQtWtS8XF5TGSpx/fp1p/s1vftMst7IFz3f754lZ48lA1SdOnVUr0GNGjXw5Zdfcn8GMBmuJJ+fkuFLzgjLRQLbr776Sl2Xs7vct4FPen3Kly+Pw4cP8/0aoAoVKqR63i1VqlTJPMwxFI6XGAR54EtcvsCXLl1qFV3LbRnbTv6lVKlS6k1lub+kq13Grpr2l/yWD3P5MjdZtmyZ2q9y9su0jqTilnHQJnLmU85s586d27yO5fOY1uH/hfskx4UEQDJcSvaF7EdL8h4MDw+3er1lvLF8mFvuVxl+ZfmBLftDvmBNXwRp7TO+3zOXvJbx8fHcnwGsVatWar9ID5/pImebZQ6J6Tr3beCTFMhHjhxRB9J8vwamxo0bpyo1cfDgQdXDFzLHS5madiFESMpGyZYxdepUlSnjueeeUykbLbOgkHezEkkKTrnIv/hnn32mrp84ccKc8lH2z19//WXcuXOn8eGHH7ab8rFWrVoqbeSaNWtUliPLlI+SIUVSPj799NMq5aP8D0hqT9uUj2FhYcZPPvlEZcqR7DpMkZ0+/fv3V2k6V6xYYZWi9c6dO1YpWiVt9rJly1SK1oYNG6qLbYrWtm3bqjTbknY1X758dlO0Dh06VO2zb7/91m6KVr7fM+7NN99U2f2OHTum3odyWzIKLVq0SN3P/Rk8LLPDCe7bwPPqq6+qz195v8p3m6S6lhTXkqlTcJ8GZgr7sLAw48iRI42HDh0yTp8+Xb3+P//8s3mdYD9eYhDkIZLPXj4AJH+9pHCUfOnkG8uXL1fBj+2lV69e5rSP77zzjnpTyodpq1atVJ0SS1euXFFv4uzZs6sUnn369FHBlSXJmS/pJWUbRYoUUR8WtmbOnGksX768+r+Q1J9SF4XcZ29/ykVqB5nIh/KAAQNUKk75gO3SpYsKlCwdP37c2KFDB1WjQL7A5Ys9MTEx1f9PzZo11T4rXbq01XOY8P2ecX379lU1KuQ1lIMheR+aAiDB/Rm8QRD3beCRVNWFChVSr6N838lty3oy3KeB6e+//1bBqRzHVKxY0Thx4kSr+4P9eEkjPzK3r4mIiIiIiMh/cE4QERERERGFFAZBREREREQUUhgEERERERFRSGEQREREREREIYVBEBERERERhRQGQUREREREFFIYBBERERERUUhhEERERERERCGFQRAREREREYUUBkFERORTly5dQv/+/VG8eHFERkaiYMGCaNeuHdauXavu12g0mDNnjq+bSUREQSTM1w0gIqLQ9sgjjyAhIQE//vgjSpcujQsXLmDp0qW4cuWKr5tGRERBij1BRETkM9evX8fq1asxZswYtGjRAiVKlED9+vUxbNgwdOrUCSVLllTrdenSRfUImW6Lv/76C7Vr10ZUVJQKnt5//30kJSWZ75f1x48fjw4dOiBLlixqnVmzZpnvl8DrxRdfRKFChdQ25LlHjRrl5VeAiIh8gUEQERH5TPbs2dVFhrvFx8enun/Tpk3q95QpU3Du3DnzbQmcevbsiZdffhl79+7FhAkTMHXqVIwcOfL/7d1PKGxhGMfxB82SWE+zU/4slKKZibJAWSgW/pWNJlOaxSxmYaFQWCmUhY3sRv4lJVnIwkqSxQhNkljNclaDhTS356k5t8lVd3HdoznfT51ymvccjo1+fu/7nqLrp6enrWm6vr6W0dFRGRkZkXQ6bZ+trq7K4eGh7O7uyv39vWxubhaFLABA6SrL5/N5t38IAIB37e/vSzQalbe3N2t2Ojo6LKw0NTU5jc7BwYH09/c713R1dUlnZ6c1RgXJZFImJyclk8k4101MTFgbVBAKhex7rK2tSTwel7u7Ozk9PbWxAADvoAkCALhKmxoNLtrK9PT0yNnZmQUVbXa+os3O3Nyc0yTpoUFK26LX11dnXDgcLrpOzwtN0NjYmKRSKamrq7NAdHJy8o1PCQD4SQhBAADX6Zqc7u5um752fn5uAWV2dvbL8blcztYAaYgpHDc3N/Lw8GD3+hsatJ6enmR+ft5aqKGhIRkYGPiHTwUA+KkIQQCAH6exsVFeXl7sa5/PJx8fH58CjK7jqa2t/XSUl//+03ZxcVF0nZ43NDQ451VVVTI8PCzr6+uys7NjU/Oy2ey3Px8AwF1skQ0AcI1ugz04OCiRSMTWAFVWVsrV1ZUsLi5KX1+fjdHNCnTL7La2NnuPUE1NjczMzEhvb6+9W0jbGw0+OkXu9vZWFhYWnPvv7e1JS0uLtLe328YHl5eXsrGxYZ8tLy/bznDNzc12vY7VdxRVV1e79vsAAPwfhCAAgGt0LU8wGJSVlRV5fHyU9/d3CQQCtr5namrKxiwtLUkikbC2xu/3y/Pzs71M9ejoyNYF6fba2hbV19fL+Ph40f11ytz29rbEYjELPFtbW9YyKQ1cGrZ0Cl1FRYW0trbK8fFxUZMEAChN7A4HAChJf9pVDgAAxb+7AAAAAHgKIQgAAACAp7AmCABQkpjtDQD4Ck0QAAAAAE8hBAEAAADwFEIQAAAAAE8hBAEAAADwFEIQAAAAAE8hBAEAAADwFEIQAAAAAE8hBAEAAAAQL/kFW8mCASUQEk8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses_test[100:], label=\"Test Loss\")\n",
    "plt.plot(losses_train[100:], label=\"Train Loss\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Losses during training\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f115cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"<s>John is in the house. Sarah is in the garden.<q> Where is John?\"\n",
    "text = text.lower()\n",
    "tokens = tf.cast(tokenizer.encode(text), tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c6a6715b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       ".widget-textarea textarea {\n",
       "    background-color: #2e2e2e !important;\n",
       "    color: #f1f1f1 !important;\n",
       "    border: 1px solid #555 !important;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ca40791cdc4a72a4f9b7017d96bc2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', disabled=True, layout=Layout(height='20em', width='80ch'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x000001C92372C3A0>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2761, in while_loop\n",
      "    while cond(*loop_vars):  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2753, in <lambda>\n",
      "    body = lambda i, lv: (i + 1, orig_body(*lv))  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\", line 655, in compute\n",
      "    return (next_i, flat_a_out, tas)  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\", line 650, in <listcomp>\n",
      "    tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_a_out)]  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 243, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 52\u001b[0m\n\u001b[0;32m     48\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1024\u001b[39m):\n\u001b[1;32m---> 52\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m     53\u001b[0m     topk_vals, _      \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mtop_k(logits, k\u001b[38;5;241m=\u001b[39mk)\n\u001b[0;32m     54\u001b[0m     kth_value         \u001b[38;5;241m=\u001b[39m topk_vals[:,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\krist\\Documents\\llm-basics\\src\\transformer.py:229\u001b[0m, in \u001b[0;36mTransformer.call\u001b[1;34m(self, tokens, training, logits)\u001b[0m\n\u001b[0;32m    226\u001b[0m x, tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(tokens, training)\n\u001b[0;32m    228\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtf_blocks:\n\u001b[1;32m--> 229\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logits:\n\u001b[0;32m    232\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munembed(x)\n",
      "File \u001b[1;32mc:\\Users\\krist\\Documents\\llm-basics\\src\\transformer.py:85\u001b[0m, in \u001b[0;36mTransformerBlock.call\u001b[1;34m(self, x_embeds, tokens, training)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_embeds, tokens, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 85\u001b[0m     x_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     x_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffnn(x_embeds, training)\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x_embeds\n",
      "File \u001b[1;32mc:\\Users\\krist\\Documents\\llm-basics\\src\\transformer.py:126\u001b[0m, in \u001b[0;36mTransformerBlock.attention\u001b[1;34m(self, x_embeds, tokens, training)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# pre-norm to keep gradients alive\u001b[39;00m\n\u001b[0;32m    125\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdol2(out)\n\u001b[1;32m--> 126\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m x_embeds\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\engine\\base_layer.py:1097\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1092\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1094\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object\n\u001b[0;32m   1096\u001b[0m ):\n\u001b[1;32m-> 1097\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m call_fn(inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:96\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\layers\\normalization\\layer_normalization.py:290\u001b[0m, in \u001b[0;36mLayerNormalization.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    287\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(inputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# Calculate the moments on the last axis (layer activations).\u001b[39;00m\n\u001b[1;32m--> 290\u001b[0m mean, variance \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmoments\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    292\u001b[0m scale, offset \u001b[38;5;241m=\u001b[39m _broadcast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma), _broadcast(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta)\n\u001b[0;32m    294\u001b[0m \u001b[38;5;66;03m# Compute layer normalization using the batch_normalization\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# function.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:1419\u001b[0m, in \u001b[0;36mmoments_v2\u001b[1;34m(x, axes, shift, keepdims, name)\u001b[0m\n\u001b[0;32m   1385\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnn.moments\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m   1386\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmoments_v2\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1391\u001b[0m     keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1392\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1393\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calculates the mean and variance of `x`.\u001b[39;00m\n\u001b[0;32m   1394\u001b[0m \n\u001b[0;32m   1395\u001b[0m \u001b[38;5;124;03m  The mean and variance are calculated by aggregating the contents of `x`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;124;03m    Two `Tensor` objects: `mean` and `variance`.\u001b[39;00m\n\u001b[0;32m   1418\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1419\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmoments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshift\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshift\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:1365\u001b[0m, in \u001b[0;36mmoments\u001b[1;34m(x, axes, shift, name, keep_dims, keepdims)\u001b[0m\n\u001b[0;32m   1363\u001b[0m y \u001b[38;5;241m=\u001b[39m math_ops\u001b[38;5;241m.\u001b[39mcast(x, dtypes\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mfloat16 \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[0;32m   1364\u001b[0m \u001b[38;5;66;03m# Compute true mean while keeping the dims for proper broadcasting.\u001b[39;00m\n\u001b[1;32m-> 1365\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[43mmath_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;66;03m# sample variance, not unbiased variance\u001b[39;00m\n\u001b[0;32m   1367\u001b[0m \u001b[38;5;66;03m# Note: stop_gradient does not change the gradient that gets\u001b[39;00m\n\u001b[0;32m   1368\u001b[0m \u001b[38;5;66;03m#       backpropagated to the mean from the variance calculation,\u001b[39;00m\n\u001b[0;32m   1369\u001b[0m \u001b[38;5;66;03m#       because that gradient is zero\u001b[39;00m\n\u001b[0;32m   1370\u001b[0m variance \u001b[38;5;241m=\u001b[39m math_ops\u001b[38;5;241m.\u001b[39mreduce_mean(\n\u001b[0;32m   1371\u001b[0m     math_ops\u001b[38;5;241m.\u001b[39msquared_difference(y, array_ops\u001b[38;5;241m.\u001b[39mstop_gradient(mean)),\n\u001b[0;32m   1372\u001b[0m     axes,\n\u001b[0;32m   1373\u001b[0m     keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   1374\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:2640\u001b[0m, in \u001b[0;36mreduce_mean\u001b[1;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[0;32m   2588\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the mean of elements across dimensions of a tensor.\u001b[39;00m\n\u001b[0;32m   2589\u001b[0m \n\u001b[0;32m   2590\u001b[0m \u001b[38;5;124;03mReduces `input_tensor` along the dimensions given in `axis` by computing the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2635\u001b[0m \u001b[38;5;124;03m@end_compatibility\u001b[39;00m\n\u001b[0;32m   2636\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2637\u001b[0m keepdims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(keepdims)\n\u001b[0;32m   2638\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _may_reduce_to_scalar(\n\u001b[0;32m   2639\u001b[0m     keepdims, axis,\n\u001b[1;32m-> 2640\u001b[0m     \u001b[43mgen_math_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2641\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_ReductionDims\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:6277\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[0;32m   6275\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   6276\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 6277\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmean_eager_fallback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   6278\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_ctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6279\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_SymbolicException:\n\u001b[0;32m   6280\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Add nodes to the TensorFlow graph.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:6309\u001b[0m, in \u001b[0;36mmean_eager_fallback\u001b[1;34m(input, axis, keep_dims, name, ctx)\u001b[0m\n\u001b[0;32m   6307\u001b[0m _inputs_flat \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28minput\u001b[39m, axis]\n\u001b[0;32m   6308\u001b[0m _attrs \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep_dims\u001b[39m\u001b[38;5;124m\"\u001b[39m, keep_dims, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_T, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTidx\u001b[39m\u001b[38;5;124m\"\u001b[39m, _attr_Tidx)\n\u001b[1;32m-> 6309\u001b[0m _result \u001b[38;5;241m=\u001b[39m \u001b[43m_execute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_inputs_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   6310\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _execute\u001b[38;5;241m.\u001b[39mmust_record_gradient():\n\u001b[0;32m   6312\u001b[0m   _execute\u001b[38;5;241m.\u001b[39mrecord_gradient(\n\u001b[0;32m   6313\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean\u001b[39m\u001b[38;5;124m\"\u001b[39m, _inputs_flat, _attrs, _result)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    ".widget-textarea textarea {\n",
    "    background-color: #2e2e2e !important;\n",
    "    color: #f1f1f1 !important;\n",
    "    border: 1px solid #555 !important;\n",
    "}\n",
    "</style>\n",
    "\"\"\"))\n",
    "\n",
    "wrapper = textwrap.TextWrapper(width=80)\n",
    "\n",
    "# create a read-only text area\n",
    "ta = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    layout=widgets.Layout(width='80ch', height='20em'),\n",
    "    disabled=True\n",
    ")\n",
    "display(ta)\n",
    "\n",
    "#text = \"<s>Recipe for Obama Burger: Ingredients: \"\n",
    "#text = \"<s>Recipe for Obama Burger: Ingredients: \"\n",
    "#text = \"<s>Make me a recipe for baked fish?<a>\"\n",
    "#text = \"<s>John is in the house. Sarah is in the garden.<q>Where is John?\"\n",
    "text = \"<s>John is in the house. Sarah is in the garden.<q>Where is Sarah?\"\n",
    "#text = \"<s>The apple is green. The banana is yellow. The tomato is red.<q> What color is the banana?\"\n",
    "#text = \"<s>The apple is green. The banana is yellow. The tomato is red.<q> What color is the banana?\"\n",
    "#text = \"<s><q>Who is Midna?<a>\"\n",
    "#text = \"<s><q>Name two presidents?<a>\"\n",
    "#text = \"<s><q>name two presidents?<a>george washington (1789-1832) and john adams.<q>tell me more about john adams.<a>\"\n",
    "#text = \"<s>Fred went to school. Then he went to church. Then he went home.<q>Where did Fred go after school?<a>\"\n",
    "#text = \"<s><q>Name a famous dictator.<a>\"\n",
    "#text = \"<s><q>Give me a long answer?<a>\"\n",
    "#text = \"<s><q>Who is the vocalist of coldplay?<a>\"\n",
    "#text = \"<s>cnn\"\n",
    "#text = \"<s><q>Why was hitler bad?<a>\"\n",
    "#text = \"<s><q>Name many colors?<a>\"\n",
    "#text = \"<s><q>Name a leader?<a>\"\n",
    "#text = \"<s><q>Name an illness?<a>police poisoning<q>what is that?<a>\"\n",
    "text = text.lower()\n",
    "tokens = tf.cast(tokenizer.encode(text), tf.int32)\n",
    "\n",
    "T = 1\n",
    "k = 50\n",
    "\n",
    "\n",
    "for i in range(1024):\n",
    "    logits = model.call(tokens)[0, -1:]\n",
    "    topk_vals, _      = tf.math.top_k(logits, k=k)\n",
    "    kth_value         = topk_vals[:,-1]\n",
    "\n",
    "    logits = tf.where(logits >= kth_value, logits, tf.constant(-np.inf, logits.dtype))\n",
    "\n",
    "    idx = tf.cast(\n",
    "        tf.random.categorical(logits / T, num_samples=1),\n",
    "        tf.int32\n",
    "    ) \n",
    "    tokens = tf.concat([tokens, idx], axis=1)\n",
    "\n",
    "    text_pred = (\n",
    "        tokenizer\n",
    "        .decode(tokens)\n",
    "        .numpy()[0]\n",
    "        .decode('utf-8')\n",
    "        .replace(\"\\n\", \" \")\n",
    "    )\n",
    "    ta.value = wrapper.fill(text_pred)  # this updates in-place\n",
    "\n",
    "    if idx[0, 0] == tokenizer.token_to_idx[\"</s>\"]:# or idx[0, 0] == tokenizer.token_to_idx[\"<q>\"]:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "581f3419",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"model_super_qa_very_large_expandedData\"\n",
    "losses_train, losses_test = pkl.load(open(\"checkpoints/losses_\" + name + \".pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c7ae3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(x, w=100):\n",
    "    return np.convolve(x, np.ones(w), 'valid') / w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d841ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d31a1d3c70>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(moving_average(losses_train[10000:], w=1), label=\"Train Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cfd04662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d31b209720>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(moving_average(losses_train[10000:], w=1000), label=\"Train Loss\")\n",
    "plt.plot(moving_average(losses_test[10000:], w=1000), label=\"Test Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a1ed386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.656181789934635,\n",
       " 3.180048368871212,\n",
       " 2.648307003080845,\n",
       " 2.8091074526309967,\n",
       " 2.7767815962433815,\n",
       " 2.982430912554264,\n",
       " 2.950876262038946,\n",
       " 2.779323920607567,\n",
       " 3.027226835489273,\n",
       " 3.3451629877090454,\n",
       " 2.8125762343406677,\n",
       " 2.749489575624466,\n",
       " 2.6862537190318108,\n",
       " 3.1763921305537224,\n",
       " 2.7770859450101852,\n",
       " 2.836370460689068,\n",
       " 3.05386471003294,\n",
       " 3.0250463485717773,\n",
       " 2.7532372027635574,\n",
       " 3.173467233777046,\n",
       " 2.8842009976506233,\n",
       " 2.9249716848134995,\n",
       " 2.8063819482922554,\n",
       " 2.9207066893577576,\n",
       " 2.8392259255051613,\n",
       " 3.072015233337879,\n",
       " 2.6695745289325714,\n",
       " 3.064947657287121,\n",
       " 3.0534869581460953,\n",
       " 2.9483791887760162,\n",
       " 2.760186418890953,\n",
       " 2.992857165634632,\n",
       " 3.0671394169330597,\n",
       " 2.811779238283634,\n",
       " 2.9837295338511467,\n",
       " 3.2271327078342438,\n",
       " 3.071956805884838,\n",
       " 3.0089734606444836,\n",
       " 2.8821495696902275,\n",
       " 3.0831585377454758,\n",
       " 2.4319462701678276,\n",
       " 2.9966388195753098,\n",
       " 2.986316926777363,\n",
       " 3.200283482670784,\n",
       " 2.807615615427494,\n",
       " 3.0506905019283295,\n",
       " 2.8199987560510635,\n",
       " 2.8519651144742966,\n",
       " 2.9968732744455338,\n",
       " 2.8059570975601673,\n",
       " 2.8184726536273956,\n",
       " 2.881971135735512,\n",
       " 2.8148792684078217,\n",
       " 3.0093940645456314,\n",
       " 2.8303071931004524,\n",
       " 2.950185351073742,\n",
       " 2.8253612518310547,\n",
       " 2.99344652146101,\n",
       " 2.8587759509682655,\n",
       " 3.0030806958675385,\n",
       " 2.7648331001400948,\n",
       " 3.2654608339071274,\n",
       " 2.6291139125823975,\n",
       " 3.043460965156555,\n",
       " 2.5949129536747932,\n",
       " 3.12819541990757,\n",
       " 3.014731377363205,\n",
       " 2.9797026813030243,\n",
       " 3.1706040427088737,\n",
       " 3.009935602545738,\n",
       " 2.933960407972336,\n",
       " 2.6494630984961987,\n",
       " 2.9807485789060593,\n",
       " 2.9438598975539207,\n",
       " 3.0098584443330765,\n",
       " 3.1611399725079536,\n",
       " 2.670781560242176,\n",
       " 3.0305641889572144,\n",
       " 2.839738056063652,\n",
       " 2.9241969361901283,\n",
       " 2.8859775736927986,\n",
       " 3.080123770982027,\n",
       " 2.856339603662491,\n",
       " 2.830078713595867,\n",
       " 3.001666449010372,\n",
       " 2.7944924011826515,\n",
       " 2.963239960372448,\n",
       " 3.1601730287075043,\n",
       " 2.957525350153446,\n",
       " 3.051824189722538,\n",
       " 3.159797817468643,\n",
       " 3.1015606448054314,\n",
       " 2.5393862053751945,\n",
       " 2.8786586076021194,\n",
       " 2.9433835595846176,\n",
       " 2.7677965834736824,\n",
       " 2.9838038086891174,\n",
       " 3.150434896349907,\n",
       " 2.935431197285652,\n",
       " 2.814832240343094,\n",
       " 2.6276708990335464,\n",
       " 2.9261372163891792,\n",
       " 2.8469909504055977,\n",
       " 3.274431437253952,\n",
       " 2.6908921599388123,\n",
       " 2.5204205252230167,\n",
       " 3.066291891038418,\n",
       " 3.0819338262081146,\n",
       " 2.9826148971915245,\n",
       " 2.78548713773489,\n",
       " 2.954497717320919,\n",
       " 3.068904720246792,\n",
       " 2.8222380578517914,\n",
       " 2.6398149877786636,\n",
       " 2.7694100439548492,\n",
       " 3.0635750889778137,\n",
       " 2.9521574899554253,\n",
       " 3.1839623227715492,\n",
       " 3.2913559079170227,\n",
       " 2.786411687731743,\n",
       " 3.1560930013656616,\n",
       " 2.9729062020778656,\n",
       " 2.635688230395317,\n",
       " 2.7657600343227386,\n",
       " 3.2067887783050537,\n",
       " 2.8119664043188095,\n",
       " 3.0886941254138947,\n",
       " 3.0237113013863564,\n",
       " 2.8071392327547073,\n",
       " 3.2412393391132355,\n",
       " 3.2398748248815536,\n",
       " 2.671566128730774,\n",
       " 2.8338228799402714,\n",
       " 3.0552574545145035,\n",
       " 3.043122485280037,\n",
       " 3.0992463678121567,\n",
       " 2.7522613033652306,\n",
       " 2.7856857925653458,\n",
       " 2.9962246865034103,\n",
       " 2.9271274507045746,\n",
       " 2.6539937406778336,\n",
       " 2.7507484406232834,\n",
       " 2.9127801582217216,\n",
       " 3.2776461094617844,\n",
       " 2.63992028683424,\n",
       " 3.004957802593708,\n",
       " 3.0124418139457703,\n",
       " 2.8260975778102875,\n",
       " 3.069982409477234,\n",
       " 2.573038212954998,\n",
       " 2.799036018550396,\n",
       " 3.185402825474739,\n",
       " 2.8247563168406487,\n",
       " 2.623592771589756,\n",
       " 3.020900323987007,\n",
       " 2.765097104012966,\n",
       " 3.047554187476635,\n",
       " 2.9458820447325706,\n",
       " 2.877212278544903,\n",
       " 2.9896403029561043,\n",
       " 2.7515391185879707,\n",
       " 3.290346682071686,\n",
       " 3.2823827415704727,\n",
       " 2.957459084689617,\n",
       " 2.6841466538608074,\n",
       " 2.763738103210926,\n",
       " 2.97555211186409,\n",
       " 2.893391266465187,\n",
       " 3.1753027513623238,\n",
       " 2.775542065501213,\n",
       " 2.6525429859757423,\n",
       " 2.9500559642910957,\n",
       " 2.805090457201004,\n",
       " 2.6641897410154343,\n",
       " 3.0413408875465393,\n",
       " 3.0349535048007965,\n",
       " 3.096307672560215,\n",
       " 3.020808443427086,\n",
       " 3.041147716343403,\n",
       " 2.8641213923692703,\n",
       " 3.0718413963913918,\n",
       " 2.6103768423199654,\n",
       " 2.8339525684714317,\n",
       " 3.057665579020977,\n",
       " 2.959145486354828,\n",
       " 2.851719915866852,\n",
       " 2.948049820959568,\n",
       " 3.0624153539538383,\n",
       " 3.038607709109783,\n",
       " 2.8750184923410416,\n",
       " 2.5870286151766777,\n",
       " 2.804035097360611,\n",
       " 2.795311287045479,\n",
       " 2.974320352077484,\n",
       " 2.9914040714502335,\n",
       " 2.9684761613607407,\n",
       " 3.161421686410904,\n",
       " 3.0839528143405914,\n",
       " 2.923310272395611,\n",
       " 2.842705324292183]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses_train[-200:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
