{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04010a08",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa3d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from src.tokenizer import TokenizerBPE, fuse_tokenized_corpus, chunk_corpus\n",
    "\n",
    "import os\n",
    "import time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm.notebook import tqdm\n",
    "from src.transformer import *\n",
    "from src.data_handling import read_first_n, sample_batch\n",
    "\n",
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b59898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 2080 SUPER, compute capability 7.5\n"
     ]
    }
   ],
   "source": [
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "576256a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_tokenized_corpus(corpus, tokenizer):\n",
    "    SOS = tf.convert_to_tensor([[tokenizer.token_to_idx[\"<s>\"]]])\n",
    "    EOS = tf.convert_to_tensor([[tokenizer.token_to_idx[\"</s>\"]]])\n",
    "\n",
    "    corpus_list = [SOS]\n",
    "    for line in tqdm(corpus):\n",
    "        corpus_list.append(line)\n",
    "        corpus_list.append(EOS)\n",
    "        corpus_list.append(SOS)\n",
    "\n",
    "    corpus = tf.concat(corpus_list[:-1], axis=1)\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def chunk_and_batch(corpus, chunk_size, batch_size, shuffle=True, repeat=False):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(corpus)\n",
    "    ds = ds.batch(chunk_size, drop_remainder=True)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=100*chunk_size, reshuffle_each_iteration=True)\n",
    "        \n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7afd4e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 256\n",
    "\n",
    "tokenizer = pkl.load(open(\"tokenizers/tokenizer_CNN16000_lowercase.pkl\", 'rb'))\n",
    "tokenizer.add_special_tokens([\"<s>\", \"</s>\"])\n",
    "tokenizer.create_hash()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8751dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random.seed(43)\n",
    "#corpus = pkl.load(open('corpus/CNN_tokenized16000_lowercase.pkl', 'rb'))\n",
    "#random.shuffle(corpus)\n",
    "#length = len(corpus)\n",
    "#train_corpus = corpus[:int(length*0.8)]\n",
    "#train_corpus = fuse_tokenized_corpus(train_corpus, tokenizer)\n",
    "\n",
    "#test_corpus = corpus[int(length*0.8):]\n",
    "#test_corpus = fuse_tokenized_corpus(test_corpus, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f472fa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pkl.dump(train_corpus, open('corpus/CNN_train_fused.pkl', 'wb'))\n",
    "#pkl.dump(test_corpus, open('corpus/CNN_test_fused.pkl', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e38fb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = pkl.load(open('corpus/CNN_train_fused.pkl', 'rb'))\n",
    "test_corpus = pkl.load(open('corpus/CNN_test_fused.pkl', 'rb'))\n",
    "\n",
    "ds_train = chunk_and_batch(train_corpus[0], chunk_size=max_seq_len, batch_size=16, shuffle=True, repeat=True)\n",
    "ds_test = chunk_and_batch(test_corpus[0], chunk_size=max_seq_len, batch_size=8, shuffle=True, repeat=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f830d881",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d47b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lr = 1e-4\n",
    "decay_steps = 20000\n",
    "decay_rate = 0.5\n",
    "decay_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_lr,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=False)\n",
    "\n",
    "warmup_steps = 1000\n",
    "lr_schedule = WarmUpThenDecay(\n",
    "    initial_learning_rate=initial_lr,\n",
    "    warmup_steps=warmup_steps,\n",
    "    decay_schedule_fn=decay_schedule)\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "embed_dim = 700\n",
    "tf_blocks = 10\n",
    "heads = 10\n",
    "ff_dim = 4*embed_dim\n",
    "weight_decay = 0.01\n",
    "dropout = 0.1\n",
    "\n",
    "unembed_dims = []\n",
    "\n",
    "model = Transformer(vocab_size=tokenizer.vocab_size,\n",
    "                    max_seq_len=max_seq_len,\n",
    "                    embed_dim=embed_dim,\n",
    "                    tf_blocks=tf_blocks,\n",
    "                    heads=heads,\n",
    "                    ff_dim = ff_dim,\n",
    "                    unembed_dims=unembed_dims,\n",
    "                    tokenizer=tokenizer,\n",
    "                    lr=lr_schedule,\n",
    "                    wd = weight_decay,\n",
    "                    dropout=dropout,\n",
    "                    )\n",
    "\n",
    "losses_train = []\n",
    "losses_test = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7970a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"model_16k_tokens_blockDiag\"\n",
    "\n",
    "\n",
    "ckpt = tf.train.Checkpoint(\n",
    "    optimizer=model.opt,\n",
    "    model=model\n",
    ")\n",
    "ckpt_manager = tf.train.CheckpointManager(\n",
    "    ckpt, \n",
    "    directory=\"checkpoints/\" + name,      # folder where ckpts are saved\n",
    "    max_to_keep=5                         # only keep 5 latest checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88b34765",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "losses_train, losses_test = pkl.load(open(\"checkpoints/losses_\" + name + \".pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6527620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 70308672\n"
     ]
    }
   ],
   "source": [
    "total_params = 0\n",
    "for var in model.parameter_list:\n",
    "    shape = var.get_shape()\n",
    "    num_params = 1\n",
    "    for dim in shape:\n",
    "        num_params *= dim\n",
    "    total_params += num_params\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "93977cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea5a233c35e406c9b0012727bd1ae31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Train Loss: 2.3366, Test Loss: 2.1772, Learning Rate: 2.41e-05\n",
      "Step 2, Train Loss: 2.3528, Test Loss: 2.2354, Learning Rate: 2.41e-05\n",
      "Step 3, Train Loss: 2.2099, Test Loss: 2.3289, Learning Rate: 2.41e-05\n",
      "Step 4, Train Loss: 2.3619, Test Loss: 2.2210, Learning Rate: 2.41e-05\n",
      "Step 5, Train Loss: 2.3546, Test Loss: 2.2714, Learning Rate: 2.41e-05\n",
      "Step 6, Train Loss: 2.3445, Test Loss: 2.2885, Learning Rate: 2.41e-05\n",
      "Step 7, Train Loss: 2.3219, Test Loss: 2.3852, Learning Rate: 2.41e-05\n",
      "Step 8, Train Loss: 2.2122, Test Loss: 2.4552, Learning Rate: 2.41e-05\n",
      "Step 9, Train Loss: 2.3676, Test Loss: 2.2330, Learning Rate: 2.41e-05\n",
      "Step 10, Train Loss: 2.3191, Test Loss: 2.2380, Learning Rate: 2.41e-05\n",
      "Step 11, Train Loss: 2.2587, Test Loss: 2.1669, Learning Rate: 2.41e-05\n",
      "Step 12, Train Loss: 2.2475, Test Loss: 2.3333, Learning Rate: 2.41e-05\n",
      "Step 13, Train Loss: 2.4222, Test Loss: 2.2924, Learning Rate: 2.41e-05\n",
      "Step 14, Train Loss: 2.3463, Test Loss: 2.3486, Learning Rate: 2.41e-05\n",
      "Step 15, Train Loss: 2.2424, Test Loss: 2.3723, Learning Rate: 2.41e-05\n",
      "Step 16, Train Loss: 2.5152, Test Loss: 2.3922, Learning Rate: 2.41e-05\n",
      "Step 17, Train Loss: 2.2185, Test Loss: 2.4741, Learning Rate: 2.41e-05\n",
      "Step 18, Train Loss: 2.3065, Test Loss: 2.2651, Learning Rate: 2.41e-05\n",
      "Step 19, Train Loss: 2.3106, Test Loss: 2.5819, Learning Rate: 2.41e-05\n",
      "Step 20, Train Loss: 2.3334, Test Loss: 2.3510, Learning Rate: 2.41e-05\n",
      "Step 21, Train Loss: 2.2662, Test Loss: 2.1806, Learning Rate: 2.41e-05\n",
      "Step 22, Train Loss: 2.2572, Test Loss: 2.4539, Learning Rate: 2.41e-05\n",
      "Step 23, Train Loss: 2.4221, Test Loss: 2.1762, Learning Rate: 2.41e-05\n",
      "Step 24, Train Loss: 2.4328, Test Loss: 2.1373, Learning Rate: 2.41e-05\n",
      "Step 25, Train Loss: 2.4790, Test Loss: 2.3012, Learning Rate: 2.41e-05\n",
      "Step 26, Train Loss: 2.3766, Test Loss: 2.2888, Learning Rate: 2.41e-05\n",
      "Step 27, Train Loss: 2.3279, Test Loss: 2.3547, Learning Rate: 2.41e-05\n",
      "Step 28, Train Loss: 2.2819, Test Loss: 2.2738, Learning Rate: 2.41e-05\n",
      "Step 29, Train Loss: 2.4183, Test Loss: 2.3785, Learning Rate: 2.41e-05\n",
      "Step 30, Train Loss: 2.3893, Test Loss: 2.3730, Learning Rate: 2.41e-05\n",
      "Step 31, Train Loss: 2.2658, Test Loss: 2.1950, Learning Rate: 2.41e-05\n",
      "Step 32, Train Loss: 2.5067, Test Loss: 2.4108, Learning Rate: 2.41e-05\n",
      "Step 33, Train Loss: 2.3742, Test Loss: 2.4201, Learning Rate: 2.41e-05\n",
      "Step 34, Train Loss: 2.4317, Test Loss: 2.4730, Learning Rate: 2.41e-05\n",
      "Step 35, Train Loss: 2.3010, Test Loss: 2.2636, Learning Rate: 2.41e-05\n",
      "Step 36, Train Loss: 2.4064, Test Loss: 2.1746, Learning Rate: 2.41e-05\n",
      "Step 37, Train Loss: 2.2002, Test Loss: 2.3892, Learning Rate: 2.41e-05\n",
      "Step 38, Train Loss: 2.4097, Test Loss: 2.2701, Learning Rate: 2.41e-05\n",
      "Step 39, Train Loss: 2.3316, Test Loss: 2.1742, Learning Rate: 2.41e-05\n",
      "Step 40, Train Loss: 2.3673, Test Loss: 2.3401, Learning Rate: 2.41e-05\n",
      "Step 41, Train Loss: 2.4729, Test Loss: 2.5423, Learning Rate: 2.41e-05\n",
      "Step 42, Train Loss: 2.2751, Test Loss: 2.2262, Learning Rate: 2.41e-05\n",
      "Step 43, Train Loss: 2.2954, Test Loss: 2.4593, Learning Rate: 2.41e-05\n",
      "Step 44, Train Loss: 2.3852, Test Loss: 2.3742, Learning Rate: 2.41e-05\n",
      "Step 45, Train Loss: 2.2478, Test Loss: 2.3419, Learning Rate: 2.41e-05\n",
      "Step 46, Train Loss: 2.3361, Test Loss: 2.2369, Learning Rate: 2.41e-05\n",
      "Step 47, Train Loss: 2.3293, Test Loss: 2.4209, Learning Rate: 2.41e-05\n",
      "Step 48, Train Loss: 2.3114, Test Loss: 2.3524, Learning Rate: 2.41e-05\n",
      "Step 49, Train Loss: 2.2736, Test Loss: 2.3926, Learning Rate: 2.41e-05\n",
      "Step 50, Train Loss: 2.3244, Test Loss: 2.4502, Learning Rate: 2.41e-05\n",
      "Step 51, Train Loss: 2.2955, Test Loss: 2.3719, Learning Rate: 2.41e-05\n",
      "Step 52, Train Loss: 2.2805, Test Loss: 2.3167, Learning Rate: 2.41e-05\n",
      "Step 53, Train Loss: 2.3184, Test Loss: 2.3026, Learning Rate: 2.41e-05\n",
      "Step 54, Train Loss: 2.4849, Test Loss: 2.4151, Learning Rate: 2.41e-05\n",
      "Step 55, Train Loss: 2.3915, Test Loss: 2.4148, Learning Rate: 2.41e-05\n",
      "Step 56, Train Loss: 2.2273, Test Loss: 2.2436, Learning Rate: 2.41e-05\n",
      "Step 57, Train Loss: 2.2455, Test Loss: 2.4940, Learning Rate: 2.41e-05\n",
      "Step 58, Train Loss: 2.2496, Test Loss: 2.3293, Learning Rate: 2.41e-05\n",
      "Step 59, Train Loss: 2.5203, Test Loss: 2.3820, Learning Rate: 2.41e-05\n",
      "Step 60, Train Loss: 2.2459, Test Loss: 2.1962, Learning Rate: 2.41e-05\n",
      "Step 61, Train Loss: 2.5181, Test Loss: 2.4964, Learning Rate: 2.41e-05\n",
      "Step 62, Train Loss: 2.3864, Test Loss: 2.2026, Learning Rate: 2.41e-05\n",
      "Step 63, Train Loss: 2.4440, Test Loss: 2.4539, Learning Rate: 2.41e-05\n",
      "Step 64, Train Loss: 2.4221, Test Loss: 2.1508, Learning Rate: 2.41e-05\n",
      "Step 65, Train Loss: 2.2345, Test Loss: 2.3753, Learning Rate: 2.41e-05\n",
      "Step 66, Train Loss: 2.3406, Test Loss: 2.2244, Learning Rate: 2.41e-05\n",
      "Step 67, Train Loss: 2.3584, Test Loss: 2.0526, Learning Rate: 2.41e-05\n",
      "Step 68, Train Loss: 2.2837, Test Loss: 2.2788, Learning Rate: 2.41e-05\n",
      "Step 69, Train Loss: 2.2641, Test Loss: 2.3416, Learning Rate: 2.41e-05\n",
      "Step 70, Train Loss: 2.3378, Test Loss: 2.2671, Learning Rate: 2.41e-05\n",
      "Step 71, Train Loss: 2.3059, Test Loss: 2.2895, Learning Rate: 2.41e-05\n",
      "Step 72, Train Loss: 2.2866, Test Loss: 2.3642, Learning Rate: 2.41e-05\n",
      "Step 73, Train Loss: 2.3861, Test Loss: 2.1187, Learning Rate: 2.41e-05\n",
      "Step 74, Train Loss: 2.4411, Test Loss: 2.2973, Learning Rate: 2.41e-05\n",
      "Step 75, Train Loss: 2.4561, Test Loss: 2.1780, Learning Rate: 2.41e-05\n",
      "Step 76, Train Loss: 2.4728, Test Loss: 2.2474, Learning Rate: 2.41e-05\n",
      "Step 77, Train Loss: 2.3073, Test Loss: 2.2673, Learning Rate: 2.41e-05\n",
      "Step 78, Train Loss: 2.2931, Test Loss: 2.3443, Learning Rate: 2.41e-05\n",
      "Step 79, Train Loss: 2.3037, Test Loss: 2.3161, Learning Rate: 2.41e-05\n",
      "Step 80, Train Loss: 2.2046, Test Loss: 2.2212, Learning Rate: 2.41e-05\n",
      "Step 81, Train Loss: 2.3339, Test Loss: 2.3729, Learning Rate: 2.41e-05\n",
      "Step 82, Train Loss: 2.3406, Test Loss: 2.4529, Learning Rate: 2.41e-05\n",
      "Step 83, Train Loss: 2.2064, Test Loss: 2.2951, Learning Rate: 2.41e-05\n",
      "Step 84, Train Loss: 2.3650, Test Loss: 2.3310, Learning Rate: 2.41e-05\n",
      "Step 85, Train Loss: 2.4995, Test Loss: 2.2195, Learning Rate: 2.41e-05\n",
      "Step 86, Train Loss: 2.4142, Test Loss: 2.2864, Learning Rate: 2.41e-05\n",
      "Step 87, Train Loss: 2.3275, Test Loss: 2.2389, Learning Rate: 2.41e-05\n",
      "Step 88, Train Loss: 2.3125, Test Loss: 2.2635, Learning Rate: 2.41e-05\n",
      "Step 89, Train Loss: 2.3462, Test Loss: 2.2756, Learning Rate: 2.41e-05\n",
      "Step 90, Train Loss: 2.3855, Test Loss: 2.3867, Learning Rate: 2.41e-05\n",
      "Step 91, Train Loss: 2.4962, Test Loss: 2.2235, Learning Rate: 2.41e-05\n",
      "Step 92, Train Loss: 2.2552, Test Loss: 2.3326, Learning Rate: 2.41e-05\n",
      "Step 93, Train Loss: 2.4053, Test Loss: 2.2292, Learning Rate: 2.41e-05\n",
      "Step 94, Train Loss: 2.3457, Test Loss: 2.3425, Learning Rate: 2.41e-05\n",
      "Step 95, Train Loss: 2.3705, Test Loss: 2.2168, Learning Rate: 2.41e-05\n",
      "Step 96, Train Loss: 2.3748, Test Loss: 2.2358, Learning Rate: 2.41e-05\n",
      "Step 97, Train Loss: 2.3153, Test Loss: 2.5473, Learning Rate: 2.41e-05\n",
      "Step 98, Train Loss: 2.3803, Test Loss: 2.6023, Learning Rate: 2.41e-05\n",
      "Step 99, Train Loss: 2.4227, Test Loss: 2.1913, Learning Rate: 2.41e-05\n",
      "Step 100, Train Loss: 2.4050, Test Loss: 2.4439, Learning Rate: 2.41e-05\n",
      "Step 101, Train Loss: 2.3882, Test Loss: 2.3175, Learning Rate: 2.41e-05\n",
      "Step 102, Train Loss: 2.3512, Test Loss: 2.4581, Learning Rate: 2.41e-05\n",
      "Step 103, Train Loss: 2.4522, Test Loss: 2.3285, Learning Rate: 2.41e-05\n",
      "Step 104, Train Loss: 2.3676, Test Loss: 2.7711, Learning Rate: 2.41e-05\n",
      "Step 105, Train Loss: 2.4329, Test Loss: 2.4147, Learning Rate: 2.41e-05\n",
      "Step 106, Train Loss: 2.4307, Test Loss: 2.1674, Learning Rate: 2.41e-05\n",
      "Step 107, Train Loss: 2.4270, Test Loss: 2.2461, Learning Rate: 2.41e-05\n",
      "Step 108, Train Loss: 2.3298, Test Loss: 2.3193, Learning Rate: 2.41e-05\n",
      "Step 109, Train Loss: 2.3357, Test Loss: 2.2165, Learning Rate: 2.41e-05\n",
      "Step 110, Train Loss: 2.2979, Test Loss: 2.5318, Learning Rate: 2.41e-05\n",
      "Step 111, Train Loss: 2.3978, Test Loss: 2.3809, Learning Rate: 2.41e-05\n",
      "Step 112, Train Loss: 2.2759, Test Loss: 2.5960, Learning Rate: 2.41e-05\n",
      "Step 113, Train Loss: 2.4518, Test Loss: 2.4408, Learning Rate: 2.41e-05\n",
      "Step 114, Train Loss: 2.4508, Test Loss: 2.3985, Learning Rate: 2.41e-05\n",
      "Step 115, Train Loss: 2.5593, Test Loss: 2.3172, Learning Rate: 2.41e-05\n",
      "Step 116, Train Loss: 2.4745, Test Loss: 2.3998, Learning Rate: 2.41e-05\n",
      "Step 117, Train Loss: 2.2169, Test Loss: 2.2403, Learning Rate: 2.41e-05\n",
      "Step 118, Train Loss: 2.3426, Test Loss: 2.4223, Learning Rate: 2.40e-05\n",
      "Step 119, Train Loss: 2.3900, Test Loss: 2.4245, Learning Rate: 2.40e-05\n",
      "Step 120, Train Loss: 2.5392, Test Loss: 2.3998, Learning Rate: 2.40e-05\n",
      "Step 121, Train Loss: 2.3220, Test Loss: 2.3643, Learning Rate: 2.40e-05\n",
      "Step 122, Train Loss: 2.2929, Test Loss: 2.3638, Learning Rate: 2.40e-05\n",
      "Step 123, Train Loss: 2.4005, Test Loss: 2.2994, Learning Rate: 2.40e-05\n",
      "Step 124, Train Loss: 2.3726, Test Loss: 2.2621, Learning Rate: 2.40e-05\n",
      "Step 125, Train Loss: 2.2781, Test Loss: 2.3827, Learning Rate: 2.40e-05\n",
      "Step 126, Train Loss: 2.4029, Test Loss: 2.4453, Learning Rate: 2.40e-05\n",
      "Step 127, Train Loss: 2.4214, Test Loss: 2.3397, Learning Rate: 2.40e-05\n",
      "Step 128, Train Loss: 2.2839, Test Loss: 2.2782, Learning Rate: 2.40e-05\n",
      "Step 129, Train Loss: 2.2216, Test Loss: 2.3334, Learning Rate: 2.40e-05\n",
      "Step 130, Train Loss: 2.2856, Test Loss: 2.3122, Learning Rate: 2.40e-05\n",
      "Step 131, Train Loss: 2.4229, Test Loss: 2.3451, Learning Rate: 2.40e-05\n",
      "Step 132, Train Loss: 2.3298, Test Loss: 2.3360, Learning Rate: 2.40e-05\n",
      "Step 133, Train Loss: 2.2354, Test Loss: 2.4331, Learning Rate: 2.40e-05\n",
      "Step 134, Train Loss: 2.3393, Test Loss: 2.3756, Learning Rate: 2.40e-05\n",
      "Step 135, Train Loss: 2.4222, Test Loss: 2.4618, Learning Rate: 2.40e-05\n",
      "Step 136, Train Loss: 2.3657, Test Loss: 2.2617, Learning Rate: 2.40e-05\n",
      "Step 137, Train Loss: 2.2758, Test Loss: 2.2240, Learning Rate: 2.40e-05\n",
      "Step 138, Train Loss: 2.3765, Test Loss: 2.4112, Learning Rate: 2.40e-05\n",
      "Step 139, Train Loss: 2.4265, Test Loss: 2.2875, Learning Rate: 2.40e-05\n",
      "Step 140, Train Loss: 2.3325, Test Loss: 2.2076, Learning Rate: 2.40e-05\n",
      "Step 141, Train Loss: 2.4108, Test Loss: 2.2875, Learning Rate: 2.40e-05\n",
      "Step 142, Train Loss: 2.3503, Test Loss: 2.4728, Learning Rate: 2.40e-05\n",
      "Step 143, Train Loss: 2.4003, Test Loss: 2.1972, Learning Rate: 2.40e-05\n",
      "Step 144, Train Loss: 2.2251, Test Loss: 2.2333, Learning Rate: 2.40e-05\n",
      "Step 145, Train Loss: 2.3232, Test Loss: 2.4685, Learning Rate: 2.40e-05\n",
      "Step 146, Train Loss: 2.4298, Test Loss: 2.5841, Learning Rate: 2.40e-05\n",
      "Step 147, Train Loss: 2.3840, Test Loss: 2.3815, Learning Rate: 2.40e-05\n",
      "Step 148, Train Loss: 2.3190, Test Loss: 2.4174, Learning Rate: 2.40e-05\n",
      "Step 149, Train Loss: 2.1993, Test Loss: 2.2816, Learning Rate: 2.40e-05\n",
      "Step 150, Train Loss: 2.2956, Test Loss: 2.2550, Learning Rate: 2.40e-05\n",
      "Step 151, Train Loss: 2.3650, Test Loss: 2.2864, Learning Rate: 2.40e-05\n",
      "Step 152, Train Loss: 2.3177, Test Loss: 2.3276, Learning Rate: 2.40e-05\n",
      "Step 153, Train Loss: 2.2344, Test Loss: 2.4081, Learning Rate: 2.40e-05\n",
      "Step 154, Train Loss: 2.3012, Test Loss: 2.3114, Learning Rate: 2.40e-05\n",
      "Step 155, Train Loss: 2.4336, Test Loss: 2.2805, Learning Rate: 2.40e-05\n",
      "Step 156, Train Loss: 2.3156, Test Loss: 2.3004, Learning Rate: 2.40e-05\n",
      "Step 157, Train Loss: 2.2684, Test Loss: 2.4307, Learning Rate: 2.40e-05\n",
      "Step 158, Train Loss: 2.3183, Test Loss: 2.4345, Learning Rate: 2.40e-05\n",
      "Step 159, Train Loss: 2.3635, Test Loss: 2.2565, Learning Rate: 2.40e-05\n",
      "Step 160, Train Loss: 2.3472, Test Loss: 2.2656, Learning Rate: 2.40e-05\n",
      "Step 161, Train Loss: 2.4288, Test Loss: 2.3997, Learning Rate: 2.40e-05\n",
      "Step 162, Train Loss: 2.3262, Test Loss: 2.3233, Learning Rate: 2.40e-05\n",
      "Step 163, Train Loss: 2.4388, Test Loss: 2.5226, Learning Rate: 2.40e-05\n",
      "Step 164, Train Loss: 2.4554, Test Loss: 2.3369, Learning Rate: 2.40e-05\n",
      "Step 165, Train Loss: 2.4310, Test Loss: 2.1893, Learning Rate: 2.40e-05\n",
      "Step 166, Train Loss: 2.4168, Test Loss: 2.2328, Learning Rate: 2.40e-05\n",
      "Step 167, Train Loss: 2.3359, Test Loss: 2.1453, Learning Rate: 2.40e-05\n",
      "Step 168, Train Loss: 2.3190, Test Loss: 2.4898, Learning Rate: 2.40e-05\n",
      "Step 169, Train Loss: 2.2844, Test Loss: 2.3501, Learning Rate: 2.40e-05\n",
      "Step 170, Train Loss: 2.2584, Test Loss: 2.4107, Learning Rate: 2.40e-05\n",
      "Step 171, Train Loss: 2.3517, Test Loss: 2.2024, Learning Rate: 2.40e-05\n",
      "Step 172, Train Loss: 2.3893, Test Loss: 2.3116, Learning Rate: 2.40e-05\n",
      "Step 173, Train Loss: 2.4777, Test Loss: 2.2134, Learning Rate: 2.40e-05\n",
      "Step 174, Train Loss: 2.2615, Test Loss: 2.2857, Learning Rate: 2.40e-05\n",
      "Step 175, Train Loss: 2.3796, Test Loss: 2.1990, Learning Rate: 2.40e-05\n",
      "Step 176, Train Loss: 2.4213, Test Loss: 2.2059, Learning Rate: 2.40e-05\n",
      "Step 177, Train Loss: 2.2839, Test Loss: 2.4236, Learning Rate: 2.40e-05\n",
      "Step 178, Train Loss: 2.4283, Test Loss: 2.2441, Learning Rate: 2.40e-05\n",
      "Step 179, Train Loss: 2.3627, Test Loss: 2.4430, Learning Rate: 2.40e-05\n",
      "Step 180, Train Loss: 2.3048, Test Loss: 2.2412, Learning Rate: 2.40e-05\n",
      "Step 181, Train Loss: 2.2684, Test Loss: 2.3536, Learning Rate: 2.40e-05\n",
      "Step 182, Train Loss: 2.3492, Test Loss: 2.3059, Learning Rate: 2.40e-05\n",
      "Step 183, Train Loss: 2.3065, Test Loss: 2.3540, Learning Rate: 2.40e-05\n",
      "Step 184, Train Loss: 2.4271, Test Loss: 2.3134, Learning Rate: 2.40e-05\n",
      "Step 185, Train Loss: 2.2628, Test Loss: 2.4528, Learning Rate: 2.40e-05\n",
      "Step 186, Train Loss: 2.2176, Test Loss: 2.4687, Learning Rate: 2.40e-05\n",
      "Step 187, Train Loss: 2.3264, Test Loss: 2.3848, Learning Rate: 2.40e-05\n",
      "Step 188, Train Loss: 2.1922, Test Loss: 2.4707, Learning Rate: 2.40e-05\n",
      "Step 189, Train Loss: 2.4600, Test Loss: 2.3192, Learning Rate: 2.40e-05\n",
      "Step 190, Train Loss: 2.3354, Test Loss: 2.3646, Learning Rate: 2.40e-05\n",
      "Step 191, Train Loss: 2.5344, Test Loss: 2.2205, Learning Rate: 2.40e-05\n",
      "Step 192, Train Loss: 2.2058, Test Loss: 2.2417, Learning Rate: 2.40e-05\n",
      "Step 193, Train Loss: 2.3737, Test Loss: 2.2736, Learning Rate: 2.40e-05\n",
      "Step 194, Train Loss: 2.2821, Test Loss: 2.3334, Learning Rate: 2.40e-05\n",
      "Step 195, Train Loss: 2.2922, Test Loss: 2.2554, Learning Rate: 2.40e-05\n",
      "Step 196, Train Loss: 2.3110, Test Loss: 2.3187, Learning Rate: 2.40e-05\n",
      "Step 197, Train Loss: 2.2845, Test Loss: 2.2886, Learning Rate: 2.40e-05\n",
      "Step 198, Train Loss: 2.3495, Test Loss: 2.3168, Learning Rate: 2.40e-05\n",
      "Step 199, Train Loss: 2.1924, Test Loss: 2.5957, Learning Rate: 2.40e-05\n",
      "Step 200, Train Loss: 2.3197, Test Loss: 2.3258, Learning Rate: 2.40e-05\n",
      "Step 201, Train Loss: 2.3736, Test Loss: 2.1238, Learning Rate: 2.40e-05\n",
      "Step 202, Train Loss: 2.3419, Test Loss: 2.2473, Learning Rate: 2.40e-05\n",
      "Step 203, Train Loss: 2.3057, Test Loss: 2.2761, Learning Rate: 2.40e-05\n",
      "Step 204, Train Loss: 2.2895, Test Loss: 2.3795, Learning Rate: 2.40e-05\n",
      "Step 205, Train Loss: 2.4941, Test Loss: 2.3593, Learning Rate: 2.40e-05\n",
      "Step 206, Train Loss: 2.3808, Test Loss: 2.4304, Learning Rate: 2.40e-05\n",
      "Step 207, Train Loss: 2.3212, Test Loss: 2.3711, Learning Rate: 2.40e-05\n",
      "Step 208, Train Loss: 2.3131, Test Loss: 2.3457, Learning Rate: 2.40e-05\n",
      "Step 209, Train Loss: 2.4134, Test Loss: 2.3835, Learning Rate: 2.40e-05\n",
      "Step 210, Train Loss: 2.4034, Test Loss: 2.3338, Learning Rate: 2.40e-05\n",
      "Step 211, Train Loss: 2.3731, Test Loss: 2.2875, Learning Rate: 2.40e-05\n",
      "Step 212, Train Loss: 2.3470, Test Loss: 2.3207, Learning Rate: 2.40e-05\n",
      "Step 213, Train Loss: 2.3993, Test Loss: 2.3515, Learning Rate: 2.40e-05\n",
      "Step 214, Train Loss: 2.3488, Test Loss: 2.4089, Learning Rate: 2.40e-05\n",
      "Step 215, Train Loss: 2.3430, Test Loss: 2.3072, Learning Rate: 2.40e-05\n",
      "Step 216, Train Loss: 2.3211, Test Loss: 2.3843, Learning Rate: 2.40e-05\n",
      "Step 217, Train Loss: 2.1983, Test Loss: 2.3522, Learning Rate: 2.40e-05\n",
      "Step 218, Train Loss: 2.2164, Test Loss: 2.2115, Learning Rate: 2.40e-05\n",
      "Step 219, Train Loss: 2.4633, Test Loss: 2.2221, Learning Rate: 2.40e-05\n",
      "Step 220, Train Loss: 2.3361, Test Loss: 2.2250, Learning Rate: 2.40e-05\n",
      "Step 221, Train Loss: 2.3154, Test Loss: 2.3737, Learning Rate: 2.40e-05\n",
      "Step 222, Train Loss: 2.3512, Test Loss: 2.2505, Learning Rate: 2.40e-05\n",
      "Step 223, Train Loss: 2.3540, Test Loss: 2.1967, Learning Rate: 2.40e-05\n",
      "Step 224, Train Loss: 2.4895, Test Loss: 2.3193, Learning Rate: 2.40e-05\n",
      "Step 225, Train Loss: 2.1702, Test Loss: 2.1061, Learning Rate: 2.40e-05\n",
      "Step 226, Train Loss: 2.3297, Test Loss: 2.3039, Learning Rate: 2.40e-05\n",
      "Step 227, Train Loss: 2.3859, Test Loss: 2.1194, Learning Rate: 2.40e-05\n",
      "Step 228, Train Loss: 2.2583, Test Loss: 2.4832, Learning Rate: 2.40e-05\n",
      "Step 229, Train Loss: 2.2258, Test Loss: 2.4657, Learning Rate: 2.40e-05\n",
      "Step 230, Train Loss: 2.2118, Test Loss: 2.3496, Learning Rate: 2.40e-05\n",
      "Step 231, Train Loss: 2.2740, Test Loss: 2.3496, Learning Rate: 2.40e-05\n",
      "Step 232, Train Loss: 2.3251, Test Loss: 2.5108, Learning Rate: 2.40e-05\n",
      "Step 233, Train Loss: 2.3567, Test Loss: 2.3782, Learning Rate: 2.40e-05\n",
      "Step 234, Train Loss: 2.2715, Test Loss: 2.4458, Learning Rate: 2.40e-05\n",
      "Step 235, Train Loss: 2.3519, Test Loss: 2.3974, Learning Rate: 2.40e-05\n",
      "Step 236, Train Loss: 2.4240, Test Loss: 2.4390, Learning Rate: 2.40e-05\n",
      "Step 237, Train Loss: 2.3373, Test Loss: 2.2678, Learning Rate: 2.40e-05\n",
      "Step 238, Train Loss: 2.3137, Test Loss: 2.1641, Learning Rate: 2.40e-05\n",
      "Step 239, Train Loss: 2.2669, Test Loss: 2.5463, Learning Rate: 2.39e-05\n",
      "Step 240, Train Loss: 2.2777, Test Loss: 2.2938, Learning Rate: 2.39e-05\n",
      "Step 241, Train Loss: 2.2502, Test Loss: 2.2666, Learning Rate: 2.39e-05\n",
      "Step 242, Train Loss: 2.2785, Test Loss: 2.2934, Learning Rate: 2.39e-05\n",
      "Step 243, Train Loss: 2.3196, Test Loss: 2.3233, Learning Rate: 2.39e-05\n",
      "Step 244, Train Loss: 2.2795, Test Loss: 2.4366, Learning Rate: 2.39e-05\n",
      "Step 245, Train Loss: 2.4211, Test Loss: 2.5559, Learning Rate: 2.39e-05\n",
      "Step 246, Train Loss: 2.4907, Test Loss: 2.3509, Learning Rate: 2.39e-05\n",
      "Step 247, Train Loss: 2.3781, Test Loss: 2.0898, Learning Rate: 2.39e-05\n",
      "Step 248, Train Loss: 2.3211, Test Loss: 2.4156, Learning Rate: 2.39e-05\n",
      "Step 249, Train Loss: 2.3740, Test Loss: 2.2916, Learning Rate: 2.39e-05\n",
      "Step 250, Train Loss: 2.3795, Test Loss: 2.4052, Learning Rate: 2.39e-05\n",
      "Step 251, Train Loss: 2.1819, Test Loss: 2.2914, Learning Rate: 2.39e-05\n",
      "Step 252, Train Loss: 2.3060, Test Loss: 2.3681, Learning Rate: 2.39e-05\n",
      "Step 253, Train Loss: 2.4180, Test Loss: 2.2396, Learning Rate: 2.39e-05\n",
      "Step 254, Train Loss: 2.2904, Test Loss: 2.3551, Learning Rate: 2.39e-05\n",
      "Step 255, Train Loss: 2.3984, Test Loss: 2.3631, Learning Rate: 2.39e-05\n",
      "Step 256, Train Loss: 2.3178, Test Loss: 2.5365, Learning Rate: 2.39e-05\n",
      "Step 257, Train Loss: 2.2601, Test Loss: 2.4794, Learning Rate: 2.39e-05\n",
      "Step 258, Train Loss: 2.2814, Test Loss: 2.3579, Learning Rate: 2.39e-05\n",
      "Step 259, Train Loss: 2.3301, Test Loss: 2.5574, Learning Rate: 2.39e-05\n",
      "Step 260, Train Loss: 2.4227, Test Loss: 2.4365, Learning Rate: 2.39e-05\n",
      "Step 261, Train Loss: 2.3534, Test Loss: 2.4927, Learning Rate: 2.39e-05\n",
      "Step 262, Train Loss: 2.3275, Test Loss: 2.0875, Learning Rate: 2.39e-05\n",
      "Step 263, Train Loss: 2.3269, Test Loss: 2.1102, Learning Rate: 2.39e-05\n",
      "Step 264, Train Loss: 2.3032, Test Loss: 2.4037, Learning Rate: 2.39e-05\n",
      "Step 265, Train Loss: 2.2072, Test Loss: 2.4960, Learning Rate: 2.39e-05\n",
      "Step 266, Train Loss: 2.3965, Test Loss: 2.2226, Learning Rate: 2.39e-05\n",
      "Step 267, Train Loss: 2.3271, Test Loss: 2.5914, Learning Rate: 2.39e-05\n",
      "Step 268, Train Loss: 2.4263, Test Loss: 2.2905, Learning Rate: 2.39e-05\n",
      "Step 269, Train Loss: 2.2840, Test Loss: 2.2535, Learning Rate: 2.39e-05\n",
      "Step 270, Train Loss: 2.4712, Test Loss: 2.3653, Learning Rate: 2.39e-05\n",
      "Step 271, Train Loss: 2.3524, Test Loss: 2.4061, Learning Rate: 2.39e-05\n",
      "Step 272, Train Loss: 2.2863, Test Loss: 2.6544, Learning Rate: 2.39e-05\n",
      "Step 273, Train Loss: 2.5037, Test Loss: 2.2661, Learning Rate: 2.39e-05\n",
      "Step 274, Train Loss: 2.2470, Test Loss: 2.2996, Learning Rate: 2.39e-05\n",
      "Step 275, Train Loss: 2.3088, Test Loss: 2.3115, Learning Rate: 2.39e-05\n",
      "Step 276, Train Loss: 2.3040, Test Loss: 2.3947, Learning Rate: 2.39e-05\n",
      "Step 277, Train Loss: 2.3189, Test Loss: 2.2359, Learning Rate: 2.39e-05\n",
      "Step 278, Train Loss: 2.2471, Test Loss: 2.5278, Learning Rate: 2.39e-05\n",
      "Step 279, Train Loss: 2.2964, Test Loss: 2.5213, Learning Rate: 2.39e-05\n",
      "Step 280, Train Loss: 2.1253, Test Loss: 2.3747, Learning Rate: 2.39e-05\n",
      "Step 281, Train Loss: 2.3298, Test Loss: 2.4680, Learning Rate: 2.39e-05\n",
      "Step 282, Train Loss: 2.3387, Test Loss: 2.2345, Learning Rate: 2.39e-05\n",
      "Step 283, Train Loss: 2.4030, Test Loss: 2.4163, Learning Rate: 2.39e-05\n",
      "Step 284, Train Loss: 2.2412, Test Loss: 2.1520, Learning Rate: 2.39e-05\n",
      "Step 285, Train Loss: 2.2909, Test Loss: 2.3018, Learning Rate: 2.39e-05\n",
      "Step 286, Train Loss: 2.3189, Test Loss: 2.2746, Learning Rate: 2.39e-05\n",
      "Step 287, Train Loss: 2.2036, Test Loss: 2.3419, Learning Rate: 2.39e-05\n",
      "Step 288, Train Loss: 2.4040, Test Loss: 2.5091, Learning Rate: 2.39e-05\n",
      "Step 289, Train Loss: 2.2077, Test Loss: 2.3008, Learning Rate: 2.39e-05\n",
      "Step 290, Train Loss: 2.3443, Test Loss: 2.3667, Learning Rate: 2.39e-05\n",
      "Step 291, Train Loss: 2.3101, Test Loss: 2.4136, Learning Rate: 2.39e-05\n",
      "Step 292, Train Loss: 2.2142, Test Loss: 2.3205, Learning Rate: 2.39e-05\n",
      "Step 293, Train Loss: 2.3763, Test Loss: 2.3329, Learning Rate: 2.39e-05\n",
      "Step 294, Train Loss: 2.3566, Test Loss: 2.2938, Learning Rate: 2.39e-05\n",
      "Step 295, Train Loss: 2.1495, Test Loss: 2.3993, Learning Rate: 2.39e-05\n",
      "Step 296, Train Loss: 2.3347, Test Loss: 2.4516, Learning Rate: 2.39e-05\n",
      "Step 297, Train Loss: 2.2979, Test Loss: 2.4454, Learning Rate: 2.39e-05\n",
      "Step 298, Train Loss: 2.3872, Test Loss: 2.4025, Learning Rate: 2.39e-05\n",
      "Step 299, Train Loss: 2.4607, Test Loss: 2.3248, Learning Rate: 2.39e-05\n",
      "Step 300, Train Loss: 2.3311, Test Loss: 2.3095, Learning Rate: 2.39e-05\n",
      "Step 301, Train Loss: 2.4027, Test Loss: 2.4765, Learning Rate: 2.39e-05\n",
      "Step 302, Train Loss: 2.3675, Test Loss: 2.1771, Learning Rate: 2.39e-05\n",
      "Step 303, Train Loss: 2.3748, Test Loss: 2.2706, Learning Rate: 2.39e-05\n",
      "Step 304, Train Loss: 2.4070, Test Loss: 2.3844, Learning Rate: 2.39e-05\n",
      "Step 305, Train Loss: 2.4354, Test Loss: 2.1826, Learning Rate: 2.39e-05\n",
      "Step 306, Train Loss: 2.4428, Test Loss: 2.3767, Learning Rate: 2.39e-05\n",
      "Step 307, Train Loss: 2.3343, Test Loss: 2.4253, Learning Rate: 2.39e-05\n",
      "Step 308, Train Loss: 2.3354, Test Loss: 1.9824, Learning Rate: 2.39e-05\n",
      "Step 309, Train Loss: 2.3149, Test Loss: 2.3733, Learning Rate: 2.39e-05\n",
      "Step 310, Train Loss: 2.4747, Test Loss: 2.5293, Learning Rate: 2.39e-05\n",
      "Step 311, Train Loss: 2.2709, Test Loss: 2.3238, Learning Rate: 2.39e-05\n",
      "Step 312, Train Loss: 2.2399, Test Loss: 2.3742, Learning Rate: 2.39e-05\n",
      "Step 313, Train Loss: 2.4172, Test Loss: 2.4158, Learning Rate: 2.39e-05\n",
      "Step 314, Train Loss: 2.3007, Test Loss: 2.2912, Learning Rate: 2.39e-05\n",
      "Step 315, Train Loss: 2.3250, Test Loss: 2.5682, Learning Rate: 2.39e-05\n",
      "Step 316, Train Loss: 2.3278, Test Loss: 2.4710, Learning Rate: 2.39e-05\n",
      "Step 317, Train Loss: 2.3910, Test Loss: 2.2867, Learning Rate: 2.39e-05\n",
      "Step 318, Train Loss: 2.3442, Test Loss: 2.3748, Learning Rate: 2.39e-05\n",
      "Step 319, Train Loss: 2.4141, Test Loss: 2.2775, Learning Rate: 2.39e-05\n",
      "Step 320, Train Loss: 2.1660, Test Loss: 2.4126, Learning Rate: 2.39e-05\n",
      "Step 321, Train Loss: 2.2291, Test Loss: 2.2648, Learning Rate: 2.39e-05\n",
      "Step 322, Train Loss: 2.2872, Test Loss: 2.5334, Learning Rate: 2.39e-05\n",
      "Step 323, Train Loss: 2.3750, Test Loss: 2.3208, Learning Rate: 2.39e-05\n",
      "Step 324, Train Loss: 2.4104, Test Loss: 2.3749, Learning Rate: 2.39e-05\n",
      "Step 325, Train Loss: 2.3226, Test Loss: 2.3558, Learning Rate: 2.39e-05\n",
      "Step 326, Train Loss: 2.2634, Test Loss: 2.6064, Learning Rate: 2.39e-05\n",
      "Step 327, Train Loss: 2.3909, Test Loss: 2.5447, Learning Rate: 2.39e-05\n",
      "Step 328, Train Loss: 2.2841, Test Loss: 2.4025, Learning Rate: 2.39e-05\n",
      "Step 329, Train Loss: 2.2841, Test Loss: 2.3641, Learning Rate: 2.39e-05\n",
      "Step 330, Train Loss: 2.1951, Test Loss: 2.2266, Learning Rate: 2.39e-05\n",
      "Step 331, Train Loss: 2.1691, Test Loss: 2.4113, Learning Rate: 2.39e-05\n",
      "Step 332, Train Loss: 2.4439, Test Loss: 2.3015, Learning Rate: 2.39e-05\n",
      "Step 333, Train Loss: 2.3326, Test Loss: 2.2466, Learning Rate: 2.39e-05\n",
      "Step 334, Train Loss: 2.2698, Test Loss: 2.4286, Learning Rate: 2.39e-05\n",
      "Step 335, Train Loss: 2.2363, Test Loss: 2.3690, Learning Rate: 2.39e-05\n",
      "Step 336, Train Loss: 2.3143, Test Loss: 2.2690, Learning Rate: 2.39e-05\n",
      "Step 337, Train Loss: 2.2419, Test Loss: 2.4440, Learning Rate: 2.39e-05\n",
      "Step 338, Train Loss: 2.2417, Test Loss: 2.1499, Learning Rate: 2.39e-05\n",
      "Step 339, Train Loss: 2.3697, Test Loss: 2.3920, Learning Rate: 2.39e-05\n",
      "Step 340, Train Loss: 2.3263, Test Loss: 2.3777, Learning Rate: 2.39e-05\n",
      "Step 341, Train Loss: 2.2772, Test Loss: 2.2818, Learning Rate: 2.39e-05\n",
      "Step 342, Train Loss: 2.3873, Test Loss: 2.3658, Learning Rate: 2.39e-05\n",
      "Step 343, Train Loss: 2.3630, Test Loss: 2.2864, Learning Rate: 2.39e-05\n",
      "Step 344, Train Loss: 2.2782, Test Loss: 2.4525, Learning Rate: 2.39e-05\n",
      "Step 345, Train Loss: 2.5047, Test Loss: 2.3090, Learning Rate: 2.39e-05\n",
      "Step 346, Train Loss: 2.3581, Test Loss: 2.2984, Learning Rate: 2.39e-05\n",
      "Step 347, Train Loss: 2.2831, Test Loss: 2.3932, Learning Rate: 2.39e-05\n",
      "Step 348, Train Loss: 2.3561, Test Loss: 2.5025, Learning Rate: 2.39e-05\n",
      "Step 349, Train Loss: 2.3574, Test Loss: 2.3899, Learning Rate: 2.39e-05\n",
      "Step 350, Train Loss: 2.3533, Test Loss: 2.3123, Learning Rate: 2.39e-05\n",
      "Step 351, Train Loss: 2.3833, Test Loss: 2.2229, Learning Rate: 2.39e-05\n",
      "Step 352, Train Loss: 2.2747, Test Loss: 2.2734, Learning Rate: 2.39e-05\n",
      "Step 353, Train Loss: 2.4318, Test Loss: 2.3575, Learning Rate: 2.39e-05\n",
      "Step 354, Train Loss: 2.3342, Test Loss: 2.4098, Learning Rate: 2.39e-05\n",
      "Step 355, Train Loss: 2.2828, Test Loss: 2.2663, Learning Rate: 2.39e-05\n",
      "Step 356, Train Loss: 2.3604, Test Loss: 2.4500, Learning Rate: 2.39e-05\n",
      "Step 357, Train Loss: 2.2823, Test Loss: 2.2924, Learning Rate: 2.39e-05\n",
      "Step 358, Train Loss: 2.4317, Test Loss: 2.2881, Learning Rate: 2.39e-05\n",
      "Step 359, Train Loss: 2.3734, Test Loss: 2.4235, Learning Rate: 2.38e-05\n",
      "Step 360, Train Loss: 2.2300, Test Loss: 2.2464, Learning Rate: 2.38e-05\n",
      "Step 361, Train Loss: 2.2589, Test Loss: 2.2526, Learning Rate: 2.38e-05\n",
      "Step 362, Train Loss: 2.3453, Test Loss: 2.2966, Learning Rate: 2.38e-05\n",
      "Step 363, Train Loss: 2.3151, Test Loss: 2.2913, Learning Rate: 2.38e-05\n",
      "Step 364, Train Loss: 2.2259, Test Loss: 2.2521, Learning Rate: 2.38e-05\n",
      "Step 365, Train Loss: 2.4062, Test Loss: 2.4522, Learning Rate: 2.38e-05\n",
      "Step 366, Train Loss: 2.2120, Test Loss: 2.3002, Learning Rate: 2.38e-05\n",
      "Step 367, Train Loss: 2.2749, Test Loss: 2.5750, Learning Rate: 2.38e-05\n",
      "Step 368, Train Loss: 2.2715, Test Loss: 2.2115, Learning Rate: 2.38e-05\n",
      "Step 369, Train Loss: 2.4877, Test Loss: 2.3331, Learning Rate: 2.38e-05\n",
      "Step 370, Train Loss: 2.5382, Test Loss: 2.1909, Learning Rate: 2.38e-05\n",
      "Step 371, Train Loss: 2.3949, Test Loss: 2.2247, Learning Rate: 2.38e-05\n",
      "Step 372, Train Loss: 2.4964, Test Loss: 2.0924, Learning Rate: 2.38e-05\n",
      "Step 373, Train Loss: 2.3356, Test Loss: 2.3170, Learning Rate: 2.38e-05\n",
      "Step 374, Train Loss: 2.2814, Test Loss: 2.4443, Learning Rate: 2.38e-05\n",
      "Step 375, Train Loss: 2.2465, Test Loss: 2.2881, Learning Rate: 2.38e-05\n",
      "Step 376, Train Loss: 2.4029, Test Loss: 2.1979, Learning Rate: 2.38e-05\n",
      "Step 377, Train Loss: 2.4043, Test Loss: 2.1987, Learning Rate: 2.38e-05\n",
      "Step 378, Train Loss: 2.2367, Test Loss: 2.3124, Learning Rate: 2.38e-05\n",
      "Step 379, Train Loss: 2.2854, Test Loss: 2.3692, Learning Rate: 2.38e-05\n",
      "Step 380, Train Loss: 2.4017, Test Loss: 2.5198, Learning Rate: 2.38e-05\n",
      "Step 381, Train Loss: 2.4153, Test Loss: 2.3115, Learning Rate: 2.38e-05\n",
      "Step 382, Train Loss: 2.2676, Test Loss: 2.3380, Learning Rate: 2.38e-05\n",
      "Step 383, Train Loss: 2.4399, Test Loss: 2.4243, Learning Rate: 2.38e-05\n",
      "Step 384, Train Loss: 2.2610, Test Loss: 2.3438, Learning Rate: 2.38e-05\n",
      "Step 385, Train Loss: 2.4487, Test Loss: 2.3926, Learning Rate: 2.38e-05\n",
      "Step 386, Train Loss: 2.4127, Test Loss: 2.3594, Learning Rate: 2.38e-05\n",
      "Step 387, Train Loss: 2.3993, Test Loss: 2.4275, Learning Rate: 2.38e-05\n",
      "Step 388, Train Loss: 2.3423, Test Loss: 2.2714, Learning Rate: 2.38e-05\n",
      "Step 389, Train Loss: 2.4110, Test Loss: 2.2990, Learning Rate: 2.38e-05\n",
      "Step 390, Train Loss: 2.3141, Test Loss: 2.4688, Learning Rate: 2.38e-05\n",
      "Step 391, Train Loss: 2.4239, Test Loss: 2.5138, Learning Rate: 2.38e-05\n",
      "Step 392, Train Loss: 2.3136, Test Loss: 2.2850, Learning Rate: 2.38e-05\n",
      "Step 393, Train Loss: 2.4416, Test Loss: 2.2771, Learning Rate: 2.38e-05\n",
      "Step 394, Train Loss: 2.3704, Test Loss: 2.2547, Learning Rate: 2.38e-05\n",
      "Step 395, Train Loss: 2.3447, Test Loss: 2.2729, Learning Rate: 2.38e-05\n",
      "Step 396, Train Loss: 2.5047, Test Loss: 2.2446, Learning Rate: 2.38e-05\n",
      "Step 397, Train Loss: 2.3406, Test Loss: 2.3451, Learning Rate: 2.38e-05\n",
      "Step 398, Train Loss: 2.4130, Test Loss: 2.3555, Learning Rate: 2.38e-05\n",
      "Step 399, Train Loss: 2.3482, Test Loss: 2.1178, Learning Rate: 2.38e-05\n",
      "Step 400, Train Loss: 2.2632, Test Loss: 2.3481, Learning Rate: 2.38e-05\n",
      "Step 401, Train Loss: 2.2728, Test Loss: 2.4768, Learning Rate: 2.38e-05\n",
      "Step 402, Train Loss: 2.5178, Test Loss: 2.1879, Learning Rate: 2.38e-05\n",
      "Step 403, Train Loss: 2.3919, Test Loss: 2.5200, Learning Rate: 2.38e-05\n",
      "Step 404, Train Loss: 2.5282, Test Loss: 2.4799, Learning Rate: 2.38e-05\n",
      "Step 405, Train Loss: 2.4082, Test Loss: 2.2316, Learning Rate: 2.38e-05\n",
      "Step 406, Train Loss: 2.4591, Test Loss: 2.2872, Learning Rate: 2.38e-05\n",
      "Step 407, Train Loss: 2.2644, Test Loss: 2.6013, Learning Rate: 2.38e-05\n",
      "Step 408, Train Loss: 2.3636, Test Loss: 2.3189, Learning Rate: 2.38e-05\n",
      "Step 409, Train Loss: 2.4010, Test Loss: 2.4825, Learning Rate: 2.38e-05\n",
      "Step 410, Train Loss: 2.4339, Test Loss: 2.2708, Learning Rate: 2.38e-05\n",
      "Step 411, Train Loss: 2.3373, Test Loss: 2.2548, Learning Rate: 2.38e-05\n",
      "Step 412, Train Loss: 2.3356, Test Loss: 2.2414, Learning Rate: 2.38e-05\n",
      "Step 413, Train Loss: 2.2091, Test Loss: 2.4437, Learning Rate: 2.38e-05\n",
      "Step 414, Train Loss: 2.3154, Test Loss: 2.3477, Learning Rate: 2.38e-05\n",
      "Step 415, Train Loss: 2.3783, Test Loss: 2.2449, Learning Rate: 2.38e-05\n",
      "Step 416, Train Loss: 2.3039, Test Loss: 2.4660, Learning Rate: 2.38e-05\n",
      "Step 417, Train Loss: 2.3714, Test Loss: 2.3037, Learning Rate: 2.38e-05\n",
      "Step 418, Train Loss: 2.1471, Test Loss: 2.2755, Learning Rate: 2.38e-05\n",
      "Step 419, Train Loss: 2.3510, Test Loss: 2.2754, Learning Rate: 2.38e-05\n",
      "Step 420, Train Loss: 2.2761, Test Loss: 2.2428, Learning Rate: 2.38e-05\n",
      "Step 421, Train Loss: 2.1865, Test Loss: 2.4459, Learning Rate: 2.38e-05\n",
      "Step 422, Train Loss: 2.2574, Test Loss: 2.4142, Learning Rate: 2.38e-05\n",
      "Step 423, Train Loss: 2.4186, Test Loss: 2.2110, Learning Rate: 2.38e-05\n",
      "Step 424, Train Loss: 2.2333, Test Loss: 2.1675, Learning Rate: 2.38e-05\n",
      "Step 425, Train Loss: 2.3785, Test Loss: 2.3044, Learning Rate: 2.38e-05\n",
      "Step 426, Train Loss: 2.3282, Test Loss: 2.6045, Learning Rate: 2.38e-05\n",
      "Step 427, Train Loss: 2.4529, Test Loss: 2.4439, Learning Rate: 2.38e-05\n",
      "Step 428, Train Loss: 2.3831, Test Loss: 2.2283, Learning Rate: 2.38e-05\n",
      "Step 429, Train Loss: 2.2509, Test Loss: 2.2300, Learning Rate: 2.38e-05\n",
      "Step 430, Train Loss: 2.2870, Test Loss: 2.4762, Learning Rate: 2.38e-05\n",
      "Step 431, Train Loss: 2.3812, Test Loss: 2.3894, Learning Rate: 2.38e-05\n",
      "Step 432, Train Loss: 2.3631, Test Loss: 2.3057, Learning Rate: 2.38e-05\n",
      "Step 433, Train Loss: 2.2877, Test Loss: 2.4581, Learning Rate: 2.38e-05\n",
      "Step 434, Train Loss: 2.2212, Test Loss: 2.4294, Learning Rate: 2.38e-05\n",
      "Step 435, Train Loss: 2.3556, Test Loss: 2.3543, Learning Rate: 2.38e-05\n",
      "Step 436, Train Loss: 2.1923, Test Loss: 2.3748, Learning Rate: 2.38e-05\n",
      "Step 437, Train Loss: 2.3007, Test Loss: 2.4194, Learning Rate: 2.38e-05\n",
      "Step 438, Train Loss: 2.2771, Test Loss: 2.2958, Learning Rate: 2.38e-05\n",
      "Step 439, Train Loss: 2.3794, Test Loss: 2.5661, Learning Rate: 2.38e-05\n",
      "Step 440, Train Loss: 2.3558, Test Loss: 2.3268, Learning Rate: 2.38e-05\n",
      "Step 441, Train Loss: 2.2471, Test Loss: 2.4315, Learning Rate: 2.38e-05\n",
      "Step 442, Train Loss: 2.3137, Test Loss: 2.2363, Learning Rate: 2.38e-05\n",
      "Step 443, Train Loss: 2.3136, Test Loss: 2.3106, Learning Rate: 2.38e-05\n",
      "Step 444, Train Loss: 2.2620, Test Loss: 2.3274, Learning Rate: 2.38e-05\n",
      "Step 445, Train Loss: 2.3926, Test Loss: 2.1546, Learning Rate: 2.38e-05\n",
      "Step 446, Train Loss: 2.2842, Test Loss: 2.4473, Learning Rate: 2.38e-05\n",
      "Step 447, Train Loss: 2.4519, Test Loss: 2.2955, Learning Rate: 2.38e-05\n",
      "Step 448, Train Loss: 2.2709, Test Loss: 2.3306, Learning Rate: 2.38e-05\n",
      "Step 449, Train Loss: 2.3416, Test Loss: 2.2110, Learning Rate: 2.38e-05\n",
      "Step 450, Train Loss: 2.2858, Test Loss: 2.2318, Learning Rate: 2.38e-05\n",
      "Step 451, Train Loss: 2.2299, Test Loss: 2.3540, Learning Rate: 2.38e-05\n",
      "Step 452, Train Loss: 2.3239, Test Loss: 2.2508, Learning Rate: 2.38e-05\n",
      "Step 453, Train Loss: 2.3713, Test Loss: 2.3861, Learning Rate: 2.38e-05\n",
      "Step 454, Train Loss: 2.3748, Test Loss: 2.2667, Learning Rate: 2.38e-05\n",
      "Step 455, Train Loss: 2.3468, Test Loss: 2.4728, Learning Rate: 2.38e-05\n",
      "Step 456, Train Loss: 2.5070, Test Loss: 2.3411, Learning Rate: 2.38e-05\n",
      "Step 457, Train Loss: 2.2360, Test Loss: 2.1906, Learning Rate: 2.38e-05\n",
      "Step 458, Train Loss: 2.3732, Test Loss: 2.3838, Learning Rate: 2.38e-05\n",
      "Step 459, Train Loss: 2.2011, Test Loss: 2.4174, Learning Rate: 2.38e-05\n",
      "Step 460, Train Loss: 2.4175, Test Loss: 2.4269, Learning Rate: 2.38e-05\n",
      "Step 461, Train Loss: 2.2657, Test Loss: 2.3037, Learning Rate: 2.38e-05\n",
      "Step 462, Train Loss: 2.4647, Test Loss: 2.3569, Learning Rate: 2.38e-05\n",
      "Step 463, Train Loss: 2.2671, Test Loss: 2.3334, Learning Rate: 2.38e-05\n",
      "Step 464, Train Loss: 2.3397, Test Loss: 2.1317, Learning Rate: 2.38e-05\n",
      "Step 465, Train Loss: 2.3944, Test Loss: 2.1036, Learning Rate: 2.38e-05\n",
      "Step 466, Train Loss: 2.4152, Test Loss: 2.3448, Learning Rate: 2.38e-05\n",
      "Step 467, Train Loss: 2.3906, Test Loss: 2.2216, Learning Rate: 2.38e-05\n",
      "Step 468, Train Loss: 2.2034, Test Loss: 2.2721, Learning Rate: 2.38e-05\n",
      "Step 469, Train Loss: 2.3380, Test Loss: 2.3042, Learning Rate: 2.38e-05\n",
      "Step 470, Train Loss: 2.2816, Test Loss: 2.3011, Learning Rate: 2.38e-05\n",
      "Step 471, Train Loss: 2.2968, Test Loss: 2.3682, Learning Rate: 2.38e-05\n",
      "Step 472, Train Loss: 2.2667, Test Loss: 2.3571, Learning Rate: 2.38e-05\n",
      "Step 473, Train Loss: 2.2204, Test Loss: 2.4379, Learning Rate: 2.38e-05\n",
      "Step 474, Train Loss: 2.3347, Test Loss: 2.2024, Learning Rate: 2.38e-05\n",
      "Step 475, Train Loss: 2.3643, Test Loss: 2.2488, Learning Rate: 2.38e-05\n",
      "Step 476, Train Loss: 2.4155, Test Loss: 2.2575, Learning Rate: 2.38e-05\n",
      "Step 477, Train Loss: 2.4055, Test Loss: 2.3100, Learning Rate: 2.38e-05\n",
      "Step 478, Train Loss: 2.2794, Test Loss: 2.3128, Learning Rate: 2.38e-05\n",
      "Step 479, Train Loss: 2.4359, Test Loss: 2.3293, Learning Rate: 2.38e-05\n",
      "Step 480, Train Loss: 2.1922, Test Loss: 2.3351, Learning Rate: 2.38e-05\n",
      "Step 481, Train Loss: 2.2398, Test Loss: 2.2082, Learning Rate: 2.37e-05\n",
      "Step 482, Train Loss: 2.3213, Test Loss: 2.4415, Learning Rate: 2.37e-05\n",
      "Step 483, Train Loss: 2.3496, Test Loss: 2.4442, Learning Rate: 2.37e-05\n",
      "Step 484, Train Loss: 2.2978, Test Loss: 2.2287, Learning Rate: 2.37e-05\n",
      "Step 485, Train Loss: 2.2041, Test Loss: 2.3019, Learning Rate: 2.37e-05\n",
      "Step 486, Train Loss: 2.3083, Test Loss: 2.0594, Learning Rate: 2.37e-05\n",
      "Step 487, Train Loss: 2.1891, Test Loss: 2.5529, Learning Rate: 2.37e-05\n",
      "Step 488, Train Loss: 2.3024, Test Loss: 2.3158, Learning Rate: 2.37e-05\n",
      "Step 489, Train Loss: 2.4859, Test Loss: 2.2219, Learning Rate: 2.37e-05\n",
      "Step 490, Train Loss: 2.3741, Test Loss: 2.2154, Learning Rate: 2.37e-05\n",
      "Step 491, Train Loss: 2.2922, Test Loss: 2.4020, Learning Rate: 2.37e-05\n",
      "Step 492, Train Loss: 2.3230, Test Loss: 2.2240, Learning Rate: 2.37e-05\n",
      "Step 493, Train Loss: 2.3956, Test Loss: 2.2522, Learning Rate: 2.37e-05\n",
      "Step 494, Train Loss: 2.2188, Test Loss: 2.2200, Learning Rate: 2.37e-05\n",
      "Step 495, Train Loss: 2.3191, Test Loss: 2.4557, Learning Rate: 2.37e-05\n",
      "Step 496, Train Loss: 2.1768, Test Loss: 2.4213, Learning Rate: 2.37e-05\n",
      "Step 497, Train Loss: 2.2717, Test Loss: 2.4110, Learning Rate: 2.37e-05\n",
      "Step 498, Train Loss: 2.3124, Test Loss: 2.2739, Learning Rate: 2.37e-05\n",
      "Step 499, Train Loss: 2.3016, Test Loss: 2.4810, Learning Rate: 2.37e-05\n",
      "Step 500, Train Loss: 2.3708, Test Loss: 2.2431, Learning Rate: 2.37e-05\n",
      "Step 501, Train Loss: 2.4811, Test Loss: 2.3760, Learning Rate: 2.37e-05\n",
      "Step 502, Train Loss: 2.3961, Test Loss: 2.3263, Learning Rate: 2.37e-05\n",
      "Step 503, Train Loss: 2.4784, Test Loss: 2.3422, Learning Rate: 2.37e-05\n",
      "Step 504, Train Loss: 2.3680, Test Loss: 2.4122, Learning Rate: 2.37e-05\n",
      "Step 505, Train Loss: 2.2313, Test Loss: 2.1996, Learning Rate: 2.37e-05\n",
      "Step 506, Train Loss: 2.3742, Test Loss: 2.2065, Learning Rate: 2.37e-05\n",
      "Step 507, Train Loss: 2.2754, Test Loss: 1.9754, Learning Rate: 2.37e-05\n",
      "Step 508, Train Loss: 2.2386, Test Loss: 2.3431, Learning Rate: 2.37e-05\n",
      "Step 509, Train Loss: 2.2865, Test Loss: 2.5343, Learning Rate: 2.37e-05\n",
      "Step 510, Train Loss: 2.2842, Test Loss: 2.3610, Learning Rate: 2.37e-05\n",
      "Step 511, Train Loss: 2.3290, Test Loss: 2.2974, Learning Rate: 2.37e-05\n",
      "Step 512, Train Loss: 2.3573, Test Loss: 2.1840, Learning Rate: 2.37e-05\n",
      "Step 513, Train Loss: 2.3201, Test Loss: 2.1701, Learning Rate: 2.37e-05\n",
      "Step 514, Train Loss: 2.3545, Test Loss: 2.0536, Learning Rate: 2.37e-05\n",
      "Step 515, Train Loss: 2.4065, Test Loss: 2.2766, Learning Rate: 2.37e-05\n",
      "Step 516, Train Loss: 2.2650, Test Loss: 2.2513, Learning Rate: 2.37e-05\n",
      "Step 517, Train Loss: 2.2082, Test Loss: 2.3651, Learning Rate: 2.37e-05\n",
      "Step 518, Train Loss: 2.3663, Test Loss: 2.2514, Learning Rate: 2.37e-05\n",
      "Step 519, Train Loss: 2.4192, Test Loss: 2.3182, Learning Rate: 2.37e-05\n",
      "Step 520, Train Loss: 2.3036, Test Loss: 2.2674, Learning Rate: 2.37e-05\n",
      "Step 521, Train Loss: 2.3129, Test Loss: 2.3935, Learning Rate: 2.37e-05\n",
      "Step 522, Train Loss: 2.3656, Test Loss: 2.4100, Learning Rate: 2.37e-05\n",
      "Step 523, Train Loss: 2.4197, Test Loss: 2.0823, Learning Rate: 2.37e-05\n",
      "Step 524, Train Loss: 2.3620, Test Loss: 2.3485, Learning Rate: 2.37e-05\n",
      "Step 525, Train Loss: 2.3578, Test Loss: 2.4571, Learning Rate: 2.37e-05\n",
      "Step 526, Train Loss: 2.3689, Test Loss: 2.2231, Learning Rate: 2.37e-05\n",
      "Step 527, Train Loss: 2.3935, Test Loss: 2.2746, Learning Rate: 2.37e-05\n",
      "Step 528, Train Loss: 2.2834, Test Loss: 2.3361, Learning Rate: 2.37e-05\n",
      "Step 529, Train Loss: 2.3731, Test Loss: 2.1878, Learning Rate: 2.37e-05\n",
      "Step 530, Train Loss: 2.4068, Test Loss: 2.3389, Learning Rate: 2.37e-05\n",
      "Step 531, Train Loss: 2.3686, Test Loss: 2.4863, Learning Rate: 2.37e-05\n",
      "Step 532, Train Loss: 2.3581, Test Loss: 2.2802, Learning Rate: 2.37e-05\n",
      "Step 533, Train Loss: 2.3698, Test Loss: 2.4821, Learning Rate: 2.37e-05\n",
      "Step 534, Train Loss: 2.5278, Test Loss: 2.4782, Learning Rate: 2.37e-05\n",
      "Step 535, Train Loss: 2.3586, Test Loss: 2.4345, Learning Rate: 2.37e-05\n",
      "Step 536, Train Loss: 2.2222, Test Loss: 2.2785, Learning Rate: 2.37e-05\n",
      "Step 537, Train Loss: 2.4047, Test Loss: 2.2189, Learning Rate: 2.37e-05\n",
      "Step 538, Train Loss: 2.2615, Test Loss: 2.5232, Learning Rate: 2.37e-05\n",
      "Step 539, Train Loss: 2.2446, Test Loss: 2.3144, Learning Rate: 2.37e-05\n",
      "Step 540, Train Loss: 2.4240, Test Loss: 2.3846, Learning Rate: 2.37e-05\n",
      "Step 541, Train Loss: 2.3891, Test Loss: 2.3293, Learning Rate: 2.37e-05\n",
      "Step 542, Train Loss: 2.3261, Test Loss: 2.4694, Learning Rate: 2.37e-05\n",
      "Step 543, Train Loss: 2.3840, Test Loss: 2.3530, Learning Rate: 2.37e-05\n",
      "Step 544, Train Loss: 2.4586, Test Loss: 2.2369, Learning Rate: 2.37e-05\n",
      "Step 545, Train Loss: 2.3994, Test Loss: 2.3332, Learning Rate: 2.37e-05\n",
      "Step 546, Train Loss: 2.2959, Test Loss: 2.2393, Learning Rate: 2.37e-05\n",
      "Step 547, Train Loss: 2.3303, Test Loss: 2.1952, Learning Rate: 2.37e-05\n",
      "Step 548, Train Loss: 2.3812, Test Loss: 2.2491, Learning Rate: 2.37e-05\n",
      "Step 549, Train Loss: 2.2986, Test Loss: 2.3706, Learning Rate: 2.37e-05\n",
      "Step 550, Train Loss: 2.3824, Test Loss: 2.1185, Learning Rate: 2.37e-05\n",
      "Step 551, Train Loss: 2.3740, Test Loss: 2.2222, Learning Rate: 2.37e-05\n",
      "Step 552, Train Loss: 2.5045, Test Loss: 2.3841, Learning Rate: 2.37e-05\n",
      "Step 553, Train Loss: 2.3038, Test Loss: 2.4468, Learning Rate: 2.37e-05\n",
      "Step 554, Train Loss: 2.2596, Test Loss: 2.0755, Learning Rate: 2.37e-05\n",
      "Step 555, Train Loss: 2.2712, Test Loss: 2.3683, Learning Rate: 2.37e-05\n",
      "Step 556, Train Loss: 2.2835, Test Loss: 2.2886, Learning Rate: 2.37e-05\n",
      "Step 557, Train Loss: 2.4065, Test Loss: 2.3719, Learning Rate: 2.37e-05\n",
      "Step 558, Train Loss: 2.3752, Test Loss: 2.3132, Learning Rate: 2.37e-05\n",
      "Step 559, Train Loss: 2.3772, Test Loss: 2.2677, Learning Rate: 2.37e-05\n",
      "Step 560, Train Loss: 2.2384, Test Loss: 2.4332, Learning Rate: 2.37e-05\n",
      "Step 561, Train Loss: 2.3718, Test Loss: 2.2297, Learning Rate: 2.37e-05\n",
      "Step 562, Train Loss: 2.2778, Test Loss: 2.4361, Learning Rate: 2.37e-05\n",
      "Step 563, Train Loss: 2.2702, Test Loss: 2.1886, Learning Rate: 2.37e-05\n",
      "Step 564, Train Loss: 2.2596, Test Loss: 2.2484, Learning Rate: 2.37e-05\n",
      "Step 565, Train Loss: 2.3544, Test Loss: 2.3847, Learning Rate: 2.37e-05\n",
      "Step 566, Train Loss: 2.4158, Test Loss: 2.4284, Learning Rate: 2.37e-05\n",
      "Step 567, Train Loss: 2.3565, Test Loss: 2.2059, Learning Rate: 2.37e-05\n",
      "Step 568, Train Loss: 2.3500, Test Loss: 2.1889, Learning Rate: 2.37e-05\n",
      "Step 569, Train Loss: 2.3534, Test Loss: 2.3978, Learning Rate: 2.37e-05\n",
      "Step 570, Train Loss: 2.3392, Test Loss: 2.4359, Learning Rate: 2.37e-05\n",
      "Step 571, Train Loss: 2.2922, Test Loss: 2.4349, Learning Rate: 2.37e-05\n",
      "Step 572, Train Loss: 2.3533, Test Loss: 2.3277, Learning Rate: 2.37e-05\n",
      "Step 573, Train Loss: 2.3697, Test Loss: 2.1977, Learning Rate: 2.37e-05\n",
      "Step 574, Train Loss: 2.2942, Test Loss: 2.4664, Learning Rate: 2.37e-05\n",
      "Step 575, Train Loss: 2.2082, Test Loss: 2.5079, Learning Rate: 2.37e-05\n",
      "Step 576, Train Loss: 2.3433, Test Loss: 2.3119, Learning Rate: 2.37e-05\n",
      "Step 577, Train Loss: 2.4170, Test Loss: 2.1995, Learning Rate: 2.37e-05\n",
      "Step 578, Train Loss: 2.4571, Test Loss: 2.2485, Learning Rate: 2.37e-05\n",
      "Step 579, Train Loss: 2.3034, Test Loss: 2.3123, Learning Rate: 2.37e-05\n",
      "Step 580, Train Loss: 2.2362, Test Loss: 2.2884, Learning Rate: 2.37e-05\n",
      "Step 581, Train Loss: 2.3320, Test Loss: 2.3820, Learning Rate: 2.37e-05\n",
      "Step 582, Train Loss: 2.2788, Test Loss: 2.1720, Learning Rate: 2.37e-05\n",
      "Step 583, Train Loss: 2.4229, Test Loss: 2.4455, Learning Rate: 2.37e-05\n",
      "Step 584, Train Loss: 2.1960, Test Loss: 2.4165, Learning Rate: 2.37e-05\n",
      "Step 585, Train Loss: 2.3257, Test Loss: 2.3976, Learning Rate: 2.37e-05\n",
      "Step 586, Train Loss: 2.2386, Test Loss: 2.3503, Learning Rate: 2.37e-05\n",
      "Step 587, Train Loss: 2.3470, Test Loss: 2.3472, Learning Rate: 2.37e-05\n",
      "Step 588, Train Loss: 2.3385, Test Loss: 2.3379, Learning Rate: 2.37e-05\n",
      "Step 589, Train Loss: 2.3434, Test Loss: 2.2020, Learning Rate: 2.37e-05\n",
      "Step 590, Train Loss: 2.3158, Test Loss: 2.3295, Learning Rate: 2.37e-05\n",
      "Step 591, Train Loss: 2.4280, Test Loss: 2.3488, Learning Rate: 2.37e-05\n",
      "Step 592, Train Loss: 2.3940, Test Loss: 2.3205, Learning Rate: 2.37e-05\n",
      "Step 593, Train Loss: 2.4144, Test Loss: 2.2545, Learning Rate: 2.37e-05\n",
      "Step 594, Train Loss: 2.4016, Test Loss: 2.5081, Learning Rate: 2.37e-05\n",
      "Step 595, Train Loss: 2.3496, Test Loss: 2.2593, Learning Rate: 2.37e-05\n",
      "Step 596, Train Loss: 2.1997, Test Loss: 2.0026, Learning Rate: 2.37e-05\n",
      "Step 597, Train Loss: 2.3454, Test Loss: 2.3497, Learning Rate: 2.37e-05\n",
      "Step 598, Train Loss: 2.4160, Test Loss: 2.3336, Learning Rate: 2.37e-05\n",
      "Step 599, Train Loss: 2.3157, Test Loss: 2.4109, Learning Rate: 2.37e-05\n",
      "Step 600, Train Loss: 2.3484, Test Loss: 2.3071, Learning Rate: 2.37e-05\n",
      "Step 601, Train Loss: 2.3554, Test Loss: 2.4665, Learning Rate: 2.37e-05\n",
      "Step 602, Train Loss: 2.3883, Test Loss: 2.2205, Learning Rate: 2.36e-05\n",
      "Step 603, Train Loss: 2.3376, Test Loss: 2.2657, Learning Rate: 2.36e-05\n",
      "Step 604, Train Loss: 2.4649, Test Loss: 2.4125, Learning Rate: 2.36e-05\n",
      "Step 605, Train Loss: 2.2845, Test Loss: 2.3057, Learning Rate: 2.36e-05\n",
      "Step 606, Train Loss: 2.2446, Test Loss: 2.3601, Learning Rate: 2.36e-05\n",
      "Step 607, Train Loss: 2.3928, Test Loss: 2.2096, Learning Rate: 2.36e-05\n",
      "Step 608, Train Loss: 2.3385, Test Loss: 2.4361, Learning Rate: 2.36e-05\n",
      "Step 609, Train Loss: 2.3467, Test Loss: 2.2065, Learning Rate: 2.36e-05\n",
      "Step 610, Train Loss: 2.3091, Test Loss: 2.1752, Learning Rate: 2.36e-05\n",
      "Step 611, Train Loss: 2.4592, Test Loss: 2.1768, Learning Rate: 2.36e-05\n",
      "Step 612, Train Loss: 2.4175, Test Loss: 2.2970, Learning Rate: 2.36e-05\n",
      "Step 613, Train Loss: 2.3078, Test Loss: 2.2519, Learning Rate: 2.36e-05\n",
      "Step 614, Train Loss: 2.3555, Test Loss: 2.3258, Learning Rate: 2.36e-05\n",
      "Step 615, Train Loss: 2.3763, Test Loss: 2.3608, Learning Rate: 2.36e-05\n",
      "Step 616, Train Loss: 2.3577, Test Loss: 2.2834, Learning Rate: 2.36e-05\n",
      "Step 617, Train Loss: 2.4374, Test Loss: 2.3422, Learning Rate: 2.36e-05\n",
      "Step 618, Train Loss: 2.2683, Test Loss: 2.4880, Learning Rate: 2.36e-05\n",
      "Step 619, Train Loss: 2.2622, Test Loss: 2.3915, Learning Rate: 2.36e-05\n",
      "Step 620, Train Loss: 2.3633, Test Loss: 2.1590, Learning Rate: 2.36e-05\n",
      "Step 621, Train Loss: 2.3769, Test Loss: 2.5565, Learning Rate: 2.36e-05\n",
      "Step 622, Train Loss: 2.4246, Test Loss: 2.1813, Learning Rate: 2.36e-05\n",
      "Step 623, Train Loss: 2.3306, Test Loss: 2.4630, Learning Rate: 2.36e-05\n",
      "Step 624, Train Loss: 2.3506, Test Loss: 2.1705, Learning Rate: 2.36e-05\n",
      "Step 625, Train Loss: 2.3079, Test Loss: 2.3462, Learning Rate: 2.36e-05\n",
      "Step 626, Train Loss: 2.4322, Test Loss: 2.5289, Learning Rate: 2.36e-05\n",
      "Step 627, Train Loss: 2.3708, Test Loss: 2.3055, Learning Rate: 2.36e-05\n",
      "Step 628, Train Loss: 2.3544, Test Loss: 2.2879, Learning Rate: 2.36e-05\n",
      "Step 629, Train Loss: 2.3633, Test Loss: 2.4713, Learning Rate: 2.36e-05\n",
      "Step 630, Train Loss: 2.3161, Test Loss: 2.2282, Learning Rate: 2.36e-05\n",
      "Step 631, Train Loss: 2.2934, Test Loss: 2.2686, Learning Rate: 2.36e-05\n",
      "Step 632, Train Loss: 2.3426, Test Loss: 2.1474, Learning Rate: 2.36e-05\n",
      "Step 633, Train Loss: 2.2920, Test Loss: 2.3232, Learning Rate: 2.36e-05\n",
      "Step 634, Train Loss: 2.3692, Test Loss: 2.1552, Learning Rate: 2.36e-05\n",
      "Step 635, Train Loss: 2.3874, Test Loss: 2.5619, Learning Rate: 2.36e-05\n",
      "Step 636, Train Loss: 2.2840, Test Loss: 2.2763, Learning Rate: 2.36e-05\n",
      "Step 637, Train Loss: 2.4897, Test Loss: 2.4263, Learning Rate: 2.36e-05\n",
      "Step 638, Train Loss: 2.2284, Test Loss: 2.2059, Learning Rate: 2.36e-05\n",
      "Step 639, Train Loss: 2.3185, Test Loss: 2.3653, Learning Rate: 2.36e-05\n",
      "Step 640, Train Loss: 2.3365, Test Loss: 2.5589, Learning Rate: 2.36e-05\n",
      "Step 641, Train Loss: 2.3676, Test Loss: 2.2863, Learning Rate: 2.36e-05\n",
      "Step 642, Train Loss: 2.4090, Test Loss: 2.3640, Learning Rate: 2.36e-05\n",
      "Step 643, Train Loss: 2.2940, Test Loss: 2.4228, Learning Rate: 2.36e-05\n",
      "Step 644, Train Loss: 2.1260, Test Loss: 2.4898, Learning Rate: 2.36e-05\n",
      "Step 645, Train Loss: 2.1671, Test Loss: 2.4037, Learning Rate: 2.36e-05\n",
      "Step 646, Train Loss: 2.3561, Test Loss: 2.4466, Learning Rate: 2.36e-05\n",
      "Step 647, Train Loss: 2.4497, Test Loss: 1.8869, Learning Rate: 2.36e-05\n",
      "Step 648, Train Loss: 2.3819, Test Loss: 2.4281, Learning Rate: 2.36e-05\n",
      "Step 649, Train Loss: 2.3309, Test Loss: 2.2647, Learning Rate: 2.36e-05\n",
      "Step 650, Train Loss: 2.2749, Test Loss: 2.2512, Learning Rate: 2.36e-05\n",
      "Step 651, Train Loss: 2.3359, Test Loss: 2.4650, Learning Rate: 2.36e-05\n",
      "Step 652, Train Loss: 2.3373, Test Loss: 2.2589, Learning Rate: 2.36e-05\n",
      "Step 653, Train Loss: 2.2356, Test Loss: 2.3017, Learning Rate: 2.36e-05\n",
      "Step 654, Train Loss: 2.2989, Test Loss: 2.3742, Learning Rate: 2.36e-05\n",
      "Step 655, Train Loss: 2.3392, Test Loss: 2.4437, Learning Rate: 2.36e-05\n",
      "Step 656, Train Loss: 2.4183, Test Loss: 2.2498, Learning Rate: 2.36e-05\n",
      "Step 657, Train Loss: 2.3245, Test Loss: 2.2810, Learning Rate: 2.36e-05\n",
      "Step 658, Train Loss: 2.3105, Test Loss: 2.1256, Learning Rate: 2.36e-05\n",
      "Step 659, Train Loss: 2.3296, Test Loss: 2.4610, Learning Rate: 2.36e-05\n",
      "Step 660, Train Loss: 2.4417, Test Loss: 2.4467, Learning Rate: 2.36e-05\n",
      "Step 661, Train Loss: 2.3533, Test Loss: 2.2098, Learning Rate: 2.36e-05\n",
      "Step 662, Train Loss: 2.4155, Test Loss: 2.4633, Learning Rate: 2.36e-05\n",
      "Step 663, Train Loss: 2.3652, Test Loss: 2.3769, Learning Rate: 2.36e-05\n",
      "Step 664, Train Loss: 2.2454, Test Loss: 2.1946, Learning Rate: 2.36e-05\n",
      "Step 665, Train Loss: 2.3971, Test Loss: 2.4258, Learning Rate: 2.36e-05\n",
      "Step 666, Train Loss: 2.2098, Test Loss: 2.2756, Learning Rate: 2.36e-05\n",
      "Step 667, Train Loss: 2.3196, Test Loss: 2.3703, Learning Rate: 2.36e-05\n",
      "Step 668, Train Loss: 2.2256, Test Loss: 2.3941, Learning Rate: 2.36e-05\n",
      "Step 669, Train Loss: 2.4363, Test Loss: 2.1654, Learning Rate: 2.36e-05\n",
      "Step 670, Train Loss: 2.2911, Test Loss: 2.3331, Learning Rate: 2.36e-05\n",
      "Step 671, Train Loss: 2.3232, Test Loss: 2.3277, Learning Rate: 2.36e-05\n",
      "Step 672, Train Loss: 2.4352, Test Loss: 2.4492, Learning Rate: 2.36e-05\n",
      "Step 673, Train Loss: 2.3862, Test Loss: 2.3794, Learning Rate: 2.36e-05\n",
      "Step 674, Train Loss: 2.2695, Test Loss: 2.3926, Learning Rate: 2.36e-05\n",
      "Step 675, Train Loss: 2.3003, Test Loss: 2.3027, Learning Rate: 2.36e-05\n",
      "Step 676, Train Loss: 2.4047, Test Loss: 2.3279, Learning Rate: 2.36e-05\n",
      "Step 677, Train Loss: 2.4401, Test Loss: 2.2923, Learning Rate: 2.36e-05\n",
      "Step 678, Train Loss: 2.3366, Test Loss: 2.5046, Learning Rate: 2.36e-05\n",
      "Step 679, Train Loss: 2.2792, Test Loss: 2.3803, Learning Rate: 2.36e-05\n",
      "Step 680, Train Loss: 2.2926, Test Loss: 2.4933, Learning Rate: 2.36e-05\n",
      "Step 681, Train Loss: 2.3335, Test Loss: 2.4785, Learning Rate: 2.36e-05\n",
      "Step 682, Train Loss: 2.1287, Test Loss: 2.4744, Learning Rate: 2.36e-05\n",
      "Step 683, Train Loss: 2.3197, Test Loss: 2.2815, Learning Rate: 2.36e-05\n",
      "Step 684, Train Loss: 2.3111, Test Loss: 2.2495, Learning Rate: 2.36e-05\n",
      "Step 685, Train Loss: 2.4087, Test Loss: 2.4262, Learning Rate: 2.36e-05\n",
      "Step 686, Train Loss: 2.2333, Test Loss: 2.3959, Learning Rate: 2.36e-05\n",
      "Step 687, Train Loss: 2.3934, Test Loss: 2.3352, Learning Rate: 2.36e-05\n",
      "Step 688, Train Loss: 2.4143, Test Loss: 2.2570, Learning Rate: 2.36e-05\n",
      "Step 689, Train Loss: 2.3913, Test Loss: 2.2643, Learning Rate: 2.36e-05\n",
      "Step 690, Train Loss: 2.3491, Test Loss: 2.3053, Learning Rate: 2.36e-05\n",
      "Step 691, Train Loss: 2.3249, Test Loss: 2.1858, Learning Rate: 2.36e-05\n",
      "Step 692, Train Loss: 2.3588, Test Loss: 2.4959, Learning Rate: 2.36e-05\n",
      "Step 693, Train Loss: 2.3962, Test Loss: 2.4169, Learning Rate: 2.36e-05\n",
      "Step 694, Train Loss: 2.2558, Test Loss: 2.3237, Learning Rate: 2.36e-05\n",
      "Step 695, Train Loss: 2.3507, Test Loss: 2.1892, Learning Rate: 2.36e-05\n",
      "Step 696, Train Loss: 2.3328, Test Loss: 2.3950, Learning Rate: 2.36e-05\n",
      "Step 697, Train Loss: 2.2709, Test Loss: 2.3444, Learning Rate: 2.36e-05\n",
      "Step 698, Train Loss: 2.4346, Test Loss: 2.4016, Learning Rate: 2.36e-05\n",
      "Step 699, Train Loss: 2.3605, Test Loss: 2.4169, Learning Rate: 2.36e-05\n",
      "Step 700, Train Loss: 2.2195, Test Loss: 2.5021, Learning Rate: 2.36e-05\n",
      "Step 701, Train Loss: 2.3373, Test Loss: 2.1835, Learning Rate: 2.36e-05\n",
      "Step 702, Train Loss: 2.2787, Test Loss: 2.1609, Learning Rate: 2.36e-05\n",
      "Step 703, Train Loss: 2.3121, Test Loss: 2.2719, Learning Rate: 2.36e-05\n",
      "Step 704, Train Loss: 2.3436, Test Loss: 2.3459, Learning Rate: 2.36e-05\n",
      "Step 705, Train Loss: 2.3552, Test Loss: 2.5309, Learning Rate: 2.36e-05\n",
      "Step 706, Train Loss: 2.3514, Test Loss: 2.4536, Learning Rate: 2.36e-05\n",
      "Step 707, Train Loss: 2.1971, Test Loss: 2.2794, Learning Rate: 2.36e-05\n",
      "Step 708, Train Loss: 2.3080, Test Loss: 2.3613, Learning Rate: 2.36e-05\n",
      "Step 709, Train Loss: 2.3381, Test Loss: 2.4572, Learning Rate: 2.36e-05\n",
      "Step 710, Train Loss: 2.3877, Test Loss: 2.2087, Learning Rate: 2.36e-05\n",
      "Step 711, Train Loss: 2.2632, Test Loss: 2.4234, Learning Rate: 2.36e-05\n",
      "Step 712, Train Loss: 2.2314, Test Loss: 2.7677, Learning Rate: 2.36e-05\n",
      "Step 713, Train Loss: 2.3604, Test Loss: 2.2184, Learning Rate: 2.36e-05\n",
      "Step 714, Train Loss: 2.3079, Test Loss: 2.1599, Learning Rate: 2.36e-05\n",
      "Step 715, Train Loss: 2.3683, Test Loss: 2.3564, Learning Rate: 2.36e-05\n",
      "Step 716, Train Loss: 2.3142, Test Loss: 2.2706, Learning Rate: 2.36e-05\n",
      "Step 717, Train Loss: 2.3896, Test Loss: 2.3662, Learning Rate: 2.36e-05\n",
      "Step 718, Train Loss: 2.2941, Test Loss: 2.0990, Learning Rate: 2.36e-05\n",
      "Step 719, Train Loss: 2.3326, Test Loss: 2.3284, Learning Rate: 2.36e-05\n",
      "Step 720, Train Loss: 2.4162, Test Loss: 2.3710, Learning Rate: 2.36e-05\n",
      "Step 721, Train Loss: 2.2166, Test Loss: 2.5578, Learning Rate: 2.36e-05\n",
      "Step 722, Train Loss: 2.3176, Test Loss: 2.2301, Learning Rate: 2.36e-05\n",
      "Step 723, Train Loss: 2.2267, Test Loss: 2.3117, Learning Rate: 2.36e-05\n",
      "Step 724, Train Loss: 2.3999, Test Loss: 2.3926, Learning Rate: 2.36e-05\n",
      "Step 725, Train Loss: 2.3854, Test Loss: 2.2375, Learning Rate: 2.35e-05\n",
      "Step 726, Train Loss: 2.4057, Test Loss: 2.2892, Learning Rate: 2.35e-05\n",
      "Step 727, Train Loss: 2.3694, Test Loss: 2.2098, Learning Rate: 2.35e-05\n",
      "Step 728, Train Loss: 2.4238, Test Loss: 2.3509, Learning Rate: 2.35e-05\n",
      "Step 729, Train Loss: 2.3166, Test Loss: 2.5019, Learning Rate: 2.35e-05\n",
      "Step 730, Train Loss: 2.3356, Test Loss: 2.3480, Learning Rate: 2.35e-05\n",
      "Step 731, Train Loss: 2.3385, Test Loss: 2.2909, Learning Rate: 2.35e-05\n",
      "Step 732, Train Loss: 2.2723, Test Loss: 2.2501, Learning Rate: 2.35e-05\n",
      "Step 733, Train Loss: 2.3124, Test Loss: 2.3732, Learning Rate: 2.35e-05\n",
      "Step 734, Train Loss: 2.2894, Test Loss: 2.4607, Learning Rate: 2.35e-05\n",
      "Step 735, Train Loss: 2.3746, Test Loss: 2.4662, Learning Rate: 2.35e-05\n",
      "Step 736, Train Loss: 2.4856, Test Loss: 2.3985, Learning Rate: 2.35e-05\n",
      "Step 737, Train Loss: 2.3426, Test Loss: 2.5091, Learning Rate: 2.35e-05\n",
      "Step 738, Train Loss: 2.3895, Test Loss: 2.3699, Learning Rate: 2.35e-05\n",
      "Step 739, Train Loss: 2.2823, Test Loss: 2.2756, Learning Rate: 2.35e-05\n",
      "Step 740, Train Loss: 2.3406, Test Loss: 2.2217, Learning Rate: 2.35e-05\n",
      "Step 741, Train Loss: 2.3858, Test Loss: 2.3703, Learning Rate: 2.35e-05\n",
      "Step 742, Train Loss: 2.3762, Test Loss: 2.2009, Learning Rate: 2.35e-05\n",
      "Step 743, Train Loss: 2.4904, Test Loss: 2.2200, Learning Rate: 2.35e-05\n",
      "Step 744, Train Loss: 2.2065, Test Loss: 2.3619, Learning Rate: 2.35e-05\n",
      "Step 745, Train Loss: 2.3093, Test Loss: 2.3560, Learning Rate: 2.35e-05\n",
      "Step 746, Train Loss: 2.2746, Test Loss: 2.3469, Learning Rate: 2.35e-05\n",
      "Step 747, Train Loss: 2.1978, Test Loss: 2.2826, Learning Rate: 2.35e-05\n",
      "Step 748, Train Loss: 2.3818, Test Loss: 2.3789, Learning Rate: 2.35e-05\n",
      "Step 749, Train Loss: 2.3311, Test Loss: 2.3103, Learning Rate: 2.35e-05\n",
      "Step 750, Train Loss: 2.3950, Test Loss: 2.1450, Learning Rate: 2.35e-05\n",
      "Step 751, Train Loss: 2.3378, Test Loss: 2.3551, Learning Rate: 2.35e-05\n",
      "Step 752, Train Loss: 2.4971, Test Loss: 2.2372, Learning Rate: 2.35e-05\n",
      "Step 753, Train Loss: 2.3874, Test Loss: 2.1662, Learning Rate: 2.35e-05\n",
      "Step 754, Train Loss: 2.3320, Test Loss: 2.4253, Learning Rate: 2.35e-05\n",
      "Step 755, Train Loss: 2.4800, Test Loss: 2.4333, Learning Rate: 2.35e-05\n",
      "Step 756, Train Loss: 2.3726, Test Loss: 2.3289, Learning Rate: 2.35e-05\n",
      "Step 757, Train Loss: 2.4195, Test Loss: 2.2387, Learning Rate: 2.35e-05\n",
      "Step 758, Train Loss: 2.4166, Test Loss: 2.3778, Learning Rate: 2.35e-05\n",
      "Step 759, Train Loss: 2.2617, Test Loss: 2.3856, Learning Rate: 2.35e-05\n",
      "Step 760, Train Loss: 2.2869, Test Loss: 2.3411, Learning Rate: 2.35e-05\n",
      "Step 761, Train Loss: 2.3820, Test Loss: 2.2175, Learning Rate: 2.35e-05\n",
      "Step 762, Train Loss: 2.2620, Test Loss: 2.0586, Learning Rate: 2.35e-05\n",
      "Step 763, Train Loss: 2.3823, Test Loss: 2.1537, Learning Rate: 2.35e-05\n",
      "Step 764, Train Loss: 2.3449, Test Loss: 2.5667, Learning Rate: 2.35e-05\n",
      "Step 765, Train Loss: 2.4004, Test Loss: 2.4149, Learning Rate: 2.35e-05\n",
      "Step 766, Train Loss: 2.4737, Test Loss: 2.4817, Learning Rate: 2.35e-05\n",
      "Step 767, Train Loss: 2.3529, Test Loss: 2.5443, Learning Rate: 2.35e-05\n",
      "Step 768, Train Loss: 2.2996, Test Loss: 2.4204, Learning Rate: 2.35e-05\n",
      "Step 769, Train Loss: 2.4170, Test Loss: 2.2936, Learning Rate: 2.35e-05\n",
      "Step 770, Train Loss: 2.2659, Test Loss: 2.3207, Learning Rate: 2.35e-05\n",
      "Step 771, Train Loss: 2.3417, Test Loss: 2.3056, Learning Rate: 2.35e-05\n",
      "Step 772, Train Loss: 2.4022, Test Loss: 2.6171, Learning Rate: 2.35e-05\n",
      "Step 773, Train Loss: 2.4079, Test Loss: 2.2228, Learning Rate: 2.35e-05\n",
      "Step 774, Train Loss: 2.2508, Test Loss: 2.2261, Learning Rate: 2.35e-05\n",
      "Step 775, Train Loss: 2.3837, Test Loss: 2.3084, Learning Rate: 2.35e-05\n",
      "Step 776, Train Loss: 2.4418, Test Loss: 2.3097, Learning Rate: 2.35e-05\n",
      "Step 777, Train Loss: 2.2106, Test Loss: 2.3125, Learning Rate: 2.35e-05\n",
      "Step 778, Train Loss: 2.1490, Test Loss: 2.2184, Learning Rate: 2.35e-05\n",
      "Step 779, Train Loss: 2.3054, Test Loss: 2.4499, Learning Rate: 2.35e-05\n",
      "Step 780, Train Loss: 2.1947, Test Loss: 2.4722, Learning Rate: 2.35e-05\n",
      "Step 781, Train Loss: 2.2347, Test Loss: 2.3669, Learning Rate: 2.35e-05\n",
      "Step 782, Train Loss: 2.3580, Test Loss: 2.5087, Learning Rate: 2.35e-05\n",
      "Step 783, Train Loss: 2.1999, Test Loss: 2.4564, Learning Rate: 2.35e-05\n",
      "Step 784, Train Loss: 2.2347, Test Loss: 2.4912, Learning Rate: 2.35e-05\n",
      "Step 785, Train Loss: 2.3527, Test Loss: 2.4122, Learning Rate: 2.35e-05\n",
      "Step 786, Train Loss: 2.3620, Test Loss: 2.3168, Learning Rate: 2.35e-05\n",
      "Step 787, Train Loss: 2.2250, Test Loss: 2.2546, Learning Rate: 2.35e-05\n",
      "Step 788, Train Loss: 2.3579, Test Loss: 2.3624, Learning Rate: 2.35e-05\n",
      "Step 789, Train Loss: 2.3251, Test Loss: 2.3515, Learning Rate: 2.35e-05\n",
      "Step 790, Train Loss: 2.3162, Test Loss: 2.4672, Learning Rate: 2.35e-05\n",
      "Step 791, Train Loss: 2.2989, Test Loss: 2.1941, Learning Rate: 2.35e-05\n",
      "Step 792, Train Loss: 2.3093, Test Loss: 2.3181, Learning Rate: 2.35e-05\n",
      "Step 793, Train Loss: 2.2912, Test Loss: 2.4046, Learning Rate: 2.35e-05\n",
      "Step 794, Train Loss: 2.3963, Test Loss: 2.3164, Learning Rate: 2.35e-05\n",
      "Step 795, Train Loss: 2.4150, Test Loss: 2.5040, Learning Rate: 2.35e-05\n",
      "Step 796, Train Loss: 2.3653, Test Loss: 2.2087, Learning Rate: 2.35e-05\n",
      "Step 797, Train Loss: 2.2638, Test Loss: 2.5766, Learning Rate: 2.35e-05\n",
      "Step 798, Train Loss: 2.2927, Test Loss: 2.2642, Learning Rate: 2.35e-05\n",
      "Step 799, Train Loss: 2.4283, Test Loss: 2.4455, Learning Rate: 2.35e-05\n",
      "Step 800, Train Loss: 2.4076, Test Loss: 2.3707, Learning Rate: 2.35e-05\n",
      "Step 801, Train Loss: 2.4643, Test Loss: 2.3765, Learning Rate: 2.35e-05\n",
      "Step 802, Train Loss: 2.3136, Test Loss: 2.2143, Learning Rate: 2.35e-05\n",
      "Step 803, Train Loss: 2.3231, Test Loss: 2.2896, Learning Rate: 2.35e-05\n",
      "Step 804, Train Loss: 2.3930, Test Loss: 2.1240, Learning Rate: 2.35e-05\n",
      "Step 805, Train Loss: 2.4055, Test Loss: 2.4523, Learning Rate: 2.35e-05\n",
      "Step 806, Train Loss: 2.2585, Test Loss: 2.1256, Learning Rate: 2.35e-05\n",
      "Step 807, Train Loss: 2.3451, Test Loss: 2.4362, Learning Rate: 2.35e-05\n",
      "Step 808, Train Loss: 2.3173, Test Loss: 2.3759, Learning Rate: 2.35e-05\n",
      "Step 809, Train Loss: 2.2061, Test Loss: 2.4209, Learning Rate: 2.35e-05\n",
      "Step 810, Train Loss: 2.3579, Test Loss: 2.3098, Learning Rate: 2.35e-05\n",
      "Step 811, Train Loss: 2.3298, Test Loss: 2.2464, Learning Rate: 2.35e-05\n",
      "Step 812, Train Loss: 2.3773, Test Loss: 2.3357, Learning Rate: 2.35e-05\n",
      "Step 813, Train Loss: 2.3445, Test Loss: 2.2852, Learning Rate: 2.35e-05\n",
      "Step 814, Train Loss: 2.2546, Test Loss: 2.2609, Learning Rate: 2.35e-05\n",
      "Step 815, Train Loss: 2.4033, Test Loss: 2.3891, Learning Rate: 2.35e-05\n",
      "Step 816, Train Loss: 2.3699, Test Loss: 2.3302, Learning Rate: 2.35e-05\n",
      "Step 817, Train Loss: 2.3192, Test Loss: 2.3494, Learning Rate: 2.35e-05\n",
      "Step 818, Train Loss: 2.3453, Test Loss: 2.1767, Learning Rate: 2.35e-05\n",
      "Step 819, Train Loss: 2.1228, Test Loss: 2.3824, Learning Rate: 2.35e-05\n",
      "Step 820, Train Loss: 2.3678, Test Loss: 2.2835, Learning Rate: 2.35e-05\n",
      "Step 821, Train Loss: 2.3932, Test Loss: 2.3553, Learning Rate: 2.35e-05\n",
      "Step 822, Train Loss: 2.3518, Test Loss: 2.3329, Learning Rate: 2.35e-05\n",
      "Step 823, Train Loss: 2.2817, Test Loss: 2.2826, Learning Rate: 2.35e-05\n",
      "Step 824, Train Loss: 2.3809, Test Loss: 2.1886, Learning Rate: 2.35e-05\n",
      "Step 825, Train Loss: 2.3456, Test Loss: 2.2496, Learning Rate: 2.35e-05\n",
      "Step 826, Train Loss: 2.3580, Test Loss: 2.4006, Learning Rate: 2.35e-05\n",
      "Step 827, Train Loss: 2.3392, Test Loss: 2.3106, Learning Rate: 2.35e-05\n",
      "Step 828, Train Loss: 2.3971, Test Loss: 2.3013, Learning Rate: 2.35e-05\n",
      "Step 829, Train Loss: 2.2525, Test Loss: 2.3024, Learning Rate: 2.35e-05\n",
      "Step 830, Train Loss: 2.3493, Test Loss: 2.3878, Learning Rate: 2.35e-05\n",
      "Step 831, Train Loss: 2.4460, Test Loss: 2.4032, Learning Rate: 2.35e-05\n",
      "Step 832, Train Loss: 2.2600, Test Loss: 2.3109, Learning Rate: 2.35e-05\n",
      "Step 833, Train Loss: 2.2767, Test Loss: 2.4917, Learning Rate: 2.35e-05\n",
      "Step 834, Train Loss: 2.2497, Test Loss: 2.4616, Learning Rate: 2.35e-05\n",
      "Step 835, Train Loss: 2.1994, Test Loss: 2.2213, Learning Rate: 2.35e-05\n",
      "Step 836, Train Loss: 2.2075, Test Loss: 2.3975, Learning Rate: 2.35e-05\n",
      "Step 837, Train Loss: 2.3273, Test Loss: 2.3464, Learning Rate: 2.35e-05\n",
      "Step 838, Train Loss: 2.3139, Test Loss: 2.4524, Learning Rate: 2.35e-05\n",
      "Step 839, Train Loss: 2.3043, Test Loss: 2.3355, Learning Rate: 2.35e-05\n",
      "Step 840, Train Loss: 2.3137, Test Loss: 2.0096, Learning Rate: 2.35e-05\n",
      "Step 841, Train Loss: 2.3691, Test Loss: 2.2161, Learning Rate: 2.35e-05\n",
      "Step 842, Train Loss: 2.3903, Test Loss: 2.4064, Learning Rate: 2.35e-05\n",
      "Step 843, Train Loss: 2.2976, Test Loss: 2.5151, Learning Rate: 2.35e-05\n",
      "Step 844, Train Loss: 2.3669, Test Loss: 2.3839, Learning Rate: 2.35e-05\n",
      "Step 845, Train Loss: 2.3060, Test Loss: 2.1929, Learning Rate: 2.35e-05\n",
      "Step 846, Train Loss: 2.3306, Test Loss: 2.3769, Learning Rate: 2.35e-05\n",
      "Step 847, Train Loss: 2.2798, Test Loss: 2.2675, Learning Rate: 2.34e-05\n",
      "Step 848, Train Loss: 2.3855, Test Loss: 2.3672, Learning Rate: 2.34e-05\n",
      "Step 849, Train Loss: 2.4353, Test Loss: 2.5059, Learning Rate: 2.34e-05\n",
      "Step 850, Train Loss: 2.4619, Test Loss: 2.3786, Learning Rate: 2.34e-05\n",
      "Step 851, Train Loss: 2.3129, Test Loss: 2.3075, Learning Rate: 2.34e-05\n",
      "Step 852, Train Loss: 2.3091, Test Loss: 2.2769, Learning Rate: 2.34e-05\n",
      "Step 853, Train Loss: 2.3601, Test Loss: 2.2930, Learning Rate: 2.34e-05\n",
      "Step 854, Train Loss: 2.2955, Test Loss: 2.3188, Learning Rate: 2.34e-05\n",
      "Step 855, Train Loss: 2.3797, Test Loss: 2.4214, Learning Rate: 2.34e-05\n",
      "Step 856, Train Loss: 2.2583, Test Loss: 2.3316, Learning Rate: 2.34e-05\n",
      "Step 857, Train Loss: 2.3708, Test Loss: 2.3296, Learning Rate: 2.34e-05\n",
      "Step 858, Train Loss: 2.2992, Test Loss: 2.3546, Learning Rate: 2.34e-05\n",
      "Step 859, Train Loss: 2.1879, Test Loss: 2.2155, Learning Rate: 2.34e-05\n",
      "Step 860, Train Loss: 2.4083, Test Loss: 2.3304, Learning Rate: 2.34e-05\n",
      "Step 861, Train Loss: 2.3815, Test Loss: 2.3626, Learning Rate: 2.34e-05\n",
      "Step 862, Train Loss: 2.4369, Test Loss: 2.3237, Learning Rate: 2.34e-05\n",
      "Step 863, Train Loss: 2.4195, Test Loss: 2.3233, Learning Rate: 2.34e-05\n",
      "Step 864, Train Loss: 2.3488, Test Loss: 2.2539, Learning Rate: 2.34e-05\n",
      "Step 865, Train Loss: 2.2960, Test Loss: 2.5052, Learning Rate: 2.34e-05\n",
      "Step 866, Train Loss: 2.3638, Test Loss: 2.2695, Learning Rate: 2.34e-05\n",
      "Step 867, Train Loss: 2.2308, Test Loss: 2.4035, Learning Rate: 2.34e-05\n",
      "Step 868, Train Loss: 2.2908, Test Loss: 2.3920, Learning Rate: 2.34e-05\n",
      "Step 869, Train Loss: 2.2769, Test Loss: 2.3378, Learning Rate: 2.34e-05\n",
      "Step 870, Train Loss: 2.3429, Test Loss: 2.4499, Learning Rate: 2.34e-05\n",
      "Step 871, Train Loss: 2.3733, Test Loss: 2.3970, Learning Rate: 2.34e-05\n",
      "Step 872, Train Loss: 2.2548, Test Loss: 2.2797, Learning Rate: 2.34e-05\n",
      "Step 873, Train Loss: 2.4675, Test Loss: 2.1646, Learning Rate: 2.34e-05\n",
      "Step 874, Train Loss: 2.3013, Test Loss: 2.2075, Learning Rate: 2.34e-05\n",
      "Step 875, Train Loss: 2.4161, Test Loss: 2.3242, Learning Rate: 2.34e-05\n",
      "Step 876, Train Loss: 2.4452, Test Loss: 2.3724, Learning Rate: 2.34e-05\n",
      "Step 877, Train Loss: 2.2332, Test Loss: 2.3965, Learning Rate: 2.34e-05\n",
      "Step 878, Train Loss: 2.3362, Test Loss: 2.3073, Learning Rate: 2.34e-05\n",
      "Step 879, Train Loss: 2.3689, Test Loss: 2.2642, Learning Rate: 2.34e-05\n",
      "Step 880, Train Loss: 2.3855, Test Loss: 2.3217, Learning Rate: 2.34e-05\n",
      "Step 881, Train Loss: 2.3527, Test Loss: 2.2942, Learning Rate: 2.34e-05\n",
      "Step 882, Train Loss: 2.4103, Test Loss: 2.3659, Learning Rate: 2.34e-05\n",
      "Step 883, Train Loss: 2.3422, Test Loss: 2.3486, Learning Rate: 2.34e-05\n",
      "Step 884, Train Loss: 2.3608, Test Loss: 2.1988, Learning Rate: 2.34e-05\n",
      "Step 885, Train Loss: 2.1894, Test Loss: 2.0923, Learning Rate: 2.34e-05\n",
      "Step 886, Train Loss: 2.3592, Test Loss: 2.5077, Learning Rate: 2.34e-05\n",
      "Step 887, Train Loss: 2.3913, Test Loss: 2.2896, Learning Rate: 2.34e-05\n",
      "Step 888, Train Loss: 2.2586, Test Loss: 2.4432, Learning Rate: 2.34e-05\n",
      "Step 889, Train Loss: 2.3374, Test Loss: 2.3688, Learning Rate: 2.34e-05\n",
      "Step 890, Train Loss: 2.4502, Test Loss: 2.3987, Learning Rate: 2.34e-05\n",
      "Step 891, Train Loss: 2.3540, Test Loss: 2.2413, Learning Rate: 2.34e-05\n",
      "Step 892, Train Loss: 2.3604, Test Loss: 2.5489, Learning Rate: 2.34e-05\n",
      "Step 893, Train Loss: 2.3252, Test Loss: 2.0897, Learning Rate: 2.34e-05\n",
      "Step 894, Train Loss: 2.3325, Test Loss: 2.3011, Learning Rate: 2.34e-05\n",
      "Step 895, Train Loss: 2.1975, Test Loss: 2.1975, Learning Rate: 2.34e-05\n",
      "Step 896, Train Loss: 2.3585, Test Loss: 2.2697, Learning Rate: 2.34e-05\n",
      "Step 897, Train Loss: 2.4348, Test Loss: 2.1886, Learning Rate: 2.34e-05\n",
      "Step 898, Train Loss: 2.3584, Test Loss: 2.2518, Learning Rate: 2.34e-05\n",
      "Step 899, Train Loss: 2.2835, Test Loss: 2.4454, Learning Rate: 2.34e-05\n",
      "Step 900, Train Loss: 2.2244, Test Loss: 2.4977, Learning Rate: 2.34e-05\n",
      "Step 901, Train Loss: 2.2862, Test Loss: 2.1911, Learning Rate: 2.34e-05\n",
      "Step 902, Train Loss: 2.2461, Test Loss: 2.3810, Learning Rate: 2.34e-05\n",
      "Step 903, Train Loss: 2.4782, Test Loss: 2.4739, Learning Rate: 2.34e-05\n",
      "Step 904, Train Loss: 2.4569, Test Loss: 2.3076, Learning Rate: 2.34e-05\n",
      "Step 905, Train Loss: 2.2964, Test Loss: 2.2207, Learning Rate: 2.34e-05\n",
      "Step 906, Train Loss: 2.2885, Test Loss: 2.1291, Learning Rate: 2.34e-05\n",
      "Step 907, Train Loss: 2.1900, Test Loss: 2.2858, Learning Rate: 2.34e-05\n",
      "Step 908, Train Loss: 2.3845, Test Loss: 2.2495, Learning Rate: 2.34e-05\n",
      "Step 909, Train Loss: 2.3397, Test Loss: 2.4928, Learning Rate: 2.34e-05\n",
      "Step 910, Train Loss: 2.3811, Test Loss: 2.4492, Learning Rate: 2.34e-05\n",
      "Step 911, Train Loss: 2.3088, Test Loss: 2.2445, Learning Rate: 2.34e-05\n",
      "Step 912, Train Loss: 2.4017, Test Loss: 2.3926, Learning Rate: 2.34e-05\n",
      "Step 913, Train Loss: 2.3307, Test Loss: 2.3099, Learning Rate: 2.34e-05\n",
      "Step 914, Train Loss: 2.3873, Test Loss: 2.3708, Learning Rate: 2.34e-05\n",
      "Step 915, Train Loss: 2.4525, Test Loss: 2.2108, Learning Rate: 2.34e-05\n",
      "Step 916, Train Loss: 2.3217, Test Loss: 2.1792, Learning Rate: 2.34e-05\n",
      "Step 917, Train Loss: 2.4003, Test Loss: 2.2741, Learning Rate: 2.34e-05\n",
      "Step 918, Train Loss: 2.4016, Test Loss: 2.1493, Learning Rate: 2.34e-05\n",
      "Step 919, Train Loss: 2.3113, Test Loss: 2.3559, Learning Rate: 2.34e-05\n",
      "Step 920, Train Loss: 2.4307, Test Loss: 2.1444, Learning Rate: 2.34e-05\n",
      "Step 921, Train Loss: 2.1825, Test Loss: 2.1587, Learning Rate: 2.34e-05\n",
      "Step 922, Train Loss: 2.4808, Test Loss: 2.3435, Learning Rate: 2.34e-05\n",
      "Step 923, Train Loss: 2.3560, Test Loss: 2.4431, Learning Rate: 2.34e-05\n",
      "Step 924, Train Loss: 2.4243, Test Loss: 2.2448, Learning Rate: 2.34e-05\n",
      "Step 925, Train Loss: 2.2958, Test Loss: 2.4068, Learning Rate: 2.34e-05\n",
      "Step 926, Train Loss: 2.3542, Test Loss: 2.2475, Learning Rate: 2.34e-05\n",
      "Step 927, Train Loss: 2.2878, Test Loss: 2.4603, Learning Rate: 2.34e-05\n",
      "Step 928, Train Loss: 2.2839, Test Loss: 2.1832, Learning Rate: 2.34e-05\n",
      "Step 929, Train Loss: 2.2794, Test Loss: 2.2461, Learning Rate: 2.34e-05\n",
      "Step 930, Train Loss: 2.2656, Test Loss: 2.3077, Learning Rate: 2.34e-05\n",
      "Step 931, Train Loss: 2.3087, Test Loss: 2.4753, Learning Rate: 2.34e-05\n",
      "Step 932, Train Loss: 2.3105, Test Loss: 2.2496, Learning Rate: 2.34e-05\n",
      "Step 933, Train Loss: 2.4334, Test Loss: 2.1639, Learning Rate: 2.34e-05\n",
      "Step 934, Train Loss: 2.2444, Test Loss: 2.3465, Learning Rate: 2.34e-05\n",
      "Step 935, Train Loss: 2.2458, Test Loss: 2.3536, Learning Rate: 2.34e-05\n",
      "Step 936, Train Loss: 2.2537, Test Loss: 2.3742, Learning Rate: 2.34e-05\n",
      "Step 937, Train Loss: 2.3482, Test Loss: 2.1721, Learning Rate: 2.34e-05\n",
      "Step 938, Train Loss: 2.2910, Test Loss: 2.2928, Learning Rate: 2.34e-05\n",
      "Step 939, Train Loss: 2.3408, Test Loss: 2.3633, Learning Rate: 2.34e-05\n",
      "Step 940, Train Loss: 2.2744, Test Loss: 2.3764, Learning Rate: 2.34e-05\n",
      "Step 941, Train Loss: 2.3606, Test Loss: 2.3632, Learning Rate: 2.34e-05\n",
      "Step 942, Train Loss: 2.3974, Test Loss: 2.2517, Learning Rate: 2.34e-05\n",
      "Step 943, Train Loss: 2.3470, Test Loss: 2.3191, Learning Rate: 2.34e-05\n",
      "Step 944, Train Loss: 2.2921, Test Loss: 2.2938, Learning Rate: 2.34e-05\n",
      "Step 945, Train Loss: 2.4614, Test Loss: 2.3851, Learning Rate: 2.34e-05\n",
      "Step 946, Train Loss: 2.2621, Test Loss: 2.2720, Learning Rate: 2.34e-05\n",
      "Step 947, Train Loss: 2.4175, Test Loss: 2.3178, Learning Rate: 2.34e-05\n",
      "Step 948, Train Loss: 2.4148, Test Loss: 2.3038, Learning Rate: 2.34e-05\n",
      "Step 949, Train Loss: 2.2206, Test Loss: 2.1338, Learning Rate: 2.34e-05\n",
      "Step 950, Train Loss: 2.2562, Test Loss: 2.2942, Learning Rate: 2.34e-05\n",
      "Step 951, Train Loss: 2.3156, Test Loss: 2.2105, Learning Rate: 2.34e-05\n",
      "Step 952, Train Loss: 2.2549, Test Loss: 2.3556, Learning Rate: 2.34e-05\n",
      "Step 953, Train Loss: 2.1869, Test Loss: 2.1695, Learning Rate: 2.34e-05\n",
      "Step 954, Train Loss: 2.3829, Test Loss: 2.3521, Learning Rate: 2.34e-05\n",
      "Step 955, Train Loss: 2.2749, Test Loss: 2.2151, Learning Rate: 2.34e-05\n",
      "Step 956, Train Loss: 2.3254, Test Loss: 2.2704, Learning Rate: 2.34e-05\n",
      "Step 957, Train Loss: 2.2166, Test Loss: 2.2603, Learning Rate: 2.34e-05\n",
      "Step 958, Train Loss: 2.3192, Test Loss: 2.5159, Learning Rate: 2.34e-05\n",
      "Step 959, Train Loss: 2.3538, Test Loss: 2.3349, Learning Rate: 2.34e-05\n",
      "Step 960, Train Loss: 2.4023, Test Loss: 2.2762, Learning Rate: 2.34e-05\n",
      "Step 961, Train Loss: 2.4224, Test Loss: 2.3471, Learning Rate: 2.34e-05\n",
      "Step 962, Train Loss: 2.3362, Test Loss: 2.3333, Learning Rate: 2.34e-05\n",
      "Step 963, Train Loss: 2.4478, Test Loss: 2.3960, Learning Rate: 2.34e-05\n",
      "Step 964, Train Loss: 2.2980, Test Loss: 2.3558, Learning Rate: 2.34e-05\n",
      "Step 965, Train Loss: 2.2361, Test Loss: 2.2439, Learning Rate: 2.34e-05\n",
      "Step 966, Train Loss: 2.4468, Test Loss: 2.1221, Learning Rate: 2.34e-05\n",
      "Step 967, Train Loss: 2.3137, Test Loss: 2.4858, Learning Rate: 2.34e-05\n",
      "Step 968, Train Loss: 2.4396, Test Loss: 2.2577, Learning Rate: 2.34e-05\n",
      "Step 969, Train Loss: 2.3956, Test Loss: 2.4778, Learning Rate: 2.34e-05\n",
      "Step 970, Train Loss: 2.3665, Test Loss: 2.2762, Learning Rate: 2.34e-05\n",
      "Step 971, Train Loss: 2.2860, Test Loss: 2.3806, Learning Rate: 2.33e-05\n",
      "Step 972, Train Loss: 2.3928, Test Loss: 2.3487, Learning Rate: 2.33e-05\n",
      "Step 973, Train Loss: 2.3188, Test Loss: 2.4090, Learning Rate: 2.33e-05\n",
      "Step 974, Train Loss: 2.3129, Test Loss: 2.2237, Learning Rate: 2.33e-05\n",
      "Step 975, Train Loss: 2.2588, Test Loss: 2.3041, Learning Rate: 2.33e-05\n",
      "Step 976, Train Loss: 2.2443, Test Loss: 2.3351, Learning Rate: 2.33e-05\n",
      "Step 977, Train Loss: 2.3521, Test Loss: 2.2577, Learning Rate: 2.33e-05\n",
      "Step 978, Train Loss: 2.3558, Test Loss: 2.3376, Learning Rate: 2.33e-05\n",
      "Step 979, Train Loss: 2.2951, Test Loss: 2.3287, Learning Rate: 2.33e-05\n",
      "Step 980, Train Loss: 2.3914, Test Loss: 2.4714, Learning Rate: 2.33e-05\n",
      "Step 981, Train Loss: 2.3658, Test Loss: 2.3531, Learning Rate: 2.33e-05\n",
      "Step 982, Train Loss: 2.3765, Test Loss: 2.3007, Learning Rate: 2.33e-05\n",
      "Step 983, Train Loss: 2.3121, Test Loss: 2.3269, Learning Rate: 2.33e-05\n",
      "Step 984, Train Loss: 2.2521, Test Loss: 2.3254, Learning Rate: 2.33e-05\n",
      "Step 985, Train Loss: 2.3582, Test Loss: 2.3137, Learning Rate: 2.33e-05\n",
      "Step 986, Train Loss: 2.3638, Test Loss: 2.2048, Learning Rate: 2.33e-05\n",
      "Step 987, Train Loss: 2.1999, Test Loss: 2.3744, Learning Rate: 2.33e-05\n",
      "Step 988, Train Loss: 2.3687, Test Loss: 2.1813, Learning Rate: 2.33e-05\n",
      "Step 989, Train Loss: 2.4315, Test Loss: 2.1416, Learning Rate: 2.33e-05\n",
      "Step 990, Train Loss: 2.3341, Test Loss: 2.1613, Learning Rate: 2.33e-05\n",
      "Step 991, Train Loss: 2.3269, Test Loss: 2.3455, Learning Rate: 2.33e-05\n",
      "Step 992, Train Loss: 2.2514, Test Loss: 2.1965, Learning Rate: 2.33e-05\n",
      "Step 993, Train Loss: 2.3794, Test Loss: 2.4518, Learning Rate: 2.33e-05\n",
      "Step 994, Train Loss: 2.2740, Test Loss: 2.4748, Learning Rate: 2.33e-05\n",
      "Step 995, Train Loss: 2.4339, Test Loss: 2.3148, Learning Rate: 2.33e-05\n",
      "Step 996, Train Loss: 2.3731, Test Loss: 2.3709, Learning Rate: 2.33e-05\n",
      "Step 997, Train Loss: 2.2296, Test Loss: 2.3221, Learning Rate: 2.33e-05\n",
      "Step 998, Train Loss: 2.2379, Test Loss: 2.1896, Learning Rate: 2.33e-05\n",
      "Step 999, Train Loss: 2.3876, Test Loss: 2.4264, Learning Rate: 2.33e-05\n",
      "Step 1000, Train Loss: 2.3473, Test Loss: 2.4400, Learning Rate: 2.33e-05\n",
      "Step 1001, Train Loss: 2.3076, Test Loss: 2.3893, Learning Rate: 2.33e-05\n",
      "Step 1002, Train Loss: 2.3115, Test Loss: 2.3526, Learning Rate: 2.33e-05\n",
      "Step 1003, Train Loss: 2.3740, Test Loss: 2.5272, Learning Rate: 2.33e-05\n",
      "Step 1004, Train Loss: 2.3178, Test Loss: 2.4358, Learning Rate: 2.33e-05\n",
      "Step 1005, Train Loss: 2.3611, Test Loss: 2.4833, Learning Rate: 2.33e-05\n",
      "Step 1006, Train Loss: 2.2679, Test Loss: 2.1903, Learning Rate: 2.33e-05\n",
      "Step 1007, Train Loss: 2.2620, Test Loss: 2.2617, Learning Rate: 2.33e-05\n",
      "Step 1008, Train Loss: 2.5134, Test Loss: 2.3665, Learning Rate: 2.33e-05\n",
      "Step 1009, Train Loss: 2.3689, Test Loss: 2.2573, Learning Rate: 2.33e-05\n",
      "Step 1010, Train Loss: 2.3111, Test Loss: 2.1590, Learning Rate: 2.33e-05\n",
      "Step 1011, Train Loss: 2.2579, Test Loss: 2.3004, Learning Rate: 2.33e-05\n",
      "Step 1012, Train Loss: 2.2927, Test Loss: 2.3131, Learning Rate: 2.33e-05\n",
      "Step 1013, Train Loss: 2.3573, Test Loss: 2.3244, Learning Rate: 2.33e-05\n",
      "Step 1014, Train Loss: 2.3690, Test Loss: 2.3180, Learning Rate: 2.33e-05\n",
      "Step 1015, Train Loss: 2.3014, Test Loss: 2.4304, Learning Rate: 2.33e-05\n",
      "Step 1016, Train Loss: 2.4273, Test Loss: 2.3151, Learning Rate: 2.33e-05\n",
      "Step 1017, Train Loss: 2.4181, Test Loss: 2.2366, Learning Rate: 2.33e-05\n",
      "Step 1018, Train Loss: 2.3353, Test Loss: 2.3147, Learning Rate: 2.33e-05\n",
      "Step 1019, Train Loss: 2.2563, Test Loss: 2.3140, Learning Rate: 2.33e-05\n",
      "Step 1020, Train Loss: 2.5072, Test Loss: 2.3054, Learning Rate: 2.33e-05\n",
      "Step 1021, Train Loss: 2.2746, Test Loss: 2.4352, Learning Rate: 2.33e-05\n",
      "Step 1022, Train Loss: 2.4166, Test Loss: 2.4501, Learning Rate: 2.33e-05\n",
      "Step 1023, Train Loss: 2.4321, Test Loss: 2.2423, Learning Rate: 2.33e-05\n",
      "Step 1024, Train Loss: 2.4782, Test Loss: 2.2693, Learning Rate: 2.33e-05\n",
      "Step 1025, Train Loss: 2.1543, Test Loss: 2.3628, Learning Rate: 2.33e-05\n",
      "Step 1026, Train Loss: 2.3439, Test Loss: 2.2856, Learning Rate: 2.33e-05\n",
      "Step 1027, Train Loss: 2.3134, Test Loss: 2.2551, Learning Rate: 2.33e-05\n",
      "Step 1028, Train Loss: 2.2760, Test Loss: 2.3256, Learning Rate: 2.33e-05\n",
      "Step 1029, Train Loss: 2.3574, Test Loss: 2.5264, Learning Rate: 2.33e-05\n",
      "Step 1030, Train Loss: 2.3445, Test Loss: 2.3051, Learning Rate: 2.33e-05\n",
      "Step 1031, Train Loss: 2.4049, Test Loss: 2.2752, Learning Rate: 2.33e-05\n",
      "Step 1032, Train Loss: 2.2970, Test Loss: 2.2032, Learning Rate: 2.33e-05\n",
      "Step 1033, Train Loss: 2.2228, Test Loss: 2.1230, Learning Rate: 2.33e-05\n",
      "Step 1034, Train Loss: 2.3016, Test Loss: 2.3103, Learning Rate: 2.33e-05\n",
      "Step 1035, Train Loss: 2.3430, Test Loss: 2.2746, Learning Rate: 2.33e-05\n",
      "Step 1036, Train Loss: 2.3748, Test Loss: 2.3938, Learning Rate: 2.33e-05\n",
      "Step 1037, Train Loss: 2.3814, Test Loss: 2.3318, Learning Rate: 2.33e-05\n",
      "Step 1038, Train Loss: 2.3285, Test Loss: 2.5280, Learning Rate: 2.33e-05\n",
      "Step 1039, Train Loss: 2.2999, Test Loss: 2.0887, Learning Rate: 2.33e-05\n",
      "Step 1040, Train Loss: 2.3516, Test Loss: 2.3606, Learning Rate: 2.33e-05\n",
      "Step 1041, Train Loss: 2.2607, Test Loss: 2.1770, Learning Rate: 2.33e-05\n",
      "Step 1042, Train Loss: 2.4518, Test Loss: 2.4400, Learning Rate: 2.33e-05\n",
      "Step 1043, Train Loss: 2.3893, Test Loss: 2.0881, Learning Rate: 2.33e-05\n",
      "Step 1044, Train Loss: 2.2237, Test Loss: 2.1434, Learning Rate: 2.33e-05\n",
      "Step 1045, Train Loss: 2.3503, Test Loss: 2.1399, Learning Rate: 2.33e-05\n",
      "Step 1046, Train Loss: 2.2539, Test Loss: 2.3755, Learning Rate: 2.33e-05\n",
      "Step 1047, Train Loss: 2.3128, Test Loss: 2.2547, Learning Rate: 2.33e-05\n",
      "Step 1048, Train Loss: 2.3957, Test Loss: 2.4646, Learning Rate: 2.33e-05\n",
      "Step 1049, Train Loss: 2.4449, Test Loss: 2.3305, Learning Rate: 2.33e-05\n",
      "Step 1050, Train Loss: 2.2653, Test Loss: 2.3620, Learning Rate: 2.33e-05\n",
      "Step 1051, Train Loss: 2.4035, Test Loss: 2.2731, Learning Rate: 2.33e-05\n",
      "Step 1052, Train Loss: 2.3409, Test Loss: 2.2626, Learning Rate: 2.33e-05\n",
      "Step 1053, Train Loss: 2.3645, Test Loss: 2.1598, Learning Rate: 2.33e-05\n",
      "Step 1054, Train Loss: 2.2847, Test Loss: 2.3207, Learning Rate: 2.33e-05\n",
      "Step 1055, Train Loss: 2.3881, Test Loss: 2.2253, Learning Rate: 2.33e-05\n",
      "Step 1056, Train Loss: 2.3725, Test Loss: 2.1610, Learning Rate: 2.33e-05\n",
      "Step 1057, Train Loss: 2.4137, Test Loss: 2.5151, Learning Rate: 2.33e-05\n",
      "Step 1058, Train Loss: 2.2969, Test Loss: 2.2615, Learning Rate: 2.33e-05\n",
      "Step 1059, Train Loss: 2.4401, Test Loss: 2.4404, Learning Rate: 2.33e-05\n",
      "Step 1060, Train Loss: 2.3971, Test Loss: 2.4097, Learning Rate: 2.33e-05\n",
      "Step 1061, Train Loss: 2.2270, Test Loss: 2.4443, Learning Rate: 2.33e-05\n",
      "Step 1062, Train Loss: 2.3254, Test Loss: 2.2625, Learning Rate: 2.33e-05\n",
      "Step 1063, Train Loss: 2.3558, Test Loss: 2.3265, Learning Rate: 2.33e-05\n",
      "Step 1064, Train Loss: 2.3062, Test Loss: 2.3348, Learning Rate: 2.33e-05\n",
      "Step 1065, Train Loss: 2.3281, Test Loss: 2.3861, Learning Rate: 2.33e-05\n",
      "Step 1066, Train Loss: 2.2864, Test Loss: 2.4669, Learning Rate: 2.33e-05\n",
      "Step 1067, Train Loss: 2.4444, Test Loss: 2.2307, Learning Rate: 2.33e-05\n",
      "Step 1068, Train Loss: 2.3900, Test Loss: 2.3832, Learning Rate: 2.33e-05\n",
      "Step 1069, Train Loss: 2.2819, Test Loss: 2.4252, Learning Rate: 2.33e-05\n",
      "Step 1070, Train Loss: 2.2080, Test Loss: 2.3949, Learning Rate: 2.33e-05\n",
      "Step 1071, Train Loss: 2.2808, Test Loss: 2.3186, Learning Rate: 2.33e-05\n",
      "Step 1072, Train Loss: 2.3701, Test Loss: 2.3885, Learning Rate: 2.33e-05\n",
      "Step 1073, Train Loss: 2.3906, Test Loss: 2.4296, Learning Rate: 2.33e-05\n",
      "Step 1074, Train Loss: 2.2213, Test Loss: 2.3704, Learning Rate: 2.33e-05\n",
      "Step 1075, Train Loss: 2.2435, Test Loss: 2.3224, Learning Rate: 2.33e-05\n",
      "Step 1076, Train Loss: 2.3815, Test Loss: 2.5398, Learning Rate: 2.33e-05\n",
      "Step 1077, Train Loss: 2.2515, Test Loss: 2.6623, Learning Rate: 2.33e-05\n",
      "Step 1078, Train Loss: 2.3204, Test Loss: 2.2728, Learning Rate: 2.33e-05\n",
      "Step 1079, Train Loss: 2.3758, Test Loss: 2.4454, Learning Rate: 2.33e-05\n",
      "Step 1080, Train Loss: 2.3779, Test Loss: 2.3485, Learning Rate: 2.33e-05\n",
      "Step 1081, Train Loss: 2.2921, Test Loss: 2.2237, Learning Rate: 2.33e-05\n",
      "Step 1082, Train Loss: 2.3261, Test Loss: 2.2334, Learning Rate: 2.33e-05\n",
      "Step 1083, Train Loss: 2.3107, Test Loss: 2.2931, Learning Rate: 2.33e-05\n",
      "Step 1084, Train Loss: 2.3628, Test Loss: 2.4963, Learning Rate: 2.33e-05\n",
      "Step 1085, Train Loss: 2.2564, Test Loss: 2.3240, Learning Rate: 2.33e-05\n",
      "Step 1086, Train Loss: 2.3382, Test Loss: 2.3284, Learning Rate: 2.33e-05\n",
      "Step 1087, Train Loss: 2.2431, Test Loss: 2.4130, Learning Rate: 2.33e-05\n",
      "Step 1088, Train Loss: 2.3742, Test Loss: 2.4247, Learning Rate: 2.33e-05\n",
      "Step 1089, Train Loss: 2.1574, Test Loss: 2.2182, Learning Rate: 2.33e-05\n",
      "Step 1090, Train Loss: 2.4720, Test Loss: 2.2794, Learning Rate: 2.33e-05\n",
      "Step 1091, Train Loss: 2.3180, Test Loss: 2.2513, Learning Rate: 2.33e-05\n",
      "Step 1092, Train Loss: 2.2496, Test Loss: 2.4311, Learning Rate: 2.33e-05\n",
      "Step 1093, Train Loss: 2.3507, Test Loss: 2.2634, Learning Rate: 2.33e-05\n",
      "Step 1094, Train Loss: 2.3535, Test Loss: 2.3225, Learning Rate: 2.32e-05\n",
      "Step 1095, Train Loss: 2.2350, Test Loss: 2.2504, Learning Rate: 2.32e-05\n",
      "Step 1096, Train Loss: 2.2822, Test Loss: 2.1165, Learning Rate: 2.32e-05\n",
      "Step 1097, Train Loss: 2.3275, Test Loss: 2.3808, Learning Rate: 2.32e-05\n",
      "Step 1098, Train Loss: 2.3370, Test Loss: 2.2884, Learning Rate: 2.32e-05\n",
      "Step 1099, Train Loss: 2.3518, Test Loss: 2.2576, Learning Rate: 2.32e-05\n",
      "Step 1100, Train Loss: 2.2749, Test Loss: 2.4796, Learning Rate: 2.32e-05\n",
      "Step 1101, Train Loss: 2.4667, Test Loss: 2.2799, Learning Rate: 2.32e-05\n",
      "Step 1102, Train Loss: 2.3616, Test Loss: 2.2236, Learning Rate: 2.32e-05\n",
      "Step 1103, Train Loss: 2.2581, Test Loss: 2.3921, Learning Rate: 2.32e-05\n",
      "Step 1104, Train Loss: 2.2214, Test Loss: 2.2029, Learning Rate: 2.32e-05\n",
      "Step 1105, Train Loss: 2.3496, Test Loss: 2.1923, Learning Rate: 2.32e-05\n",
      "Step 1106, Train Loss: 2.2930, Test Loss: 2.3123, Learning Rate: 2.32e-05\n",
      "Step 1107, Train Loss: 2.3807, Test Loss: 2.4748, Learning Rate: 2.32e-05\n",
      "Step 1108, Train Loss: 2.4188, Test Loss: 2.2486, Learning Rate: 2.32e-05\n",
      "Step 1109, Train Loss: 2.1938, Test Loss: 2.3992, Learning Rate: 2.32e-05\n",
      "Step 1110, Train Loss: 2.3909, Test Loss: 2.1737, Learning Rate: 2.32e-05\n",
      "Step 1111, Train Loss: 2.3827, Test Loss: 2.2717, Learning Rate: 2.32e-05\n",
      "Step 1112, Train Loss: 2.2154, Test Loss: 2.2879, Learning Rate: 2.32e-05\n",
      "Step 1113, Train Loss: 2.3105, Test Loss: 2.1965, Learning Rate: 2.32e-05\n",
      "Step 1114, Train Loss: 2.3391, Test Loss: 2.1845, Learning Rate: 2.32e-05\n",
      "Step 1115, Train Loss: 2.2706, Test Loss: 2.2595, Learning Rate: 2.32e-05\n",
      "Step 1116, Train Loss: 2.3907, Test Loss: 2.3810, Learning Rate: 2.32e-05\n",
      "Step 1117, Train Loss: 2.3209, Test Loss: 2.2869, Learning Rate: 2.32e-05\n",
      "Step 1118, Train Loss: 2.3783, Test Loss: 2.3563, Learning Rate: 2.32e-05\n",
      "Step 1119, Train Loss: 2.3252, Test Loss: 2.2839, Learning Rate: 2.32e-05\n",
      "Step 1120, Train Loss: 2.3954, Test Loss: 2.2970, Learning Rate: 2.32e-05\n",
      "Step 1121, Train Loss: 2.3437, Test Loss: 2.3722, Learning Rate: 2.32e-05\n",
      "Step 1122, Train Loss: 2.3408, Test Loss: 2.2287, Learning Rate: 2.32e-05\n",
      "Step 1123, Train Loss: 2.3059, Test Loss: 2.2841, Learning Rate: 2.32e-05\n",
      "Step 1124, Train Loss: 2.2783, Test Loss: 2.3364, Learning Rate: 2.32e-05\n",
      "Step 1125, Train Loss: 2.3283, Test Loss: 2.2494, Learning Rate: 2.32e-05\n",
      "Step 1126, Train Loss: 2.2817, Test Loss: 2.2174, Learning Rate: 2.32e-05\n",
      "Step 1127, Train Loss: 2.3116, Test Loss: 2.1837, Learning Rate: 2.32e-05\n",
      "Step 1128, Train Loss: 2.2705, Test Loss: 2.5001, Learning Rate: 2.32e-05\n",
      "Step 1129, Train Loss: 2.1463, Test Loss: 2.5919, Learning Rate: 2.32e-05\n",
      "Step 1130, Train Loss: 2.3308, Test Loss: 2.3809, Learning Rate: 2.32e-05\n",
      "Step 1131, Train Loss: 2.2635, Test Loss: 2.2898, Learning Rate: 2.32e-05\n",
      "Step 1132, Train Loss: 2.3470, Test Loss: 2.5322, Learning Rate: 2.32e-05\n",
      "Step 1133, Train Loss: 2.3685, Test Loss: 2.4769, Learning Rate: 2.32e-05\n",
      "Step 1134, Train Loss: 2.3527, Test Loss: 2.3949, Learning Rate: 2.32e-05\n",
      "Step 1135, Train Loss: 2.3420, Test Loss: 2.3538, Learning Rate: 2.32e-05\n",
      "Step 1136, Train Loss: 2.3061, Test Loss: 2.2431, Learning Rate: 2.32e-05\n",
      "Step 1137, Train Loss: 2.4209, Test Loss: 2.4440, Learning Rate: 2.32e-05\n",
      "Step 1138, Train Loss: 2.2694, Test Loss: 2.2741, Learning Rate: 2.32e-05\n",
      "Step 1139, Train Loss: 2.3605, Test Loss: 2.2111, Learning Rate: 2.32e-05\n",
      "Step 1140, Train Loss: 2.4671, Test Loss: 2.2390, Learning Rate: 2.32e-05\n",
      "Step 1141, Train Loss: 2.2616, Test Loss: 2.2832, Learning Rate: 2.32e-05\n",
      "Step 1142, Train Loss: 2.4954, Test Loss: 2.2596, Learning Rate: 2.32e-05\n",
      "Step 1143, Train Loss: 2.2928, Test Loss: 2.2997, Learning Rate: 2.32e-05\n",
      "Step 1144, Train Loss: 2.2788, Test Loss: 2.1382, Learning Rate: 2.32e-05\n",
      "Step 1145, Train Loss: 2.1560, Test Loss: 2.2616, Learning Rate: 2.32e-05\n",
      "Step 1146, Train Loss: 2.3228, Test Loss: 2.4447, Learning Rate: 2.32e-05\n",
      "Step 1147, Train Loss: 2.2669, Test Loss: 2.2185, Learning Rate: 2.32e-05\n",
      "Step 1148, Train Loss: 2.4062, Test Loss: 2.3283, Learning Rate: 2.32e-05\n",
      "Step 1149, Train Loss: 2.4808, Test Loss: 2.2754, Learning Rate: 2.32e-05\n",
      "Step 1150, Train Loss: 2.4358, Test Loss: 2.5323, Learning Rate: 2.32e-05\n",
      "Step 1151, Train Loss: 2.3401, Test Loss: 2.3224, Learning Rate: 2.32e-05\n",
      "Step 1152, Train Loss: 2.2689, Test Loss: 2.2658, Learning Rate: 2.32e-05\n",
      "Step 1153, Train Loss: 2.4092, Test Loss: 2.4080, Learning Rate: 2.32e-05\n",
      "Step 1154, Train Loss: 2.4842, Test Loss: 2.1957, Learning Rate: 2.32e-05\n",
      "Step 1155, Train Loss: 2.3403, Test Loss: 2.2540, Learning Rate: 2.32e-05\n",
      "Step 1156, Train Loss: 2.4243, Test Loss: 2.3852, Learning Rate: 2.32e-05\n",
      "Step 1157, Train Loss: 2.2302, Test Loss: 2.2578, Learning Rate: 2.32e-05\n",
      "Step 1158, Train Loss: 2.2867, Test Loss: 2.4089, Learning Rate: 2.32e-05\n",
      "Step 1159, Train Loss: 2.3660, Test Loss: 2.2898, Learning Rate: 2.32e-05\n",
      "Step 1160, Train Loss: 2.2432, Test Loss: 2.2792, Learning Rate: 2.32e-05\n",
      "Step 1161, Train Loss: 2.2821, Test Loss: 2.3811, Learning Rate: 2.32e-05\n",
      "Step 1162, Train Loss: 2.3771, Test Loss: 2.3600, Learning Rate: 2.32e-05\n",
      "Step 1163, Train Loss: 2.2888, Test Loss: 2.4403, Learning Rate: 2.32e-05\n",
      "Step 1164, Train Loss: 2.2857, Test Loss: 2.4862, Learning Rate: 2.32e-05\n",
      "Step 1165, Train Loss: 2.3895, Test Loss: 2.1217, Learning Rate: 2.32e-05\n",
      "Step 1166, Train Loss: 2.4483, Test Loss: 2.4059, Learning Rate: 2.32e-05\n",
      "Step 1167, Train Loss: 2.2450, Test Loss: 2.5005, Learning Rate: 2.32e-05\n",
      "Step 1168, Train Loss: 2.3666, Test Loss: 2.2423, Learning Rate: 2.32e-05\n",
      "Step 1169, Train Loss: 2.2940, Test Loss: 2.4352, Learning Rate: 2.32e-05\n",
      "Step 1170, Train Loss: 2.2147, Test Loss: 2.3900, Learning Rate: 2.32e-05\n",
      "Step 1171, Train Loss: 2.3515, Test Loss: 2.3780, Learning Rate: 2.32e-05\n",
      "Step 1172, Train Loss: 2.4663, Test Loss: 2.3490, Learning Rate: 2.32e-05\n",
      "Step 1173, Train Loss: 2.3048, Test Loss: 2.1847, Learning Rate: 2.32e-05\n",
      "Step 1174, Train Loss: 2.2256, Test Loss: 2.1595, Learning Rate: 2.32e-05\n",
      "Step 1175, Train Loss: 2.3087, Test Loss: 2.5250, Learning Rate: 2.32e-05\n",
      "Step 1176, Train Loss: 2.4708, Test Loss: 2.2860, Learning Rate: 2.32e-05\n",
      "Step 1177, Train Loss: 2.4003, Test Loss: 2.4877, Learning Rate: 2.32e-05\n",
      "Step 1178, Train Loss: 2.4078, Test Loss: 2.2882, Learning Rate: 2.32e-05\n",
      "Step 1179, Train Loss: 2.2103, Test Loss: 2.2759, Learning Rate: 2.32e-05\n",
      "Step 1180, Train Loss: 2.3287, Test Loss: 2.1871, Learning Rate: 2.32e-05\n",
      "Step 1181, Train Loss: 2.3371, Test Loss: 2.2201, Learning Rate: 2.32e-05\n",
      "Step 1182, Train Loss: 2.2757, Test Loss: 2.3313, Learning Rate: 2.32e-05\n",
      "Step 1183, Train Loss: 2.3928, Test Loss: 2.3514, Learning Rate: 2.32e-05\n",
      "Step 1184, Train Loss: 2.3598, Test Loss: 2.2062, Learning Rate: 2.32e-05\n",
      "Step 1185, Train Loss: 2.3124, Test Loss: 2.1492, Learning Rate: 2.32e-05\n",
      "Step 1186, Train Loss: 2.2885, Test Loss: 2.4395, Learning Rate: 2.32e-05\n",
      "Step 1187, Train Loss: 2.4893, Test Loss: 2.4903, Learning Rate: 2.32e-05\n",
      "Step 1188, Train Loss: 2.2902, Test Loss: 2.3056, Learning Rate: 2.32e-05\n",
      "Step 1189, Train Loss: 2.2965, Test Loss: 2.2949, Learning Rate: 2.32e-05\n",
      "Step 1190, Train Loss: 2.3023, Test Loss: 2.3291, Learning Rate: 2.32e-05\n",
      "Step 1191, Train Loss: 2.2832, Test Loss: 2.2491, Learning Rate: 2.32e-05\n",
      "Step 1192, Train Loss: 2.3123, Test Loss: 2.2095, Learning Rate: 2.32e-05\n",
      "Step 1193, Train Loss: 2.5293, Test Loss: 2.3638, Learning Rate: 2.32e-05\n",
      "Step 1194, Train Loss: 2.3274, Test Loss: 2.2109, Learning Rate: 2.32e-05\n",
      "Step 1195, Train Loss: 2.3176, Test Loss: 2.4303, Learning Rate: 2.32e-05\n",
      "Step 1196, Train Loss: 2.3166, Test Loss: 2.3157, Learning Rate: 2.32e-05\n",
      "Step 1197, Train Loss: 2.2218, Test Loss: 2.1088, Learning Rate: 2.32e-05\n",
      "Step 1198, Train Loss: 2.3513, Test Loss: 2.2737, Learning Rate: 2.32e-05\n",
      "Step 1199, Train Loss: 2.3599, Test Loss: 2.2887, Learning Rate: 2.32e-05\n",
      "Step 1200, Train Loss: 2.3376, Test Loss: 2.2582, Learning Rate: 2.32e-05\n",
      "Step 1201, Train Loss: 2.3121, Test Loss: 2.2053, Learning Rate: 2.32e-05\n",
      "Step 1202, Train Loss: 2.3605, Test Loss: 2.3922, Learning Rate: 2.32e-05\n",
      "Step 1203, Train Loss: 2.2394, Test Loss: 2.1469, Learning Rate: 2.32e-05\n",
      "Step 1204, Train Loss: 2.2721, Test Loss: 2.3008, Learning Rate: 2.32e-05\n",
      "Step 1205, Train Loss: 2.3987, Test Loss: 2.4108, Learning Rate: 2.32e-05\n",
      "Step 1206, Train Loss: 2.2441, Test Loss: 2.3195, Learning Rate: 2.32e-05\n",
      "Step 1207, Train Loss: 2.3004, Test Loss: 2.2853, Learning Rate: 2.32e-05\n",
      "Step 1208, Train Loss: 2.4489, Test Loss: 2.2633, Learning Rate: 2.32e-05\n",
      "Step 1209, Train Loss: 2.3050, Test Loss: 2.4810, Learning Rate: 2.32e-05\n",
      "Step 1210, Train Loss: 2.4092, Test Loss: 2.1941, Learning Rate: 2.32e-05\n",
      "Step 1211, Train Loss: 2.4169, Test Loss: 2.2693, Learning Rate: 2.32e-05\n",
      "Step 1212, Train Loss: 2.3285, Test Loss: 2.1869, Learning Rate: 2.32e-05\n",
      "Step 1213, Train Loss: 2.2823, Test Loss: 2.3390, Learning Rate: 2.32e-05\n",
      "Step 1214, Train Loss: 2.4457, Test Loss: 2.1853, Learning Rate: 2.32e-05\n",
      "Step 1215, Train Loss: 2.3243, Test Loss: 2.3898, Learning Rate: 2.32e-05\n",
      "Step 1216, Train Loss: 2.1462, Test Loss: 2.2296, Learning Rate: 2.32e-05\n",
      "Step 1217, Train Loss: 2.4182, Test Loss: 2.5392, Learning Rate: 2.32e-05\n",
      "Step 1218, Train Loss: 2.3071, Test Loss: 2.2826, Learning Rate: 2.32e-05\n",
      "Step 1219, Train Loss: 2.3761, Test Loss: 2.2163, Learning Rate: 2.31e-05\n",
      "Step 1220, Train Loss: 2.2884, Test Loss: 2.2313, Learning Rate: 2.31e-05\n",
      "Step 1221, Train Loss: 2.3709, Test Loss: 2.2242, Learning Rate: 2.31e-05\n",
      "Step 1222, Train Loss: 2.3679, Test Loss: 2.2478, Learning Rate: 2.31e-05\n",
      "Step 1223, Train Loss: 2.2967, Test Loss: 2.3210, Learning Rate: 2.31e-05\n",
      "Step 1224, Train Loss: 2.3354, Test Loss: 2.2363, Learning Rate: 2.31e-05\n",
      "Step 1225, Train Loss: 2.3344, Test Loss: 2.2760, Learning Rate: 2.31e-05\n",
      "Step 1226, Train Loss: 2.3031, Test Loss: 2.1683, Learning Rate: 2.31e-05\n",
      "Step 1227, Train Loss: 2.3202, Test Loss: 2.3315, Learning Rate: 2.31e-05\n",
      "Step 1228, Train Loss: 2.3737, Test Loss: 2.2295, Learning Rate: 2.31e-05\n",
      "Step 1229, Train Loss: 2.2387, Test Loss: 2.1387, Learning Rate: 2.31e-05\n",
      "Step 1230, Train Loss: 2.3398, Test Loss: 2.2835, Learning Rate: 2.31e-05\n",
      "Step 1231, Train Loss: 2.1794, Test Loss: 2.2503, Learning Rate: 2.31e-05\n",
      "Step 1232, Train Loss: 2.3012, Test Loss: 2.4836, Learning Rate: 2.31e-05\n",
      "Step 1233, Train Loss: 2.4090, Test Loss: 2.3122, Learning Rate: 2.31e-05\n",
      "Step 1234, Train Loss: 2.2912, Test Loss: 2.5457, Learning Rate: 2.31e-05\n",
      "Step 1235, Train Loss: 2.3201, Test Loss: 2.6046, Learning Rate: 2.31e-05\n",
      "Step 1236, Train Loss: 2.4156, Test Loss: 2.2209, Learning Rate: 2.31e-05\n",
      "Step 1237, Train Loss: 2.3543, Test Loss: 2.2376, Learning Rate: 2.31e-05\n",
      "Step 1238, Train Loss: 2.2618, Test Loss: 2.2625, Learning Rate: 2.31e-05\n",
      "Step 1239, Train Loss: 2.2636, Test Loss: 2.1863, Learning Rate: 2.31e-05\n",
      "Step 1240, Train Loss: 2.3743, Test Loss: 2.3743, Learning Rate: 2.31e-05\n",
      "Step 1241, Train Loss: 2.4554, Test Loss: 2.1450, Learning Rate: 2.31e-05\n",
      "Step 1242, Train Loss: 2.1106, Test Loss: 2.3842, Learning Rate: 2.31e-05\n",
      "Step 1243, Train Loss: 2.4161, Test Loss: 2.3983, Learning Rate: 2.31e-05\n",
      "Step 1244, Train Loss: 2.2667, Test Loss: 2.4325, Learning Rate: 2.31e-05\n",
      "Step 1245, Train Loss: 2.2582, Test Loss: 2.6190, Learning Rate: 2.31e-05\n",
      "Step 1246, Train Loss: 2.2962, Test Loss: 2.0899, Learning Rate: 2.31e-05\n",
      "Step 1247, Train Loss: 2.4562, Test Loss: 2.1327, Learning Rate: 2.31e-05\n",
      "Step 1248, Train Loss: 2.2108, Test Loss: 2.2178, Learning Rate: 2.31e-05\n",
      "Step 1249, Train Loss: 2.2440, Test Loss: 2.3115, Learning Rate: 2.31e-05\n",
      "Step 1250, Train Loss: 2.2989, Test Loss: 2.3202, Learning Rate: 2.31e-05\n",
      "Step 1251, Train Loss: 2.2532, Test Loss: 2.3749, Learning Rate: 2.31e-05\n",
      "Step 1252, Train Loss: 2.3554, Test Loss: 2.2549, Learning Rate: 2.31e-05\n",
      "Step 1253, Train Loss: 2.3096, Test Loss: 2.4038, Learning Rate: 2.31e-05\n",
      "Step 1254, Train Loss: 2.3932, Test Loss: 2.4152, Learning Rate: 2.31e-05\n",
      "Step 1255, Train Loss: 2.2924, Test Loss: 2.0903, Learning Rate: 2.31e-05\n",
      "Step 1256, Train Loss: 2.4654, Test Loss: 2.3680, Learning Rate: 2.31e-05\n",
      "Step 1257, Train Loss: 2.4803, Test Loss: 2.3228, Learning Rate: 2.31e-05\n",
      "Step 1258, Train Loss: 2.3125, Test Loss: 2.4421, Learning Rate: 2.31e-05\n",
      "Step 1259, Train Loss: 2.3488, Test Loss: 2.3007, Learning Rate: 2.31e-05\n",
      "Step 1260, Train Loss: 2.3693, Test Loss: 2.2510, Learning Rate: 2.31e-05\n",
      "Step 1261, Train Loss: 2.2566, Test Loss: 2.3328, Learning Rate: 2.31e-05\n",
      "Step 1262, Train Loss: 2.1426, Test Loss: 2.4440, Learning Rate: 2.31e-05\n",
      "Step 1263, Train Loss: 2.3071, Test Loss: 2.3944, Learning Rate: 2.31e-05\n",
      "Step 1264, Train Loss: 2.2074, Test Loss: 2.1476, Learning Rate: 2.31e-05\n",
      "Step 1265, Train Loss: 2.3253, Test Loss: 2.5387, Learning Rate: 2.31e-05\n",
      "Step 1266, Train Loss: 2.2184, Test Loss: 2.4082, Learning Rate: 2.31e-05\n",
      "Step 1267, Train Loss: 2.2779, Test Loss: 2.2183, Learning Rate: 2.31e-05\n",
      "Step 1268, Train Loss: 2.3882, Test Loss: 2.3721, Learning Rate: 2.31e-05\n",
      "Step 1269, Train Loss: 2.4308, Test Loss: 2.1744, Learning Rate: 2.31e-05\n",
      "Step 1270, Train Loss: 2.2790, Test Loss: 2.4392, Learning Rate: 2.31e-05\n",
      "Step 1271, Train Loss: 2.3349, Test Loss: 2.2046, Learning Rate: 2.31e-05\n",
      "Step 1272, Train Loss: 2.3867, Test Loss: 2.3898, Learning Rate: 2.31e-05\n",
      "Step 1273, Train Loss: 2.3675, Test Loss: 2.2500, Learning Rate: 2.31e-05\n",
      "Step 1274, Train Loss: 2.3053, Test Loss: 2.4091, Learning Rate: 2.31e-05\n",
      "Step 1275, Train Loss: 2.4105, Test Loss: 2.2277, Learning Rate: 2.31e-05\n",
      "Step 1276, Train Loss: 2.3519, Test Loss: 2.1234, Learning Rate: 2.31e-05\n",
      "Step 1277, Train Loss: 2.3748, Test Loss: 2.3968, Learning Rate: 2.31e-05\n",
      "Step 1278, Train Loss: 2.3689, Test Loss: 2.5759, Learning Rate: 2.31e-05\n",
      "Step 1279, Train Loss: 2.3114, Test Loss: 2.2740, Learning Rate: 2.31e-05\n",
      "Step 1280, Train Loss: 2.3646, Test Loss: 2.2621, Learning Rate: 2.31e-05\n",
      "Step 1281, Train Loss: 2.2919, Test Loss: 2.6227, Learning Rate: 2.31e-05\n",
      "Step 1282, Train Loss: 2.3176, Test Loss: 2.3939, Learning Rate: 2.31e-05\n",
      "Step 1283, Train Loss: 2.3266, Test Loss: 2.2730, Learning Rate: 2.31e-05\n",
      "Step 1284, Train Loss: 2.3604, Test Loss: 2.2264, Learning Rate: 2.31e-05\n",
      "Step 1285, Train Loss: 2.2887, Test Loss: 2.3637, Learning Rate: 2.31e-05\n",
      "Step 1286, Train Loss: 2.3326, Test Loss: 2.4492, Learning Rate: 2.31e-05\n",
      "Step 1287, Train Loss: 2.3263, Test Loss: 2.3966, Learning Rate: 2.31e-05\n",
      "Step 1288, Train Loss: 2.2634, Test Loss: 2.1555, Learning Rate: 2.31e-05\n",
      "Step 1289, Train Loss: 2.1482, Test Loss: 2.2518, Learning Rate: 2.31e-05\n",
      "Step 1290, Train Loss: 2.2831, Test Loss: 2.3020, Learning Rate: 2.31e-05\n",
      "Step 1291, Train Loss: 2.5390, Test Loss: 2.2605, Learning Rate: 2.31e-05\n",
      "Step 1292, Train Loss: 2.4358, Test Loss: 2.3925, Learning Rate: 2.31e-05\n",
      "Step 1293, Train Loss: 2.3049, Test Loss: 2.4223, Learning Rate: 2.31e-05\n",
      "Step 1294, Train Loss: 2.2691, Test Loss: 2.2894, Learning Rate: 2.31e-05\n",
      "Step 1295, Train Loss: 2.3257, Test Loss: 2.2126, Learning Rate: 2.31e-05\n",
      "Step 1296, Train Loss: 2.3167, Test Loss: 2.2328, Learning Rate: 2.31e-05\n",
      "Step 1297, Train Loss: 2.3334, Test Loss: 2.2898, Learning Rate: 2.31e-05\n",
      "Step 1298, Train Loss: 2.1214, Test Loss: 2.2010, Learning Rate: 2.31e-05\n",
      "Step 1299, Train Loss: 2.3239, Test Loss: 2.3157, Learning Rate: 2.31e-05\n",
      "Step 1300, Train Loss: 2.3032, Test Loss: 2.3173, Learning Rate: 2.31e-05\n",
      "Step 1301, Train Loss: 2.3268, Test Loss: 2.5290, Learning Rate: 2.31e-05\n",
      "Step 1302, Train Loss: 2.3574, Test Loss: 2.2645, Learning Rate: 2.31e-05\n",
      "Step 1303, Train Loss: 2.2781, Test Loss: 2.3353, Learning Rate: 2.31e-05\n",
      "Step 1304, Train Loss: 2.3636, Test Loss: 2.3354, Learning Rate: 2.31e-05\n",
      "Step 1305, Train Loss: 2.5491, Test Loss: 2.3788, Learning Rate: 2.31e-05\n",
      "Step 1306, Train Loss: 2.1743, Test Loss: 2.3716, Learning Rate: 2.31e-05\n",
      "Step 1307, Train Loss: 2.3088, Test Loss: 2.3371, Learning Rate: 2.31e-05\n",
      "Step 1308, Train Loss: 2.3808, Test Loss: 2.4442, Learning Rate: 2.31e-05\n",
      "Step 1309, Train Loss: 2.3576, Test Loss: 2.2705, Learning Rate: 2.31e-05\n",
      "Step 1310, Train Loss: 2.2898, Test Loss: 2.2294, Learning Rate: 2.31e-05\n",
      "Step 1311, Train Loss: 2.3573, Test Loss: 2.2672, Learning Rate: 2.31e-05\n",
      "Step 1312, Train Loss: 2.2872, Test Loss: 2.2104, Learning Rate: 2.31e-05\n",
      "Step 1313, Train Loss: 2.4550, Test Loss: 2.3358, Learning Rate: 2.31e-05\n",
      "Step 1314, Train Loss: 2.3397, Test Loss: 2.2926, Learning Rate: 2.31e-05\n",
      "Step 1315, Train Loss: 2.3530, Test Loss: 2.3058, Learning Rate: 2.31e-05\n",
      "Step 1316, Train Loss: 2.3339, Test Loss: 2.1958, Learning Rate: 2.31e-05\n",
      "Step 1317, Train Loss: 2.2703, Test Loss: 2.3996, Learning Rate: 2.31e-05\n",
      "Step 1318, Train Loss: 2.2675, Test Loss: 2.4485, Learning Rate: 2.31e-05\n",
      "Step 1319, Train Loss: 2.4438, Test Loss: 2.2845, Learning Rate: 2.31e-05\n",
      "Step 1320, Train Loss: 2.3420, Test Loss: 2.3854, Learning Rate: 2.31e-05\n",
      "Step 1321, Train Loss: 2.2449, Test Loss: 2.4855, Learning Rate: 2.31e-05\n",
      "Step 1322, Train Loss: 2.3324, Test Loss: 2.2582, Learning Rate: 2.31e-05\n",
      "Step 1323, Train Loss: 2.3246, Test Loss: 2.5171, Learning Rate: 2.31e-05\n",
      "Step 1324, Train Loss: 2.3126, Test Loss: 2.2054, Learning Rate: 2.31e-05\n",
      "Step 1325, Train Loss: 2.2384, Test Loss: 2.4221, Learning Rate: 2.31e-05\n",
      "Step 1326, Train Loss: 2.2370, Test Loss: 2.2494, Learning Rate: 2.31e-05\n",
      "Step 1327, Train Loss: 2.0950, Test Loss: 2.3558, Learning Rate: 2.31e-05\n",
      "Step 1328, Train Loss: 2.2729, Test Loss: 2.4354, Learning Rate: 2.31e-05\n",
      "Step 1329, Train Loss: 2.3330, Test Loss: 2.2940, Learning Rate: 2.31e-05\n",
      "Step 1330, Train Loss: 2.4006, Test Loss: 2.3443, Learning Rate: 2.31e-05\n",
      "Step 1331, Train Loss: 2.3012, Test Loss: 2.3374, Learning Rate: 2.31e-05\n",
      "Step 1332, Train Loss: 2.2664, Test Loss: 2.4108, Learning Rate: 2.31e-05\n",
      "Step 1333, Train Loss: 2.3809, Test Loss: 2.3633, Learning Rate: 2.31e-05\n",
      "Step 1334, Train Loss: 2.2770, Test Loss: 2.2815, Learning Rate: 2.31e-05\n",
      "Step 1335, Train Loss: 2.2377, Test Loss: 2.2059, Learning Rate: 2.31e-05\n",
      "Step 1336, Train Loss: 2.3682, Test Loss: 2.3495, Learning Rate: 2.31e-05\n",
      "Step 1337, Train Loss: 2.4657, Test Loss: 2.2982, Learning Rate: 2.31e-05\n",
      "Step 1338, Train Loss: 2.3580, Test Loss: 2.5980, Learning Rate: 2.31e-05\n",
      "Step 1339, Train Loss: 2.3469, Test Loss: 2.3247, Learning Rate: 2.31e-05\n",
      "Step 1340, Train Loss: 2.3411, Test Loss: 2.4867, Learning Rate: 2.31e-05\n",
      "Step 1341, Train Loss: 2.3295, Test Loss: 2.5257, Learning Rate: 2.31e-05\n",
      "Step 1342, Train Loss: 2.2908, Test Loss: 2.3696, Learning Rate: 2.31e-05\n",
      "Step 1343, Train Loss: 2.4355, Test Loss: 2.1873, Learning Rate: 2.31e-05\n",
      "Step 1344, Train Loss: 2.3292, Test Loss: 2.2975, Learning Rate: 2.30e-05\n",
      "Step 1345, Train Loss: 2.3141, Test Loss: 2.1995, Learning Rate: 2.30e-05\n",
      "Step 1346, Train Loss: 2.3703, Test Loss: 2.3280, Learning Rate: 2.30e-05\n",
      "Step 1347, Train Loss: 2.2859, Test Loss: 2.2055, Learning Rate: 2.30e-05\n",
      "Step 1348, Train Loss: 2.3066, Test Loss: 2.4873, Learning Rate: 2.30e-05\n",
      "Step 1349, Train Loss: 2.4602, Test Loss: 2.4035, Learning Rate: 2.30e-05\n",
      "Step 1350, Train Loss: 2.2753, Test Loss: 2.3230, Learning Rate: 2.30e-05\n",
      "Step 1351, Train Loss: 2.4279, Test Loss: 2.2527, Learning Rate: 2.30e-05\n",
      "Step 1352, Train Loss: 2.3452, Test Loss: 2.4852, Learning Rate: 2.30e-05\n",
      "Step 1353, Train Loss: 2.4303, Test Loss: 2.3004, Learning Rate: 2.30e-05\n",
      "Step 1354, Train Loss: 2.2578, Test Loss: 2.2747, Learning Rate: 2.30e-05\n",
      "Step 1355, Train Loss: 2.3042, Test Loss: 2.3294, Learning Rate: 2.30e-05\n",
      "Step 1356, Train Loss: 2.3265, Test Loss: 2.3136, Learning Rate: 2.30e-05\n",
      "Step 1357, Train Loss: 2.3200, Test Loss: 2.3186, Learning Rate: 2.30e-05\n",
      "Step 1358, Train Loss: 2.3074, Test Loss: 2.1478, Learning Rate: 2.30e-05\n",
      "Step 1359, Train Loss: 2.2741, Test Loss: 2.5834, Learning Rate: 2.30e-05\n",
      "Step 1360, Train Loss: 2.3317, Test Loss: 2.2832, Learning Rate: 2.30e-05\n",
      "Step 1361, Train Loss: 2.4035, Test Loss: 2.3101, Learning Rate: 2.30e-05\n",
      "Step 1362, Train Loss: 2.1268, Test Loss: 2.1351, Learning Rate: 2.30e-05\n",
      "Step 1363, Train Loss: 2.4019, Test Loss: 2.2741, Learning Rate: 2.30e-05\n",
      "Step 1364, Train Loss: 2.3186, Test Loss: 2.2979, Learning Rate: 2.30e-05\n",
      "Step 1365, Train Loss: 2.4169, Test Loss: 2.5282, Learning Rate: 2.30e-05\n",
      "Step 1366, Train Loss: 2.2530, Test Loss: 2.3009, Learning Rate: 2.30e-05\n",
      "Step 1367, Train Loss: 2.3574, Test Loss: 2.3529, Learning Rate: 2.30e-05\n",
      "Step 1368, Train Loss: 2.4473, Test Loss: 2.1531, Learning Rate: 2.30e-05\n",
      "Step 1369, Train Loss: 2.3449, Test Loss: 2.2712, Learning Rate: 2.30e-05\n",
      "Step 1370, Train Loss: 2.3614, Test Loss: 2.2959, Learning Rate: 2.30e-05\n",
      "Step 1371, Train Loss: 2.3202, Test Loss: 2.4195, Learning Rate: 2.30e-05\n",
      "Step 1372, Train Loss: 2.3044, Test Loss: 2.0556, Learning Rate: 2.30e-05\n",
      "Step 1373, Train Loss: 2.2415, Test Loss: 2.1764, Learning Rate: 2.30e-05\n",
      "Step 1374, Train Loss: 2.4411, Test Loss: 2.4864, Learning Rate: 2.30e-05\n",
      "Step 1375, Train Loss: 2.3342, Test Loss: 2.2996, Learning Rate: 2.30e-05\n",
      "Step 1376, Train Loss: 2.3649, Test Loss: 2.4247, Learning Rate: 2.30e-05\n",
      "Step 1377, Train Loss: 2.3988, Test Loss: 2.2643, Learning Rate: 2.30e-05\n",
      "Step 1378, Train Loss: 2.3519, Test Loss: 2.3354, Learning Rate: 2.30e-05\n",
      "Step 1379, Train Loss: 2.2680, Test Loss: 2.4754, Learning Rate: 2.30e-05\n",
      "Step 1380, Train Loss: 2.3577, Test Loss: 2.5236, Learning Rate: 2.30e-05\n",
      "Step 1381, Train Loss: 2.4047, Test Loss: 1.8654, Learning Rate: 2.30e-05\n",
      "Step 1382, Train Loss: 2.3207, Test Loss: 2.3788, Learning Rate: 2.30e-05\n",
      "Step 1383, Train Loss: 2.2336, Test Loss: 2.2202, Learning Rate: 2.30e-05\n",
      "Step 1384, Train Loss: 2.1880, Test Loss: 2.3206, Learning Rate: 2.30e-05\n",
      "Step 1385, Train Loss: 2.3394, Test Loss: 2.3113, Learning Rate: 2.30e-05\n",
      "Step 1386, Train Loss: 2.4414, Test Loss: 2.4505, Learning Rate: 2.30e-05\n",
      "Step 1387, Train Loss: 2.2562, Test Loss: 2.2803, Learning Rate: 2.30e-05\n",
      "Step 1388, Train Loss: 2.4316, Test Loss: 2.3506, Learning Rate: 2.30e-05\n",
      "Step 1389, Train Loss: 2.2808, Test Loss: 2.2770, Learning Rate: 2.30e-05\n",
      "Step 1390, Train Loss: 2.3483, Test Loss: 2.2973, Learning Rate: 2.30e-05\n",
      "Step 1391, Train Loss: 2.4324, Test Loss: 2.4303, Learning Rate: 2.30e-05\n",
      "Step 1392, Train Loss: 2.3258, Test Loss: 2.2934, Learning Rate: 2.30e-05\n",
      "Step 1393, Train Loss: 2.4121, Test Loss: 2.2811, Learning Rate: 2.30e-05\n",
      "Step 1394, Train Loss: 2.3785, Test Loss: 2.1371, Learning Rate: 2.30e-05\n",
      "Step 1395, Train Loss: 2.3623, Test Loss: 2.4346, Learning Rate: 2.30e-05\n",
      "Step 1396, Train Loss: 2.2315, Test Loss: 2.2952, Learning Rate: 2.30e-05\n",
      "Step 1397, Train Loss: 2.2665, Test Loss: 2.4040, Learning Rate: 2.30e-05\n",
      "Step 1398, Train Loss: 2.2618, Test Loss: 2.4687, Learning Rate: 2.30e-05\n",
      "Step 1399, Train Loss: 2.4286, Test Loss: 2.2210, Learning Rate: 2.30e-05\n",
      "Step 1400, Train Loss: 2.3503, Test Loss: 2.3504, Learning Rate: 2.30e-05\n",
      "Step 1401, Train Loss: 2.2985, Test Loss: 2.3175, Learning Rate: 2.30e-05\n",
      "Step 1402, Train Loss: 2.3785, Test Loss: 2.2138, Learning Rate: 2.30e-05\n",
      "Step 1403, Train Loss: 2.4535, Test Loss: 2.3387, Learning Rate: 2.30e-05\n",
      "Step 1404, Train Loss: 2.1511, Test Loss: 2.2479, Learning Rate: 2.30e-05\n",
      "Step 1405, Train Loss: 2.3228, Test Loss: 2.3517, Learning Rate: 2.30e-05\n",
      "Step 1406, Train Loss: 2.3319, Test Loss: 2.2489, Learning Rate: 2.30e-05\n",
      "Step 1407, Train Loss: 2.4231, Test Loss: 2.2143, Learning Rate: 2.30e-05\n",
      "Step 1408, Train Loss: 2.4050, Test Loss: 2.1851, Learning Rate: 2.30e-05\n",
      "Step 1409, Train Loss: 2.1510, Test Loss: 2.2628, Learning Rate: 2.30e-05\n",
      "Step 1410, Train Loss: 2.3302, Test Loss: 2.3626, Learning Rate: 2.30e-05\n",
      "Step 1411, Train Loss: 2.4877, Test Loss: 2.2782, Learning Rate: 2.30e-05\n",
      "Step 1412, Train Loss: 2.1832, Test Loss: 2.6114, Learning Rate: 2.30e-05\n",
      "Step 1413, Train Loss: 2.3185, Test Loss: 2.3003, Learning Rate: 2.30e-05\n",
      "Step 1414, Train Loss: 2.3337, Test Loss: 2.2894, Learning Rate: 2.30e-05\n",
      "Step 1415, Train Loss: 2.3515, Test Loss: 2.3312, Learning Rate: 2.30e-05\n",
      "Step 1416, Train Loss: 2.3258, Test Loss: 2.4116, Learning Rate: 2.30e-05\n",
      "Step 1417, Train Loss: 2.2762, Test Loss: 2.3209, Learning Rate: 2.30e-05\n",
      "Step 1418, Train Loss: 2.3556, Test Loss: 2.4137, Learning Rate: 2.30e-05\n",
      "Step 1419, Train Loss: 2.3319, Test Loss: 2.1455, Learning Rate: 2.30e-05\n",
      "Step 1420, Train Loss: 2.2674, Test Loss: 2.3488, Learning Rate: 2.30e-05\n",
      "Step 1421, Train Loss: 2.3771, Test Loss: 2.3913, Learning Rate: 2.30e-05\n",
      "Step 1422, Train Loss: 2.4306, Test Loss: 2.3473, Learning Rate: 2.30e-05\n",
      "Step 1423, Train Loss: 2.3383, Test Loss: 2.3669, Learning Rate: 2.30e-05\n",
      "Step 1424, Train Loss: 2.4092, Test Loss: 2.3427, Learning Rate: 2.30e-05\n",
      "Step 1425, Train Loss: 2.3523, Test Loss: 2.3807, Learning Rate: 2.30e-05\n",
      "Step 1426, Train Loss: 2.3582, Test Loss: 2.4775, Learning Rate: 2.30e-05\n",
      "Step 1427, Train Loss: 2.3234, Test Loss: 2.2896, Learning Rate: 2.30e-05\n",
      "Step 1428, Train Loss: 2.3473, Test Loss: 2.2345, Learning Rate: 2.30e-05\n",
      "Step 1429, Train Loss: 2.3098, Test Loss: 2.0814, Learning Rate: 2.30e-05\n",
      "Step 1430, Train Loss: 2.5027, Test Loss: 2.3768, Learning Rate: 2.30e-05\n",
      "Step 1431, Train Loss: 2.3343, Test Loss: 2.2509, Learning Rate: 2.30e-05\n",
      "Step 1432, Train Loss: 2.2026, Test Loss: 2.1157, Learning Rate: 2.30e-05\n",
      "Step 1433, Train Loss: 2.3869, Test Loss: 2.4162, Learning Rate: 2.30e-05\n",
      "Step 1434, Train Loss: 2.4059, Test Loss: 2.3665, Learning Rate: 2.30e-05\n",
      "Step 1435, Train Loss: 2.3741, Test Loss: 2.2920, Learning Rate: 2.30e-05\n",
      "Step 1436, Train Loss: 2.4038, Test Loss: 2.2516, Learning Rate: 2.30e-05\n",
      "Step 1437, Train Loss: 2.3544, Test Loss: 2.4636, Learning Rate: 2.30e-05\n",
      "Step 1438, Train Loss: 2.3983, Test Loss: 2.3645, Learning Rate: 2.30e-05\n",
      "Step 1439, Train Loss: 2.2690, Test Loss: 2.3572, Learning Rate: 2.30e-05\n",
      "Step 1440, Train Loss: 2.2787, Test Loss: 2.0515, Learning Rate: 2.30e-05\n",
      "Step 1441, Train Loss: 2.3645, Test Loss: 2.4646, Learning Rate: 2.30e-05\n",
      "Step 1442, Train Loss: 2.2748, Test Loss: 2.2322, Learning Rate: 2.30e-05\n",
      "Step 1443, Train Loss: 2.2931, Test Loss: 2.3236, Learning Rate: 2.30e-05\n",
      "Step 1444, Train Loss: 2.3498, Test Loss: 2.2765, Learning Rate: 2.30e-05\n",
      "Step 1445, Train Loss: 2.3645, Test Loss: 2.3422, Learning Rate: 2.30e-05\n",
      "Step 1446, Train Loss: 2.3564, Test Loss: 2.3296, Learning Rate: 2.30e-05\n",
      "Step 1447, Train Loss: 2.3361, Test Loss: 2.3186, Learning Rate: 2.30e-05\n",
      "Step 1448, Train Loss: 2.3350, Test Loss: 2.3357, Learning Rate: 2.30e-05\n",
      "Step 1449, Train Loss: 2.2600, Test Loss: 2.2985, Learning Rate: 2.30e-05\n",
      "Step 1450, Train Loss: 2.3420, Test Loss: 2.1764, Learning Rate: 2.30e-05\n",
      "Step 1451, Train Loss: 2.2922, Test Loss: 2.3202, Learning Rate: 2.30e-05\n",
      "Step 1452, Train Loss: 2.2740, Test Loss: 2.2693, Learning Rate: 2.30e-05\n",
      "Step 1453, Train Loss: 2.2764, Test Loss: 2.4273, Learning Rate: 2.30e-05\n",
      "Step 1454, Train Loss: 2.3132, Test Loss: 2.3256, Learning Rate: 2.30e-05\n",
      "Step 1455, Train Loss: 2.3540, Test Loss: 2.2610, Learning Rate: 2.30e-05\n",
      "Step 1456, Train Loss: 2.3017, Test Loss: 2.3565, Learning Rate: 2.30e-05\n",
      "Step 1457, Train Loss: 2.2033, Test Loss: 2.3238, Learning Rate: 2.30e-05\n",
      "Step 1458, Train Loss: 2.2816, Test Loss: 2.4302, Learning Rate: 2.30e-05\n",
      "Step 1459, Train Loss: 2.3870, Test Loss: 2.2714, Learning Rate: 2.30e-05\n",
      "Step 1460, Train Loss: 2.3531, Test Loss: 2.4013, Learning Rate: 2.30e-05\n",
      "Step 1461, Train Loss: 2.2531, Test Loss: 2.3707, Learning Rate: 2.30e-05\n",
      "Step 1462, Train Loss: 2.3385, Test Loss: 2.2624, Learning Rate: 2.30e-05\n",
      "Step 1463, Train Loss: 2.2270, Test Loss: 2.3061, Learning Rate: 2.30e-05\n",
      "Step 1464, Train Loss: 2.3536, Test Loss: 2.3117, Learning Rate: 2.30e-05\n",
      "Step 1465, Train Loss: 2.3259, Test Loss: 2.5775, Learning Rate: 2.30e-05\n",
      "Step 1466, Train Loss: 2.3445, Test Loss: 2.3514, Learning Rate: 2.30e-05\n",
      "Step 1467, Train Loss: 2.4113, Test Loss: 2.3772, Learning Rate: 2.30e-05\n",
      "Step 1468, Train Loss: 2.2779, Test Loss: 2.1954, Learning Rate: 2.30e-05\n",
      "Step 1469, Train Loss: 2.2217, Test Loss: 2.2688, Learning Rate: 2.29e-05\n",
      "Step 1470, Train Loss: 2.2972, Test Loss: 2.2522, Learning Rate: 2.29e-05\n",
      "Step 1471, Train Loss: 2.1365, Test Loss: 2.4419, Learning Rate: 2.29e-05\n",
      "Step 1472, Train Loss: 2.2477, Test Loss: 2.3242, Learning Rate: 2.29e-05\n",
      "Step 1473, Train Loss: 2.2709, Test Loss: 2.2540, Learning Rate: 2.29e-05\n",
      "Step 1474, Train Loss: 2.2485, Test Loss: 2.1674, Learning Rate: 2.29e-05\n",
      "Step 1475, Train Loss: 2.2270, Test Loss: 2.3138, Learning Rate: 2.29e-05\n",
      "Step 1476, Train Loss: 2.1942, Test Loss: 2.1800, Learning Rate: 2.29e-05\n",
      "Step 1477, Train Loss: 2.4034, Test Loss: 2.3390, Learning Rate: 2.29e-05\n",
      "Step 1478, Train Loss: 2.4377, Test Loss: 2.2517, Learning Rate: 2.29e-05\n",
      "Step 1479, Train Loss: 2.4426, Test Loss: 2.4637, Learning Rate: 2.29e-05\n",
      "Step 1480, Train Loss: 2.2547, Test Loss: 2.4767, Learning Rate: 2.29e-05\n",
      "Step 1481, Train Loss: 2.3319, Test Loss: 2.3788, Learning Rate: 2.29e-05\n",
      "Step 1482, Train Loss: 2.2080, Test Loss: 2.1831, Learning Rate: 2.29e-05\n",
      "Step 1483, Train Loss: 2.3261, Test Loss: 2.4473, Learning Rate: 2.29e-05\n",
      "Step 1484, Train Loss: 2.3210, Test Loss: 2.3984, Learning Rate: 2.29e-05\n",
      "Step 1485, Train Loss: 2.3190, Test Loss: 2.3991, Learning Rate: 2.29e-05\n",
      "Step 1486, Train Loss: 2.3159, Test Loss: 2.4715, Learning Rate: 2.29e-05\n",
      "Step 1487, Train Loss: 2.4070, Test Loss: 2.2417, Learning Rate: 2.29e-05\n",
      "Step 1488, Train Loss: 2.3021, Test Loss: 2.5129, Learning Rate: 2.29e-05\n",
      "Step 1489, Train Loss: 2.3336, Test Loss: 2.4797, Learning Rate: 2.29e-05\n",
      "Step 1490, Train Loss: 2.3354, Test Loss: 2.5378, Learning Rate: 2.29e-05\n",
      "Step 1491, Train Loss: 2.2306, Test Loss: 2.2977, Learning Rate: 2.29e-05\n",
      "Step 1492, Train Loss: 2.2820, Test Loss: 2.2929, Learning Rate: 2.29e-05\n",
      "Step 1493, Train Loss: 2.3971, Test Loss: 2.4991, Learning Rate: 2.29e-05\n",
      "Step 1494, Train Loss: 2.3359, Test Loss: 2.3643, Learning Rate: 2.29e-05\n",
      "Step 1495, Train Loss: 2.3213, Test Loss: 2.2828, Learning Rate: 2.29e-05\n",
      "Step 1496, Train Loss: 2.3532, Test Loss: 2.3260, Learning Rate: 2.29e-05\n",
      "Step 1497, Train Loss: 2.3810, Test Loss: 2.3103, Learning Rate: 2.29e-05\n",
      "Step 1498, Train Loss: 2.3542, Test Loss: 2.4778, Learning Rate: 2.29e-05\n",
      "Step 1499, Train Loss: 2.2676, Test Loss: 2.4810, Learning Rate: 2.29e-05\n",
      "Step 1500, Train Loss: 2.4380, Test Loss: 2.2907, Learning Rate: 2.29e-05\n",
      "Step 1501, Train Loss: 2.1697, Test Loss: 2.2761, Learning Rate: 2.29e-05\n",
      "Step 1502, Train Loss: 2.3758, Test Loss: 2.2010, Learning Rate: 2.29e-05\n",
      "Step 1503, Train Loss: 2.4465, Test Loss: 2.2796, Learning Rate: 2.29e-05\n",
      "Step 1504, Train Loss: 2.2644, Test Loss: 2.3880, Learning Rate: 2.29e-05\n",
      "Step 1505, Train Loss: 2.4561, Test Loss: 2.3597, Learning Rate: 2.29e-05\n",
      "Step 1506, Train Loss: 2.3033, Test Loss: 2.3668, Learning Rate: 2.29e-05\n",
      "Step 1507, Train Loss: 2.4036, Test Loss: 2.2677, Learning Rate: 2.29e-05\n",
      "Step 1508, Train Loss: 2.3978, Test Loss: 2.3822, Learning Rate: 2.29e-05\n",
      "Step 1509, Train Loss: 2.4113, Test Loss: 2.3715, Learning Rate: 2.29e-05\n",
      "Step 1510, Train Loss: 2.3489, Test Loss: 2.1720, Learning Rate: 2.29e-05\n",
      "Step 1511, Train Loss: 2.2624, Test Loss: 2.1754, Learning Rate: 2.29e-05\n",
      "Step 1512, Train Loss: 2.4160, Test Loss: 2.4137, Learning Rate: 2.29e-05\n",
      "Step 1513, Train Loss: 2.3634, Test Loss: 2.3585, Learning Rate: 2.29e-05\n",
      "Step 1514, Train Loss: 2.3831, Test Loss: 2.3300, Learning Rate: 2.29e-05\n",
      "Step 1515, Train Loss: 2.3436, Test Loss: 2.1470, Learning Rate: 2.29e-05\n",
      "Step 1516, Train Loss: 2.4201, Test Loss: 2.2851, Learning Rate: 2.29e-05\n",
      "Step 1517, Train Loss: 2.3773, Test Loss: 2.5299, Learning Rate: 2.29e-05\n",
      "Step 1518, Train Loss: 2.2141, Test Loss: 2.5284, Learning Rate: 2.29e-05\n",
      "Step 1519, Train Loss: 2.5253, Test Loss: 2.3903, Learning Rate: 2.29e-05\n",
      "Step 1520, Train Loss: 2.2113, Test Loss: 2.4054, Learning Rate: 2.29e-05\n",
      "Step 1521, Train Loss: 2.2804, Test Loss: 2.2750, Learning Rate: 2.29e-05\n",
      "Step 1522, Train Loss: 2.3724, Test Loss: 2.3749, Learning Rate: 2.29e-05\n",
      "Step 1523, Train Loss: 2.4035, Test Loss: 2.3723, Learning Rate: 2.29e-05\n",
      "Step 1524, Train Loss: 2.1969, Test Loss: 2.4032, Learning Rate: 2.29e-05\n",
      "Step 1525, Train Loss: 2.3753, Test Loss: 2.4592, Learning Rate: 2.29e-05\n",
      "Step 1526, Train Loss: 2.3407, Test Loss: 2.1093, Learning Rate: 2.29e-05\n",
      "Step 1527, Train Loss: 2.2904, Test Loss: 2.4023, Learning Rate: 2.29e-05\n",
      "Step 1528, Train Loss: 2.4379, Test Loss: 2.3036, Learning Rate: 2.29e-05\n",
      "Step 1529, Train Loss: 2.2887, Test Loss: 2.3991, Learning Rate: 2.29e-05\n",
      "Step 1530, Train Loss: 2.2311, Test Loss: 1.9677, Learning Rate: 2.29e-05\n",
      "Step 1531, Train Loss: 2.3919, Test Loss: 2.2589, Learning Rate: 2.29e-05\n",
      "Step 1532, Train Loss: 2.3143, Test Loss: 2.2550, Learning Rate: 2.29e-05\n",
      "Step 1533, Train Loss: 2.4216, Test Loss: 2.4425, Learning Rate: 2.29e-05\n",
      "Step 1534, Train Loss: 2.3729, Test Loss: 2.1862, Learning Rate: 2.29e-05\n",
      "Step 1535, Train Loss: 2.3833, Test Loss: 2.3963, Learning Rate: 2.29e-05\n",
      "Step 1536, Train Loss: 2.2323, Test Loss: 2.2733, Learning Rate: 2.29e-05\n",
      "Step 1537, Train Loss: 2.4108, Test Loss: 2.3364, Learning Rate: 2.29e-05\n",
      "Step 1538, Train Loss: 2.3539, Test Loss: 2.2865, Learning Rate: 2.29e-05\n",
      "Step 1539, Train Loss: 2.4079, Test Loss: 2.4149, Learning Rate: 2.29e-05\n",
      "Step 1540, Train Loss: 2.3001, Test Loss: 2.4585, Learning Rate: 2.29e-05\n",
      "Step 1541, Train Loss: 2.3106, Test Loss: 2.3110, Learning Rate: 2.29e-05\n",
      "Step 1542, Train Loss: 2.2945, Test Loss: 2.4268, Learning Rate: 2.29e-05\n",
      "Step 1543, Train Loss: 2.2571, Test Loss: 2.3362, Learning Rate: 2.29e-05\n",
      "Step 1544, Train Loss: 2.3136, Test Loss: 2.2499, Learning Rate: 2.29e-05\n",
      "Step 1545, Train Loss: 2.3481, Test Loss: 2.2601, Learning Rate: 2.29e-05\n",
      "Step 1546, Train Loss: 2.2814, Test Loss: 2.2623, Learning Rate: 2.29e-05\n",
      "Step 1547, Train Loss: 2.2015, Test Loss: 2.3321, Learning Rate: 2.29e-05\n",
      "Step 1548, Train Loss: 2.3687, Test Loss: 2.2495, Learning Rate: 2.29e-05\n",
      "Step 1549, Train Loss: 2.4905, Test Loss: 2.3549, Learning Rate: 2.29e-05\n",
      "Step 1550, Train Loss: 2.4493, Test Loss: 2.3820, Learning Rate: 2.29e-05\n",
      "Step 1551, Train Loss: 2.3058, Test Loss: 2.3423, Learning Rate: 2.29e-05\n",
      "Step 1552, Train Loss: 2.3051, Test Loss: 2.3185, Learning Rate: 2.29e-05\n",
      "Step 1553, Train Loss: 2.3471, Test Loss: 2.1873, Learning Rate: 2.29e-05\n",
      "Step 1554, Train Loss: 2.3003, Test Loss: 2.2810, Learning Rate: 2.29e-05\n",
      "Step 1555, Train Loss: 2.2406, Test Loss: 2.3733, Learning Rate: 2.29e-05\n",
      "Step 1556, Train Loss: 2.3405, Test Loss: 2.3717, Learning Rate: 2.29e-05\n",
      "Step 1557, Train Loss: 2.4133, Test Loss: 2.2590, Learning Rate: 2.29e-05\n",
      "Step 1558, Train Loss: 2.2592, Test Loss: 2.2923, Learning Rate: 2.29e-05\n",
      "Step 1559, Train Loss: 2.2897, Test Loss: 2.2785, Learning Rate: 2.29e-05\n",
      "Step 1560, Train Loss: 2.2827, Test Loss: 2.0901, Learning Rate: 2.29e-05\n",
      "Step 1561, Train Loss: 2.1332, Test Loss: 2.1558, Learning Rate: 2.29e-05\n",
      "Step 1562, Train Loss: 2.3293, Test Loss: 2.2743, Learning Rate: 2.29e-05\n",
      "Step 1563, Train Loss: 2.4141, Test Loss: 2.2972, Learning Rate: 2.29e-05\n",
      "Step 1564, Train Loss: 2.1938, Test Loss: 2.6333, Learning Rate: 2.29e-05\n",
      "Step 1565, Train Loss: 2.3281, Test Loss: 2.5239, Learning Rate: 2.29e-05\n",
      "Step 1566, Train Loss: 2.3883, Test Loss: 2.5247, Learning Rate: 2.29e-05\n",
      "Step 1567, Train Loss: 2.4807, Test Loss: 2.2700, Learning Rate: 2.29e-05\n",
      "Step 1568, Train Loss: 2.2269, Test Loss: 2.3225, Learning Rate: 2.29e-05\n",
      "Step 1569, Train Loss: 2.3282, Test Loss: 2.4699, Learning Rate: 2.29e-05\n",
      "Step 1570, Train Loss: 2.5125, Test Loss: 2.3599, Learning Rate: 2.29e-05\n",
      "Step 1571, Train Loss: 2.2747, Test Loss: 2.4247, Learning Rate: 2.29e-05\n",
      "Step 1572, Train Loss: 2.2869, Test Loss: 2.2194, Learning Rate: 2.29e-05\n",
      "Step 1573, Train Loss: 2.2809, Test Loss: 2.2078, Learning Rate: 2.29e-05\n",
      "Step 1574, Train Loss: 2.4105, Test Loss: 2.1829, Learning Rate: 2.29e-05\n",
      "Step 1575, Train Loss: 2.2612, Test Loss: 2.3654, Learning Rate: 2.29e-05\n",
      "Step 1576, Train Loss: 2.3315, Test Loss: 2.4524, Learning Rate: 2.29e-05\n",
      "Step 1577, Train Loss: 2.3753, Test Loss: 2.3569, Learning Rate: 2.29e-05\n",
      "Step 1578, Train Loss: 2.3875, Test Loss: 2.4524, Learning Rate: 2.29e-05\n",
      "Step 1579, Train Loss: 2.3339, Test Loss: 2.3464, Learning Rate: 2.29e-05\n",
      "Step 1580, Train Loss: 2.3330, Test Loss: 2.4170, Learning Rate: 2.29e-05\n",
      "Step 1581, Train Loss: 2.3882, Test Loss: 2.4166, Learning Rate: 2.29e-05\n",
      "Step 1582, Train Loss: 2.3138, Test Loss: 2.2080, Learning Rate: 2.29e-05\n",
      "Step 1583, Train Loss: 2.2850, Test Loss: 2.3348, Learning Rate: 2.29e-05\n",
      "Step 1584, Train Loss: 2.6005, Test Loss: 2.4453, Learning Rate: 2.29e-05\n",
      "Step 1585, Train Loss: 2.1503, Test Loss: 2.4715, Learning Rate: 2.29e-05\n",
      "Step 1586, Train Loss: 2.3032, Test Loss: 2.1264, Learning Rate: 2.29e-05\n",
      "Step 1587, Train Loss: 2.3598, Test Loss: 2.3986, Learning Rate: 2.29e-05\n",
      "Step 1588, Train Loss: 2.2027, Test Loss: 2.3959, Learning Rate: 2.29e-05\n",
      "Step 1589, Train Loss: 2.2289, Test Loss: 2.2098, Learning Rate: 2.29e-05\n",
      "Step 1590, Train Loss: 2.2751, Test Loss: 2.2922, Learning Rate: 2.29e-05\n",
      "Step 1591, Train Loss: 2.4384, Test Loss: 2.4513, Learning Rate: 2.29e-05\n",
      "Step 1592, Train Loss: 2.3809, Test Loss: 2.5727, Learning Rate: 2.29e-05\n",
      "Step 1593, Train Loss: 2.3643, Test Loss: 2.3634, Learning Rate: 2.29e-05\n",
      "Step 1594, Train Loss: 2.3778, Test Loss: 2.3037, Learning Rate: 2.29e-05\n",
      "Step 1595, Train Loss: 2.4332, Test Loss: 2.3833, Learning Rate: 2.28e-05\n",
      "Step 1596, Train Loss: 2.2724, Test Loss: 2.2447, Learning Rate: 2.28e-05\n",
      "Step 1597, Train Loss: 2.3387, Test Loss: 2.2709, Learning Rate: 2.28e-05\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x000001F90E31F400>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2761, in while_loop\n",
      "    while cond(*loop_vars):  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2753, in <lambda>\n",
      "    body = lambda i, lv: (i + 1, orig_body(*lv))  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\", line 655, in compute\n",
      "    return (next_i, flat_a_out, tas)  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\", line 650, in <listcomp>\n",
      "    tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_a_out)]  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 243, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n",
      "Step 1598, Train Loss: 2.3156, Test Loss: 2.4978, Learning Rate: 2.28e-05\n",
      "Step 1599, Train Loss: 2.3770, Test Loss: 2.2304, Learning Rate: 2.28e-05\n",
      "Step 1600, Train Loss: 2.3014, Test Loss: 2.2820, Learning Rate: 2.28e-05\n",
      "Step 1601, Train Loss: 2.4314, Test Loss: 2.3045, Learning Rate: 2.28e-05\n",
      "Step 1602, Train Loss: 2.3462, Test Loss: 2.2521, Learning Rate: 2.28e-05\n",
      "Step 1603, Train Loss: 2.2813, Test Loss: 2.2549, Learning Rate: 2.28e-05\n",
      "Step 1604, Train Loss: 2.3867, Test Loss: 2.3128, Learning Rate: 2.28e-05\n",
      "Step 1605, Train Loss: 2.3817, Test Loss: 2.2960, Learning Rate: 2.28e-05\n",
      "Step 1606, Train Loss: 2.3905, Test Loss: 2.5237, Learning Rate: 2.28e-05\n",
      "Step 1607, Train Loss: 2.3509, Test Loss: 2.3711, Learning Rate: 2.28e-05\n",
      "Step 1608, Train Loss: 2.3881, Test Loss: 2.3213, Learning Rate: 2.28e-05\n",
      "Step 1609, Train Loss: 2.5806, Test Loss: 2.4345, Learning Rate: 2.28e-05\n",
      "Step 1610, Train Loss: 2.3134, Test Loss: 2.3473, Learning Rate: 2.28e-05\n",
      "Step 1611, Train Loss: 2.3860, Test Loss: 2.3979, Learning Rate: 2.28e-05\n",
      "Step 1612, Train Loss: 2.3995, Test Loss: 2.5286, Learning Rate: 2.28e-05\n",
      "Step 1613, Train Loss: 2.2657, Test Loss: 2.2756, Learning Rate: 2.28e-05\n",
      "Step 1614, Train Loss: 2.3584, Test Loss: 2.0820, Learning Rate: 2.28e-05\n",
      "Step 1615, Train Loss: 2.2938, Test Loss: 2.4176, Learning Rate: 2.28e-05\n",
      "Step 1616, Train Loss: 2.2999, Test Loss: 2.1676, Learning Rate: 2.28e-05\n",
      "Step 1617, Train Loss: 2.3679, Test Loss: 2.2054, Learning Rate: 2.28e-05\n",
      "Step 1618, Train Loss: 2.3358, Test Loss: 2.2421, Learning Rate: 2.28e-05\n",
      "Step 1619, Train Loss: 2.3314, Test Loss: 2.2463, Learning Rate: 2.28e-05\n",
      "Step 1620, Train Loss: 2.3923, Test Loss: 2.2389, Learning Rate: 2.28e-05\n",
      "Step 1621, Train Loss: 2.4934, Test Loss: 2.4509, Learning Rate: 2.28e-05\n",
      "Step 1622, Train Loss: 2.2669, Test Loss: 2.1899, Learning Rate: 2.28e-05\n",
      "Step 1623, Train Loss: 2.2070, Test Loss: 2.0642, Learning Rate: 2.28e-05\n",
      "Step 1624, Train Loss: 2.4328, Test Loss: 2.2716, Learning Rate: 2.28e-05\n",
      "Step 1625, Train Loss: 2.1567, Test Loss: 2.3971, Learning Rate: 2.28e-05\n",
      "Step 1626, Train Loss: 2.2725, Test Loss: 2.2816, Learning Rate: 2.28e-05\n",
      "Step 1627, Train Loss: 2.2825, Test Loss: 2.1895, Learning Rate: 2.28e-05\n",
      "Step 1628, Train Loss: 2.2301, Test Loss: 2.4060, Learning Rate: 2.28e-05\n",
      "Step 1629, Train Loss: 2.3027, Test Loss: 2.2073, Learning Rate: 2.28e-05\n",
      "Step 1630, Train Loss: 2.3838, Test Loss: 2.4075, Learning Rate: 2.28e-05\n",
      "Step 1631, Train Loss: 2.5897, Test Loss: 2.0816, Learning Rate: 2.28e-05\n",
      "Step 1632, Train Loss: 2.3303, Test Loss: 2.4051, Learning Rate: 2.28e-05\n",
      "Step 1633, Train Loss: 2.3349, Test Loss: 2.5911, Learning Rate: 2.28e-05\n",
      "Step 1634, Train Loss: 2.4208, Test Loss: 2.2464, Learning Rate: 2.28e-05\n",
      "Step 1635, Train Loss: 2.2613, Test Loss: 2.3855, Learning Rate: 2.28e-05\n",
      "Step 1636, Train Loss: 2.4475, Test Loss: 2.5609, Learning Rate: 2.28e-05\n",
      "Step 1637, Train Loss: 2.2825, Test Loss: 2.4532, Learning Rate: 2.28e-05\n",
      "Step 1638, Train Loss: 2.3042, Test Loss: 2.4207, Learning Rate: 2.28e-05\n",
      "Step 1639, Train Loss: 2.2681, Test Loss: 2.3287, Learning Rate: 2.28e-05\n",
      "Step 1640, Train Loss: 2.3003, Test Loss: 2.3153, Learning Rate: 2.28e-05\n",
      "Step 1641, Train Loss: 2.3954, Test Loss: 2.2810, Learning Rate: 2.28e-05\n",
      "Step 1642, Train Loss: 2.3481, Test Loss: 2.3198, Learning Rate: 2.28e-05\n",
      "Step 1643, Train Loss: 2.4445, Test Loss: 2.2666, Learning Rate: 2.28e-05\n",
      "Step 1644, Train Loss: 2.3364, Test Loss: 2.1263, Learning Rate: 2.28e-05\n",
      "Step 1645, Train Loss: 2.1577, Test Loss: 2.0916, Learning Rate: 2.28e-05\n",
      "Step 1646, Train Loss: 2.4440, Test Loss: 2.3827, Learning Rate: 2.28e-05\n",
      "Step 1647, Train Loss: 2.2688, Test Loss: 2.3488, Learning Rate: 2.28e-05\n",
      "Step 1648, Train Loss: 2.4069, Test Loss: 2.2579, Learning Rate: 2.28e-05\n",
      "Step 1649, Train Loss: 2.3972, Test Loss: 2.1658, Learning Rate: 2.28e-05\n",
      "Step 1650, Train Loss: 2.3417, Test Loss: 2.2771, Learning Rate: 2.28e-05\n",
      "Step 1651, Train Loss: 2.3260, Test Loss: 2.3186, Learning Rate: 2.28e-05\n",
      "Step 1652, Train Loss: 2.3572, Test Loss: 2.3862, Learning Rate: 2.28e-05\n",
      "Step 1653, Train Loss: 2.2456, Test Loss: 2.1731, Learning Rate: 2.28e-05\n",
      "Step 1654, Train Loss: 2.3396, Test Loss: 2.2647, Learning Rate: 2.28e-05\n",
      "Step 1655, Train Loss: 2.5271, Test Loss: 2.2823, Learning Rate: 2.28e-05\n",
      "Step 1656, Train Loss: 2.3482, Test Loss: 1.9877, Learning Rate: 2.28e-05\n",
      "Step 1657, Train Loss: 2.3014, Test Loss: 2.3740, Learning Rate: 2.28e-05\n",
      "Step 1658, Train Loss: 2.1977, Test Loss: 2.2943, Learning Rate: 2.28e-05\n",
      "Step 1659, Train Loss: 2.3463, Test Loss: 2.3744, Learning Rate: 2.28e-05\n",
      "Step 1660, Train Loss: 2.2353, Test Loss: 2.4989, Learning Rate: 2.28e-05\n",
      "Step 1661, Train Loss: 2.2904, Test Loss: 2.2159, Learning Rate: 2.28e-05\n",
      "Step 1662, Train Loss: 2.3585, Test Loss: 2.2500, Learning Rate: 2.28e-05\n",
      "Step 1663, Train Loss: 2.3803, Test Loss: 2.1618, Learning Rate: 2.28e-05\n",
      "Step 1664, Train Loss: 2.1622, Test Loss: 2.3356, Learning Rate: 2.28e-05\n",
      "Step 1665, Train Loss: 2.3453, Test Loss: 2.2565, Learning Rate: 2.28e-05\n",
      "Step 1666, Train Loss: 2.3804, Test Loss: 2.2993, Learning Rate: 2.28e-05\n",
      "Step 1667, Train Loss: 2.4273, Test Loss: 2.3960, Learning Rate: 2.28e-05\n",
      "Step 1668, Train Loss: 2.2991, Test Loss: 2.2524, Learning Rate: 2.28e-05\n",
      "Step 1669, Train Loss: 2.4755, Test Loss: 2.2913, Learning Rate: 2.28e-05\n",
      "Step 1670, Train Loss: 2.3350, Test Loss: 2.2866, Learning Rate: 2.28e-05\n",
      "Step 1671, Train Loss: 2.2227, Test Loss: 2.3458, Learning Rate: 2.28e-05\n",
      "Step 1672, Train Loss: 2.3583, Test Loss: 2.4458, Learning Rate: 2.28e-05\n",
      "Step 1673, Train Loss: 2.1741, Test Loss: 2.2060, Learning Rate: 2.28e-05\n",
      "Step 1674, Train Loss: 2.4078, Test Loss: 2.4234, Learning Rate: 2.28e-05\n",
      "Step 1675, Train Loss: 2.3672, Test Loss: 2.3972, Learning Rate: 2.28e-05\n",
      "Step 1676, Train Loss: 2.3026, Test Loss: 2.3610, Learning Rate: 2.28e-05\n",
      "Step 1677, Train Loss: 2.3911, Test Loss: 2.3743, Learning Rate: 2.28e-05\n",
      "Step 1678, Train Loss: 2.3097, Test Loss: 2.2347, Learning Rate: 2.28e-05\n",
      "Step 1679, Train Loss: 2.2618, Test Loss: 2.2167, Learning Rate: 2.28e-05\n",
      "Step 1680, Train Loss: 2.4750, Test Loss: 2.3083, Learning Rate: 2.28e-05\n",
      "Step 1681, Train Loss: 2.4211, Test Loss: 2.1587, Learning Rate: 2.28e-05\n",
      "Step 1682, Train Loss: 2.3518, Test Loss: 2.2828, Learning Rate: 2.28e-05\n",
      "Step 1683, Train Loss: 2.3795, Test Loss: 2.3891, Learning Rate: 2.28e-05\n",
      "Step 1684, Train Loss: 2.3887, Test Loss: 2.1386, Learning Rate: 2.28e-05\n",
      "Step 1685, Train Loss: 2.2857, Test Loss: 2.3790, Learning Rate: 2.28e-05\n",
      "Step 1686, Train Loss: 2.2946, Test Loss: 2.2737, Learning Rate: 2.28e-05\n",
      "Step 1687, Train Loss: 2.3641, Test Loss: 2.2135, Learning Rate: 2.28e-05\n",
      "Step 1688, Train Loss: 2.2569, Test Loss: 2.2244, Learning Rate: 2.28e-05\n",
      "Step 1689, Train Loss: 2.4491, Test Loss: 2.2942, Learning Rate: 2.28e-05\n",
      "Step 1690, Train Loss: 2.2855, Test Loss: 2.3387, Learning Rate: 2.28e-05\n",
      "Step 1691, Train Loss: 2.4030, Test Loss: 2.2428, Learning Rate: 2.28e-05\n",
      "Step 1692, Train Loss: 2.3350, Test Loss: 2.2530, Learning Rate: 2.28e-05\n",
      "Step 1693, Train Loss: 2.3048, Test Loss: 2.2629, Learning Rate: 2.28e-05\n",
      "Step 1694, Train Loss: 2.3101, Test Loss: 2.3987, Learning Rate: 2.28e-05\n",
      "Step 1695, Train Loss: 2.2630, Test Loss: 2.4386, Learning Rate: 2.28e-05\n",
      "Step 1696, Train Loss: 2.1637, Test Loss: 2.3641, Learning Rate: 2.28e-05\n",
      "Step 1697, Train Loss: 2.2388, Test Loss: 2.2596, Learning Rate: 2.28e-05\n",
      "Step 1698, Train Loss: 2.3047, Test Loss: 2.2430, Learning Rate: 2.28e-05\n",
      "Step 1699, Train Loss: 2.2483, Test Loss: 2.1811, Learning Rate: 2.28e-05\n",
      "Step 1700, Train Loss: 2.3344, Test Loss: 2.3587, Learning Rate: 2.28e-05\n",
      "Step 1701, Train Loss: 2.3198, Test Loss: 2.4550, Learning Rate: 2.28e-05\n",
      "Step 1702, Train Loss: 2.4055, Test Loss: 2.2770, Learning Rate: 2.28e-05\n",
      "Step 1703, Train Loss: 2.3789, Test Loss: 2.0914, Learning Rate: 2.28e-05\n",
      "Step 1704, Train Loss: 2.3525, Test Loss: 2.3882, Learning Rate: 2.28e-05\n",
      "Step 1705, Train Loss: 2.3298, Test Loss: 2.2940, Learning Rate: 2.28e-05\n",
      "Step 1706, Train Loss: 2.4375, Test Loss: 2.2733, Learning Rate: 2.28e-05\n",
      "Step 1707, Train Loss: 2.3565, Test Loss: 2.2841, Learning Rate: 2.28e-05\n",
      "Step 1708, Train Loss: 2.3417, Test Loss: 2.4238, Learning Rate: 2.28e-05\n",
      "Step 1709, Train Loss: 2.4008, Test Loss: 2.3426, Learning Rate: 2.28e-05\n",
      "Step 1710, Train Loss: 2.3266, Test Loss: 2.3623, Learning Rate: 2.28e-05\n",
      "Step 1711, Train Loss: 2.3687, Test Loss: 2.2145, Learning Rate: 2.28e-05\n",
      "Step 1712, Train Loss: 2.3376, Test Loss: 2.4716, Learning Rate: 2.28e-05\n",
      "Step 1713, Train Loss: 2.2940, Test Loss: 2.3927, Learning Rate: 2.28e-05\n",
      "Step 1714, Train Loss: 2.3809, Test Loss: 2.0898, Learning Rate: 2.28e-05\n",
      "Step 1715, Train Loss: 2.4646, Test Loss: 2.1314, Learning Rate: 2.28e-05\n",
      "Step 1716, Train Loss: 2.2741, Test Loss: 2.1862, Learning Rate: 2.28e-05\n",
      "Step 1717, Train Loss: 2.3275, Test Loss: 2.3638, Learning Rate: 2.28e-05\n",
      "Step 1718, Train Loss: 2.3374, Test Loss: 2.2119, Learning Rate: 2.28e-05\n",
      "Step 1719, Train Loss: 2.3440, Test Loss: 2.5338, Learning Rate: 2.28e-05\n",
      "Step 1720, Train Loss: 2.3704, Test Loss: 2.3673, Learning Rate: 2.28e-05\n",
      "Step 1721, Train Loss: 2.3045, Test Loss: 2.4672, Learning Rate: 2.28e-05\n",
      "Step 1722, Train Loss: 2.1581, Test Loss: 2.2207, Learning Rate: 2.27e-05\n",
      "Step 1723, Train Loss: 2.3586, Test Loss: 2.3076, Learning Rate: 2.27e-05\n",
      "Step 1724, Train Loss: 2.3989, Test Loss: 2.2520, Learning Rate: 2.27e-05\n",
      "Step 1725, Train Loss: 2.3667, Test Loss: 2.3530, Learning Rate: 2.27e-05\n",
      "Step 1726, Train Loss: 2.2916, Test Loss: 2.3567, Learning Rate: 2.27e-05\n",
      "Step 1727, Train Loss: 2.3185, Test Loss: 2.2966, Learning Rate: 2.27e-05\n",
      "Step 1728, Train Loss: 2.2943, Test Loss: 2.3812, Learning Rate: 2.27e-05\n",
      "Step 1729, Train Loss: 2.3645, Test Loss: 2.1265, Learning Rate: 2.27e-05\n",
      "Step 1730, Train Loss: 2.3112, Test Loss: 2.3842, Learning Rate: 2.27e-05\n",
      "Step 1731, Train Loss: 2.2846, Test Loss: 2.2142, Learning Rate: 2.27e-05\n",
      "Step 1732, Train Loss: 2.3876, Test Loss: 2.3170, Learning Rate: 2.27e-05\n",
      "Step 1733, Train Loss: 2.2128, Test Loss: 2.5507, Learning Rate: 2.27e-05\n",
      "Step 1734, Train Loss: 2.2917, Test Loss: 2.3661, Learning Rate: 2.27e-05\n",
      "Step 1735, Train Loss: 2.2376, Test Loss: 2.2786, Learning Rate: 2.27e-05\n",
      "Step 1736, Train Loss: 2.4835, Test Loss: 2.3712, Learning Rate: 2.27e-05\n",
      "Step 1737, Train Loss: 2.3618, Test Loss: 2.4076, Learning Rate: 2.27e-05\n",
      "Step 1738, Train Loss: 2.4207, Test Loss: 2.3511, Learning Rate: 2.27e-05\n",
      "Step 1739, Train Loss: 2.2850, Test Loss: 2.3436, Learning Rate: 2.27e-05\n",
      "Step 1740, Train Loss: 2.3840, Test Loss: 2.2095, Learning Rate: 2.27e-05\n",
      "Step 1741, Train Loss: 2.3256, Test Loss: 2.1792, Learning Rate: 2.27e-05\n",
      "Step 1742, Train Loss: 2.4455, Test Loss: 2.3776, Learning Rate: 2.27e-05\n",
      "Step 1743, Train Loss: 2.2611, Test Loss: 2.1781, Learning Rate: 2.27e-05\n",
      "Step 1744, Train Loss: 2.2710, Test Loss: 2.3949, Learning Rate: 2.27e-05\n",
      "Step 1745, Train Loss: 2.3090, Test Loss: 2.6088, Learning Rate: 2.27e-05\n",
      "Step 1746, Train Loss: 2.3962, Test Loss: 2.2500, Learning Rate: 2.27e-05\n",
      "Step 1747, Train Loss: 2.4021, Test Loss: 2.3560, Learning Rate: 2.27e-05\n",
      "Step 1748, Train Loss: 2.2357, Test Loss: 2.3262, Learning Rate: 2.27e-05\n",
      "Step 1749, Train Loss: 2.3085, Test Loss: 2.3878, Learning Rate: 2.27e-05\n",
      "Step 1750, Train Loss: 2.3163, Test Loss: 2.5131, Learning Rate: 2.27e-05\n",
      "Step 1751, Train Loss: 2.1817, Test Loss: 2.1183, Learning Rate: 2.27e-05\n",
      "Step 1752, Train Loss: 2.2936, Test Loss: 2.2502, Learning Rate: 2.27e-05\n",
      "Step 1753, Train Loss: 2.4244, Test Loss: 2.3164, Learning Rate: 2.27e-05\n",
      "Step 1754, Train Loss: 2.3991, Test Loss: 2.3022, Learning Rate: 2.27e-05\n",
      "Step 1755, Train Loss: 2.3765, Test Loss: 2.2409, Learning Rate: 2.27e-05\n",
      "Step 1756, Train Loss: 2.4194, Test Loss: 2.2581, Learning Rate: 2.27e-05\n",
      "Step 1757, Train Loss: 2.3625, Test Loss: 2.2737, Learning Rate: 2.27e-05\n",
      "Step 1758, Train Loss: 2.4779, Test Loss: 2.3619, Learning Rate: 2.27e-05\n",
      "Step 1759, Train Loss: 2.2604, Test Loss: 2.3207, Learning Rate: 2.27e-05\n",
      "Step 1760, Train Loss: 2.3220, Test Loss: 2.2340, Learning Rate: 2.27e-05\n",
      "Step 1761, Train Loss: 2.3504, Test Loss: 2.3531, Learning Rate: 2.27e-05\n",
      "Step 1762, Train Loss: 2.3339, Test Loss: 2.2739, Learning Rate: 2.27e-05\n",
      "Step 1763, Train Loss: 2.2891, Test Loss: 2.3668, Learning Rate: 2.27e-05\n",
      "Step 1764, Train Loss: 2.3784, Test Loss: 2.2868, Learning Rate: 2.27e-05\n",
      "Step 1765, Train Loss: 2.3743, Test Loss: 2.2080, Learning Rate: 2.27e-05\n",
      "Step 1766, Train Loss: 2.4009, Test Loss: 2.4272, Learning Rate: 2.27e-05\n",
      "Step 1767, Train Loss: 2.2884, Test Loss: 2.2451, Learning Rate: 2.27e-05\n",
      "Step 1768, Train Loss: 2.2167, Test Loss: 2.2776, Learning Rate: 2.27e-05\n",
      "Step 1769, Train Loss: 2.3329, Test Loss: 2.2934, Learning Rate: 2.27e-05\n",
      "Step 1770, Train Loss: 2.3726, Test Loss: 2.3971, Learning Rate: 2.27e-05\n",
      "Step 1771, Train Loss: 2.3685, Test Loss: 2.2220, Learning Rate: 2.27e-05\n",
      "Step 1772, Train Loss: 2.2730, Test Loss: 2.2511, Learning Rate: 2.27e-05\n",
      "Step 1773, Train Loss: 2.3194, Test Loss: 2.2607, Learning Rate: 2.27e-05\n",
      "Step 1774, Train Loss: 2.2760, Test Loss: 2.3422, Learning Rate: 2.27e-05\n",
      "Step 1775, Train Loss: 2.2470, Test Loss: 2.2850, Learning Rate: 2.27e-05\n",
      "Step 1776, Train Loss: 2.2566, Test Loss: 2.2637, Learning Rate: 2.27e-05\n",
      "Step 1777, Train Loss: 2.3051, Test Loss: 2.2794, Learning Rate: 2.27e-05\n",
      "Step 1778, Train Loss: 2.3146, Test Loss: 2.2688, Learning Rate: 2.27e-05\n",
      "Step 1779, Train Loss: 2.3656, Test Loss: 2.3333, Learning Rate: 2.27e-05\n",
      "Step 1780, Train Loss: 2.3424, Test Loss: 2.3337, Learning Rate: 2.27e-05\n",
      "Step 1781, Train Loss: 2.3764, Test Loss: 2.4122, Learning Rate: 2.27e-05\n",
      "Step 1782, Train Loss: 2.3208, Test Loss: 2.4068, Learning Rate: 2.27e-05\n",
      "Step 1783, Train Loss: 2.2313, Test Loss: 2.4301, Learning Rate: 2.27e-05\n",
      "Step 1784, Train Loss: 2.3160, Test Loss: 2.3458, Learning Rate: 2.27e-05\n",
      "Step 1785, Train Loss: 2.3290, Test Loss: 2.3245, Learning Rate: 2.27e-05\n",
      "Step 1786, Train Loss: 2.2682, Test Loss: 2.1579, Learning Rate: 2.27e-05\n",
      "Step 1787, Train Loss: 2.3428, Test Loss: 2.3404, Learning Rate: 2.27e-05\n",
      "Step 1788, Train Loss: 2.4566, Test Loss: 2.1151, Learning Rate: 2.27e-05\n",
      "Step 1789, Train Loss: 2.3158, Test Loss: 2.3563, Learning Rate: 2.27e-05\n",
      "Step 1790, Train Loss: 2.2396, Test Loss: 2.0772, Learning Rate: 2.27e-05\n",
      "Step 1791, Train Loss: 2.3074, Test Loss: 2.2706, Learning Rate: 2.27e-05\n",
      "Step 1792, Train Loss: 2.3832, Test Loss: 2.3202, Learning Rate: 2.27e-05\n",
      "Step 1793, Train Loss: 2.3044, Test Loss: 2.2241, Learning Rate: 2.27e-05\n",
      "Step 1794, Train Loss: 2.3680, Test Loss: 2.2444, Learning Rate: 2.27e-05\n",
      "Step 1795, Train Loss: 2.2130, Test Loss: 2.3379, Learning Rate: 2.27e-05\n",
      "Step 1796, Train Loss: 2.4039, Test Loss: 2.3296, Learning Rate: 2.27e-05\n",
      "Step 1797, Train Loss: 2.3465, Test Loss: 2.2900, Learning Rate: 2.27e-05\n",
      "Step 1798, Train Loss: 2.3691, Test Loss: 2.4004, Learning Rate: 2.27e-05\n",
      "Step 1799, Train Loss: 2.3894, Test Loss: 2.4078, Learning Rate: 2.27e-05\n",
      "Step 1800, Train Loss: 2.3607, Test Loss: 2.3286, Learning Rate: 2.27e-05\n",
      "Step 1801, Train Loss: 2.4372, Test Loss: 2.3480, Learning Rate: 2.27e-05\n",
      "Step 1802, Train Loss: 2.3340, Test Loss: 2.5167, Learning Rate: 2.27e-05\n",
      "Step 1803, Train Loss: 2.3793, Test Loss: 2.2497, Learning Rate: 2.27e-05\n",
      "Step 1804, Train Loss: 2.3609, Test Loss: 2.3820, Learning Rate: 2.27e-05\n",
      "Step 1805, Train Loss: 2.3345, Test Loss: 2.3188, Learning Rate: 2.27e-05\n",
      "Step 1806, Train Loss: 2.3631, Test Loss: 2.2583, Learning Rate: 2.27e-05\n",
      "Step 1807, Train Loss: 2.2562, Test Loss: 2.5456, Learning Rate: 2.27e-05\n",
      "Step 1808, Train Loss: 2.3106, Test Loss: 2.0906, Learning Rate: 2.27e-05\n",
      "Step 1809, Train Loss: 2.4119, Test Loss: 2.4285, Learning Rate: 2.27e-05\n",
      "Step 1810, Train Loss: 2.3599, Test Loss: 2.2922, Learning Rate: 2.27e-05\n",
      "Step 1811, Train Loss: 2.3656, Test Loss: 2.1902, Learning Rate: 2.27e-05\n",
      "Step 1812, Train Loss: 2.4039, Test Loss: 2.1436, Learning Rate: 2.27e-05\n",
      "Step 1813, Train Loss: 2.2737, Test Loss: 2.3632, Learning Rate: 2.27e-05\n",
      "Step 1814, Train Loss: 2.4127, Test Loss: 2.2528, Learning Rate: 2.27e-05\n",
      "Step 1815, Train Loss: 2.2347, Test Loss: 2.4577, Learning Rate: 2.27e-05\n",
      "Step 1816, Train Loss: 2.2492, Test Loss: 2.3147, Learning Rate: 2.27e-05\n",
      "Step 1817, Train Loss: 2.4267, Test Loss: 2.3266, Learning Rate: 2.27e-05\n",
      "Step 1818, Train Loss: 2.3584, Test Loss: 2.3293, Learning Rate: 2.27e-05\n",
      "Step 1819, Train Loss: 2.2651, Test Loss: 2.3815, Learning Rate: 2.27e-05\n",
      "Step 1820, Train Loss: 2.2855, Test Loss: 2.2329, Learning Rate: 2.27e-05\n",
      "Step 1821, Train Loss: 2.3019, Test Loss: 2.2096, Learning Rate: 2.27e-05\n",
      "Step 1822, Train Loss: 2.3337, Test Loss: 2.3569, Learning Rate: 2.27e-05\n",
      "Step 1823, Train Loss: 2.3035, Test Loss: 2.3714, Learning Rate: 2.27e-05\n",
      "Step 1824, Train Loss: 2.3374, Test Loss: 2.4122, Learning Rate: 2.27e-05\n",
      "Step 1825, Train Loss: 2.3049, Test Loss: 2.4185, Learning Rate: 2.27e-05\n",
      "Step 1826, Train Loss: 2.3935, Test Loss: 2.3605, Learning Rate: 2.27e-05\n",
      "Step 1827, Train Loss: 2.3744, Test Loss: 2.3291, Learning Rate: 2.27e-05\n",
      "Step 1828, Train Loss: 2.3501, Test Loss: 2.4678, Learning Rate: 2.27e-05\n",
      "Step 1829, Train Loss: 2.3379, Test Loss: 1.9882, Learning Rate: 2.27e-05\n",
      "Step 1830, Train Loss: 2.3585, Test Loss: 2.3424, Learning Rate: 2.27e-05\n",
      "Step 1831, Train Loss: 2.3449, Test Loss: 2.0881, Learning Rate: 2.27e-05\n",
      "Step 1832, Train Loss: 2.2476, Test Loss: 2.1210, Learning Rate: 2.27e-05\n",
      "Step 1833, Train Loss: 2.2501, Test Loss: 2.4031, Learning Rate: 2.27e-05\n",
      "Step 1834, Train Loss: 2.4475, Test Loss: 2.0778, Learning Rate: 2.27e-05\n",
      "Step 1835, Train Loss: 2.2952, Test Loss: 2.3536, Learning Rate: 2.27e-05\n",
      "Step 1836, Train Loss: 2.3172, Test Loss: 2.4914, Learning Rate: 2.27e-05\n",
      "Step 1837, Train Loss: 2.3242, Test Loss: 2.4504, Learning Rate: 2.27e-05\n",
      "Step 1838, Train Loss: 2.2724, Test Loss: 2.3896, Learning Rate: 2.27e-05\n",
      "Step 1839, Train Loss: 2.2539, Test Loss: 2.3099, Learning Rate: 2.27e-05\n",
      "Step 1840, Train Loss: 2.2285, Test Loss: 2.2681, Learning Rate: 2.27e-05\n",
      "Step 1841, Train Loss: 2.2992, Test Loss: 2.2068, Learning Rate: 2.27e-05\n",
      "Step 1842, Train Loss: 2.3840, Test Loss: 2.3778, Learning Rate: 2.27e-05\n",
      "Step 1843, Train Loss: 2.3612, Test Loss: 2.2450, Learning Rate: 2.27e-05\n",
      "Step 1844, Train Loss: 2.3407, Test Loss: 2.3981, Learning Rate: 2.27e-05\n",
      "Step 1845, Train Loss: 2.1629, Test Loss: 2.3890, Learning Rate: 2.27e-05\n",
      "Step 1846, Train Loss: 2.3859, Test Loss: 2.3972, Learning Rate: 2.27e-05\n",
      "Step 1847, Train Loss: 2.3357, Test Loss: 2.2641, Learning Rate: 2.27e-05\n",
      "Step 1848, Train Loss: 2.2885, Test Loss: 2.2965, Learning Rate: 2.27e-05\n",
      "Step 1849, Train Loss: 2.2811, Test Loss: 2.4581, Learning Rate: 2.26e-05\n",
      "Step 1850, Train Loss: 2.3224, Test Loss: 2.3511, Learning Rate: 2.26e-05\n",
      "Step 1851, Train Loss: 2.3635, Test Loss: 2.3083, Learning Rate: 2.26e-05\n",
      "Step 1852, Train Loss: 2.3399, Test Loss: 2.2232, Learning Rate: 2.26e-05\n",
      "Step 1853, Train Loss: 2.2811, Test Loss: 2.3077, Learning Rate: 2.26e-05\n",
      "Step 1854, Train Loss: 2.3792, Test Loss: 2.0286, Learning Rate: 2.26e-05\n",
      "Step 1855, Train Loss: 2.4332, Test Loss: 2.4284, Learning Rate: 2.26e-05\n",
      "Step 1856, Train Loss: 2.3581, Test Loss: 2.2976, Learning Rate: 2.26e-05\n",
      "Step 1857, Train Loss: 2.4192, Test Loss: 2.4350, Learning Rate: 2.26e-05\n",
      "Step 1858, Train Loss: 2.2488, Test Loss: 2.1928, Learning Rate: 2.26e-05\n",
      "Step 1859, Train Loss: 2.4421, Test Loss: 2.3340, Learning Rate: 2.26e-05\n",
      "Step 1860, Train Loss: 2.2747, Test Loss: 2.1515, Learning Rate: 2.26e-05\n",
      "Step 1861, Train Loss: 2.3339, Test Loss: 2.2685, Learning Rate: 2.26e-05\n",
      "Step 1862, Train Loss: 2.4045, Test Loss: 2.4344, Learning Rate: 2.26e-05\n",
      "Step 1863, Train Loss: 2.1354, Test Loss: 2.2203, Learning Rate: 2.26e-05\n",
      "Step 1864, Train Loss: 2.3664, Test Loss: 2.1417, Learning Rate: 2.26e-05\n",
      "Step 1865, Train Loss: 2.3570, Test Loss: 2.2755, Learning Rate: 2.26e-05\n",
      "Step 1866, Train Loss: 2.5078, Test Loss: 2.3916, Learning Rate: 2.26e-05\n",
      "Step 1867, Train Loss: 2.2952, Test Loss: 2.2844, Learning Rate: 2.26e-05\n",
      "Step 1868, Train Loss: 2.3809, Test Loss: 2.4426, Learning Rate: 2.26e-05\n",
      "Step 1869, Train Loss: 2.3987, Test Loss: 2.4954, Learning Rate: 2.26e-05\n",
      "Step 1870, Train Loss: 2.4165, Test Loss: 2.5109, Learning Rate: 2.26e-05\n",
      "Step 1871, Train Loss: 2.2687, Test Loss: 2.3791, Learning Rate: 2.26e-05\n",
      "Step 1872, Train Loss: 2.3456, Test Loss: 2.2746, Learning Rate: 2.26e-05\n",
      "Step 1873, Train Loss: 2.3375, Test Loss: 2.2440, Learning Rate: 2.26e-05\n",
      "Step 1874, Train Loss: 2.3763, Test Loss: 2.3398, Learning Rate: 2.26e-05\n",
      "Step 1875, Train Loss: 2.1976, Test Loss: 2.3090, Learning Rate: 2.26e-05\n",
      "Step 1876, Train Loss: 2.3189, Test Loss: 2.3337, Learning Rate: 2.26e-05\n",
      "Step 1877, Train Loss: 2.2653, Test Loss: 2.3229, Learning Rate: 2.26e-05\n",
      "Step 1878, Train Loss: 2.3295, Test Loss: 2.2819, Learning Rate: 2.26e-05\n",
      "Step 1879, Train Loss: 2.2225, Test Loss: 2.3803, Learning Rate: 2.26e-05\n",
      "Step 1880, Train Loss: 2.4782, Test Loss: 2.3193, Learning Rate: 2.26e-05\n",
      "Step 1881, Train Loss: 2.2970, Test Loss: 2.2551, Learning Rate: 2.26e-05\n",
      "Step 1882, Train Loss: 2.4464, Test Loss: 2.1898, Learning Rate: 2.26e-05\n",
      "Step 1883, Train Loss: 2.2914, Test Loss: 2.3247, Learning Rate: 2.26e-05\n",
      "Step 1884, Train Loss: 2.2235, Test Loss: 2.2979, Learning Rate: 2.26e-05\n",
      "Step 1885, Train Loss: 2.3545, Test Loss: 2.3601, Learning Rate: 2.26e-05\n",
      "Step 1886, Train Loss: 2.3552, Test Loss: 2.2165, Learning Rate: 2.26e-05\n",
      "Step 1887, Train Loss: 2.3733, Test Loss: 2.2243, Learning Rate: 2.26e-05\n",
      "Step 1888, Train Loss: 2.3065, Test Loss: 2.3385, Learning Rate: 2.26e-05\n",
      "Step 1889, Train Loss: 2.2796, Test Loss: 2.3683, Learning Rate: 2.26e-05\n",
      "Step 1890, Train Loss: 2.2578, Test Loss: 2.3230, Learning Rate: 2.26e-05\n",
      "Step 1891, Train Loss: 2.3169, Test Loss: 2.1466, Learning Rate: 2.26e-05\n",
      "Step 1892, Train Loss: 2.3362, Test Loss: 2.5261, Learning Rate: 2.26e-05\n",
      "Step 1893, Train Loss: 2.3797, Test Loss: 2.3311, Learning Rate: 2.26e-05\n",
      "Step 1894, Train Loss: 2.2144, Test Loss: 2.3300, Learning Rate: 2.26e-05\n",
      "Step 1895, Train Loss: 2.4768, Test Loss: 2.5514, Learning Rate: 2.26e-05\n",
      "Step 1896, Train Loss: 2.2348, Test Loss: 2.3293, Learning Rate: 2.26e-05\n",
      "Step 1897, Train Loss: 2.4030, Test Loss: 2.3199, Learning Rate: 2.26e-05\n",
      "Step 1898, Train Loss: 2.3475, Test Loss: 2.4090, Learning Rate: 2.26e-05\n",
      "Step 1899, Train Loss: 2.1915, Test Loss: 2.3796, Learning Rate: 2.26e-05\n",
      "Step 1900, Train Loss: 2.3715, Test Loss: 2.4294, Learning Rate: 2.26e-05\n",
      "Step 1901, Train Loss: 2.3897, Test Loss: 2.3291, Learning Rate: 2.26e-05\n",
      "Step 1902, Train Loss: 2.2768, Test Loss: 2.4372, Learning Rate: 2.26e-05\n",
      "Step 1903, Train Loss: 2.3586, Test Loss: 2.4774, Learning Rate: 2.26e-05\n",
      "Step 1904, Train Loss: 2.4108, Test Loss: 2.2660, Learning Rate: 2.26e-05\n",
      "Step 1905, Train Loss: 2.4396, Test Loss: 2.4041, Learning Rate: 2.26e-05\n",
      "Step 1906, Train Loss: 2.2458, Test Loss: 2.5186, Learning Rate: 2.26e-05\n",
      "Step 1907, Train Loss: 2.3737, Test Loss: 2.4496, Learning Rate: 2.26e-05\n",
      "Step 1908, Train Loss: 2.2933, Test Loss: 2.3601, Learning Rate: 2.26e-05\n",
      "Step 1909, Train Loss: 2.2969, Test Loss: 2.5037, Learning Rate: 2.26e-05\n",
      "Step 1910, Train Loss: 2.2520, Test Loss: 2.1758, Learning Rate: 2.26e-05\n",
      "Step 1911, Train Loss: 2.3177, Test Loss: 2.4138, Learning Rate: 2.26e-05\n",
      "Step 1912, Train Loss: 2.1956, Test Loss: 2.1711, Learning Rate: 2.26e-05\n",
      "Step 1913, Train Loss: 2.2839, Test Loss: 2.3093, Learning Rate: 2.26e-05\n",
      "Step 1914, Train Loss: 2.3519, Test Loss: 2.2922, Learning Rate: 2.26e-05\n",
      "Step 1915, Train Loss: 2.3835, Test Loss: 2.4222, Learning Rate: 2.26e-05\n",
      "Step 1916, Train Loss: 2.2524, Test Loss: 2.1589, Learning Rate: 2.26e-05\n",
      "Step 1917, Train Loss: 2.2947, Test Loss: 2.3554, Learning Rate: 2.26e-05\n",
      "Step 1918, Train Loss: 2.3494, Test Loss: 2.2915, Learning Rate: 2.26e-05\n",
      "Step 1919, Train Loss: 2.2754, Test Loss: 2.3408, Learning Rate: 2.26e-05\n",
      "Step 1920, Train Loss: 2.2034, Test Loss: 2.2204, Learning Rate: 2.26e-05\n",
      "Step 1921, Train Loss: 2.3132, Test Loss: 2.3382, Learning Rate: 2.26e-05\n",
      "Step 1922, Train Loss: 2.2442, Test Loss: 2.3262, Learning Rate: 2.26e-05\n",
      "Step 1923, Train Loss: 2.3057, Test Loss: 2.3757, Learning Rate: 2.26e-05\n",
      "Step 1924, Train Loss: 2.2939, Test Loss: 2.4084, Learning Rate: 2.26e-05\n",
      "Step 1925, Train Loss: 2.3905, Test Loss: 2.2764, Learning Rate: 2.26e-05\n",
      "Step 1926, Train Loss: 2.4240, Test Loss: 2.2646, Learning Rate: 2.26e-05\n",
      "Step 1927, Train Loss: 2.3546, Test Loss: 2.2911, Learning Rate: 2.26e-05\n",
      "Step 1928, Train Loss: 2.3298, Test Loss: 2.3117, Learning Rate: 2.26e-05\n",
      "Step 1929, Train Loss: 2.2341, Test Loss: 2.5045, Learning Rate: 2.26e-05\n",
      "Step 1930, Train Loss: 2.2777, Test Loss: 2.5926, Learning Rate: 2.26e-05\n",
      "Step 1931, Train Loss: 2.2579, Test Loss: 2.4436, Learning Rate: 2.26e-05\n",
      "Step 1932, Train Loss: 2.1744, Test Loss: 2.3023, Learning Rate: 2.26e-05\n",
      "Step 1933, Train Loss: 2.2601, Test Loss: 2.3700, Learning Rate: 2.26e-05\n",
      "Step 1934, Train Loss: 2.5656, Test Loss: 2.3531, Learning Rate: 2.26e-05\n",
      "Step 1935, Train Loss: 2.3645, Test Loss: 2.2280, Learning Rate: 2.26e-05\n",
      "Step 1936, Train Loss: 2.2819, Test Loss: 2.3664, Learning Rate: 2.26e-05\n",
      "Step 1937, Train Loss: 2.1610, Test Loss: 2.3544, Learning Rate: 2.26e-05\n",
      "Step 1938, Train Loss: 2.2169, Test Loss: 2.1941, Learning Rate: 2.26e-05\n",
      "Step 1939, Train Loss: 2.4799, Test Loss: 2.4390, Learning Rate: 2.26e-05\n",
      "Step 1940, Train Loss: 2.3905, Test Loss: 2.3490, Learning Rate: 2.26e-05\n",
      "Step 1941, Train Loss: 2.4501, Test Loss: 2.3709, Learning Rate: 2.26e-05\n",
      "Step 1942, Train Loss: 2.4235, Test Loss: 2.4833, Learning Rate: 2.26e-05\n",
      "Step 1943, Train Loss: 2.3287, Test Loss: 2.2522, Learning Rate: 2.26e-05\n",
      "Step 1944, Train Loss: 2.2734, Test Loss: 2.3719, Learning Rate: 2.26e-05\n",
      "Step 1945, Train Loss: 2.4811, Test Loss: 2.5088, Learning Rate: 2.26e-05\n",
      "Step 1946, Train Loss: 2.3044, Test Loss: 2.3026, Learning Rate: 2.26e-05\n",
      "Step 1947, Train Loss: 2.4405, Test Loss: 2.4363, Learning Rate: 2.26e-05\n",
      "Step 1948, Train Loss: 2.4251, Test Loss: 2.5774, Learning Rate: 2.26e-05\n",
      "Step 1949, Train Loss: 2.4872, Test Loss: 2.3323, Learning Rate: 2.26e-05\n",
      "Step 1950, Train Loss: 2.4212, Test Loss: 2.4872, Learning Rate: 2.26e-05\n",
      "Step 1951, Train Loss: 2.3398, Test Loss: 2.4188, Learning Rate: 2.26e-05\n",
      "Step 1952, Train Loss: 2.2768, Test Loss: 2.3493, Learning Rate: 2.26e-05\n",
      "Step 1953, Train Loss: 2.3601, Test Loss: 2.4563, Learning Rate: 2.26e-05\n",
      "Step 1954, Train Loss: 2.3767, Test Loss: 2.3424, Learning Rate: 2.26e-05\n",
      "Step 1955, Train Loss: 2.1924, Test Loss: 2.3217, Learning Rate: 2.26e-05\n",
      "Step 1956, Train Loss: 2.3496, Test Loss: 2.1464, Learning Rate: 2.26e-05\n",
      "Step 1957, Train Loss: 2.3291, Test Loss: 2.1260, Learning Rate: 2.26e-05\n",
      "Step 1958, Train Loss: 2.3852, Test Loss: 2.6105, Learning Rate: 2.26e-05\n",
      "Step 1959, Train Loss: 2.2987, Test Loss: 2.3107, Learning Rate: 2.26e-05\n",
      "Step 1960, Train Loss: 2.2569, Test Loss: 2.3877, Learning Rate: 2.26e-05\n",
      "Step 1961, Train Loss: 2.3887, Test Loss: 2.1845, Learning Rate: 2.26e-05\n",
      "Step 1962, Train Loss: 2.3153, Test Loss: 2.1987, Learning Rate: 2.26e-05\n",
      "Step 1963, Train Loss: 2.2839, Test Loss: 2.3482, Learning Rate: 2.26e-05\n",
      "Step 1964, Train Loss: 2.3012, Test Loss: 2.3362, Learning Rate: 2.26e-05\n",
      "Step 1965, Train Loss: 2.2920, Test Loss: 2.4381, Learning Rate: 2.26e-05\n",
      "Step 1966, Train Loss: 2.3904, Test Loss: 2.3795, Learning Rate: 2.26e-05\n",
      "Step 1967, Train Loss: 2.3406, Test Loss: 2.5286, Learning Rate: 2.26e-05\n",
      "Step 1968, Train Loss: 2.3896, Test Loss: 2.2002, Learning Rate: 2.26e-05\n",
      "Step 1969, Train Loss: 2.3151, Test Loss: 2.5576, Learning Rate: 2.26e-05\n",
      "Step 1970, Train Loss: 2.3456, Test Loss: 2.3461, Learning Rate: 2.26e-05\n",
      "Step 1971, Train Loss: 2.4001, Test Loss: 2.4068, Learning Rate: 2.26e-05\n",
      "Step 1972, Train Loss: 2.4390, Test Loss: 2.3209, Learning Rate: 2.26e-05\n",
      "Step 1973, Train Loss: 2.3799, Test Loss: 2.2221, Learning Rate: 2.26e-05\n",
      "Step 1974, Train Loss: 2.2036, Test Loss: 2.2137, Learning Rate: 2.26e-05\n",
      "Step 1975, Train Loss: 2.3073, Test Loss: 2.3992, Learning Rate: 2.26e-05\n",
      "Step 1976, Train Loss: 2.3216, Test Loss: 2.2269, Learning Rate: 2.26e-05\n",
      "Step 1977, Train Loss: 2.3615, Test Loss: 2.3541, Learning Rate: 2.25e-05\n",
      "Step 1978, Train Loss: 2.3847, Test Loss: 2.2976, Learning Rate: 2.25e-05\n",
      "Step 1979, Train Loss: 2.3091, Test Loss: 2.3001, Learning Rate: 2.25e-05\n",
      "Step 1980, Train Loss: 2.3481, Test Loss: 2.2185, Learning Rate: 2.25e-05\n",
      "Step 1981, Train Loss: 2.3564, Test Loss: 2.5374, Learning Rate: 2.25e-05\n",
      "Step 1982, Train Loss: 2.3804, Test Loss: 2.4638, Learning Rate: 2.25e-05\n",
      "Step 1983, Train Loss: 2.4602, Test Loss: 2.1968, Learning Rate: 2.25e-05\n",
      "Step 1984, Train Loss: 2.4413, Test Loss: 2.2842, Learning Rate: 2.25e-05\n",
      "Step 1985, Train Loss: 2.3263, Test Loss: 2.3319, Learning Rate: 2.25e-05\n",
      "Step 1986, Train Loss: 2.3984, Test Loss: 2.3072, Learning Rate: 2.25e-05\n",
      "Step 1987, Train Loss: 2.3454, Test Loss: 2.3689, Learning Rate: 2.25e-05\n",
      "Step 1988, Train Loss: 2.3351, Test Loss: 2.3792, Learning Rate: 2.25e-05\n",
      "Step 1989, Train Loss: 2.4644, Test Loss: 2.1129, Learning Rate: 2.25e-05\n",
      "Step 1990, Train Loss: 2.4513, Test Loss: 2.2769, Learning Rate: 2.25e-05\n",
      "Step 1991, Train Loss: 2.3482, Test Loss: 2.2313, Learning Rate: 2.25e-05\n",
      "Step 1992, Train Loss: 2.3936, Test Loss: 2.1826, Learning Rate: 2.25e-05\n",
      "Step 1993, Train Loss: 2.2609, Test Loss: 2.1055, Learning Rate: 2.25e-05\n",
      "Step 1994, Train Loss: 2.3953, Test Loss: 2.2776, Learning Rate: 2.25e-05\n",
      "Step 1995, Train Loss: 2.3811, Test Loss: 2.4563, Learning Rate: 2.25e-05\n",
      "Step 1996, Train Loss: 2.3629, Test Loss: 2.2672, Learning Rate: 2.25e-05\n",
      "Step 1997, Train Loss: 2.2482, Test Loss: 2.3180, Learning Rate: 2.25e-05\n",
      "Step 1998, Train Loss: 2.3588, Test Loss: 2.2929, Learning Rate: 2.25e-05\n",
      "Step 1999, Train Loss: 2.3714, Test Loss: 2.2326, Learning Rate: 2.25e-05\n",
      "Step 2000, Train Loss: 2.4076, Test Loss: 2.2442, Learning Rate: 2.25e-05\n",
      "Step 2001, Train Loss: 2.3413, Test Loss: 2.2299, Learning Rate: 2.25e-05\n",
      "Step 2002, Train Loss: 2.2259, Test Loss: 2.3879, Learning Rate: 2.25e-05\n",
      "Step 2003, Train Loss: 2.3819, Test Loss: 2.2120, Learning Rate: 2.25e-05\n",
      "Step 2004, Train Loss: 2.2720, Test Loss: 2.3324, Learning Rate: 2.25e-05\n",
      "Step 2005, Train Loss: 2.2430, Test Loss: 2.2497, Learning Rate: 2.25e-05\n",
      "Step 2006, Train Loss: 2.4851, Test Loss: 2.4755, Learning Rate: 2.25e-05\n",
      "Step 2007, Train Loss: 2.0007, Test Loss: 2.3508, Learning Rate: 2.25e-05\n",
      "Step 2008, Train Loss: 2.3487, Test Loss: 2.2794, Learning Rate: 2.25e-05\n",
      "Step 2009, Train Loss: 2.2008, Test Loss: 2.3641, Learning Rate: 2.25e-05\n",
      "Step 2010, Train Loss: 2.2017, Test Loss: 2.4525, Learning Rate: 2.25e-05\n",
      "Step 2011, Train Loss: 2.1814, Test Loss: 2.3352, Learning Rate: 2.25e-05\n",
      "Step 2012, Train Loss: 2.2737, Test Loss: 2.2950, Learning Rate: 2.25e-05\n",
      "Step 2013, Train Loss: 2.4303, Test Loss: 2.2167, Learning Rate: 2.25e-05\n",
      "Step 2014, Train Loss: 2.3301, Test Loss: 2.3710, Learning Rate: 2.25e-05\n",
      "Step 2015, Train Loss: 2.3433, Test Loss: 2.3155, Learning Rate: 2.25e-05\n",
      "Step 2016, Train Loss: 2.2803, Test Loss: 2.3305, Learning Rate: 2.25e-05\n",
      "Step 2017, Train Loss: 2.4472, Test Loss: 2.4028, Learning Rate: 2.25e-05\n",
      "Step 2018, Train Loss: 2.4991, Test Loss: 2.3810, Learning Rate: 2.25e-05\n",
      "Step 2019, Train Loss: 2.5389, Test Loss: 2.1695, Learning Rate: 2.25e-05\n",
      "Step 2020, Train Loss: 2.3567, Test Loss: 2.1935, Learning Rate: 2.25e-05\n",
      "Step 2021, Train Loss: 2.4071, Test Loss: 2.3115, Learning Rate: 2.25e-05\n",
      "Step 2022, Train Loss: 2.3192, Test Loss: 2.4062, Learning Rate: 2.25e-05\n",
      "Step 2023, Train Loss: 2.3342, Test Loss: 2.3181, Learning Rate: 2.25e-05\n",
      "Step 2024, Train Loss: 2.3207, Test Loss: 2.3667, Learning Rate: 2.25e-05\n",
      "Step 2025, Train Loss: 2.2921, Test Loss: 2.4090, Learning Rate: 2.25e-05\n",
      "Step 2026, Train Loss: 2.1814, Test Loss: 2.3019, Learning Rate: 2.25e-05\n",
      "Step 2027, Train Loss: 2.2105, Test Loss: 2.3241, Learning Rate: 2.25e-05\n",
      "Step 2028, Train Loss: 2.3897, Test Loss: 2.3628, Learning Rate: 2.25e-05\n",
      "Step 2029, Train Loss: 2.3714, Test Loss: 2.1893, Learning Rate: 2.25e-05\n",
      "Step 2030, Train Loss: 2.4697, Test Loss: 2.3883, Learning Rate: 2.25e-05\n",
      "Step 2031, Train Loss: 2.3657, Test Loss: 2.2294, Learning Rate: 2.25e-05\n",
      "Step 2032, Train Loss: 2.2994, Test Loss: 2.2856, Learning Rate: 2.25e-05\n",
      "Step 2033, Train Loss: 2.3360, Test Loss: 2.3482, Learning Rate: 2.25e-05\n",
      "Step 2034, Train Loss: 2.4262, Test Loss: 2.3949, Learning Rate: 2.25e-05\n",
      "Step 2035, Train Loss: 2.2356, Test Loss: 2.2530, Learning Rate: 2.25e-05\n",
      "Step 2036, Train Loss: 2.2089, Test Loss: 2.3305, Learning Rate: 2.25e-05\n",
      "Step 2037, Train Loss: 2.2554, Test Loss: 2.4486, Learning Rate: 2.25e-05\n",
      "Step 2038, Train Loss: 2.2003, Test Loss: 2.2869, Learning Rate: 2.25e-05\n",
      "Step 2039, Train Loss: 2.2710, Test Loss: 2.3145, Learning Rate: 2.25e-05\n",
      "Step 2040, Train Loss: 2.3206, Test Loss: 2.5296, Learning Rate: 2.25e-05\n",
      "Step 2041, Train Loss: 2.3142, Test Loss: 2.3999, Learning Rate: 2.25e-05\n",
      "Step 2042, Train Loss: 2.4111, Test Loss: 2.3301, Learning Rate: 2.25e-05\n",
      "Step 2043, Train Loss: 2.2817, Test Loss: 2.3262, Learning Rate: 2.25e-05\n",
      "Step 2044, Train Loss: 2.3663, Test Loss: 2.3487, Learning Rate: 2.25e-05\n",
      "Step 2045, Train Loss: 2.3406, Test Loss: 2.4807, Learning Rate: 2.25e-05\n",
      "Step 2046, Train Loss: 2.4436, Test Loss: 2.4644, Learning Rate: 2.25e-05\n",
      "Step 2047, Train Loss: 2.3249, Test Loss: 2.4176, Learning Rate: 2.25e-05\n",
      "Step 2048, Train Loss: 2.2589, Test Loss: 2.5249, Learning Rate: 2.25e-05\n",
      "Step 2049, Train Loss: 2.2997, Test Loss: 2.4648, Learning Rate: 2.25e-05\n",
      "Step 2050, Train Loss: 2.2103, Test Loss: 2.1867, Learning Rate: 2.25e-05\n",
      "Step 2051, Train Loss: 2.3044, Test Loss: 2.3701, Learning Rate: 2.25e-05\n",
      "Step 2052, Train Loss: 2.4004, Test Loss: 2.4278, Learning Rate: 2.25e-05\n",
      "Step 2053, Train Loss: 2.2996, Test Loss: 2.4028, Learning Rate: 2.25e-05\n",
      "Step 2054, Train Loss: 2.3689, Test Loss: 2.0704, Learning Rate: 2.25e-05\n",
      "Step 2055, Train Loss: 2.2728, Test Loss: 2.2363, Learning Rate: 2.25e-05\n",
      "Step 2056, Train Loss: 2.3924, Test Loss: 2.3411, Learning Rate: 2.25e-05\n",
      "Step 2057, Train Loss: 2.2311, Test Loss: 2.3987, Learning Rate: 2.25e-05\n",
      "Step 2058, Train Loss: 2.2511, Test Loss: 2.5053, Learning Rate: 2.25e-05\n",
      "Step 2059, Train Loss: 2.3840, Test Loss: 2.4161, Learning Rate: 2.25e-05\n",
      "Step 2060, Train Loss: 2.1938, Test Loss: 2.2527, Learning Rate: 2.25e-05\n",
      "Step 2061, Train Loss: 2.4449, Test Loss: 2.1766, Learning Rate: 2.25e-05\n",
      "Step 2062, Train Loss: 2.3274, Test Loss: 2.4169, Learning Rate: 2.25e-05\n",
      "Step 2063, Train Loss: 2.3125, Test Loss: 2.2910, Learning Rate: 2.25e-05\n",
      "Step 2064, Train Loss: 2.3003, Test Loss: 2.1206, Learning Rate: 2.25e-05\n",
      "Step 2065, Train Loss: 2.3197, Test Loss: 2.2168, Learning Rate: 2.25e-05\n",
      "Step 2066, Train Loss: 2.3136, Test Loss: 2.4025, Learning Rate: 2.25e-05\n",
      "Step 2067, Train Loss: 2.2675, Test Loss: 2.4984, Learning Rate: 2.25e-05\n",
      "Step 2068, Train Loss: 2.3969, Test Loss: 2.2290, Learning Rate: 2.25e-05\n",
      "Step 2069, Train Loss: 2.1772, Test Loss: 2.3752, Learning Rate: 2.25e-05\n",
      "Step 2070, Train Loss: 2.3448, Test Loss: 2.5039, Learning Rate: 2.25e-05\n",
      "Step 2071, Train Loss: 2.3722, Test Loss: 2.2127, Learning Rate: 2.25e-05\n",
      "Step 2072, Train Loss: 2.2593, Test Loss: 2.3908, Learning Rate: 2.25e-05\n",
      "Step 2073, Train Loss: 2.2125, Test Loss: 2.2450, Learning Rate: 2.25e-05\n",
      "Step 2074, Train Loss: 2.2916, Test Loss: 2.2005, Learning Rate: 2.25e-05\n",
      "Step 2075, Train Loss: 2.3737, Test Loss: 2.1322, Learning Rate: 2.25e-05\n",
      "Step 2076, Train Loss: 2.2435, Test Loss: 2.2047, Learning Rate: 2.25e-05\n",
      "Step 2077, Train Loss: 2.3199, Test Loss: 2.2424, Learning Rate: 2.25e-05\n",
      "Step 2078, Train Loss: 2.4664, Test Loss: 2.3154, Learning Rate: 2.25e-05\n",
      "Step 2079, Train Loss: 2.2719, Test Loss: 2.2837, Learning Rate: 2.25e-05\n",
      "Step 2080, Train Loss: 2.2899, Test Loss: 2.4134, Learning Rate: 2.25e-05\n",
      "Step 2081, Train Loss: 2.1766, Test Loss: 2.2627, Learning Rate: 2.25e-05\n",
      "Step 2082, Train Loss: 2.3331, Test Loss: 2.2131, Learning Rate: 2.25e-05\n",
      "Step 2083, Train Loss: 2.1974, Test Loss: 2.4407, Learning Rate: 2.25e-05\n",
      "Step 2084, Train Loss: 2.4418, Test Loss: 2.3007, Learning Rate: 2.25e-05\n",
      "Step 2085, Train Loss: 2.2567, Test Loss: 2.3142, Learning Rate: 2.25e-05\n",
      "Step 2086, Train Loss: 2.3900, Test Loss: 2.3179, Learning Rate: 2.25e-05\n",
      "Step 2087, Train Loss: 2.2364, Test Loss: 2.2246, Learning Rate: 2.25e-05\n",
      "Step 2088, Train Loss: 2.3280, Test Loss: 2.4914, Learning Rate: 2.25e-05\n",
      "Step 2089, Train Loss: 2.4270, Test Loss: 2.3898, Learning Rate: 2.25e-05\n",
      "Step 2090, Train Loss: 2.4837, Test Loss: 2.4200, Learning Rate: 2.25e-05\n",
      "Step 2091, Train Loss: 2.3133, Test Loss: 2.4922, Learning Rate: 2.25e-05\n",
      "Step 2092, Train Loss: 2.4135, Test Loss: 2.3447, Learning Rate: 2.25e-05\n",
      "Step 2093, Train Loss: 2.4340, Test Loss: 2.1692, Learning Rate: 2.25e-05\n",
      "Step 2094, Train Loss: 2.4225, Test Loss: 2.3168, Learning Rate: 2.25e-05\n",
      "Step 2095, Train Loss: 2.3759, Test Loss: 2.3930, Learning Rate: 2.25e-05\n",
      "Step 2096, Train Loss: 2.1975, Test Loss: 2.1439, Learning Rate: 2.25e-05\n",
      "Step 2097, Train Loss: 2.2150, Test Loss: 2.5232, Learning Rate: 2.25e-05\n",
      "Step 2098, Train Loss: 2.3858, Test Loss: 2.1871, Learning Rate: 2.25e-05\n",
      "Step 2099, Train Loss: 2.3627, Test Loss: 2.3365, Learning Rate: 2.25e-05\n",
      "Step 2100, Train Loss: 2.4092, Test Loss: 2.2732, Learning Rate: 2.25e-05\n",
      "Step 2101, Train Loss: 2.2157, Test Loss: 2.2796, Learning Rate: 2.25e-05\n",
      "Step 2102, Train Loss: 2.1829, Test Loss: 2.2323, Learning Rate: 2.25e-05\n",
      "Step 2103, Train Loss: 2.2514, Test Loss: 2.2755, Learning Rate: 2.25e-05\n",
      "Step 2104, Train Loss: 2.4646, Test Loss: 2.4075, Learning Rate: 2.25e-05\n",
      "Step 2105, Train Loss: 2.4241, Test Loss: 2.2662, Learning Rate: 2.24e-05\n",
      "Step 2106, Train Loss: 2.4762, Test Loss: 2.3767, Learning Rate: 2.24e-05\n",
      "Step 2107, Train Loss: 2.2013, Test Loss: 2.3018, Learning Rate: 2.24e-05\n",
      "Step 2108, Train Loss: 2.3272, Test Loss: 2.2995, Learning Rate: 2.24e-05\n",
      "Step 2109, Train Loss: 2.3080, Test Loss: 2.5469, Learning Rate: 2.24e-05\n",
      "Step 2110, Train Loss: 2.3215, Test Loss: 2.3593, Learning Rate: 2.24e-05\n",
      "Step 2111, Train Loss: 2.2260, Test Loss: 2.6148, Learning Rate: 2.24e-05\n",
      "Step 2112, Train Loss: 2.4364, Test Loss: 2.4164, Learning Rate: 2.24e-05\n",
      "Step 2113, Train Loss: 2.2669, Test Loss: 2.3464, Learning Rate: 2.24e-05\n",
      "Step 2114, Train Loss: 2.2557, Test Loss: 2.4313, Learning Rate: 2.24e-05\n",
      "Step 2115, Train Loss: 2.2478, Test Loss: 2.2390, Learning Rate: 2.24e-05\n",
      "Step 2116, Train Loss: 2.2480, Test Loss: 2.2855, Learning Rate: 2.24e-05\n",
      "Step 2117, Train Loss: 2.3459, Test Loss: 2.3929, Learning Rate: 2.24e-05\n",
      "Step 2118, Train Loss: 2.4103, Test Loss: 2.1256, Learning Rate: 2.24e-05\n",
      "Step 2119, Train Loss: 2.3618, Test Loss: 2.2890, Learning Rate: 2.24e-05\n",
      "Step 2120, Train Loss: 2.3256, Test Loss: 2.3629, Learning Rate: 2.24e-05\n",
      "Step 2121, Train Loss: 2.3499, Test Loss: 2.4811, Learning Rate: 2.24e-05\n",
      "Step 2122, Train Loss: 2.3112, Test Loss: 2.1171, Learning Rate: 2.24e-05\n",
      "Step 2123, Train Loss: 2.2388, Test Loss: 2.3068, Learning Rate: 2.24e-05\n",
      "Step 2124, Train Loss: 2.2188, Test Loss: 2.2295, Learning Rate: 2.24e-05\n",
      "Step 2125, Train Loss: 2.4049, Test Loss: 2.2066, Learning Rate: 2.24e-05\n",
      "Step 2126, Train Loss: 2.2933, Test Loss: 2.4080, Learning Rate: 2.24e-05\n",
      "Step 2127, Train Loss: 2.4046, Test Loss: 2.3438, Learning Rate: 2.24e-05\n",
      "Step 2128, Train Loss: 2.3698, Test Loss: 2.1825, Learning Rate: 2.24e-05\n",
      "Step 2129, Train Loss: 2.4749, Test Loss: 2.4560, Learning Rate: 2.24e-05\n",
      "Step 2130, Train Loss: 2.3976, Test Loss: 2.4811, Learning Rate: 2.24e-05\n",
      "Step 2131, Train Loss: 2.3756, Test Loss: 2.2134, Learning Rate: 2.24e-05\n",
      "Step 2132, Train Loss: 2.3325, Test Loss: 2.2867, Learning Rate: 2.24e-05\n",
      "Step 2133, Train Loss: 2.1397, Test Loss: 2.1393, Learning Rate: 2.24e-05\n",
      "Step 2134, Train Loss: 2.3695, Test Loss: 2.3680, Learning Rate: 2.24e-05\n",
      "Step 2135, Train Loss: 2.3379, Test Loss: 2.2143, Learning Rate: 2.24e-05\n",
      "Step 2136, Train Loss: 2.5281, Test Loss: 2.4820, Learning Rate: 2.24e-05\n",
      "Step 2137, Train Loss: 2.4759, Test Loss: 2.3928, Learning Rate: 2.24e-05\n",
      "Step 2138, Train Loss: 2.2649, Test Loss: 2.3292, Learning Rate: 2.24e-05\n",
      "Step 2139, Train Loss: 2.3546, Test Loss: 2.2344, Learning Rate: 2.24e-05\n",
      "Step 2140, Train Loss: 2.3783, Test Loss: 2.2209, Learning Rate: 2.24e-05\n",
      "Step 2141, Train Loss: 2.3339, Test Loss: 2.4712, Learning Rate: 2.24e-05\n",
      "Step 2142, Train Loss: 2.3468, Test Loss: 2.3416, Learning Rate: 2.24e-05\n",
      "Step 2143, Train Loss: 2.2629, Test Loss: 2.4557, Learning Rate: 2.24e-05\n",
      "Step 2144, Train Loss: 2.2201, Test Loss: 2.3367, Learning Rate: 2.24e-05\n",
      "Step 2145, Train Loss: 2.3033, Test Loss: 2.0255, Learning Rate: 2.24e-05\n",
      "Step 2146, Train Loss: 2.3316, Test Loss: 2.0951, Learning Rate: 2.24e-05\n",
      "Step 2147, Train Loss: 2.3133, Test Loss: 2.4805, Learning Rate: 2.24e-05\n",
      "Step 2148, Train Loss: 2.1232, Test Loss: 2.3985, Learning Rate: 2.24e-05\n",
      "Step 2149, Train Loss: 2.3000, Test Loss: 2.4109, Learning Rate: 2.24e-05\n",
      "Step 2150, Train Loss: 2.2314, Test Loss: 2.4890, Learning Rate: 2.24e-05\n",
      "Step 2151, Train Loss: 2.2369, Test Loss: 2.3336, Learning Rate: 2.24e-05\n",
      "Step 2152, Train Loss: 2.4098, Test Loss: 2.2585, Learning Rate: 2.24e-05\n",
      "Step 2153, Train Loss: 2.4186, Test Loss: 2.2982, Learning Rate: 2.24e-05\n",
      "Step 2154, Train Loss: 2.3967, Test Loss: 2.3689, Learning Rate: 2.24e-05\n",
      "Step 2155, Train Loss: 2.3722, Test Loss: 2.3982, Learning Rate: 2.24e-05\n",
      "Step 2156, Train Loss: 2.2750, Test Loss: 2.3842, Learning Rate: 2.24e-05\n",
      "Step 2157, Train Loss: 2.2260, Test Loss: 2.2778, Learning Rate: 2.24e-05\n",
      "Step 2158, Train Loss: 2.3238, Test Loss: 2.2881, Learning Rate: 2.24e-05\n",
      "Step 2159, Train Loss: 2.4591, Test Loss: 2.3715, Learning Rate: 2.24e-05\n",
      "Step 2160, Train Loss: 2.2936, Test Loss: 2.3658, Learning Rate: 2.24e-05\n",
      "Step 2161, Train Loss: 2.3003, Test Loss: 2.3328, Learning Rate: 2.24e-05\n",
      "Step 2162, Train Loss: 2.3942, Test Loss: 2.3134, Learning Rate: 2.24e-05\n",
      "Step 2163, Train Loss: 2.2891, Test Loss: 2.4193, Learning Rate: 2.24e-05\n",
      "Step 2164, Train Loss: 2.4012, Test Loss: 2.3046, Learning Rate: 2.24e-05\n",
      "Step 2165, Train Loss: 2.4405, Test Loss: 2.1902, Learning Rate: 2.24e-05\n",
      "Step 2166, Train Loss: 2.1593, Test Loss: 2.2192, Learning Rate: 2.24e-05\n",
      "Step 2167, Train Loss: 2.2853, Test Loss: 2.3275, Learning Rate: 2.24e-05\n",
      "Step 2168, Train Loss: 2.2689, Test Loss: 2.2222, Learning Rate: 2.24e-05\n",
      "Step 2169, Train Loss: 2.2928, Test Loss: 2.3241, Learning Rate: 2.24e-05\n",
      "Step 2170, Train Loss: 2.2824, Test Loss: 2.3146, Learning Rate: 2.24e-05\n",
      "Step 2171, Train Loss: 2.6436, Test Loss: 2.5710, Learning Rate: 2.24e-05\n",
      "Step 2172, Train Loss: 2.2724, Test Loss: 2.2881, Learning Rate: 2.24e-05\n",
      "Step 2173, Train Loss: 2.3851, Test Loss: 2.3055, Learning Rate: 2.24e-05\n",
      "Step 2174, Train Loss: 2.3655, Test Loss: 2.3278, Learning Rate: 2.24e-05\n",
      "Step 2175, Train Loss: 2.4056, Test Loss: 2.5887, Learning Rate: 2.24e-05\n",
      "Step 2176, Train Loss: 2.2640, Test Loss: 2.3780, Learning Rate: 2.24e-05\n",
      "Step 2177, Train Loss: 2.3716, Test Loss: 2.3260, Learning Rate: 2.24e-05\n",
      "Step 2178, Train Loss: 2.4366, Test Loss: 2.4555, Learning Rate: 2.24e-05\n",
      "Step 2179, Train Loss: 2.3691, Test Loss: 2.1358, Learning Rate: 2.24e-05\n",
      "Step 2180, Train Loss: 2.3970, Test Loss: 2.1943, Learning Rate: 2.24e-05\n",
      "Step 2181, Train Loss: 2.2256, Test Loss: 2.1771, Learning Rate: 2.24e-05\n",
      "Step 2182, Train Loss: 2.3940, Test Loss: 2.0916, Learning Rate: 2.24e-05\n",
      "Step 2183, Train Loss: 2.3123, Test Loss: 2.2909, Learning Rate: 2.24e-05\n",
      "Step 2184, Train Loss: 2.2945, Test Loss: 2.3826, Learning Rate: 2.24e-05\n",
      "Step 2185, Train Loss: 2.3452, Test Loss: 2.0735, Learning Rate: 2.24e-05\n",
      "Step 2186, Train Loss: 2.3707, Test Loss: 2.1497, Learning Rate: 2.24e-05\n",
      "Step 2187, Train Loss: 2.2149, Test Loss: 2.2130, Learning Rate: 2.24e-05\n",
      "Step 2188, Train Loss: 2.3831, Test Loss: 2.3784, Learning Rate: 2.24e-05\n",
      "Step 2189, Train Loss: 2.2591, Test Loss: 2.3873, Learning Rate: 2.24e-05\n",
      "Step 2190, Train Loss: 2.3110, Test Loss: 2.3736, Learning Rate: 2.24e-05\n",
      "Step 2191, Train Loss: 2.2686, Test Loss: 2.4538, Learning Rate: 2.24e-05\n",
      "Step 2192, Train Loss: 2.2947, Test Loss: 2.2999, Learning Rate: 2.24e-05\n",
      "Step 2193, Train Loss: 2.3108, Test Loss: 2.4413, Learning Rate: 2.24e-05\n",
      "Step 2194, Train Loss: 2.3139, Test Loss: 2.4136, Learning Rate: 2.24e-05\n",
      "Step 2195, Train Loss: 2.3602, Test Loss: 2.3546, Learning Rate: 2.24e-05\n",
      "Step 2196, Train Loss: 2.3131, Test Loss: 2.1562, Learning Rate: 2.24e-05\n",
      "Step 2197, Train Loss: 2.3269, Test Loss: 2.2010, Learning Rate: 2.24e-05\n",
      "Step 2198, Train Loss: 2.2843, Test Loss: 2.3081, Learning Rate: 2.24e-05\n",
      "Step 2199, Train Loss: 2.3224, Test Loss: 2.3477, Learning Rate: 2.24e-05\n",
      "Step 2200, Train Loss: 2.4333, Test Loss: 2.4673, Learning Rate: 2.24e-05\n",
      "Step 2201, Train Loss: 2.4940, Test Loss: 2.2943, Learning Rate: 2.24e-05\n",
      "Step 2202, Train Loss: 2.4029, Test Loss: 2.2554, Learning Rate: 2.24e-05\n",
      "Step 2203, Train Loss: 2.2554, Test Loss: 2.2234, Learning Rate: 2.24e-05\n",
      "Step 2204, Train Loss: 2.1897, Test Loss: 2.3698, Learning Rate: 2.24e-05\n",
      "Step 2205, Train Loss: 2.2502, Test Loss: 2.4276, Learning Rate: 2.24e-05\n",
      "Step 2206, Train Loss: 2.2847, Test Loss: 2.5040, Learning Rate: 2.24e-05\n",
      "Step 2207, Train Loss: 2.3457, Test Loss: 2.1288, Learning Rate: 2.24e-05\n",
      "Step 2208, Train Loss: 2.2329, Test Loss: 2.2998, Learning Rate: 2.24e-05\n",
      "Step 2209, Train Loss: 2.3799, Test Loss: 2.2841, Learning Rate: 2.24e-05\n",
      "Step 2210, Train Loss: 2.4295, Test Loss: 2.3042, Learning Rate: 2.24e-05\n",
      "Step 2211, Train Loss: 2.3670, Test Loss: 2.0745, Learning Rate: 2.24e-05\n",
      "Step 2212, Train Loss: 2.3132, Test Loss: 2.3090, Learning Rate: 2.24e-05\n",
      "Step 2213, Train Loss: 2.2133, Test Loss: 2.2504, Learning Rate: 2.24e-05\n",
      "Step 2214, Train Loss: 2.2597, Test Loss: 2.1889, Learning Rate: 2.24e-05\n",
      "Step 2215, Train Loss: 2.2735, Test Loss: 2.2144, Learning Rate: 2.24e-05\n",
      "Step 2216, Train Loss: 2.2215, Test Loss: 2.2561, Learning Rate: 2.24e-05\n",
      "Step 2217, Train Loss: 2.3524, Test Loss: 2.4071, Learning Rate: 2.24e-05\n",
      "Step 2218, Train Loss: 2.3117, Test Loss: 2.1805, Learning Rate: 2.24e-05\n",
      "Step 2219, Train Loss: 2.3676, Test Loss: 2.2745, Learning Rate: 2.24e-05\n",
      "Step 2220, Train Loss: 2.3454, Test Loss: 2.3258, Learning Rate: 2.24e-05\n",
      "Step 2221, Train Loss: 2.3759, Test Loss: 2.4815, Learning Rate: 2.24e-05\n",
      "Step 2222, Train Loss: 2.2907, Test Loss: 2.2025, Learning Rate: 2.24e-05\n",
      "Step 2223, Train Loss: 2.3338, Test Loss: 2.1552, Learning Rate: 2.24e-05\n",
      "Step 2224, Train Loss: 2.3089, Test Loss: 2.3910, Learning Rate: 2.24e-05\n",
      "Step 2225, Train Loss: 2.4413, Test Loss: 2.2720, Learning Rate: 2.24e-05\n",
      "Step 2226, Train Loss: 2.3448, Test Loss: 2.3147, Learning Rate: 2.24e-05\n",
      "Step 2227, Train Loss: 2.3400, Test Loss: 2.2068, Learning Rate: 2.24e-05\n",
      "Step 2228, Train Loss: 2.3230, Test Loss: 2.4581, Learning Rate: 2.24e-05\n",
      "Step 2229, Train Loss: 2.3719, Test Loss: 2.2264, Learning Rate: 2.24e-05\n",
      "Step 2230, Train Loss: 2.2033, Test Loss: 2.3809, Learning Rate: 2.24e-05\n",
      "Step 2231, Train Loss: 2.2870, Test Loss: 2.2903, Learning Rate: 2.24e-05\n",
      "Step 2232, Train Loss: 2.1666, Test Loss: 2.4360, Learning Rate: 2.24e-05\n",
      "Step 2233, Train Loss: 2.2881, Test Loss: 2.3918, Learning Rate: 2.24e-05\n",
      "Step 2234, Train Loss: 2.4483, Test Loss: 2.2655, Learning Rate: 2.23e-05\n",
      "Step 2235, Train Loss: 2.3883, Test Loss: 2.3685, Learning Rate: 2.23e-05\n",
      "Step 2236, Train Loss: 2.4273, Test Loss: 2.3102, Learning Rate: 2.23e-05\n",
      "Step 2237, Train Loss: 2.3623, Test Loss: 2.4297, Learning Rate: 2.23e-05\n",
      "Step 2238, Train Loss: 2.3008, Test Loss: 2.2588, Learning Rate: 2.23e-05\n",
      "Step 2239, Train Loss: 2.3489, Test Loss: 2.2804, Learning Rate: 2.23e-05\n",
      "Step 2240, Train Loss: 2.2641, Test Loss: 2.3398, Learning Rate: 2.23e-05\n",
      "Step 2241, Train Loss: 2.4242, Test Loss: 2.3679, Learning Rate: 2.23e-05\n",
      "Step 2242, Train Loss: 2.2934, Test Loss: 2.4201, Learning Rate: 2.23e-05\n",
      "Step 2243, Train Loss: 2.4343, Test Loss: 2.3547, Learning Rate: 2.23e-05\n",
      "Step 2244, Train Loss: 2.3620, Test Loss: 2.4855, Learning Rate: 2.23e-05\n",
      "Step 2245, Train Loss: 2.3005, Test Loss: 2.4238, Learning Rate: 2.23e-05\n",
      "Step 2246, Train Loss: 2.2739, Test Loss: 2.3991, Learning Rate: 2.23e-05\n",
      "Step 2247, Train Loss: 2.3953, Test Loss: 2.4680, Learning Rate: 2.23e-05\n",
      "Step 2248, Train Loss: 2.4062, Test Loss: 2.1774, Learning Rate: 2.23e-05\n",
      "Step 2249, Train Loss: 2.3967, Test Loss: 2.3602, Learning Rate: 2.23e-05\n",
      "Step 2250, Train Loss: 2.2903, Test Loss: 2.0920, Learning Rate: 2.23e-05\n",
      "Step 2251, Train Loss: 2.3538, Test Loss: 2.2840, Learning Rate: 2.23e-05\n",
      "Step 2252, Train Loss: 2.3752, Test Loss: 2.1393, Learning Rate: 2.23e-05\n",
      "Step 2253, Train Loss: 2.2611, Test Loss: 2.2065, Learning Rate: 2.23e-05\n",
      "Step 2254, Train Loss: 2.2649, Test Loss: 2.2374, Learning Rate: 2.23e-05\n",
      "Step 2255, Train Loss: 2.3492, Test Loss: 2.3559, Learning Rate: 2.23e-05\n",
      "Step 2256, Train Loss: 2.3898, Test Loss: 2.4092, Learning Rate: 2.23e-05\n",
      "Step 2257, Train Loss: 2.2959, Test Loss: 2.3496, Learning Rate: 2.23e-05\n",
      "Step 2258, Train Loss: 2.3169, Test Loss: 2.3104, Learning Rate: 2.23e-05\n",
      "Step 2259, Train Loss: 2.2206, Test Loss: 2.1409, Learning Rate: 2.23e-05\n",
      "Step 2260, Train Loss: 2.2671, Test Loss: 2.2687, Learning Rate: 2.23e-05\n",
      "Step 2261, Train Loss: 2.2453, Test Loss: 2.1335, Learning Rate: 2.23e-05\n",
      "Step 2262, Train Loss: 2.3342, Test Loss: 2.2438, Learning Rate: 2.23e-05\n",
      "Step 2263, Train Loss: 2.4789, Test Loss: 2.2813, Learning Rate: 2.23e-05\n",
      "Step 2264, Train Loss: 2.2962, Test Loss: 2.4436, Learning Rate: 2.23e-05\n",
      "Step 2265, Train Loss: 2.2868, Test Loss: 2.3016, Learning Rate: 2.23e-05\n",
      "Step 2266, Train Loss: 2.2951, Test Loss: 2.2809, Learning Rate: 2.23e-05\n",
      "Step 2267, Train Loss: 2.4519, Test Loss: 2.3559, Learning Rate: 2.23e-05\n",
      "Step 2268, Train Loss: 2.3593, Test Loss: 2.1868, Learning Rate: 2.23e-05\n",
      "Step 2269, Train Loss: 2.2983, Test Loss: 2.3184, Learning Rate: 2.23e-05\n",
      "Step 2270, Train Loss: 2.1673, Test Loss: 2.4227, Learning Rate: 2.23e-05\n",
      "Step 2271, Train Loss: 2.3398, Test Loss: 2.4364, Learning Rate: 2.23e-05\n",
      "Step 2272, Train Loss: 2.2474, Test Loss: 2.4441, Learning Rate: 2.23e-05\n",
      "Step 2273, Train Loss: 2.2104, Test Loss: 2.1764, Learning Rate: 2.23e-05\n",
      "Step 2274, Train Loss: 2.2864, Test Loss: 2.3399, Learning Rate: 2.23e-05\n",
      "Step 2275, Train Loss: 2.3054, Test Loss: 2.3529, Learning Rate: 2.23e-05\n",
      "Step 2276, Train Loss: 2.1648, Test Loss: 2.3251, Learning Rate: 2.23e-05\n",
      "Step 2277, Train Loss: 2.3950, Test Loss: 2.4265, Learning Rate: 2.23e-05\n",
      "Step 2278, Train Loss: 2.2993, Test Loss: 2.4142, Learning Rate: 2.23e-05\n",
      "Step 2279, Train Loss: 2.2583, Test Loss: 2.4262, Learning Rate: 2.23e-05\n",
      "Step 2280, Train Loss: 2.3037, Test Loss: 2.3581, Learning Rate: 2.23e-05\n",
      "Step 2281, Train Loss: 2.3037, Test Loss: 2.2742, Learning Rate: 2.23e-05\n",
      "Step 2282, Train Loss: 2.5134, Test Loss: 2.4140, Learning Rate: 2.23e-05\n",
      "Step 2283, Train Loss: 2.4200, Test Loss: 2.2455, Learning Rate: 2.23e-05\n",
      "Step 2284, Train Loss: 2.2568, Test Loss: 2.2926, Learning Rate: 2.23e-05\n",
      "Step 2285, Train Loss: 2.4012, Test Loss: 2.4772, Learning Rate: 2.23e-05\n",
      "Step 2286, Train Loss: 2.4022, Test Loss: 2.2728, Learning Rate: 2.23e-05\n",
      "Step 2287, Train Loss: 2.2813, Test Loss: 2.3693, Learning Rate: 2.23e-05\n",
      "Step 2288, Train Loss: 2.3417, Test Loss: 2.3171, Learning Rate: 2.23e-05\n",
      "Step 2289, Train Loss: 2.3818, Test Loss: 2.3906, Learning Rate: 2.23e-05\n",
      "Step 2290, Train Loss: 2.2902, Test Loss: 2.1279, Learning Rate: 2.23e-05\n",
      "Step 2291, Train Loss: 2.1953, Test Loss: 2.3836, Learning Rate: 2.23e-05\n",
      "Step 2292, Train Loss: 2.3449, Test Loss: 2.2939, Learning Rate: 2.23e-05\n",
      "Step 2293, Train Loss: 2.3396, Test Loss: 2.2712, Learning Rate: 2.23e-05\n",
      "Step 2294, Train Loss: 2.3406, Test Loss: 2.1812, Learning Rate: 2.23e-05\n",
      "Step 2295, Train Loss: 2.3800, Test Loss: 2.2787, Learning Rate: 2.23e-05\n",
      "Step 2296, Train Loss: 2.4063, Test Loss: 2.1939, Learning Rate: 2.23e-05\n",
      "Step 2297, Train Loss: 2.2276, Test Loss: 2.2395, Learning Rate: 2.23e-05\n",
      "Step 2298, Train Loss: 2.3305, Test Loss: 2.3474, Learning Rate: 2.23e-05\n",
      "Step 2299, Train Loss: 2.3207, Test Loss: 2.2377, Learning Rate: 2.23e-05\n",
      "Step 2300, Train Loss: 2.3249, Test Loss: 2.2244, Learning Rate: 2.23e-05\n",
      "Step 2301, Train Loss: 2.2994, Test Loss: 1.9844, Learning Rate: 2.23e-05\n",
      "Step 2302, Train Loss: 2.3701, Test Loss: 2.3077, Learning Rate: 2.23e-05\n",
      "Step 2303, Train Loss: 2.3451, Test Loss: 2.2018, Learning Rate: 2.23e-05\n",
      "Step 2304, Train Loss: 2.2473, Test Loss: 2.1780, Learning Rate: 2.23e-05\n",
      "Step 2305, Train Loss: 2.3771, Test Loss: 2.3591, Learning Rate: 2.23e-05\n",
      "Step 2306, Train Loss: 2.3120, Test Loss: 2.3138, Learning Rate: 2.23e-05\n",
      "Step 2307, Train Loss: 2.2537, Test Loss: 2.2873, Learning Rate: 2.23e-05\n",
      "Step 2308, Train Loss: 2.4571, Test Loss: 2.3834, Learning Rate: 2.23e-05\n",
      "Step 2309, Train Loss: 2.1950, Test Loss: 2.3459, Learning Rate: 2.23e-05\n",
      "Step 2310, Train Loss: 2.3683, Test Loss: 2.2027, Learning Rate: 2.23e-05\n",
      "Step 2311, Train Loss: 2.3417, Test Loss: 2.3180, Learning Rate: 2.23e-05\n",
      "Step 2312, Train Loss: 2.2935, Test Loss: 2.2991, Learning Rate: 2.23e-05\n",
      "Step 2313, Train Loss: 2.4130, Test Loss: 2.2112, Learning Rate: 2.23e-05\n",
      "Step 2314, Train Loss: 2.2845, Test Loss: 2.2059, Learning Rate: 2.23e-05\n",
      "Step 2315, Train Loss: 2.3361, Test Loss: 2.5255, Learning Rate: 2.23e-05\n",
      "Step 2316, Train Loss: 2.2751, Test Loss: 2.2888, Learning Rate: 2.23e-05\n",
      "Step 2317, Train Loss: 2.3834, Test Loss: 2.4037, Learning Rate: 2.23e-05\n",
      "Step 2318, Train Loss: 2.3074, Test Loss: 2.4187, Learning Rate: 2.23e-05\n",
      "Step 2319, Train Loss: 2.2685, Test Loss: 2.2862, Learning Rate: 2.23e-05\n",
      "Step 2320, Train Loss: 2.3111, Test Loss: 2.4834, Learning Rate: 2.23e-05\n",
      "Step 2321, Train Loss: 2.2716, Test Loss: 2.2847, Learning Rate: 2.23e-05\n",
      "Step 2322, Train Loss: 2.2968, Test Loss: 2.3653, Learning Rate: 2.23e-05\n",
      "Step 2323, Train Loss: 2.3070, Test Loss: 2.2892, Learning Rate: 2.23e-05\n",
      "Step 2324, Train Loss: 2.2259, Test Loss: 2.3147, Learning Rate: 2.23e-05\n",
      "Step 2325, Train Loss: 2.2815, Test Loss: 2.3207, Learning Rate: 2.23e-05\n",
      "Step 2326, Train Loss: 2.3647, Test Loss: 2.1951, Learning Rate: 2.23e-05\n",
      "Step 2327, Train Loss: 2.3947, Test Loss: 2.5781, Learning Rate: 2.23e-05\n",
      "Step 2328, Train Loss: 2.3095, Test Loss: 2.4305, Learning Rate: 2.23e-05\n",
      "Step 2329, Train Loss: 2.3406, Test Loss: 2.2499, Learning Rate: 2.23e-05\n",
      "Step 2330, Train Loss: 2.4833, Test Loss: 2.2253, Learning Rate: 2.23e-05\n",
      "Step 2331, Train Loss: 2.2494, Test Loss: 2.3047, Learning Rate: 2.23e-05\n",
      "Step 2332, Train Loss: 2.3446, Test Loss: 2.2653, Learning Rate: 2.23e-05\n",
      "Step 2333, Train Loss: 2.4035, Test Loss: 2.1353, Learning Rate: 2.23e-05\n",
      "Step 2334, Train Loss: 2.3184, Test Loss: 2.5903, Learning Rate: 2.23e-05\n",
      "Step 2335, Train Loss: 2.4043, Test Loss: 2.5269, Learning Rate: 2.23e-05\n",
      "Step 2336, Train Loss: 2.3488, Test Loss: 2.4333, Learning Rate: 2.23e-05\n",
      "Step 2337, Train Loss: 2.3750, Test Loss: 2.3012, Learning Rate: 2.23e-05\n",
      "Step 2338, Train Loss: 2.3067, Test Loss: 2.3626, Learning Rate: 2.23e-05\n",
      "Step 2339, Train Loss: 2.2710, Test Loss: 2.3331, Learning Rate: 2.23e-05\n",
      "Step 2340, Train Loss: 2.3062, Test Loss: 2.3628, Learning Rate: 2.23e-05\n",
      "Step 2341, Train Loss: 2.3341, Test Loss: 2.1609, Learning Rate: 2.23e-05\n",
      "Step 2342, Train Loss: 2.3355, Test Loss: 2.2715, Learning Rate: 2.23e-05\n",
      "Step 2343, Train Loss: 2.2724, Test Loss: 2.4189, Learning Rate: 2.23e-05\n",
      "Step 2344, Train Loss: 2.3055, Test Loss: 2.4029, Learning Rate: 2.23e-05\n",
      "Step 2345, Train Loss: 2.1849, Test Loss: 2.1271, Learning Rate: 2.23e-05\n",
      "Step 2346, Train Loss: 2.3217, Test Loss: 2.1231, Learning Rate: 2.23e-05\n",
      "Step 2347, Train Loss: 2.3943, Test Loss: 2.4118, Learning Rate: 2.23e-05\n",
      "Step 2348, Train Loss: 2.2401, Test Loss: 2.4786, Learning Rate: 2.23e-05\n",
      "Step 2349, Train Loss: 2.2808, Test Loss: 2.2888, Learning Rate: 2.23e-05\n",
      "Step 2350, Train Loss: 2.3046, Test Loss: 2.3040, Learning Rate: 2.23e-05\n",
      "Step 2351, Train Loss: 2.3855, Test Loss: 2.1676, Learning Rate: 2.23e-05\n",
      "Step 2352, Train Loss: 2.3202, Test Loss: 2.3746, Learning Rate: 2.23e-05\n",
      "Step 2353, Train Loss: 2.2846, Test Loss: 2.5457, Learning Rate: 2.23e-05\n",
      "Step 2354, Train Loss: 2.3914, Test Loss: 2.2505, Learning Rate: 2.23e-05\n",
      "Step 2355, Train Loss: 2.2573, Test Loss: 2.2776, Learning Rate: 2.23e-05\n",
      "Step 2356, Train Loss: 2.2529, Test Loss: 2.3575, Learning Rate: 2.23e-05\n",
      "Step 2357, Train Loss: 2.3919, Test Loss: 2.2627, Learning Rate: 2.23e-05\n",
      "Step 2358, Train Loss: 2.3434, Test Loss: 2.5068, Learning Rate: 2.23e-05\n",
      "Step 2359, Train Loss: 2.3446, Test Loss: 2.3153, Learning Rate: 2.23e-05\n",
      "Step 2360, Train Loss: 2.3256, Test Loss: 2.3152, Learning Rate: 2.23e-05\n",
      "Step 2361, Train Loss: 2.3028, Test Loss: 2.2688, Learning Rate: 2.23e-05\n",
      "Step 2362, Train Loss: 2.3234, Test Loss: 2.3148, Learning Rate: 2.23e-05\n",
      "Step 2363, Train Loss: 2.2746, Test Loss: 2.3061, Learning Rate: 2.22e-05\n",
      "Step 2364, Train Loss: 2.0113, Test Loss: 2.2781, Learning Rate: 2.22e-05\n",
      "Step 2365, Train Loss: 2.4098, Test Loss: 2.3256, Learning Rate: 2.22e-05\n",
      "Step 2366, Train Loss: 2.1489, Test Loss: 2.3148, Learning Rate: 2.22e-05\n",
      "Step 2367, Train Loss: 2.3379, Test Loss: 2.3668, Learning Rate: 2.22e-05\n",
      "Step 2368, Train Loss: 2.4828, Test Loss: 2.2429, Learning Rate: 2.22e-05\n",
      "Step 2369, Train Loss: 2.4583, Test Loss: 2.3674, Learning Rate: 2.22e-05\n",
      "Step 2370, Train Loss: 2.2941, Test Loss: 2.1773, Learning Rate: 2.22e-05\n",
      "Step 2371, Train Loss: 2.2875, Test Loss: 2.4261, Learning Rate: 2.22e-05\n",
      "Step 2372, Train Loss: 2.4540, Test Loss: 2.2113, Learning Rate: 2.22e-05\n",
      "Step 2373, Train Loss: 2.3316, Test Loss: 2.4888, Learning Rate: 2.22e-05\n",
      "Step 2374, Train Loss: 2.3661, Test Loss: 2.3932, Learning Rate: 2.22e-05\n",
      "Step 2375, Train Loss: 2.4401, Test Loss: 2.4551, Learning Rate: 2.22e-05\n",
      "Step 2376, Train Loss: 2.1461, Test Loss: 2.5252, Learning Rate: 2.22e-05\n",
      "Step 2377, Train Loss: 2.2426, Test Loss: 2.2555, Learning Rate: 2.22e-05\n",
      "Step 2378, Train Loss: 2.3222, Test Loss: 2.2158, Learning Rate: 2.22e-05\n",
      "Step 2379, Train Loss: 2.2389, Test Loss: 2.2626, Learning Rate: 2.22e-05\n",
      "Step 2380, Train Loss: 2.4526, Test Loss: 2.2784, Learning Rate: 2.22e-05\n",
      "Step 2381, Train Loss: 2.4503, Test Loss: 2.3531, Learning Rate: 2.22e-05\n",
      "Step 2382, Train Loss: 2.2350, Test Loss: 2.3159, Learning Rate: 2.22e-05\n",
      "Step 2383, Train Loss: 2.2017, Test Loss: 2.5114, Learning Rate: 2.22e-05\n",
      "Step 2384, Train Loss: 2.3618, Test Loss: 2.2772, Learning Rate: 2.22e-05\n",
      "Step 2385, Train Loss: 2.2966, Test Loss: 2.3647, Learning Rate: 2.22e-05\n",
      "Step 2386, Train Loss: 2.1281, Test Loss: 2.2623, Learning Rate: 2.22e-05\n",
      "Step 2387, Train Loss: 2.3106, Test Loss: 2.3184, Learning Rate: 2.22e-05\n",
      "Step 2388, Train Loss: 2.1967, Test Loss: 2.3463, Learning Rate: 2.22e-05\n",
      "Step 2389, Train Loss: 2.2508, Test Loss: 2.3889, Learning Rate: 2.22e-05\n",
      "Step 2390, Train Loss: 2.2981, Test Loss: 2.1623, Learning Rate: 2.22e-05\n",
      "Step 2391, Train Loss: 2.3914, Test Loss: 2.1440, Learning Rate: 2.22e-05\n",
      "Step 2392, Train Loss: 2.4065, Test Loss: 2.2549, Learning Rate: 2.22e-05\n",
      "Step 2393, Train Loss: 2.4037, Test Loss: 2.4313, Learning Rate: 2.22e-05\n",
      "Step 2394, Train Loss: 2.4048, Test Loss: 2.3736, Learning Rate: 2.22e-05\n",
      "Step 2395, Train Loss: 2.5045, Test Loss: 2.1940, Learning Rate: 2.22e-05\n",
      "Step 2396, Train Loss: 2.3955, Test Loss: 2.4092, Learning Rate: 2.22e-05\n",
      "Step 2397, Train Loss: 2.3263, Test Loss: 2.2758, Learning Rate: 2.22e-05\n",
      "Step 2398, Train Loss: 2.4046, Test Loss: 2.3063, Learning Rate: 2.22e-05\n",
      "Step 2399, Train Loss: 2.4032, Test Loss: 2.2652, Learning Rate: 2.22e-05\n",
      "Step 2400, Train Loss: 2.3783, Test Loss: 2.4080, Learning Rate: 2.22e-05\n",
      "Step 2401, Train Loss: 2.3027, Test Loss: 2.2642, Learning Rate: 2.22e-05\n",
      "Step 2402, Train Loss: 2.2265, Test Loss: 2.2096, Learning Rate: 2.22e-05\n",
      "Step 2403, Train Loss: 2.2457, Test Loss: 2.4095, Learning Rate: 2.22e-05\n",
      "Step 2404, Train Loss: 2.3163, Test Loss: 2.3370, Learning Rate: 2.22e-05\n",
      "Step 2405, Train Loss: 2.3962, Test Loss: 2.4639, Learning Rate: 2.22e-05\n",
      "Step 2406, Train Loss: 2.2682, Test Loss: 2.3663, Learning Rate: 2.22e-05\n",
      "Step 2407, Train Loss: 2.2317, Test Loss: 2.2748, Learning Rate: 2.22e-05\n",
      "Step 2408, Train Loss: 2.2627, Test Loss: 2.2593, Learning Rate: 2.22e-05\n",
      "Step 2409, Train Loss: 2.2296, Test Loss: 2.3152, Learning Rate: 2.22e-05\n",
      "Step 2410, Train Loss: 2.3620, Test Loss: 2.3936, Learning Rate: 2.22e-05\n",
      "Step 2411, Train Loss: 2.2767, Test Loss: 2.1503, Learning Rate: 2.22e-05\n",
      "Step 2412, Train Loss: 2.4223, Test Loss: 2.4122, Learning Rate: 2.22e-05\n",
      "Step 2413, Train Loss: 2.3709, Test Loss: 2.1733, Learning Rate: 2.22e-05\n",
      "Step 2414, Train Loss: 2.3922, Test Loss: 2.3647, Learning Rate: 2.22e-05\n",
      "Step 2415, Train Loss: 2.3004, Test Loss: 2.2483, Learning Rate: 2.22e-05\n",
      "Step 2416, Train Loss: 2.3537, Test Loss: 2.3276, Learning Rate: 2.22e-05\n",
      "Step 2417, Train Loss: 2.2997, Test Loss: 2.3655, Learning Rate: 2.22e-05\n",
      "Step 2418, Train Loss: 2.2043, Test Loss: 2.2148, Learning Rate: 2.22e-05\n",
      "Step 2419, Train Loss: 2.2761, Test Loss: 2.2809, Learning Rate: 2.22e-05\n",
      "Step 2420, Train Loss: 2.3651, Test Loss: 2.5897, Learning Rate: 2.22e-05\n",
      "Step 2421, Train Loss: 2.2277, Test Loss: 2.2537, Learning Rate: 2.22e-05\n",
      "Step 2422, Train Loss: 2.3960, Test Loss: 2.4189, Learning Rate: 2.22e-05\n",
      "Step 2423, Train Loss: 2.4747, Test Loss: 2.4662, Learning Rate: 2.22e-05\n",
      "Step 2424, Train Loss: 2.3472, Test Loss: 2.2288, Learning Rate: 2.22e-05\n",
      "Step 2425, Train Loss: 2.2479, Test Loss: 2.3154, Learning Rate: 2.22e-05\n",
      "Step 2426, Train Loss: 2.4413, Test Loss: 2.2287, Learning Rate: 2.22e-05\n",
      "Step 2427, Train Loss: 2.3120, Test Loss: 2.1071, Learning Rate: 2.22e-05\n",
      "Step 2428, Train Loss: 2.4148, Test Loss: 2.2170, Learning Rate: 2.22e-05\n",
      "Step 2429, Train Loss: 2.3076, Test Loss: 2.3298, Learning Rate: 2.22e-05\n",
      "Step 2430, Train Loss: 2.2386, Test Loss: 2.3109, Learning Rate: 2.22e-05\n",
      "Step 2431, Train Loss: 2.2585, Test Loss: 2.4245, Learning Rate: 2.22e-05\n",
      "Step 2432, Train Loss: 2.1995, Test Loss: 2.3132, Learning Rate: 2.22e-05\n",
      "Step 2433, Train Loss: 2.3619, Test Loss: 2.2208, Learning Rate: 2.22e-05\n",
      "Step 2434, Train Loss: 2.3142, Test Loss: 2.1845, Learning Rate: 2.22e-05\n",
      "Step 2435, Train Loss: 2.3026, Test Loss: 2.6281, Learning Rate: 2.22e-05\n",
      "Step 2436, Train Loss: 2.4602, Test Loss: 2.4325, Learning Rate: 2.22e-05\n",
      "Step 2437, Train Loss: 2.4060, Test Loss: 2.4298, Learning Rate: 2.22e-05\n",
      "Step 2438, Train Loss: 2.2546, Test Loss: 2.3621, Learning Rate: 2.22e-05\n",
      "Step 2439, Train Loss: 2.3750, Test Loss: 2.3351, Learning Rate: 2.22e-05\n",
      "Step 2440, Train Loss: 2.4356, Test Loss: 2.3061, Learning Rate: 2.22e-05\n",
      "Step 2441, Train Loss: 2.3059, Test Loss: 2.3881, Learning Rate: 2.22e-05\n",
      "Step 2442, Train Loss: 2.3076, Test Loss: 2.1989, Learning Rate: 2.22e-05\n",
      "Step 2443, Train Loss: 2.4272, Test Loss: 2.2731, Learning Rate: 2.22e-05\n",
      "Step 2444, Train Loss: 2.2421, Test Loss: 2.2986, Learning Rate: 2.22e-05\n",
      "Step 2445, Train Loss: 2.3393, Test Loss: 2.5431, Learning Rate: 2.22e-05\n",
      "Step 2446, Train Loss: 2.3911, Test Loss: 2.4214, Learning Rate: 2.22e-05\n",
      "Step 2447, Train Loss: 2.3134, Test Loss: 2.3633, Learning Rate: 2.22e-05\n",
      "Step 2448, Train Loss: 2.4205, Test Loss: 2.3346, Learning Rate: 2.22e-05\n",
      "Step 2449, Train Loss: 2.4387, Test Loss: 2.2738, Learning Rate: 2.22e-05\n",
      "Step 2450, Train Loss: 2.3399, Test Loss: 2.0965, Learning Rate: 2.22e-05\n",
      "Step 2451, Train Loss: 2.3470, Test Loss: 2.1783, Learning Rate: 2.22e-05\n",
      "Step 2452, Train Loss: 2.3423, Test Loss: 2.3997, Learning Rate: 2.22e-05\n",
      "Step 2453, Train Loss: 2.4011, Test Loss: 2.4132, Learning Rate: 2.22e-05\n",
      "Step 2454, Train Loss: 2.2931, Test Loss: 2.3768, Learning Rate: 2.22e-05\n",
      "Step 2455, Train Loss: 2.3693, Test Loss: 2.3068, Learning Rate: 2.22e-05\n",
      "Step 2456, Train Loss: 2.3027, Test Loss: 2.3935, Learning Rate: 2.22e-05\n",
      "Step 2457, Train Loss: 2.3018, Test Loss: 2.1625, Learning Rate: 2.22e-05\n",
      "Step 2458, Train Loss: 2.3878, Test Loss: 2.3719, Learning Rate: 2.22e-05\n",
      "Step 2459, Train Loss: 2.3396, Test Loss: 2.3327, Learning Rate: 2.22e-05\n",
      "Step 2460, Train Loss: 2.3175, Test Loss: 2.3711, Learning Rate: 2.22e-05\n",
      "Step 2461, Train Loss: 2.3480, Test Loss: 2.4944, Learning Rate: 2.22e-05\n",
      "Step 2462, Train Loss: 2.3538, Test Loss: 2.3462, Learning Rate: 2.22e-05\n",
      "Step 2463, Train Loss: 2.3493, Test Loss: 2.2091, Learning Rate: 2.22e-05\n",
      "Step 2464, Train Loss: 2.2419, Test Loss: 2.3618, Learning Rate: 2.22e-05\n",
      "Step 2465, Train Loss: 2.3381, Test Loss: 2.2366, Learning Rate: 2.22e-05\n",
      "Step 2466, Train Loss: 2.3803, Test Loss: 2.1448, Learning Rate: 2.22e-05\n",
      "Step 2467, Train Loss: 2.3578, Test Loss: 2.2153, Learning Rate: 2.22e-05\n",
      "Step 2468, Train Loss: 2.1178, Test Loss: 2.5674, Learning Rate: 2.22e-05\n",
      "Step 2469, Train Loss: 2.3379, Test Loss: 2.3239, Learning Rate: 2.22e-05\n",
      "Step 2470, Train Loss: 2.4250, Test Loss: 2.4863, Learning Rate: 2.22e-05\n",
      "Step 2471, Train Loss: 2.1936, Test Loss: 2.2272, Learning Rate: 2.22e-05\n",
      "Step 2472, Train Loss: 2.3139, Test Loss: 2.2858, Learning Rate: 2.22e-05\n",
      "Step 2473, Train Loss: 2.3252, Test Loss: 2.3217, Learning Rate: 2.22e-05\n",
      "Step 2474, Train Loss: 2.3777, Test Loss: 2.1943, Learning Rate: 2.22e-05\n",
      "Step 2475, Train Loss: 2.2844, Test Loss: 2.4455, Learning Rate: 2.22e-05\n",
      "Step 2476, Train Loss: 2.2877, Test Loss: 2.4406, Learning Rate: 2.22e-05\n",
      "Step 2477, Train Loss: 2.3526, Test Loss: 2.3089, Learning Rate: 2.22e-05\n",
      "Step 2478, Train Loss: 2.3517, Test Loss: 2.4368, Learning Rate: 2.22e-05\n",
      "Step 2479, Train Loss: 2.3802, Test Loss: 2.5455, Learning Rate: 2.22e-05\n",
      "Step 2480, Train Loss: 2.3620, Test Loss: 2.1883, Learning Rate: 2.22e-05\n",
      "Step 2481, Train Loss: 2.3676, Test Loss: 2.3155, Learning Rate: 2.22e-05\n",
      "Step 2482, Train Loss: 2.3345, Test Loss: 2.4006, Learning Rate: 2.22e-05\n",
      "Step 2483, Train Loss: 2.2619, Test Loss: 2.3444, Learning Rate: 2.22e-05\n",
      "Step 2484, Train Loss: 2.2912, Test Loss: 2.2969, Learning Rate: 2.22e-05\n",
      "Step 2485, Train Loss: 2.3023, Test Loss: 2.3665, Learning Rate: 2.22e-05\n",
      "Step 2486, Train Loss: 2.4726, Test Loss: 2.2792, Learning Rate: 2.22e-05\n",
      "Step 2487, Train Loss: 2.4472, Test Loss: 2.2656, Learning Rate: 2.22e-05\n",
      "Step 2488, Train Loss: 2.3367, Test Loss: 2.0941, Learning Rate: 2.22e-05\n",
      "Step 2489, Train Loss: 2.3272, Test Loss: 2.3569, Learning Rate: 2.22e-05\n",
      "Step 2490, Train Loss: 2.3003, Test Loss: 2.3903, Learning Rate: 2.22e-05\n",
      "Step 2491, Train Loss: 2.3238, Test Loss: 2.3098, Learning Rate: 2.22e-05\n",
      "Step 2492, Train Loss: 2.2998, Test Loss: 2.5258, Learning Rate: 2.22e-05\n",
      "Step 2493, Train Loss: 2.4879, Test Loss: 2.3190, Learning Rate: 2.21e-05\n",
      "Step 2494, Train Loss: 2.2207, Test Loss: 2.3541, Learning Rate: 2.21e-05\n",
      "Step 2495, Train Loss: 2.4457, Test Loss: 2.4004, Learning Rate: 2.21e-05\n",
      "Step 2496, Train Loss: 2.3068, Test Loss: 2.2513, Learning Rate: 2.21e-05\n",
      "Step 2497, Train Loss: 2.3154, Test Loss: 2.3786, Learning Rate: 2.21e-05\n",
      "Step 2498, Train Loss: 2.3079, Test Loss: 2.1904, Learning Rate: 2.21e-05\n",
      "Step 2499, Train Loss: 2.4279, Test Loss: 2.3910, Learning Rate: 2.21e-05\n",
      "Step 2500, Train Loss: 2.3455, Test Loss: 2.3557, Learning Rate: 2.21e-05\n",
      "Step 2501, Train Loss: 2.3407, Test Loss: 2.2377, Learning Rate: 2.21e-05\n",
      "Step 2502, Train Loss: 2.2505, Test Loss: 2.5379, Learning Rate: 2.21e-05\n",
      "Step 2503, Train Loss: 2.3458, Test Loss: 2.3089, Learning Rate: 2.21e-05\n",
      "Step 2504, Train Loss: 2.2360, Test Loss: 2.3407, Learning Rate: 2.21e-05\n",
      "Step 2505, Train Loss: 2.3266, Test Loss: 2.2306, Learning Rate: 2.21e-05\n",
      "Step 2506, Train Loss: 2.3785, Test Loss: 2.2490, Learning Rate: 2.21e-05\n",
      "Step 2507, Train Loss: 2.3870, Test Loss: 2.2930, Learning Rate: 2.21e-05\n",
      "Step 2508, Train Loss: 2.3843, Test Loss: 2.5449, Learning Rate: 2.21e-05\n",
      "Step 2509, Train Loss: 2.3767, Test Loss: 2.3524, Learning Rate: 2.21e-05\n",
      "Step 2510, Train Loss: 2.2570, Test Loss: 2.4008, Learning Rate: 2.21e-05\n",
      "Step 2511, Train Loss: 2.3558, Test Loss: 2.5074, Learning Rate: 2.21e-05\n",
      "Step 2512, Train Loss: 2.3846, Test Loss: 2.4643, Learning Rate: 2.21e-05\n",
      "Step 2513, Train Loss: 2.3115, Test Loss: 2.2859, Learning Rate: 2.21e-05\n",
      "Step 2514, Train Loss: 2.3986, Test Loss: 2.2187, Learning Rate: 2.21e-05\n",
      "Step 2515, Train Loss: 2.2364, Test Loss: 2.2608, Learning Rate: 2.21e-05\n",
      "Step 2516, Train Loss: 2.3933, Test Loss: 2.6273, Learning Rate: 2.21e-05\n",
      "Step 2517, Train Loss: 2.3217, Test Loss: 2.2987, Learning Rate: 2.21e-05\n",
      "Step 2518, Train Loss: 2.3126, Test Loss: 2.3702, Learning Rate: 2.21e-05\n",
      "Step 2519, Train Loss: 2.3319, Test Loss: 2.3014, Learning Rate: 2.21e-05\n",
      "Step 2520, Train Loss: 2.2785, Test Loss: 2.3662, Learning Rate: 2.21e-05\n",
      "Step 2521, Train Loss: 2.3895, Test Loss: 2.2940, Learning Rate: 2.21e-05\n",
      "Step 2522, Train Loss: 2.3249, Test Loss: 2.3203, Learning Rate: 2.21e-05\n",
      "Step 2523, Train Loss: 2.3581, Test Loss: 2.4343, Learning Rate: 2.21e-05\n",
      "Step 2524, Train Loss: 2.4609, Test Loss: 2.1017, Learning Rate: 2.21e-05\n",
      "Step 2525, Train Loss: 2.2843, Test Loss: 2.2237, Learning Rate: 2.21e-05\n",
      "Step 2526, Train Loss: 2.3452, Test Loss: 2.3467, Learning Rate: 2.21e-05\n",
      "Step 2527, Train Loss: 2.3808, Test Loss: 2.2144, Learning Rate: 2.21e-05\n",
      "Step 2528, Train Loss: 2.2602, Test Loss: 2.3292, Learning Rate: 2.21e-05\n",
      "Step 2529, Train Loss: 2.3680, Test Loss: 2.5370, Learning Rate: 2.21e-05\n",
      "Step 2530, Train Loss: 2.3556, Test Loss: 2.4006, Learning Rate: 2.21e-05\n",
      "Step 2531, Train Loss: 2.3646, Test Loss: 2.4803, Learning Rate: 2.21e-05\n",
      "Step 2532, Train Loss: 2.3101, Test Loss: 2.4846, Learning Rate: 2.21e-05\n",
      "Step 2533, Train Loss: 2.3428, Test Loss: 2.5150, Learning Rate: 2.21e-05\n",
      "Step 2534, Train Loss: 2.2709, Test Loss: 2.4434, Learning Rate: 2.21e-05\n",
      "Step 2535, Train Loss: 2.3507, Test Loss: 2.4225, Learning Rate: 2.21e-05\n",
      "Step 2536, Train Loss: 2.3926, Test Loss: 2.2294, Learning Rate: 2.21e-05\n",
      "Step 2537, Train Loss: 2.4026, Test Loss: 2.2205, Learning Rate: 2.21e-05\n",
      "Step 2538, Train Loss: 2.2904, Test Loss: 1.9714, Learning Rate: 2.21e-05\n",
      "Step 2539, Train Loss: 2.3257, Test Loss: 2.3574, Learning Rate: 2.21e-05\n",
      "Step 2540, Train Loss: 2.2658, Test Loss: 2.4704, Learning Rate: 2.21e-05\n",
      "Step 2541, Train Loss: 2.2623, Test Loss: 2.4803, Learning Rate: 2.21e-05\n",
      "Step 2542, Train Loss: 2.3426, Test Loss: 2.3075, Learning Rate: 2.21e-05\n",
      "Step 2543, Train Loss: 2.2690, Test Loss: 2.3539, Learning Rate: 2.21e-05\n",
      "Step 2544, Train Loss: 2.4029, Test Loss: 2.4366, Learning Rate: 2.21e-05\n",
      "Step 2545, Train Loss: 2.3387, Test Loss: 2.1568, Learning Rate: 2.21e-05\n",
      "Step 2546, Train Loss: 2.3429, Test Loss: 2.3778, Learning Rate: 2.21e-05\n",
      "Step 2547, Train Loss: 2.2796, Test Loss: 2.3354, Learning Rate: 2.21e-05\n",
      "Step 2548, Train Loss: 2.3080, Test Loss: 2.3056, Learning Rate: 2.21e-05\n",
      "Step 2549, Train Loss: 2.3756, Test Loss: 2.3939, Learning Rate: 2.21e-05\n",
      "Step 2550, Train Loss: 2.2001, Test Loss: 2.2639, Learning Rate: 2.21e-05\n",
      "Step 2551, Train Loss: 2.2977, Test Loss: 2.1161, Learning Rate: 2.21e-05\n",
      "Step 2552, Train Loss: 2.2821, Test Loss: 2.2596, Learning Rate: 2.21e-05\n",
      "Step 2553, Train Loss: 2.3505, Test Loss: 2.1834, Learning Rate: 2.21e-05\n",
      "Step 2554, Train Loss: 2.2758, Test Loss: 2.3795, Learning Rate: 2.21e-05\n",
      "Step 2555, Train Loss: 2.4001, Test Loss: 2.3088, Learning Rate: 2.21e-05\n",
      "Step 2556, Train Loss: 2.4287, Test Loss: 2.3884, Learning Rate: 2.21e-05\n",
      "Step 2557, Train Loss: 2.3864, Test Loss: 2.0970, Learning Rate: 2.21e-05\n",
      "Step 2558, Train Loss: 2.3878, Test Loss: 2.3305, Learning Rate: 2.21e-05\n",
      "Step 2559, Train Loss: 2.3662, Test Loss: 2.3044, Learning Rate: 2.21e-05\n",
      "Step 2560, Train Loss: 2.3659, Test Loss: 2.2650, Learning Rate: 2.21e-05\n",
      "Step 2561, Train Loss: 2.3201, Test Loss: 2.3349, Learning Rate: 2.21e-05\n",
      "Step 2562, Train Loss: 2.3558, Test Loss: 2.3873, Learning Rate: 2.21e-05\n",
      "Step 2563, Train Loss: 2.4315, Test Loss: 2.4205, Learning Rate: 2.21e-05\n",
      "Step 2564, Train Loss: 2.3559, Test Loss: 2.4632, Learning Rate: 2.21e-05\n",
      "Step 2565, Train Loss: 2.3281, Test Loss: 2.1958, Learning Rate: 2.21e-05\n",
      "Step 2566, Train Loss: 2.3216, Test Loss: 2.4163, Learning Rate: 2.21e-05\n",
      "Step 2567, Train Loss: 2.2667, Test Loss: 2.2269, Learning Rate: 2.21e-05\n",
      "Step 2568, Train Loss: 2.3472, Test Loss: 2.4200, Learning Rate: 2.21e-05\n",
      "Step 2569, Train Loss: 2.5591, Test Loss: 2.3401, Learning Rate: 2.21e-05\n",
      "Step 2570, Train Loss: 2.2902, Test Loss: 2.2082, Learning Rate: 2.21e-05\n",
      "Step 2571, Train Loss: 2.3785, Test Loss: 2.4294, Learning Rate: 2.21e-05\n",
      "Step 2572, Train Loss: 2.2725, Test Loss: 2.4352, Learning Rate: 2.21e-05\n",
      "Step 2573, Train Loss: 2.2452, Test Loss: 2.5519, Learning Rate: 2.21e-05\n",
      "Step 2574, Train Loss: 2.4826, Test Loss: 2.2694, Learning Rate: 2.21e-05\n",
      "Step 2575, Train Loss: 2.1800, Test Loss: 2.2805, Learning Rate: 2.21e-05\n",
      "Step 2576, Train Loss: 2.4271, Test Loss: 2.2471, Learning Rate: 2.21e-05\n",
      "Step 2577, Train Loss: 2.4294, Test Loss: 2.3091, Learning Rate: 2.21e-05\n",
      "Step 2578, Train Loss: 2.5425, Test Loss: 2.3501, Learning Rate: 2.21e-05\n",
      "Step 2579, Train Loss: 2.3535, Test Loss: 2.1818, Learning Rate: 2.21e-05\n",
      "Step 2580, Train Loss: 2.3336, Test Loss: 2.5585, Learning Rate: 2.21e-05\n",
      "Step 2581, Train Loss: 2.1843, Test Loss: 2.4212, Learning Rate: 2.21e-05\n",
      "Step 2582, Train Loss: 2.2793, Test Loss: 2.4538, Learning Rate: 2.21e-05\n",
      "Step 2583, Train Loss: 2.3602, Test Loss: 2.2740, Learning Rate: 2.21e-05\n",
      "Step 2584, Train Loss: 2.1964, Test Loss: 2.2600, Learning Rate: 2.21e-05\n",
      "Step 2585, Train Loss: 2.3646, Test Loss: 2.2350, Learning Rate: 2.21e-05\n",
      "Step 2586, Train Loss: 2.4492, Test Loss: 2.2569, Learning Rate: 2.21e-05\n",
      "Step 2587, Train Loss: 2.3523, Test Loss: 2.1742, Learning Rate: 2.21e-05\n",
      "Step 2588, Train Loss: 2.4397, Test Loss: 2.3781, Learning Rate: 2.21e-05\n",
      "Step 2589, Train Loss: 2.3460, Test Loss: 2.3420, Learning Rate: 2.21e-05\n",
      "Step 2590, Train Loss: 2.4183, Test Loss: 2.2135, Learning Rate: 2.21e-05\n",
      "Step 2591, Train Loss: 2.3572, Test Loss: 2.2122, Learning Rate: 2.21e-05\n",
      "Step 2592, Train Loss: 2.3329, Test Loss: 2.2573, Learning Rate: 2.21e-05\n",
      "Step 2593, Train Loss: 2.4462, Test Loss: 2.2475, Learning Rate: 2.21e-05\n",
      "Step 2594, Train Loss: 2.2089, Test Loss: 2.2786, Learning Rate: 2.21e-05\n",
      "Step 2595, Train Loss: 2.4068, Test Loss: 2.3802, Learning Rate: 2.21e-05\n",
      "Step 2596, Train Loss: 2.3492, Test Loss: 2.3145, Learning Rate: 2.21e-05\n",
      "Step 2597, Train Loss: 2.4858, Test Loss: 2.3288, Learning Rate: 2.21e-05\n",
      "Step 2598, Train Loss: 2.3006, Test Loss: 2.3487, Learning Rate: 2.21e-05\n",
      "Step 2599, Train Loss: 2.3156, Test Loss: 2.2221, Learning Rate: 2.21e-05\n",
      "Step 2600, Train Loss: 2.2738, Test Loss: 2.3099, Learning Rate: 2.21e-05\n",
      "Step 2601, Train Loss: 2.4914, Test Loss: 2.3875, Learning Rate: 2.21e-05\n",
      "Step 2602, Train Loss: 2.2265, Test Loss: 2.4300, Learning Rate: 2.21e-05\n",
      "Step 2603, Train Loss: 2.2579, Test Loss: 2.3571, Learning Rate: 2.21e-05\n",
      "Step 2604, Train Loss: 2.3014, Test Loss: 2.2224, Learning Rate: 2.21e-05\n",
      "Step 2605, Train Loss: 2.2675, Test Loss: 2.2819, Learning Rate: 2.21e-05\n",
      "Step 2606, Train Loss: 2.2599, Test Loss: 2.3578, Learning Rate: 2.21e-05\n",
      "Step 2607, Train Loss: 2.4340, Test Loss: 2.3891, Learning Rate: 2.21e-05\n",
      "Step 2608, Train Loss: 2.2246, Test Loss: 2.2257, Learning Rate: 2.21e-05\n",
      "Step 2609, Train Loss: 2.2810, Test Loss: 2.3738, Learning Rate: 2.21e-05\n",
      "Step 2610, Train Loss: 2.2583, Test Loss: 2.3662, Learning Rate: 2.21e-05\n",
      "Step 2611, Train Loss: 2.3287, Test Loss: 2.2343, Learning Rate: 2.21e-05\n",
      "Step 2612, Train Loss: 2.3020, Test Loss: 2.3884, Learning Rate: 2.21e-05\n",
      "Step 2613, Train Loss: 2.2661, Test Loss: 2.2259, Learning Rate: 2.21e-05\n",
      "Step 2614, Train Loss: 2.4497, Test Loss: 2.4131, Learning Rate: 2.21e-05\n",
      "Step 2615, Train Loss: 2.1910, Test Loss: 2.3969, Learning Rate: 2.21e-05\n",
      "Step 2616, Train Loss: 2.4785, Test Loss: 2.2724, Learning Rate: 2.21e-05\n",
      "Step 2617, Train Loss: 2.2752, Test Loss: 2.3680, Learning Rate: 2.21e-05\n",
      "Step 2618, Train Loss: 2.3322, Test Loss: 2.4652, Learning Rate: 2.21e-05\n",
      "Step 2619, Train Loss: 2.3190, Test Loss: 2.3028, Learning Rate: 2.21e-05\n",
      "Step 2620, Train Loss: 2.1523, Test Loss: 2.3028, Learning Rate: 2.21e-05\n",
      "Step 2621, Train Loss: 2.2694, Test Loss: 2.1665, Learning Rate: 2.21e-05\n",
      "Step 2622, Train Loss: 2.3237, Test Loss: 2.2482, Learning Rate: 2.21e-05\n",
      "Step 2623, Train Loss: 2.2403, Test Loss: 2.2687, Learning Rate: 2.20e-05\n",
      "Step 2624, Train Loss: 2.3175, Test Loss: 2.2555, Learning Rate: 2.20e-05\n",
      "Step 2625, Train Loss: 2.1806, Test Loss: 2.3218, Learning Rate: 2.20e-05\n",
      "Step 2626, Train Loss: 2.1962, Test Loss: 2.3192, Learning Rate: 2.20e-05\n",
      "Step 2627, Train Loss: 2.3711, Test Loss: 2.3355, Learning Rate: 2.20e-05\n",
      "Step 2628, Train Loss: 2.3177, Test Loss: 2.1407, Learning Rate: 2.20e-05\n",
      "Step 2629, Train Loss: 2.3429, Test Loss: 2.3968, Learning Rate: 2.20e-05\n",
      "Step 2630, Train Loss: 2.2072, Test Loss: 2.3806, Learning Rate: 2.20e-05\n",
      "Step 2631, Train Loss: 2.1989, Test Loss: 2.2854, Learning Rate: 2.20e-05\n",
      "Step 2632, Train Loss: 2.3432, Test Loss: 2.3115, Learning Rate: 2.20e-05\n",
      "Step 2633, Train Loss: 2.5246, Test Loss: 2.3823, Learning Rate: 2.20e-05\n",
      "Step 2634, Train Loss: 2.3551, Test Loss: 2.4857, Learning Rate: 2.20e-05\n",
      "Step 2635, Train Loss: 2.3068, Test Loss: 2.2305, Learning Rate: 2.20e-05\n",
      "Step 2636, Train Loss: 2.2770, Test Loss: 2.3311, Learning Rate: 2.20e-05\n",
      "Step 2637, Train Loss: 2.4541, Test Loss: 2.3444, Learning Rate: 2.20e-05\n",
      "Step 2638, Train Loss: 2.3006, Test Loss: 2.2077, Learning Rate: 2.20e-05\n",
      "Step 2639, Train Loss: 2.3387, Test Loss: 2.4245, Learning Rate: 2.20e-05\n",
      "Step 2640, Train Loss: 2.3600, Test Loss: 2.2958, Learning Rate: 2.20e-05\n",
      "Step 2641, Train Loss: 2.1297, Test Loss: 2.0373, Learning Rate: 2.20e-05\n",
      "Step 2642, Train Loss: 2.2447, Test Loss: 2.2227, Learning Rate: 2.20e-05\n",
      "Step 2643, Train Loss: 2.3402, Test Loss: 2.5741, Learning Rate: 2.20e-05\n",
      "Step 2644, Train Loss: 2.3789, Test Loss: 2.0901, Learning Rate: 2.20e-05\n",
      "Step 2645, Train Loss: 2.2225, Test Loss: 2.1868, Learning Rate: 2.20e-05\n",
      "Step 2646, Train Loss: 2.3096, Test Loss: 2.1897, Learning Rate: 2.20e-05\n",
      "Step 2647, Train Loss: 2.2680, Test Loss: 2.2323, Learning Rate: 2.20e-05\n",
      "Step 2648, Train Loss: 2.3485, Test Loss: 1.9508, Learning Rate: 2.20e-05\n",
      "Step 2649, Train Loss: 2.3763, Test Loss: 2.2886, Learning Rate: 2.20e-05\n",
      "Step 2650, Train Loss: 2.3442, Test Loss: 2.4012, Learning Rate: 2.20e-05\n",
      "Step 2651, Train Loss: 2.2160, Test Loss: 2.3719, Learning Rate: 2.20e-05\n",
      "Step 2652, Train Loss: 2.2224, Test Loss: 2.3053, Learning Rate: 2.20e-05\n",
      "Step 2653, Train Loss: 2.3440, Test Loss: 2.2174, Learning Rate: 2.20e-05\n",
      "Step 2654, Train Loss: 2.2894, Test Loss: 2.3434, Learning Rate: 2.20e-05\n",
      "Step 2655, Train Loss: 2.3303, Test Loss: 2.3267, Learning Rate: 2.20e-05\n",
      "Step 2656, Train Loss: 2.2952, Test Loss: 2.6198, Learning Rate: 2.20e-05\n",
      "Step 2657, Train Loss: 2.3252, Test Loss: 2.1967, Learning Rate: 2.20e-05\n",
      "Step 2658, Train Loss: 2.4269, Test Loss: 2.2606, Learning Rate: 2.20e-05\n",
      "Step 2659, Train Loss: 2.2474, Test Loss: 2.3080, Learning Rate: 2.20e-05\n",
      "Step 2660, Train Loss: 2.4658, Test Loss: 2.1890, Learning Rate: 2.20e-05\n",
      "Step 2661, Train Loss: 2.4009, Test Loss: 2.2400, Learning Rate: 2.20e-05\n",
      "Step 2662, Train Loss: 2.2977, Test Loss: 2.1418, Learning Rate: 2.20e-05\n",
      "Step 2663, Train Loss: 2.3568, Test Loss: 2.3027, Learning Rate: 2.20e-05\n",
      "Step 2664, Train Loss: 2.2765, Test Loss: 2.4275, Learning Rate: 2.20e-05\n",
      "Step 2665, Train Loss: 2.3385, Test Loss: 2.2534, Learning Rate: 2.20e-05\n",
      "Step 2666, Train Loss: 2.5320, Test Loss: 2.4425, Learning Rate: 2.20e-05\n",
      "Step 2667, Train Loss: 2.4403, Test Loss: 2.3273, Learning Rate: 2.20e-05\n",
      "Step 2668, Train Loss: 2.0935, Test Loss: 2.2130, Learning Rate: 2.20e-05\n",
      "Step 2669, Train Loss: 2.3886, Test Loss: 2.3962, Learning Rate: 2.20e-05\n",
      "Step 2670, Train Loss: 2.1226, Test Loss: 2.3184, Learning Rate: 2.20e-05\n",
      "Step 2671, Train Loss: 2.1656, Test Loss: 2.2088, Learning Rate: 2.20e-05\n",
      "Step 2672, Train Loss: 2.5055, Test Loss: 2.3599, Learning Rate: 2.20e-05\n",
      "Step 2673, Train Loss: 2.3701, Test Loss: 2.3982, Learning Rate: 2.20e-05\n",
      "Step 2674, Train Loss: 2.4828, Test Loss: 2.3839, Learning Rate: 2.20e-05\n",
      "Step 2675, Train Loss: 2.4016, Test Loss: 2.1842, Learning Rate: 2.20e-05\n",
      "Step 2676, Train Loss: 2.2988, Test Loss: 2.2694, Learning Rate: 2.20e-05\n",
      "Step 2677, Train Loss: 2.3156, Test Loss: 2.6716, Learning Rate: 2.20e-05\n",
      "Step 2678, Train Loss: 2.3373, Test Loss: 2.5628, Learning Rate: 2.20e-05\n",
      "Step 2679, Train Loss: 2.4890, Test Loss: 2.4581, Learning Rate: 2.20e-05\n",
      "Step 2680, Train Loss: 2.2756, Test Loss: 2.2712, Learning Rate: 2.20e-05\n",
      "Step 2681, Train Loss: 2.1057, Test Loss: 2.3689, Learning Rate: 2.20e-05\n",
      "Step 2682, Train Loss: 2.3709, Test Loss: 2.3433, Learning Rate: 2.20e-05\n",
      "Step 2683, Train Loss: 2.3558, Test Loss: 2.3447, Learning Rate: 2.20e-05\n",
      "Step 2684, Train Loss: 2.2575, Test Loss: 2.2275, Learning Rate: 2.20e-05\n",
      "Step 2685, Train Loss: 2.2872, Test Loss: 2.4865, Learning Rate: 2.20e-05\n",
      "Step 2686, Train Loss: 2.4220, Test Loss: 2.4006, Learning Rate: 2.20e-05\n",
      "Step 2687, Train Loss: 2.2832, Test Loss: 2.3505, Learning Rate: 2.20e-05\n",
      "Step 2688, Train Loss: 2.2850, Test Loss: 2.2394, Learning Rate: 2.20e-05\n",
      "Step 2689, Train Loss: 2.3332, Test Loss: 2.1851, Learning Rate: 2.20e-05\n",
      "Step 2690, Train Loss: 2.3688, Test Loss: 2.3256, Learning Rate: 2.20e-05\n",
      "Step 2691, Train Loss: 2.4393, Test Loss: 2.5174, Learning Rate: 2.20e-05\n",
      "Step 2692, Train Loss: 2.3333, Test Loss: 2.4424, Learning Rate: 2.20e-05\n",
      "Step 2693, Train Loss: 2.2804, Test Loss: 2.4788, Learning Rate: 2.20e-05\n",
      "Step 2694, Train Loss: 2.2645, Test Loss: 2.2161, Learning Rate: 2.20e-05\n",
      "Step 2695, Train Loss: 2.4045, Test Loss: 2.3162, Learning Rate: 2.20e-05\n",
      "Step 2696, Train Loss: 2.2893, Test Loss: 2.3999, Learning Rate: 2.20e-05\n",
      "Step 2697, Train Loss: 2.4999, Test Loss: 2.3466, Learning Rate: 2.20e-05\n",
      "Step 2698, Train Loss: 2.3641, Test Loss: 2.2429, Learning Rate: 2.20e-05\n",
      "Step 2699, Train Loss: 2.2457, Test Loss: 2.3483, Learning Rate: 2.20e-05\n",
      "Step 2700, Train Loss: 2.3797, Test Loss: 2.3254, Learning Rate: 2.20e-05\n",
      "Step 2701, Train Loss: 2.3518, Test Loss: 2.3937, Learning Rate: 2.20e-05\n",
      "Step 2702, Train Loss: 2.2477, Test Loss: 2.4261, Learning Rate: 2.20e-05\n",
      "Step 2703, Train Loss: 2.3724, Test Loss: 2.1902, Learning Rate: 2.20e-05\n",
      "Step 2704, Train Loss: 2.3459, Test Loss: 2.2787, Learning Rate: 2.20e-05\n",
      "Step 2705, Train Loss: 2.3251, Test Loss: 2.7433, Learning Rate: 2.20e-05\n",
      "Step 2706, Train Loss: 2.3509, Test Loss: 2.3007, Learning Rate: 2.20e-05\n",
      "Step 2707, Train Loss: 2.2000, Test Loss: 2.2529, Learning Rate: 2.20e-05\n",
      "Step 2708, Train Loss: 2.4263, Test Loss: 2.2922, Learning Rate: 2.20e-05\n",
      "Step 2709, Train Loss: 2.3866, Test Loss: 2.4219, Learning Rate: 2.20e-05\n",
      "Step 2710, Train Loss: 2.3388, Test Loss: 2.2498, Learning Rate: 2.20e-05\n",
      "Step 2711, Train Loss: 2.3592, Test Loss: 2.0207, Learning Rate: 2.20e-05\n",
      "Step 2712, Train Loss: 2.3985, Test Loss: 2.2545, Learning Rate: 2.20e-05\n",
      "Step 2713, Train Loss: 2.3593, Test Loss: 2.4997, Learning Rate: 2.20e-05\n",
      "Step 2714, Train Loss: 2.3630, Test Loss: 2.3870, Learning Rate: 2.20e-05\n",
      "Step 2715, Train Loss: 2.2708, Test Loss: 2.4368, Learning Rate: 2.20e-05\n",
      "Step 2716, Train Loss: 2.2615, Test Loss: 2.5707, Learning Rate: 2.20e-05\n",
      "Step 2717, Train Loss: 2.3059, Test Loss: 2.2775, Learning Rate: 2.20e-05\n",
      "Step 2718, Train Loss: 2.3087, Test Loss: 2.2336, Learning Rate: 2.20e-05\n",
      "Step 2719, Train Loss: 2.4214, Test Loss: 2.2203, Learning Rate: 2.20e-05\n",
      "Step 2720, Train Loss: 2.4365, Test Loss: 2.3830, Learning Rate: 2.20e-05\n",
      "Step 2721, Train Loss: 2.3283, Test Loss: 2.3078, Learning Rate: 2.20e-05\n",
      "Step 2722, Train Loss: 2.2362, Test Loss: 2.3591, Learning Rate: 2.20e-05\n",
      "Step 2723, Train Loss: 2.4136, Test Loss: 2.3287, Learning Rate: 2.20e-05\n",
      "Step 2724, Train Loss: 2.3858, Test Loss: 2.3727, Learning Rate: 2.20e-05\n",
      "Step 2725, Train Loss: 2.4524, Test Loss: 2.3334, Learning Rate: 2.20e-05\n",
      "Step 2726, Train Loss: 2.2487, Test Loss: 2.2211, Learning Rate: 2.20e-05\n",
      "Step 2727, Train Loss: 2.2016, Test Loss: 2.3984, Learning Rate: 2.20e-05\n",
      "Step 2728, Train Loss: 2.2986, Test Loss: 2.2356, Learning Rate: 2.20e-05\n",
      "Step 2729, Train Loss: 2.4187, Test Loss: 2.1195, Learning Rate: 2.20e-05\n",
      "Step 2730, Train Loss: 2.2116, Test Loss: 2.2900, Learning Rate: 2.20e-05\n",
      "Step 2731, Train Loss: 2.3706, Test Loss: 2.3208, Learning Rate: 2.20e-05\n",
      "Step 2732, Train Loss: 2.2502, Test Loss: 2.3699, Learning Rate: 2.20e-05\n",
      "Step 2733, Train Loss: 2.4285, Test Loss: 2.2490, Learning Rate: 2.20e-05\n",
      "Step 2734, Train Loss: 2.3827, Test Loss: 2.4167, Learning Rate: 2.20e-05\n",
      "Step 2735, Train Loss: 2.4508, Test Loss: 2.6063, Learning Rate: 2.20e-05\n",
      "Step 2736, Train Loss: 2.3340, Test Loss: 2.4417, Learning Rate: 2.20e-05\n",
      "Step 2737, Train Loss: 2.2946, Test Loss: 2.2774, Learning Rate: 2.20e-05\n",
      "Step 2738, Train Loss: 2.2894, Test Loss: 2.2395, Learning Rate: 2.20e-05\n",
      "Step 2739, Train Loss: 2.4157, Test Loss: 2.0842, Learning Rate: 2.20e-05\n",
      "Step 2740, Train Loss: 2.2014, Test Loss: 2.2277, Learning Rate: 2.20e-05\n",
      "Step 2741, Train Loss: 2.3077, Test Loss: 2.3769, Learning Rate: 2.20e-05\n",
      "Step 2742, Train Loss: 2.3772, Test Loss: 2.2087, Learning Rate: 2.20e-05\n",
      "Step 2743, Train Loss: 2.3283, Test Loss: 2.3686, Learning Rate: 2.20e-05\n",
      "Step 2744, Train Loss: 2.3122, Test Loss: 2.2834, Learning Rate: 2.20e-05\n",
      "Step 2745, Train Loss: 2.3319, Test Loss: 2.3778, Learning Rate: 2.20e-05\n",
      "Step 2746, Train Loss: 2.3102, Test Loss: 2.2732, Learning Rate: 2.20e-05\n",
      "Step 2747, Train Loss: 2.1963, Test Loss: 2.1989, Learning Rate: 2.20e-05\n",
      "Step 2748, Train Loss: 2.4759, Test Loss: 2.3966, Learning Rate: 2.20e-05\n",
      "Step 2749, Train Loss: 2.3011, Test Loss: 2.5620, Learning Rate: 2.20e-05\n",
      "Step 2750, Train Loss: 2.2688, Test Loss: 2.1852, Learning Rate: 2.20e-05\n",
      "Step 2751, Train Loss: 2.3147, Test Loss: 2.3843, Learning Rate: 2.20e-05\n",
      "Step 2752, Train Loss: 2.4654, Test Loss: 2.2389, Learning Rate: 2.20e-05\n",
      "Step 2753, Train Loss: 2.2263, Test Loss: 2.2100, Learning Rate: 2.20e-05\n",
      "Step 2754, Train Loss: 2.3990, Test Loss: 2.4855, Learning Rate: 2.20e-05\n",
      "Step 2755, Train Loss: 2.3120, Test Loss: 2.1819, Learning Rate: 2.19e-05\n",
      "Step 2756, Train Loss: 2.4820, Test Loss: 2.2645, Learning Rate: 2.19e-05\n",
      "Step 2757, Train Loss: 2.3427, Test Loss: 2.1934, Learning Rate: 2.19e-05\n",
      "Step 2758, Train Loss: 2.1833, Test Loss: 2.4773, Learning Rate: 2.19e-05\n",
      "Step 2759, Train Loss: 2.3753, Test Loss: 2.2038, Learning Rate: 2.19e-05\n",
      "Step 2760, Train Loss: 2.3880, Test Loss: 2.2164, Learning Rate: 2.19e-05\n",
      "Step 2761, Train Loss: 2.4012, Test Loss: 2.4182, Learning Rate: 2.19e-05\n",
      "Step 2762, Train Loss: 2.2982, Test Loss: 2.2935, Learning Rate: 2.19e-05\n",
      "Step 2763, Train Loss: 2.4010, Test Loss: 2.3888, Learning Rate: 2.19e-05\n",
      "Step 2764, Train Loss: 2.3016, Test Loss: 2.3291, Learning Rate: 2.19e-05\n",
      "Step 2765, Train Loss: 2.3794, Test Loss: 2.2292, Learning Rate: 2.19e-05\n",
      "Step 2766, Train Loss: 2.1805, Test Loss: 2.4366, Learning Rate: 2.19e-05\n",
      "Step 2767, Train Loss: 2.4229, Test Loss: 2.2200, Learning Rate: 2.19e-05\n",
      "Step 2768, Train Loss: 2.2307, Test Loss: 2.3611, Learning Rate: 2.19e-05\n",
      "Step 2769, Train Loss: 2.2065, Test Loss: 2.4352, Learning Rate: 2.19e-05\n",
      "Step 2770, Train Loss: 2.3302, Test Loss: 2.4562, Learning Rate: 2.19e-05\n",
      "Step 2771, Train Loss: 2.2585, Test Loss: 2.2634, Learning Rate: 2.19e-05\n",
      "Step 2772, Train Loss: 2.3635, Test Loss: 2.3156, Learning Rate: 2.19e-05\n",
      "Step 2773, Train Loss: 2.1663, Test Loss: 2.2742, Learning Rate: 2.19e-05\n",
      "Step 2774, Train Loss: 2.2423, Test Loss: 2.3615, Learning Rate: 2.19e-05\n",
      "Step 2775, Train Loss: 2.3311, Test Loss: 2.2216, Learning Rate: 2.19e-05\n",
      "Step 2776, Train Loss: 2.5929, Test Loss: 2.2147, Learning Rate: 2.19e-05\n",
      "Step 2777, Train Loss: 2.3681, Test Loss: 2.2609, Learning Rate: 2.19e-05\n",
      "Step 2778, Train Loss: 2.3483, Test Loss: 2.5473, Learning Rate: 2.19e-05\n",
      "Step 2779, Train Loss: 2.3380, Test Loss: 2.3354, Learning Rate: 2.19e-05\n",
      "Step 2780, Train Loss: 2.4332, Test Loss: 2.3429, Learning Rate: 2.19e-05\n",
      "Step 2781, Train Loss: 2.3720, Test Loss: 2.1167, Learning Rate: 2.19e-05\n",
      "Step 2782, Train Loss: 2.3248, Test Loss: 2.1651, Learning Rate: 2.19e-05\n",
      "Step 2783, Train Loss: 2.4301, Test Loss: 2.3529, Learning Rate: 2.19e-05\n",
      "Step 2784, Train Loss: 2.3556, Test Loss: 2.6100, Learning Rate: 2.19e-05\n",
      "Step 2785, Train Loss: 2.3963, Test Loss: 2.4023, Learning Rate: 2.19e-05\n",
      "Step 2786, Train Loss: 2.4107, Test Loss: 2.1894, Learning Rate: 2.19e-05\n",
      "Step 2787, Train Loss: 2.3723, Test Loss: 2.2955, Learning Rate: 2.19e-05\n",
      "Step 2788, Train Loss: 2.2325, Test Loss: 2.1205, Learning Rate: 2.19e-05\n",
      "Step 2789, Train Loss: 2.4156, Test Loss: 2.2011, Learning Rate: 2.19e-05\n",
      "Step 2790, Train Loss: 2.3162, Test Loss: 2.2991, Learning Rate: 2.19e-05\n",
      "Step 2791, Train Loss: 2.3473, Test Loss: 2.2851, Learning Rate: 2.19e-05\n",
      "Step 2792, Train Loss: 2.2505, Test Loss: 2.4402, Learning Rate: 2.19e-05\n",
      "Step 2793, Train Loss: 2.3061, Test Loss: 2.3763, Learning Rate: 2.19e-05\n",
      "Step 2794, Train Loss: 2.2826, Test Loss: 2.4556, Learning Rate: 2.19e-05\n",
      "Step 2795, Train Loss: 2.3399, Test Loss: 2.3285, Learning Rate: 2.19e-05\n",
      "Step 2796, Train Loss: 2.3788, Test Loss: 2.4386, Learning Rate: 2.19e-05\n",
      "Step 2797, Train Loss: 2.1731, Test Loss: 2.4388, Learning Rate: 2.19e-05\n",
      "Step 2798, Train Loss: 2.3356, Test Loss: 2.3034, Learning Rate: 2.19e-05\n",
      "Step 2799, Train Loss: 2.2946, Test Loss: 2.4195, Learning Rate: 2.19e-05\n",
      "Step 2800, Train Loss: 2.4168, Test Loss: 2.2100, Learning Rate: 2.19e-05\n",
      "Step 2801, Train Loss: 2.2872, Test Loss: 2.2787, Learning Rate: 2.19e-05\n",
      "Step 2802, Train Loss: 2.3968, Test Loss: 2.2164, Learning Rate: 2.19e-05\n",
      "Step 2803, Train Loss: 2.3244, Test Loss: 2.3052, Learning Rate: 2.19e-05\n",
      "Step 2804, Train Loss: 2.2799, Test Loss: 2.1911, Learning Rate: 2.19e-05\n",
      "Step 2805, Train Loss: 2.2625, Test Loss: 2.2886, Learning Rate: 2.19e-05\n",
      "Step 2806, Train Loss: 2.3965, Test Loss: 2.2307, Learning Rate: 2.19e-05\n",
      "Step 2807, Train Loss: 2.2610, Test Loss: 2.4471, Learning Rate: 2.19e-05\n",
      "Step 2808, Train Loss: 2.3220, Test Loss: 2.4272, Learning Rate: 2.19e-05\n",
      "Step 2809, Train Loss: 2.3245, Test Loss: 2.2083, Learning Rate: 2.19e-05\n",
      "Step 2810, Train Loss: 2.2915, Test Loss: 2.1303, Learning Rate: 2.19e-05\n",
      "Step 2811, Train Loss: 2.4225, Test Loss: 2.2175, Learning Rate: 2.19e-05\n",
      "Step 2812, Train Loss: 2.2420, Test Loss: 2.1642, Learning Rate: 2.19e-05\n",
      "Step 2813, Train Loss: 2.5349, Test Loss: 2.2483, Learning Rate: 2.19e-05\n",
      "Step 2814, Train Loss: 2.2944, Test Loss: 2.3105, Learning Rate: 2.19e-05\n",
      "Step 2815, Train Loss: 2.2709, Test Loss: 2.1641, Learning Rate: 2.19e-05\n",
      "Step 2816, Train Loss: 2.2533, Test Loss: 2.2003, Learning Rate: 2.19e-05\n",
      "Step 2817, Train Loss: 2.3014, Test Loss: 2.2785, Learning Rate: 2.19e-05\n",
      "Step 2818, Train Loss: 2.3387, Test Loss: 2.3993, Learning Rate: 2.19e-05\n",
      "Step 2819, Train Loss: 2.5012, Test Loss: 2.5887, Learning Rate: 2.19e-05\n",
      "Step 2820, Train Loss: 2.3640, Test Loss: 2.3076, Learning Rate: 2.19e-05\n",
      "Step 2821, Train Loss: 2.3657, Test Loss: 2.4945, Learning Rate: 2.19e-05\n",
      "Step 2822, Train Loss: 2.4277, Test Loss: 2.3308, Learning Rate: 2.19e-05\n",
      "Step 2823, Train Loss: 2.2655, Test Loss: 2.0259, Learning Rate: 2.19e-05\n",
      "Step 2824, Train Loss: 2.3795, Test Loss: 2.3875, Learning Rate: 2.19e-05\n",
      "Step 2825, Train Loss: 2.3086, Test Loss: 2.1054, Learning Rate: 2.19e-05\n",
      "Step 2826, Train Loss: 2.3302, Test Loss: 2.2677, Learning Rate: 2.19e-05\n",
      "Step 2827, Train Loss: 2.3912, Test Loss: 2.3569, Learning Rate: 2.19e-05\n",
      "Step 2828, Train Loss: 2.3527, Test Loss: 2.2123, Learning Rate: 2.19e-05\n",
      "Step 2829, Train Loss: 2.3426, Test Loss: 2.3895, Learning Rate: 2.19e-05\n",
      "Step 2830, Train Loss: 2.3144, Test Loss: 2.2328, Learning Rate: 2.19e-05\n",
      "Step 2831, Train Loss: 2.3029, Test Loss: 2.1619, Learning Rate: 2.19e-05\n",
      "Step 2832, Train Loss: 2.2116, Test Loss: 2.3880, Learning Rate: 2.19e-05\n",
      "Step 2833, Train Loss: 2.4233, Test Loss: 2.1616, Learning Rate: 2.19e-05\n",
      "Step 2834, Train Loss: 2.3534, Test Loss: 2.4369, Learning Rate: 2.19e-05\n",
      "Step 2835, Train Loss: 2.2900, Test Loss: 2.3729, Learning Rate: 2.19e-05\n",
      "Step 2836, Train Loss: 2.3522, Test Loss: 2.1928, Learning Rate: 2.19e-05\n",
      "Step 2837, Train Loss: 2.3050, Test Loss: 2.2042, Learning Rate: 2.19e-05\n",
      "Step 2838, Train Loss: 2.4513, Test Loss: 2.1569, Learning Rate: 2.19e-05\n",
      "Step 2839, Train Loss: 2.3440, Test Loss: 2.3398, Learning Rate: 2.19e-05\n",
      "Step 2840, Train Loss: 2.3545, Test Loss: 2.3175, Learning Rate: 2.19e-05\n",
      "Step 2841, Train Loss: 2.2739, Test Loss: 2.3778, Learning Rate: 2.19e-05\n",
      "Step 2842, Train Loss: 2.3409, Test Loss: 2.2319, Learning Rate: 2.19e-05\n",
      "Step 2843, Train Loss: 2.4743, Test Loss: 2.4854, Learning Rate: 2.19e-05\n",
      "Step 2844, Train Loss: 2.3366, Test Loss: 2.2158, Learning Rate: 2.19e-05\n",
      "Step 2845, Train Loss: 2.3582, Test Loss: 2.1027, Learning Rate: 2.19e-05\n",
      "Step 2846, Train Loss: 2.3014, Test Loss: 2.3079, Learning Rate: 2.19e-05\n",
      "Step 2847, Train Loss: 2.3761, Test Loss: 2.7080, Learning Rate: 2.19e-05\n",
      "Step 2848, Train Loss: 2.3700, Test Loss: 2.4494, Learning Rate: 2.19e-05\n",
      "Step 2849, Train Loss: 2.2508, Test Loss: 2.3631, Learning Rate: 2.19e-05\n",
      "Step 2850, Train Loss: 2.3182, Test Loss: 2.3241, Learning Rate: 2.19e-05\n",
      "Step 2851, Train Loss: 2.3091, Test Loss: 2.4215, Learning Rate: 2.19e-05\n",
      "Step 2852, Train Loss: 2.3478, Test Loss: 2.4145, Learning Rate: 2.19e-05\n",
      "Step 2853, Train Loss: 2.2730, Test Loss: 2.2962, Learning Rate: 2.19e-05\n",
      "Step 2854, Train Loss: 2.3508, Test Loss: 2.2528, Learning Rate: 2.19e-05\n",
      "Step 2855, Train Loss: 2.4432, Test Loss: 2.1163, Learning Rate: 2.19e-05\n",
      "Step 2856, Train Loss: 2.3108, Test Loss: 2.1818, Learning Rate: 2.19e-05\n",
      "Step 2857, Train Loss: 2.3831, Test Loss: 2.3491, Learning Rate: 2.19e-05\n",
      "Step 2858, Train Loss: 2.3098, Test Loss: 2.4572, Learning Rate: 2.19e-05\n",
      "Step 2859, Train Loss: 2.2555, Test Loss: 2.3155, Learning Rate: 2.19e-05\n",
      "Step 2860, Train Loss: 2.4238, Test Loss: 2.1302, Learning Rate: 2.19e-05\n",
      "Step 2861, Train Loss: 2.3877, Test Loss: 2.3326, Learning Rate: 2.19e-05\n",
      "Step 2862, Train Loss: 2.2177, Test Loss: 2.4514, Learning Rate: 2.19e-05\n",
      "Step 2863, Train Loss: 2.4237, Test Loss: 2.6089, Learning Rate: 2.19e-05\n",
      "Step 2864, Train Loss: 2.4474, Test Loss: 2.2942, Learning Rate: 2.19e-05\n",
      "Step 2865, Train Loss: 2.1496, Test Loss: 2.2922, Learning Rate: 2.19e-05\n",
      "Step 2866, Train Loss: 2.4231, Test Loss: 2.3294, Learning Rate: 2.19e-05\n",
      "Step 2867, Train Loss: 2.3650, Test Loss: 2.3504, Learning Rate: 2.19e-05\n",
      "Step 2868, Train Loss: 2.1892, Test Loss: 2.4554, Learning Rate: 2.19e-05\n",
      "Step 2869, Train Loss: 2.4547, Test Loss: 2.4027, Learning Rate: 2.19e-05\n",
      "Step 2870, Train Loss: 2.3585, Test Loss: 2.1022, Learning Rate: 2.19e-05\n",
      "Step 2871, Train Loss: 2.2638, Test Loss: 2.0983, Learning Rate: 2.19e-05\n",
      "Step 2872, Train Loss: 2.3211, Test Loss: 2.3341, Learning Rate: 2.19e-05\n",
      "Step 2873, Train Loss: 2.3642, Test Loss: 2.7638, Learning Rate: 2.19e-05\n",
      "Step 2874, Train Loss: 2.2556, Test Loss: 2.3761, Learning Rate: 2.19e-05\n",
      "Step 2875, Train Loss: 2.3289, Test Loss: 2.6719, Learning Rate: 2.19e-05\n",
      "Step 2876, Train Loss: 2.3502, Test Loss: 2.1556, Learning Rate: 2.19e-05\n",
      "Step 2877, Train Loss: 2.4276, Test Loss: 2.3575, Learning Rate: 2.19e-05\n",
      "Step 2878, Train Loss: 2.1950, Test Loss: 2.3076, Learning Rate: 2.19e-05\n",
      "Step 2879, Train Loss: 2.4298, Test Loss: 2.2080, Learning Rate: 2.19e-05\n",
      "Step 2880, Train Loss: 2.3774, Test Loss: 2.8959, Learning Rate: 2.19e-05\n",
      "Step 2881, Train Loss: 2.1608, Test Loss: 2.2836, Learning Rate: 2.19e-05\n",
      "Step 2882, Train Loss: 2.2418, Test Loss: 2.3777, Learning Rate: 2.19e-05\n",
      "Step 2883, Train Loss: 2.4291, Test Loss: 2.2366, Learning Rate: 2.19e-05\n",
      "Step 2884, Train Loss: 2.2812, Test Loss: 2.2213, Learning Rate: 2.19e-05\n",
      "Step 2885, Train Loss: 2.4556, Test Loss: 2.3130, Learning Rate: 2.19e-05\n",
      "Step 2886, Train Loss: 2.3809, Test Loss: 2.4562, Learning Rate: 2.18e-05\n",
      "Step 2887, Train Loss: 2.3781, Test Loss: 2.4888, Learning Rate: 2.18e-05\n",
      "Step 2888, Train Loss: 2.2761, Test Loss: 2.3360, Learning Rate: 2.18e-05\n",
      "Step 2889, Train Loss: 2.3498, Test Loss: 2.2102, Learning Rate: 2.18e-05\n",
      "Step 2890, Train Loss: 2.3180, Test Loss: 2.3109, Learning Rate: 2.18e-05\n",
      "Step 2891, Train Loss: 2.3595, Test Loss: 2.4477, Learning Rate: 2.18e-05\n",
      "Step 2892, Train Loss: 2.3394, Test Loss: 2.3878, Learning Rate: 2.18e-05\n",
      "Step 2893, Train Loss: 2.3707, Test Loss: 2.2482, Learning Rate: 2.18e-05\n",
      "Step 2894, Train Loss: 2.5674, Test Loss: 2.0117, Learning Rate: 2.18e-05\n",
      "Step 2895, Train Loss: 2.3343, Test Loss: 2.4212, Learning Rate: 2.18e-05\n",
      "Step 2896, Train Loss: 2.2573, Test Loss: 2.2563, Learning Rate: 2.18e-05\n",
      "Step 2897, Train Loss: 2.2875, Test Loss: 2.2610, Learning Rate: 2.18e-05\n",
      "Step 2898, Train Loss: 2.3353, Test Loss: 2.2469, Learning Rate: 2.18e-05\n",
      "Step 2899, Train Loss: 2.3901, Test Loss: 2.4469, Learning Rate: 2.18e-05\n",
      "Step 2900, Train Loss: 2.3151, Test Loss: 2.2356, Learning Rate: 2.18e-05\n",
      "Step 2901, Train Loss: 2.2872, Test Loss: 2.5589, Learning Rate: 2.18e-05\n",
      "Step 2902, Train Loss: 2.3890, Test Loss: 2.2445, Learning Rate: 2.18e-05\n",
      "Step 2903, Train Loss: 2.4229, Test Loss: 2.3435, Learning Rate: 2.18e-05\n",
      "Step 2904, Train Loss: 2.2726, Test Loss: 2.3962, Learning Rate: 2.18e-05\n",
      "Step 2905, Train Loss: 2.2930, Test Loss: 2.2964, Learning Rate: 2.18e-05\n",
      "Step 2906, Train Loss: 2.3707, Test Loss: 2.2605, Learning Rate: 2.18e-05\n",
      "Step 2907, Train Loss: 2.3081, Test Loss: 2.3337, Learning Rate: 2.18e-05\n",
      "Step 2908, Train Loss: 2.3483, Test Loss: 2.3339, Learning Rate: 2.18e-05\n",
      "Step 2909, Train Loss: 2.3251, Test Loss: 2.2496, Learning Rate: 2.18e-05\n",
      "Step 2910, Train Loss: 2.4480, Test Loss: 2.3670, Learning Rate: 2.18e-05\n",
      "Step 2911, Train Loss: 2.3419, Test Loss: 2.4594, Learning Rate: 2.18e-05\n",
      "Step 2912, Train Loss: 2.4247, Test Loss: 2.3772, Learning Rate: 2.18e-05\n",
      "Step 2913, Train Loss: 2.3417, Test Loss: 2.4605, Learning Rate: 2.18e-05\n",
      "Step 2914, Train Loss: 2.3680, Test Loss: 2.5457, Learning Rate: 2.18e-05\n",
      "Step 2915, Train Loss: 2.3289, Test Loss: 2.2481, Learning Rate: 2.18e-05\n",
      "Step 2916, Train Loss: 2.2677, Test Loss: 2.2098, Learning Rate: 2.18e-05\n",
      "Step 2917, Train Loss: 2.3737, Test Loss: 2.4280, Learning Rate: 2.18e-05\n",
      "Step 2918, Train Loss: 2.3514, Test Loss: 2.3710, Learning Rate: 2.18e-05\n",
      "Step 2919, Train Loss: 2.3164, Test Loss: 2.1100, Learning Rate: 2.18e-05\n",
      "Step 2920, Train Loss: 2.3119, Test Loss: 2.3155, Learning Rate: 2.18e-05\n",
      "Step 2921, Train Loss: 2.5190, Test Loss: 2.0669, Learning Rate: 2.18e-05\n",
      "Step 2922, Train Loss: 2.3144, Test Loss: 2.6040, Learning Rate: 2.18e-05\n",
      "Step 2923, Train Loss: 2.3601, Test Loss: 2.2837, Learning Rate: 2.18e-05\n",
      "Step 2924, Train Loss: 2.2332, Test Loss: 2.3797, Learning Rate: 2.18e-05\n",
      "Step 2925, Train Loss: 2.3219, Test Loss: 2.2209, Learning Rate: 2.18e-05\n",
      "Step 2926, Train Loss: 2.4439, Test Loss: 2.2870, Learning Rate: 2.18e-05\n",
      "Step 2927, Train Loss: 2.2748, Test Loss: 2.3400, Learning Rate: 2.18e-05\n",
      "Step 2928, Train Loss: 2.3340, Test Loss: 2.2385, Learning Rate: 2.18e-05\n",
      "Step 2929, Train Loss: 2.4462, Test Loss: 2.2658, Learning Rate: 2.18e-05\n",
      "Step 2930, Train Loss: 2.3942, Test Loss: 2.3550, Learning Rate: 2.18e-05\n",
      "Step 2931, Train Loss: 2.3231, Test Loss: 2.3631, Learning Rate: 2.18e-05\n",
      "Step 2932, Train Loss: 2.3371, Test Loss: 2.2167, Learning Rate: 2.18e-05\n",
      "Step 2933, Train Loss: 2.3208, Test Loss: 2.1317, Learning Rate: 2.18e-05\n",
      "Step 2934, Train Loss: 2.4658, Test Loss: 2.4177, Learning Rate: 2.18e-05\n",
      "Step 2935, Train Loss: 2.3721, Test Loss: 2.2110, Learning Rate: 2.18e-05\n",
      "Step 2936, Train Loss: 2.2847, Test Loss: 2.2893, Learning Rate: 2.18e-05\n",
      "Step 2937, Train Loss: 2.3389, Test Loss: 2.4667, Learning Rate: 2.18e-05\n",
      "Step 2938, Train Loss: 2.3381, Test Loss: 2.2037, Learning Rate: 2.18e-05\n",
      "Step 2939, Train Loss: 2.2521, Test Loss: 2.1870, Learning Rate: 2.18e-05\n",
      "Step 2940, Train Loss: 2.3591, Test Loss: 2.2044, Learning Rate: 2.18e-05\n",
      "Step 2941, Train Loss: 2.2960, Test Loss: 2.3299, Learning Rate: 2.18e-05\n",
      "Step 2942, Train Loss: 2.2947, Test Loss: 2.4841, Learning Rate: 2.18e-05\n",
      "Step 2943, Train Loss: 2.2036, Test Loss: 2.2699, Learning Rate: 2.18e-05\n",
      "Step 2944, Train Loss: 2.2495, Test Loss: 2.2930, Learning Rate: 2.18e-05\n",
      "Step 2945, Train Loss: 2.3349, Test Loss: 2.3602, Learning Rate: 2.18e-05\n",
      "Step 2946, Train Loss: 2.2942, Test Loss: 2.3711, Learning Rate: 2.18e-05\n",
      "Step 2947, Train Loss: 2.3902, Test Loss: 2.2644, Learning Rate: 2.18e-05\n",
      "Step 2948, Train Loss: 2.3231, Test Loss: 2.3210, Learning Rate: 2.18e-05\n",
      "Step 2949, Train Loss: 2.2560, Test Loss: 2.4525, Learning Rate: 2.18e-05\n",
      "Step 2950, Train Loss: 2.2848, Test Loss: 2.0830, Learning Rate: 2.18e-05\n",
      "Step 2951, Train Loss: 2.2374, Test Loss: 2.3975, Learning Rate: 2.18e-05\n",
      "Step 2952, Train Loss: 2.4085, Test Loss: 2.3061, Learning Rate: 2.18e-05\n",
      "Step 2953, Train Loss: 2.2836, Test Loss: 2.3603, Learning Rate: 2.18e-05\n",
      "Step 2954, Train Loss: 2.2475, Test Loss: 2.4247, Learning Rate: 2.18e-05\n",
      "Step 2955, Train Loss: 2.3630, Test Loss: 2.3964, Learning Rate: 2.18e-05\n",
      "Step 2956, Train Loss: 2.2682, Test Loss: 2.2967, Learning Rate: 2.18e-05\n",
      "Step 2957, Train Loss: 2.3301, Test Loss: 2.3147, Learning Rate: 2.18e-05\n",
      "Step 2958, Train Loss: 2.3454, Test Loss: 2.3677, Learning Rate: 2.18e-05\n",
      "Step 2959, Train Loss: 2.4644, Test Loss: 2.3717, Learning Rate: 2.18e-05\n",
      "Step 2960, Train Loss: 2.3618, Test Loss: 2.3987, Learning Rate: 2.18e-05\n",
      "Step 2961, Train Loss: 2.3261, Test Loss: 2.4290, Learning Rate: 2.18e-05\n",
      "Step 2962, Train Loss: 2.2626, Test Loss: 2.3352, Learning Rate: 2.18e-05\n",
      "Step 2963, Train Loss: 2.2587, Test Loss: 2.3542, Learning Rate: 2.18e-05\n",
      "Step 2964, Train Loss: 2.3243, Test Loss: 2.2251, Learning Rate: 2.18e-05\n",
      "Step 2965, Train Loss: 2.2643, Test Loss: 2.2564, Learning Rate: 2.18e-05\n",
      "Step 2966, Train Loss: 2.3584, Test Loss: 2.2294, Learning Rate: 2.18e-05\n",
      "Step 2967, Train Loss: 2.3507, Test Loss: 2.2023, Learning Rate: 2.18e-05\n",
      "Step 2968, Train Loss: 2.3107, Test Loss: 2.2488, Learning Rate: 2.18e-05\n",
      "Step 2969, Train Loss: 2.1887, Test Loss: 2.4695, Learning Rate: 2.18e-05\n",
      "Step 2970, Train Loss: 2.3364, Test Loss: 2.4362, Learning Rate: 2.18e-05\n",
      "Step 2971, Train Loss: 2.2094, Test Loss: 2.4073, Learning Rate: 2.18e-05\n",
      "Step 2972, Train Loss: 2.3995, Test Loss: 2.2316, Learning Rate: 2.18e-05\n",
      "Step 2973, Train Loss: 2.2448, Test Loss: 2.3505, Learning Rate: 2.18e-05\n",
      "Step 2974, Train Loss: 2.3281, Test Loss: 2.1006, Learning Rate: 2.18e-05\n",
      "Step 2975, Train Loss: 2.3116, Test Loss: 2.0035, Learning Rate: 2.18e-05\n",
      "Step 2976, Train Loss: 2.4133, Test Loss: 2.5096, Learning Rate: 2.18e-05\n",
      "Step 2977, Train Loss: 2.3534, Test Loss: 2.3776, Learning Rate: 2.18e-05\n",
      "Step 2978, Train Loss: 2.4596, Test Loss: 2.4336, Learning Rate: 2.18e-05\n",
      "Step 2979, Train Loss: 2.2439, Test Loss: 2.1303, Learning Rate: 2.18e-05\n",
      "Step 2980, Train Loss: 2.2870, Test Loss: 2.4373, Learning Rate: 2.18e-05\n",
      "Step 2981, Train Loss: 2.2159, Test Loss: 2.4024, Learning Rate: 2.18e-05\n",
      "Step 2982, Train Loss: 2.3772, Test Loss: 2.1182, Learning Rate: 2.18e-05\n",
      "Step 2983, Train Loss: 2.2596, Test Loss: 2.3517, Learning Rate: 2.18e-05\n",
      "Step 2984, Train Loss: 2.2412, Test Loss: 2.3921, Learning Rate: 2.18e-05\n",
      "Step 2985, Train Loss: 2.2424, Test Loss: 2.2543, Learning Rate: 2.18e-05\n",
      "Step 2986, Train Loss: 2.2352, Test Loss: 2.4130, Learning Rate: 2.18e-05\n",
      "Step 2987, Train Loss: 2.3079, Test Loss: 2.1552, Learning Rate: 2.18e-05\n",
      "Step 2988, Train Loss: 2.2407, Test Loss: 2.3562, Learning Rate: 2.18e-05\n",
      "Step 2989, Train Loss: 2.3359, Test Loss: 2.3410, Learning Rate: 2.18e-05\n",
      "Step 2990, Train Loss: 2.3113, Test Loss: 2.4888, Learning Rate: 2.18e-05\n",
      "Step 2991, Train Loss: 2.1620, Test Loss: 2.2923, Learning Rate: 2.18e-05\n",
      "Step 2992, Train Loss: 2.2484, Test Loss: 2.1132, Learning Rate: 2.18e-05\n",
      "Step 2993, Train Loss: 2.2047, Test Loss: 2.3511, Learning Rate: 2.18e-05\n",
      "Step 2994, Train Loss: 2.2822, Test Loss: 2.2666, Learning Rate: 2.18e-05\n",
      "Step 2995, Train Loss: 2.3792, Test Loss: 2.2820, Learning Rate: 2.18e-05\n",
      "Step 2996, Train Loss: 2.3516, Test Loss: 2.2811, Learning Rate: 2.18e-05\n",
      "Step 2997, Train Loss: 2.4637, Test Loss: 2.2845, Learning Rate: 2.18e-05\n",
      "Step 2998, Train Loss: 2.3106, Test Loss: 2.3511, Learning Rate: 2.18e-05\n",
      "Step 2999, Train Loss: 2.3761, Test Loss: 2.1561, Learning Rate: 2.18e-05\n",
      "Step 3000, Train Loss: 2.2243, Test Loss: 2.3089, Learning Rate: 2.18e-05\n",
      "Step 3001, Train Loss: 2.3065, Test Loss: 2.2466, Learning Rate: 2.18e-05\n",
      "Step 3002, Train Loss: 2.4732, Test Loss: 2.2317, Learning Rate: 2.18e-05\n",
      "Step 3003, Train Loss: 2.4138, Test Loss: 2.1690, Learning Rate: 2.18e-05\n",
      "Step 3004, Train Loss: 2.3896, Test Loss: 2.3699, Learning Rate: 2.18e-05\n",
      "Step 3005, Train Loss: 2.3351, Test Loss: 2.2216, Learning Rate: 2.18e-05\n",
      "Step 3006, Train Loss: 2.3605, Test Loss: 2.3660, Learning Rate: 2.18e-05\n",
      "Step 3007, Train Loss: 2.3151, Test Loss: 2.4642, Learning Rate: 2.18e-05\n",
      "Step 3008, Train Loss: 2.2440, Test Loss: 2.0524, Learning Rate: 2.18e-05\n",
      "Step 3009, Train Loss: 2.2448, Test Loss: 2.4491, Learning Rate: 2.18e-05\n",
      "Step 3010, Train Loss: 2.2589, Test Loss: 2.3308, Learning Rate: 2.18e-05\n",
      "Step 3011, Train Loss: 2.2449, Test Loss: 2.2953, Learning Rate: 2.18e-05\n",
      "Step 3012, Train Loss: 2.4252, Test Loss: 2.4304, Learning Rate: 2.18e-05\n",
      "Step 3013, Train Loss: 2.4295, Test Loss: 2.3093, Learning Rate: 2.18e-05\n",
      "Step 3014, Train Loss: 2.3106, Test Loss: 2.4935, Learning Rate: 2.18e-05\n",
      "Step 3015, Train Loss: 2.2834, Test Loss: 2.4775, Learning Rate: 2.18e-05\n",
      "Step 3016, Train Loss: 2.3378, Test Loss: 2.3392, Learning Rate: 2.18e-05\n",
      "Step 3017, Train Loss: 2.3375, Test Loss: 2.3206, Learning Rate: 2.18e-05\n",
      "Step 3018, Train Loss: 2.3274, Test Loss: 2.3012, Learning Rate: 2.18e-05\n",
      "Step 3019, Train Loss: 2.4122, Test Loss: 2.4079, Learning Rate: 2.17e-05\n",
      "Step 3020, Train Loss: 2.4676, Test Loss: 2.1893, Learning Rate: 2.17e-05\n",
      "Step 3021, Train Loss: 2.3885, Test Loss: 2.3570, Learning Rate: 2.17e-05\n",
      "Step 3022, Train Loss: 2.4229, Test Loss: 2.2711, Learning Rate: 2.17e-05\n",
      "Step 3023, Train Loss: 2.1912, Test Loss: 2.2485, Learning Rate: 2.17e-05\n",
      "Step 3024, Train Loss: 2.2042, Test Loss: 2.2955, Learning Rate: 2.17e-05\n",
      "Step 3025, Train Loss: 2.3200, Test Loss: 2.1600, Learning Rate: 2.17e-05\n",
      "Step 3026, Train Loss: 2.2601, Test Loss: 2.3596, Learning Rate: 2.17e-05\n",
      "Step 3027, Train Loss: 2.3748, Test Loss: 2.2610, Learning Rate: 2.17e-05\n",
      "Step 3028, Train Loss: 2.2717, Test Loss: 2.3544, Learning Rate: 2.17e-05\n",
      "Step 3029, Train Loss: 2.4183, Test Loss: 2.2333, Learning Rate: 2.17e-05\n",
      "Step 3030, Train Loss: 2.2121, Test Loss: 2.1375, Learning Rate: 2.17e-05\n",
      "Step 3031, Train Loss: 2.3011, Test Loss: 2.3106, Learning Rate: 2.17e-05\n",
      "Step 3032, Train Loss: 2.2890, Test Loss: 2.3622, Learning Rate: 2.17e-05\n",
      "Step 3033, Train Loss: 2.2467, Test Loss: 2.1595, Learning Rate: 2.17e-05\n",
      "Step 3034, Train Loss: 2.3687, Test Loss: 2.2875, Learning Rate: 2.17e-05\n",
      "Step 3035, Train Loss: 2.4056, Test Loss: 2.3200, Learning Rate: 2.17e-05\n",
      "Step 3036, Train Loss: 2.3700, Test Loss: 2.1308, Learning Rate: 2.17e-05\n",
      "Step 3037, Train Loss: 2.2139, Test Loss: 2.1775, Learning Rate: 2.17e-05\n",
      "Step 3038, Train Loss: 2.2083, Test Loss: 2.1659, Learning Rate: 2.17e-05\n",
      "Step 3039, Train Loss: 2.3361, Test Loss: 2.3300, Learning Rate: 2.17e-05\n",
      "Step 3040, Train Loss: 2.4841, Test Loss: 2.2587, Learning Rate: 2.17e-05\n",
      "Step 3041, Train Loss: 2.4075, Test Loss: 2.3851, Learning Rate: 2.17e-05\n",
      "Step 3042, Train Loss: 2.3729, Test Loss: 2.1916, Learning Rate: 2.17e-05\n",
      "Step 3043, Train Loss: 2.3443, Test Loss: 2.4177, Learning Rate: 2.17e-05\n",
      "Step 3044, Train Loss: 2.2272, Test Loss: 2.3327, Learning Rate: 2.17e-05\n",
      "Step 3045, Train Loss: 2.3289, Test Loss: 2.1800, Learning Rate: 2.17e-05\n",
      "Step 3046, Train Loss: 2.4137, Test Loss: 2.4906, Learning Rate: 2.17e-05\n",
      "Step 3047, Train Loss: 2.2142, Test Loss: 2.2982, Learning Rate: 2.17e-05\n",
      "Step 3048, Train Loss: 2.3593, Test Loss: 2.2707, Learning Rate: 2.17e-05\n",
      "Step 3049, Train Loss: 2.2862, Test Loss: 2.3170, Learning Rate: 2.17e-05\n",
      "Step 3050, Train Loss: 2.2755, Test Loss: 2.3972, Learning Rate: 2.17e-05\n",
      "Step 3051, Train Loss: 2.3173, Test Loss: 2.1697, Learning Rate: 2.17e-05\n",
      "Step 3052, Train Loss: 2.1841, Test Loss: 2.4568, Learning Rate: 2.17e-05\n",
      "Step 3053, Train Loss: 2.2440, Test Loss: 2.3566, Learning Rate: 2.17e-05\n",
      "Step 3054, Train Loss: 2.3676, Test Loss: 2.3012, Learning Rate: 2.17e-05\n",
      "Step 3055, Train Loss: 2.2828, Test Loss: 2.2298, Learning Rate: 2.17e-05\n",
      "Step 3056, Train Loss: 2.2801, Test Loss: 2.3534, Learning Rate: 2.17e-05\n",
      "Step 3057, Train Loss: 2.2752, Test Loss: 2.3442, Learning Rate: 2.17e-05\n",
      "Step 3058, Train Loss: 2.3354, Test Loss: 2.4650, Learning Rate: 2.17e-05\n",
      "Step 3059, Train Loss: 2.2714, Test Loss: 2.3244, Learning Rate: 2.17e-05\n",
      "Step 3060, Train Loss: 2.2845, Test Loss: 2.1757, Learning Rate: 2.17e-05\n",
      "Step 3061, Train Loss: 2.3424, Test Loss: 2.2417, Learning Rate: 2.17e-05\n",
      "Step 3062, Train Loss: 2.4070, Test Loss: 2.3158, Learning Rate: 2.17e-05\n",
      "Step 3063, Train Loss: 2.3324, Test Loss: 2.1915, Learning Rate: 2.17e-05\n",
      "Step 3064, Train Loss: 2.3488, Test Loss: 2.1556, Learning Rate: 2.17e-05\n",
      "Step 3065, Train Loss: 2.2459, Test Loss: 2.3070, Learning Rate: 2.17e-05\n",
      "Step 3066, Train Loss: 2.3747, Test Loss: 2.4796, Learning Rate: 2.17e-05\n",
      "Step 3067, Train Loss: 2.3427, Test Loss: 2.2503, Learning Rate: 2.17e-05\n",
      "Step 3068, Train Loss: 2.3556, Test Loss: 2.2691, Learning Rate: 2.17e-05\n",
      "Step 3069, Train Loss: 2.2755, Test Loss: 2.4829, Learning Rate: 2.17e-05\n",
      "Step 3070, Train Loss: 2.2233, Test Loss: 2.0519, Learning Rate: 2.17e-05\n",
      "Step 3071, Train Loss: 2.2577, Test Loss: 2.0078, Learning Rate: 2.17e-05\n",
      "Step 3072, Train Loss: 2.2667, Test Loss: 2.1550, Learning Rate: 2.17e-05\n",
      "Step 3073, Train Loss: 2.3066, Test Loss: 2.4280, Learning Rate: 2.17e-05\n",
      "Step 3074, Train Loss: 2.3882, Test Loss: 2.4748, Learning Rate: 2.17e-05\n",
      "Step 3075, Train Loss: 2.3948, Test Loss: 2.5163, Learning Rate: 2.17e-05\n",
      "Step 3076, Train Loss: 2.1962, Test Loss: 2.2850, Learning Rate: 2.17e-05\n",
      "Step 3077, Train Loss: 2.3213, Test Loss: 2.3103, Learning Rate: 2.17e-05\n",
      "Step 3078, Train Loss: 2.3068, Test Loss: 2.2998, Learning Rate: 2.17e-05\n",
      "Step 3079, Train Loss: 2.2413, Test Loss: 2.4173, Learning Rate: 2.17e-05\n",
      "Step 3080, Train Loss: 2.2615, Test Loss: 2.2667, Learning Rate: 2.17e-05\n",
      "Step 3081, Train Loss: 2.3186, Test Loss: 2.2875, Learning Rate: 2.17e-05\n",
      "Step 3082, Train Loss: 2.3609, Test Loss: 2.2231, Learning Rate: 2.17e-05\n",
      "Step 3083, Train Loss: 2.2233, Test Loss: 2.3809, Learning Rate: 2.17e-05\n",
      "Step 3084, Train Loss: 2.3995, Test Loss: 2.4063, Learning Rate: 2.17e-05\n",
      "Step 3085, Train Loss: 2.3166, Test Loss: 2.0890, Learning Rate: 2.17e-05\n",
      "Step 3086, Train Loss: 2.2885, Test Loss: 2.2677, Learning Rate: 2.17e-05\n",
      "Step 3087, Train Loss: 2.2293, Test Loss: 2.4067, Learning Rate: 2.17e-05\n",
      "Step 3088, Train Loss: 2.1934, Test Loss: 2.4311, Learning Rate: 2.17e-05\n",
      "Step 3089, Train Loss: 2.3388, Test Loss: 2.3482, Learning Rate: 2.17e-05\n",
      "Step 3090, Train Loss: 2.3442, Test Loss: 2.4950, Learning Rate: 2.17e-05\n",
      "Step 3091, Train Loss: 2.3383, Test Loss: 2.3342, Learning Rate: 2.17e-05\n",
      "Step 3092, Train Loss: 2.3268, Test Loss: 2.4164, Learning Rate: 2.17e-05\n",
      "Step 3093, Train Loss: 2.4191, Test Loss: 2.4099, Learning Rate: 2.17e-05\n",
      "Step 3094, Train Loss: 2.2917, Test Loss: 2.1386, Learning Rate: 2.17e-05\n",
      "Step 3095, Train Loss: 2.4050, Test Loss: 2.2853, Learning Rate: 2.17e-05\n",
      "Step 3096, Train Loss: 2.3117, Test Loss: 2.5887, Learning Rate: 2.17e-05\n",
      "Step 3097, Train Loss: 2.2988, Test Loss: 2.4366, Learning Rate: 2.17e-05\n",
      "Step 3098, Train Loss: 2.3122, Test Loss: 2.3502, Learning Rate: 2.17e-05\n",
      "Step 3099, Train Loss: 2.4094, Test Loss: 2.3481, Learning Rate: 2.17e-05\n",
      "Step 3100, Train Loss: 2.1989, Test Loss: 2.4445, Learning Rate: 2.17e-05\n",
      "Step 3101, Train Loss: 2.3049, Test Loss: 2.2269, Learning Rate: 2.17e-05\n",
      "Step 3102, Train Loss: 2.4242, Test Loss: 2.2461, Learning Rate: 2.17e-05\n",
      "Step 3103, Train Loss: 2.3144, Test Loss: 2.2117, Learning Rate: 2.17e-05\n",
      "Step 3104, Train Loss: 2.2406, Test Loss: 2.4153, Learning Rate: 2.17e-05\n",
      "Step 3105, Train Loss: 2.2566, Test Loss: 2.0950, Learning Rate: 2.17e-05\n",
      "Step 3106, Train Loss: 2.3945, Test Loss: 2.2628, Learning Rate: 2.17e-05\n",
      "Step 3107, Train Loss: 2.3144, Test Loss: 2.1825, Learning Rate: 2.17e-05\n",
      "Step 3108, Train Loss: 2.2786, Test Loss: 2.3364, Learning Rate: 2.17e-05\n",
      "Step 3109, Train Loss: 2.3378, Test Loss: 2.3004, Learning Rate: 2.17e-05\n",
      "Step 3110, Train Loss: 2.3046, Test Loss: 2.2385, Learning Rate: 2.17e-05\n",
      "Step 3111, Train Loss: 2.4443, Test Loss: 2.4807, Learning Rate: 2.17e-05\n",
      "Step 3112, Train Loss: 2.2100, Test Loss: 2.7071, Learning Rate: 2.17e-05\n",
      "Step 3113, Train Loss: 2.3531, Test Loss: 2.1746, Learning Rate: 2.17e-05\n",
      "Step 3114, Train Loss: 2.3476, Test Loss: 2.1857, Learning Rate: 2.17e-05\n",
      "Step 3115, Train Loss: 2.4110, Test Loss: 2.4588, Learning Rate: 2.17e-05\n",
      "Step 3116, Train Loss: 2.2589, Test Loss: 2.1692, Learning Rate: 2.17e-05\n",
      "Step 3117, Train Loss: 2.4103, Test Loss: 2.2083, Learning Rate: 2.17e-05\n",
      "Step 3118, Train Loss: 2.1705, Test Loss: 2.2872, Learning Rate: 2.17e-05\n",
      "Step 3119, Train Loss: 2.3593, Test Loss: 2.3090, Learning Rate: 2.17e-05\n",
      "Step 3120, Train Loss: 2.3401, Test Loss: 2.2905, Learning Rate: 2.17e-05\n",
      "Step 3121, Train Loss: 2.3807, Test Loss: 2.1789, Learning Rate: 2.17e-05\n",
      "Step 3122, Train Loss: 2.4517, Test Loss: 2.3207, Learning Rate: 2.17e-05\n",
      "Step 3123, Train Loss: 2.3352, Test Loss: 2.3652, Learning Rate: 2.17e-05\n",
      "Step 3124, Train Loss: 2.5329, Test Loss: 2.1519, Learning Rate: 2.17e-05\n",
      "Step 3125, Train Loss: 2.2891, Test Loss: 2.1781, Learning Rate: 2.17e-05\n",
      "Step 3126, Train Loss: 2.3002, Test Loss: 2.2710, Learning Rate: 2.17e-05\n",
      "Step 3127, Train Loss: 2.4098, Test Loss: 2.3709, Learning Rate: 2.17e-05\n",
      "Step 3128, Train Loss: 2.4754, Test Loss: 2.3987, Learning Rate: 2.17e-05\n",
      "Step 3129, Train Loss: 2.2000, Test Loss: 2.5299, Learning Rate: 2.17e-05\n",
      "Step 3130, Train Loss: 2.2337, Test Loss: 2.3988, Learning Rate: 2.17e-05\n",
      "Step 3131, Train Loss: 2.3307, Test Loss: 2.4032, Learning Rate: 2.17e-05\n",
      "Step 3132, Train Loss: 2.4376, Test Loss: 2.3957, Learning Rate: 2.17e-05\n",
      "Step 3133, Train Loss: 2.3146, Test Loss: 2.1856, Learning Rate: 2.17e-05\n",
      "Step 3134, Train Loss: 2.3843, Test Loss: 2.4532, Learning Rate: 2.17e-05\n",
      "Step 3135, Train Loss: 2.3222, Test Loss: 2.3769, Learning Rate: 2.17e-05\n",
      "Step 3136, Train Loss: 2.3265, Test Loss: 2.3973, Learning Rate: 2.17e-05\n",
      "Step 3137, Train Loss: 2.4127, Test Loss: 2.1797, Learning Rate: 2.17e-05\n",
      "Step 3138, Train Loss: 2.3559, Test Loss: 2.2269, Learning Rate: 2.17e-05\n",
      "Step 3139, Train Loss: 2.2761, Test Loss: 2.3953, Learning Rate: 2.17e-05\n",
      "Step 3140, Train Loss: 2.2295, Test Loss: 2.3473, Learning Rate: 2.17e-05\n",
      "Step 3141, Train Loss: 2.3239, Test Loss: 2.6000, Learning Rate: 2.17e-05\n",
      "Step 3142, Train Loss: 2.4315, Test Loss: 2.2839, Learning Rate: 2.17e-05\n",
      "Step 3143, Train Loss: 2.3142, Test Loss: 2.3972, Learning Rate: 2.17e-05\n",
      "Step 3144, Train Loss: 2.2290, Test Loss: 2.2104, Learning Rate: 2.17e-05\n",
      "Step 3145, Train Loss: 2.3761, Test Loss: 2.3250, Learning Rate: 2.17e-05\n",
      "Step 3146, Train Loss: 2.3811, Test Loss: 2.2840, Learning Rate: 2.17e-05\n",
      "Step 3147, Train Loss: 2.3390, Test Loss: 2.2822, Learning Rate: 2.17e-05\n",
      "Step 3148, Train Loss: 2.2697, Test Loss: 2.2218, Learning Rate: 2.17e-05\n",
      "Step 3149, Train Loss: 2.3317, Test Loss: 2.2352, Learning Rate: 2.17e-05\n",
      "Step 3150, Train Loss: 2.2350, Test Loss: 2.3696, Learning Rate: 2.17e-05\n",
      "Step 3151, Train Loss: 2.4428, Test Loss: 2.3420, Learning Rate: 2.17e-05\n",
      "Step 3152, Train Loss: 2.4556, Test Loss: 2.3805, Learning Rate: 2.16e-05\n",
      "Step 3153, Train Loss: 2.2411, Test Loss: 2.2763, Learning Rate: 2.16e-05\n",
      "Step 3154, Train Loss: 2.3004, Test Loss: 2.4059, Learning Rate: 2.16e-05\n",
      "Step 3155, Train Loss: 2.2734, Test Loss: 2.5144, Learning Rate: 2.16e-05\n",
      "Step 3156, Train Loss: 2.3530, Test Loss: 2.4061, Learning Rate: 2.16e-05\n",
      "Step 3157, Train Loss: 2.3234, Test Loss: 2.2624, Learning Rate: 2.16e-05\n",
      "Step 3158, Train Loss: 2.3822, Test Loss: 2.3456, Learning Rate: 2.16e-05\n",
      "Step 3159, Train Loss: 2.2791, Test Loss: 2.2669, Learning Rate: 2.16e-05\n",
      "Step 3160, Train Loss: 2.3789, Test Loss: 2.2566, Learning Rate: 2.16e-05\n",
      "Step 3161, Train Loss: 2.1108, Test Loss: 1.9298, Learning Rate: 2.16e-05\n",
      "Step 3162, Train Loss: 2.3344, Test Loss: 2.3799, Learning Rate: 2.16e-05\n",
      "Step 3163, Train Loss: 2.3427, Test Loss: 2.1725, Learning Rate: 2.16e-05\n",
      "Step 3164, Train Loss: 2.3241, Test Loss: 2.3131, Learning Rate: 2.16e-05\n",
      "Step 3165, Train Loss: 2.3016, Test Loss: 2.0541, Learning Rate: 2.16e-05\n",
      "Step 3166, Train Loss: 2.3100, Test Loss: 2.3984, Learning Rate: 2.16e-05\n",
      "Step 3167, Train Loss: 2.2933, Test Loss: 2.3198, Learning Rate: 2.16e-05\n",
      "Step 3168, Train Loss: 2.3199, Test Loss: 2.1714, Learning Rate: 2.16e-05\n",
      "Step 3169, Train Loss: 2.2469, Test Loss: 2.1205, Learning Rate: 2.16e-05\n",
      "Step 3170, Train Loss: 2.2327, Test Loss: 2.7724, Learning Rate: 2.16e-05\n",
      "Step 3171, Train Loss: 2.2783, Test Loss: 2.2129, Learning Rate: 2.16e-05\n",
      "Step 3172, Train Loss: 2.2419, Test Loss: 2.2723, Learning Rate: 2.16e-05\n",
      "Step 3173, Train Loss: 2.3857, Test Loss: 2.1828, Learning Rate: 2.16e-05\n",
      "Step 3174, Train Loss: 2.3913, Test Loss: 2.2633, Learning Rate: 2.16e-05\n",
      "Step 3175, Train Loss: 2.2713, Test Loss: 2.6094, Learning Rate: 2.16e-05\n",
      "Step 3176, Train Loss: 2.3304, Test Loss: 2.2089, Learning Rate: 2.16e-05\n",
      "Step 3177, Train Loss: 2.3600, Test Loss: 2.3906, Learning Rate: 2.16e-05\n",
      "Step 3178, Train Loss: 2.3221, Test Loss: 2.3930, Learning Rate: 2.16e-05\n",
      "Step 3179, Train Loss: 2.2698, Test Loss: 2.3962, Learning Rate: 2.16e-05\n",
      "Step 3180, Train Loss: 2.4844, Test Loss: 2.3974, Learning Rate: 2.16e-05\n",
      "Step 3181, Train Loss: 2.3820, Test Loss: 2.2626, Learning Rate: 2.16e-05\n",
      "Step 3182, Train Loss: 2.2253, Test Loss: 2.2928, Learning Rate: 2.16e-05\n",
      "Step 3183, Train Loss: 2.3966, Test Loss: 2.3932, Learning Rate: 2.16e-05\n",
      "Step 3184, Train Loss: 2.2335, Test Loss: 2.3685, Learning Rate: 2.16e-05\n",
      "Step 3185, Train Loss: 2.3343, Test Loss: 2.2823, Learning Rate: 2.16e-05\n",
      "Step 3186, Train Loss: 2.3823, Test Loss: 2.2190, Learning Rate: 2.16e-05\n",
      "Step 3187, Train Loss: 2.3405, Test Loss: 2.4605, Learning Rate: 2.16e-05\n",
      "Step 3188, Train Loss: 2.4070, Test Loss: 2.2573, Learning Rate: 2.16e-05\n",
      "Step 3189, Train Loss: 2.2738, Test Loss: 2.3710, Learning Rate: 2.16e-05\n",
      "Step 3190, Train Loss: 2.5116, Test Loss: 2.1946, Learning Rate: 2.16e-05\n",
      "Step 3191, Train Loss: 2.3364, Test Loss: 2.2499, Learning Rate: 2.16e-05\n",
      "Step 3192, Train Loss: 2.3088, Test Loss: 2.1483, Learning Rate: 2.16e-05\n",
      "Step 3193, Train Loss: 2.3291, Test Loss: 2.2260, Learning Rate: 2.16e-05\n",
      "Step 3194, Train Loss: 2.2415, Test Loss: 2.4340, Learning Rate: 2.16e-05\n",
      "Step 3195, Train Loss: 2.2212, Test Loss: 2.3829, Learning Rate: 2.16e-05\n",
      "Step 3196, Train Loss: 2.4215, Test Loss: 2.4198, Learning Rate: 2.16e-05\n",
      "Step 3197, Train Loss: 2.3390, Test Loss: 2.2913, Learning Rate: 2.16e-05\n",
      "Step 3198, Train Loss: 2.3627, Test Loss: 2.3632, Learning Rate: 2.16e-05\n",
      "Step 3199, Train Loss: 2.4469, Test Loss: 2.3067, Learning Rate: 2.16e-05\n",
      "Step 3200, Train Loss: 2.2857, Test Loss: 2.2377, Learning Rate: 2.16e-05\n",
      "Step 3201, Train Loss: 2.3481, Test Loss: 2.1284, Learning Rate: 2.16e-05\n",
      "Step 3202, Train Loss: 2.4001, Test Loss: 2.1434, Learning Rate: 2.16e-05\n",
      "Step 3203, Train Loss: 2.2647, Test Loss: 2.3840, Learning Rate: 2.16e-05\n",
      "Step 3204, Train Loss: 2.2854, Test Loss: 2.4462, Learning Rate: 2.16e-05\n",
      "Step 3205, Train Loss: 2.2809, Test Loss: 2.3288, Learning Rate: 2.16e-05\n",
      "Step 3206, Train Loss: 2.3169, Test Loss: 2.1326, Learning Rate: 2.16e-05\n",
      "Step 3207, Train Loss: 2.3439, Test Loss: 2.3422, Learning Rate: 2.16e-05\n",
      "Step 3208, Train Loss: 2.4492, Test Loss: 2.5050, Learning Rate: 2.16e-05\n",
      "Step 3209, Train Loss: 2.2527, Test Loss: 2.2913, Learning Rate: 2.16e-05\n",
      "Step 3210, Train Loss: 2.2953, Test Loss: 2.3816, Learning Rate: 2.16e-05\n",
      "Step 3211, Train Loss: 2.3774, Test Loss: 2.4056, Learning Rate: 2.16e-05\n",
      "Step 3212, Train Loss: 2.2413, Test Loss: 2.2985, Learning Rate: 2.16e-05\n",
      "Step 3213, Train Loss: 2.2430, Test Loss: 2.3954, Learning Rate: 2.16e-05\n",
      "Step 3214, Train Loss: 2.2462, Test Loss: 2.2120, Learning Rate: 2.16e-05\n",
      "Step 3215, Train Loss: 2.3795, Test Loss: 2.2022, Learning Rate: 2.16e-05\n",
      "Step 3216, Train Loss: 2.2695, Test Loss: 2.2994, Learning Rate: 2.16e-05\n",
      "Step 3217, Train Loss: 2.2989, Test Loss: 2.4701, Learning Rate: 2.16e-05\n",
      "Step 3218, Train Loss: 2.4368, Test Loss: 2.2527, Learning Rate: 2.16e-05\n",
      "Step 3219, Train Loss: 2.1356, Test Loss: 2.3734, Learning Rate: 2.16e-05\n",
      "Step 3220, Train Loss: 2.3241, Test Loss: 2.2696, Learning Rate: 2.16e-05\n",
      "Step 3221, Train Loss: 2.4632, Test Loss: 2.3159, Learning Rate: 2.16e-05\n",
      "Step 3222, Train Loss: 2.2510, Test Loss: 2.3847, Learning Rate: 2.16e-05\n",
      "Step 3223, Train Loss: 2.2469, Test Loss: 2.3241, Learning Rate: 2.16e-05\n",
      "Step 3224, Train Loss: 2.3670, Test Loss: 2.1739, Learning Rate: 2.16e-05\n",
      "Step 3225, Train Loss: 2.3635, Test Loss: 2.4294, Learning Rate: 2.16e-05\n",
      "Step 3226, Train Loss: 2.2577, Test Loss: 2.3232, Learning Rate: 2.16e-05\n",
      "Step 3227, Train Loss: 2.3111, Test Loss: 2.3214, Learning Rate: 2.16e-05\n",
      "Step 3228, Train Loss: 2.4970, Test Loss: 2.2362, Learning Rate: 2.16e-05\n",
      "Step 3229, Train Loss: 2.2736, Test Loss: 2.6676, Learning Rate: 2.16e-05\n",
      "Step 3230, Train Loss: 2.2495, Test Loss: 2.3977, Learning Rate: 2.16e-05\n",
      "Step 3231, Train Loss: 2.2658, Test Loss: 2.4272, Learning Rate: 2.16e-05\n",
      "Step 3232, Train Loss: 2.3494, Test Loss: 2.2704, Learning Rate: 2.16e-05\n",
      "Step 3233, Train Loss: 2.3391, Test Loss: 2.3749, Learning Rate: 2.16e-05\n",
      "Step 3234, Train Loss: 2.1711, Test Loss: 2.3358, Learning Rate: 2.16e-05\n",
      "Step 3235, Train Loss: 2.4177, Test Loss: 2.3561, Learning Rate: 2.16e-05\n",
      "Step 3236, Train Loss: 2.2918, Test Loss: 2.2159, Learning Rate: 2.16e-05\n",
      "Step 3237, Train Loss: 2.3423, Test Loss: 2.3821, Learning Rate: 2.16e-05\n",
      "Step 3238, Train Loss: 2.3228, Test Loss: 2.3086, Learning Rate: 2.16e-05\n",
      "Step 3239, Train Loss: 2.2926, Test Loss: 2.4778, Learning Rate: 2.16e-05\n",
      "Step 3240, Train Loss: 2.2830, Test Loss: 2.2014, Learning Rate: 2.16e-05\n",
      "Step 3241, Train Loss: 2.2881, Test Loss: 2.2436, Learning Rate: 2.16e-05\n",
      "Step 3242, Train Loss: 2.3145, Test Loss: 2.2562, Learning Rate: 2.16e-05\n",
      "Step 3243, Train Loss: 2.3786, Test Loss: 2.2828, Learning Rate: 2.16e-05\n",
      "Step 3244, Train Loss: 2.3945, Test Loss: 2.4270, Learning Rate: 2.16e-05\n",
      "Step 3245, Train Loss: 2.2841, Test Loss: 2.4095, Learning Rate: 2.16e-05\n",
      "Step 3246, Train Loss: 2.2720, Test Loss: 2.1595, Learning Rate: 2.16e-05\n",
      "Step 3247, Train Loss: 2.4006, Test Loss: 2.2147, Learning Rate: 2.16e-05\n",
      "Step 3248, Train Loss: 2.3599, Test Loss: 2.1441, Learning Rate: 2.16e-05\n",
      "Step 3249, Train Loss: 2.4124, Test Loss: 2.5503, Learning Rate: 2.16e-05\n",
      "Step 3250, Train Loss: 2.3436, Test Loss: 2.2284, Learning Rate: 2.16e-05\n",
      "Step 3251, Train Loss: 2.2232, Test Loss: 2.3627, Learning Rate: 2.16e-05\n",
      "Step 3252, Train Loss: 2.3705, Test Loss: 2.2811, Learning Rate: 2.16e-05\n",
      "Step 3253, Train Loss: 2.3208, Test Loss: 2.3418, Learning Rate: 2.16e-05\n",
      "Step 3254, Train Loss: 2.3454, Test Loss: 2.3207, Learning Rate: 2.16e-05\n",
      "Step 3255, Train Loss: 2.4312, Test Loss: 2.0826, Learning Rate: 2.16e-05\n",
      "Step 3256, Train Loss: 2.2432, Test Loss: 2.5248, Learning Rate: 2.16e-05\n",
      "Step 3257, Train Loss: 2.3519, Test Loss: 2.5407, Learning Rate: 2.16e-05\n",
      "Step 3258, Train Loss: 2.3399, Test Loss: 2.4767, Learning Rate: 2.16e-05\n",
      "Step 3259, Train Loss: 2.4192, Test Loss: 2.3134, Learning Rate: 2.16e-05\n",
      "Step 3260, Train Loss: 2.4111, Test Loss: 2.3817, Learning Rate: 2.16e-05\n",
      "Step 3261, Train Loss: 2.1835, Test Loss: 2.1190, Learning Rate: 2.16e-05\n",
      "Step 3262, Train Loss: 2.2620, Test Loss: 2.2701, Learning Rate: 2.16e-05\n",
      "Step 3263, Train Loss: 2.2664, Test Loss: 2.3481, Learning Rate: 2.16e-05\n",
      "Step 3264, Train Loss: 2.4322, Test Loss: 2.4007, Learning Rate: 2.16e-05\n",
      "Step 3265, Train Loss: 2.2904, Test Loss: 2.2712, Learning Rate: 2.16e-05\n",
      "Step 3266, Train Loss: 2.2386, Test Loss: 2.4080, Learning Rate: 2.16e-05\n",
      "Step 3267, Train Loss: 2.3011, Test Loss: 2.4184, Learning Rate: 2.16e-05\n",
      "Step 3268, Train Loss: 2.2929, Test Loss: 2.3919, Learning Rate: 2.16e-05\n",
      "Step 3269, Train Loss: 2.2551, Test Loss: 2.2470, Learning Rate: 2.16e-05\n",
      "Step 3270, Train Loss: 2.3142, Test Loss: 2.3066, Learning Rate: 2.16e-05\n",
      "Step 3271, Train Loss: 2.3281, Test Loss: 2.3385, Learning Rate: 2.16e-05\n",
      "Step 3272, Train Loss: 2.5337, Test Loss: 2.2238, Learning Rate: 2.16e-05\n",
      "Step 3273, Train Loss: 2.2759, Test Loss: 2.4450, Learning Rate: 2.16e-05\n",
      "Step 3274, Train Loss: 2.4183, Test Loss: 2.3339, Learning Rate: 2.16e-05\n",
      "Step 3275, Train Loss: 2.3653, Test Loss: 2.3039, Learning Rate: 2.16e-05\n",
      "Step 3276, Train Loss: 2.3113, Test Loss: 2.2910, Learning Rate: 2.16e-05\n",
      "Step 3277, Train Loss: 2.2865, Test Loss: 2.3228, Learning Rate: 2.16e-05\n",
      "Step 3278, Train Loss: 2.4273, Test Loss: 2.3308, Learning Rate: 2.16e-05\n",
      "Step 3279, Train Loss: 2.2279, Test Loss: 2.2285, Learning Rate: 2.16e-05\n",
      "Step 3280, Train Loss: 2.4005, Test Loss: 2.1553, Learning Rate: 2.16e-05\n",
      "Step 3281, Train Loss: 2.4167, Test Loss: 2.1755, Learning Rate: 2.16e-05\n",
      "Step 3282, Train Loss: 2.3514, Test Loss: 2.3115, Learning Rate: 2.16e-05\n",
      "Step 3283, Train Loss: 2.3385, Test Loss: 2.4128, Learning Rate: 2.16e-05\n",
      "Step 3284, Train Loss: 2.2059, Test Loss: 2.3318, Learning Rate: 2.16e-05\n",
      "Step 3285, Train Loss: 2.3583, Test Loss: 2.3108, Learning Rate: 2.15e-05\n",
      "Step 3286, Train Loss: 2.3214, Test Loss: 2.4924, Learning Rate: 2.15e-05\n",
      "Step 3287, Train Loss: 2.4065, Test Loss: 2.2784, Learning Rate: 2.15e-05\n",
      "Step 3288, Train Loss: 2.2064, Test Loss: 2.2441, Learning Rate: 2.15e-05\n",
      "Step 3289, Train Loss: 2.3737, Test Loss: 2.2500, Learning Rate: 2.15e-05\n",
      "Step 3290, Train Loss: 2.3187, Test Loss: 2.4419, Learning Rate: 2.15e-05\n",
      "Step 3291, Train Loss: 2.2463, Test Loss: 2.2006, Learning Rate: 2.15e-05\n",
      "Step 3292, Train Loss: 2.4041, Test Loss: 2.2741, Learning Rate: 2.15e-05\n",
      "Step 3293, Train Loss: 2.3695, Test Loss: 2.1172, Learning Rate: 2.15e-05\n",
      "Step 3294, Train Loss: 2.2074, Test Loss: 2.3014, Learning Rate: 2.15e-05\n",
      "Step 3295, Train Loss: 2.3184, Test Loss: 2.4031, Learning Rate: 2.15e-05\n",
      "Step 3296, Train Loss: 2.3763, Test Loss: 2.4572, Learning Rate: 2.15e-05\n",
      "Step 3297, Train Loss: 2.3318, Test Loss: 2.3605, Learning Rate: 2.15e-05\n",
      "Step 3298, Train Loss: 2.3491, Test Loss: 2.4587, Learning Rate: 2.15e-05\n",
      "Step 3299, Train Loss: 2.2077, Test Loss: 2.2574, Learning Rate: 2.15e-05\n",
      "Step 3300, Train Loss: 2.3834, Test Loss: 2.2489, Learning Rate: 2.15e-05\n",
      "Step 3301, Train Loss: 2.2619, Test Loss: 2.2846, Learning Rate: 2.15e-05\n",
      "Step 3302, Train Loss: 2.2813, Test Loss: 2.3003, Learning Rate: 2.15e-05\n",
      "Step 3303, Train Loss: 2.3988, Test Loss: 2.5286, Learning Rate: 2.15e-05\n",
      "Step 3304, Train Loss: 2.4082, Test Loss: 2.2147, Learning Rate: 2.15e-05\n",
      "Step 3305, Train Loss: 2.3406, Test Loss: 2.2127, Learning Rate: 2.15e-05\n",
      "Step 3306, Train Loss: 2.2196, Test Loss: 2.2014, Learning Rate: 2.15e-05\n",
      "Step 3307, Train Loss: 2.3141, Test Loss: 2.0265, Learning Rate: 2.15e-05\n",
      "Step 3308, Train Loss: 2.4352, Test Loss: 2.2515, Learning Rate: 2.15e-05\n",
      "Step 3309, Train Loss: 2.3198, Test Loss: 2.4126, Learning Rate: 2.15e-05\n",
      "Step 3310, Train Loss: 2.2199, Test Loss: 2.2985, Learning Rate: 2.15e-05\n",
      "Step 3311, Train Loss: 2.1607, Test Loss: 2.3251, Learning Rate: 2.15e-05\n",
      "Step 3312, Train Loss: 2.4325, Test Loss: 2.5545, Learning Rate: 2.15e-05\n",
      "Step 3313, Train Loss: 2.2019, Test Loss: 2.2766, Learning Rate: 2.15e-05\n",
      "Step 3314, Train Loss: 2.3332, Test Loss: 2.3023, Learning Rate: 2.15e-05\n",
      "Step 3315, Train Loss: 2.4182, Test Loss: 2.2701, Learning Rate: 2.15e-05\n",
      "Step 3316, Train Loss: 2.2431, Test Loss: 2.2611, Learning Rate: 2.15e-05\n",
      "Step 3317, Train Loss: 2.3182, Test Loss: 2.2976, Learning Rate: 2.15e-05\n",
      "Step 3318, Train Loss: 2.3613, Test Loss: 2.2595, Learning Rate: 2.15e-05\n",
      "Step 3319, Train Loss: 2.1661, Test Loss: 2.2653, Learning Rate: 2.15e-05\n",
      "Step 3320, Train Loss: 2.3153, Test Loss: 2.3262, Learning Rate: 2.15e-05\n",
      "Step 3321, Train Loss: 2.3290, Test Loss: 2.2826, Learning Rate: 2.15e-05\n",
      "Step 3322, Train Loss: 2.2491, Test Loss: 2.3777, Learning Rate: 2.15e-05\n",
      "Step 3323, Train Loss: 2.4929, Test Loss: 2.2430, Learning Rate: 2.15e-05\n",
      "Step 3324, Train Loss: 2.4214, Test Loss: 2.2939, Learning Rate: 2.15e-05\n",
      "Step 3325, Train Loss: 2.2077, Test Loss: 2.3014, Learning Rate: 2.15e-05\n",
      "Step 3326, Train Loss: 2.3576, Test Loss: 1.9087, Learning Rate: 2.15e-05\n",
      "Step 3327, Train Loss: 2.3871, Test Loss: 2.3233, Learning Rate: 2.15e-05\n",
      "Step 3328, Train Loss: 2.3170, Test Loss: 2.3196, Learning Rate: 2.15e-05\n",
      "Step 3329, Train Loss: 2.2155, Test Loss: 2.3219, Learning Rate: 2.15e-05\n",
      "Step 3330, Train Loss: 2.3705, Test Loss: 2.3995, Learning Rate: 2.15e-05\n",
      "Step 3331, Train Loss: 2.3356, Test Loss: 2.2952, Learning Rate: 2.15e-05\n",
      "Step 3332, Train Loss: 2.3951, Test Loss: 2.4763, Learning Rate: 2.15e-05\n",
      "Step 3333, Train Loss: 2.3004, Test Loss: 2.4122, Learning Rate: 2.15e-05\n",
      "Step 3334, Train Loss: 2.4703, Test Loss: 2.2007, Learning Rate: 2.15e-05\n",
      "Step 3335, Train Loss: 2.3273, Test Loss: 2.3858, Learning Rate: 2.15e-05\n",
      "Step 3336, Train Loss: 2.2879, Test Loss: 2.2051, Learning Rate: 2.15e-05\n",
      "Step 3337, Train Loss: 2.2274, Test Loss: 2.2526, Learning Rate: 2.15e-05\n",
      "Step 3338, Train Loss: 2.3926, Test Loss: 2.8358, Learning Rate: 2.15e-05\n",
      "Step 3339, Train Loss: 2.3670, Test Loss: 2.3707, Learning Rate: 2.15e-05\n",
      "Step 3340, Train Loss: 2.2208, Test Loss: 2.0567, Learning Rate: 2.15e-05\n",
      "Step 3341, Train Loss: 2.2344, Test Loss: 2.3759, Learning Rate: 2.15e-05\n",
      "Step 3342, Train Loss: 2.2349, Test Loss: 2.3277, Learning Rate: 2.15e-05\n",
      "Step 3343, Train Loss: 2.2262, Test Loss: 2.2754, Learning Rate: 2.15e-05\n",
      "Step 3344, Train Loss: 2.3669, Test Loss: 2.5347, Learning Rate: 2.15e-05\n",
      "Step 3345, Train Loss: 2.4330, Test Loss: 2.2434, Learning Rate: 2.15e-05\n",
      "Step 3346, Train Loss: 2.4165, Test Loss: 2.2721, Learning Rate: 2.15e-05\n",
      "Step 3347, Train Loss: 2.3829, Test Loss: 2.1833, Learning Rate: 2.15e-05\n",
      "Step 3348, Train Loss: 2.3152, Test Loss: 2.1206, Learning Rate: 2.15e-05\n",
      "Step 3349, Train Loss: 2.0964, Test Loss: 2.3959, Learning Rate: 2.15e-05\n",
      "Step 3350, Train Loss: 2.2611, Test Loss: 2.0317, Learning Rate: 2.15e-05\n",
      "Step 3351, Train Loss: 2.3171, Test Loss: 2.3280, Learning Rate: 2.15e-05\n",
      "Step 3352, Train Loss: 2.2230, Test Loss: 2.2782, Learning Rate: 2.15e-05\n",
      "Step 3353, Train Loss: 2.3538, Test Loss: 2.3197, Learning Rate: 2.15e-05\n",
      "Step 3354, Train Loss: 2.2947, Test Loss: 2.2640, Learning Rate: 2.15e-05\n",
      "Step 3355, Train Loss: 2.4260, Test Loss: 2.3481, Learning Rate: 2.15e-05\n",
      "Step 3356, Train Loss: 2.2204, Test Loss: 2.3001, Learning Rate: 2.15e-05\n",
      "Step 3357, Train Loss: 2.5143, Test Loss: 2.0737, Learning Rate: 2.15e-05\n",
      "Step 3358, Train Loss: 2.1700, Test Loss: 2.4387, Learning Rate: 2.15e-05\n",
      "Step 3359, Train Loss: 2.2887, Test Loss: 2.3129, Learning Rate: 2.15e-05\n",
      "Step 3360, Train Loss: 2.2567, Test Loss: 2.4322, Learning Rate: 2.15e-05\n",
      "Step 3361, Train Loss: 2.3738, Test Loss: 2.2617, Learning Rate: 2.15e-05\n",
      "Step 3362, Train Loss: 2.2978, Test Loss: 2.3235, Learning Rate: 2.15e-05\n",
      "Step 3363, Train Loss: 2.3180, Test Loss: 2.1666, Learning Rate: 2.15e-05\n",
      "Step 3364, Train Loss: 2.2986, Test Loss: 2.3219, Learning Rate: 2.15e-05\n",
      "Step 3365, Train Loss: 2.3180, Test Loss: 2.4585, Learning Rate: 2.15e-05\n",
      "Step 3366, Train Loss: 2.3548, Test Loss: 2.4957, Learning Rate: 2.15e-05\n",
      "Step 3367, Train Loss: 2.2786, Test Loss: 2.3303, Learning Rate: 2.15e-05\n",
      "Step 3368, Train Loss: 2.3365, Test Loss: 2.3369, Learning Rate: 2.15e-05\n",
      "Step 3369, Train Loss: 2.3654, Test Loss: 2.4010, Learning Rate: 2.15e-05\n",
      "Step 3370, Train Loss: 2.3643, Test Loss: 2.0694, Learning Rate: 2.15e-05\n",
      "Step 3371, Train Loss: 2.2097, Test Loss: 2.2553, Learning Rate: 2.15e-05\n",
      "Step 3372, Train Loss: 2.3158, Test Loss: 2.4665, Learning Rate: 2.15e-05\n",
      "Step 3373, Train Loss: 2.3372, Test Loss: 2.4899, Learning Rate: 2.15e-05\n",
      "Step 3374, Train Loss: 2.3158, Test Loss: 2.3287, Learning Rate: 2.15e-05\n",
      "Step 3375, Train Loss: 2.3839, Test Loss: 2.3435, Learning Rate: 2.15e-05\n",
      "Step 3376, Train Loss: 2.3543, Test Loss: 2.1441, Learning Rate: 2.15e-05\n",
      "Step 3377, Train Loss: 2.4104, Test Loss: 2.3677, Learning Rate: 2.15e-05\n",
      "Step 3378, Train Loss: 2.4616, Test Loss: 2.1719, Learning Rate: 2.15e-05\n",
      "Step 3379, Train Loss: 2.2900, Test Loss: 2.3229, Learning Rate: 2.15e-05\n",
      "Step 3380, Train Loss: 2.4201, Test Loss: 2.1492, Learning Rate: 2.15e-05\n",
      "Step 3381, Train Loss: 2.2468, Test Loss: 2.3032, Learning Rate: 2.15e-05\n",
      "Step 3382, Train Loss: 2.2635, Test Loss: 2.2672, Learning Rate: 2.15e-05\n",
      "Step 3383, Train Loss: 2.2411, Test Loss: 2.3395, Learning Rate: 2.15e-05\n",
      "Step 3384, Train Loss: 2.4313, Test Loss: 2.4526, Learning Rate: 2.15e-05\n",
      "Step 3385, Train Loss: 2.3877, Test Loss: 2.2121, Learning Rate: 2.15e-05\n",
      "Step 3386, Train Loss: 2.3749, Test Loss: 2.2406, Learning Rate: 2.15e-05\n",
      "Step 3387, Train Loss: 2.3517, Test Loss: 2.2325, Learning Rate: 2.15e-05\n",
      "Step 3388, Train Loss: 2.3269, Test Loss: 2.2903, Learning Rate: 2.15e-05\n",
      "Step 3389, Train Loss: 2.2111, Test Loss: 2.4469, Learning Rate: 2.15e-05\n",
      "Step 3390, Train Loss: 2.2865, Test Loss: 2.4082, Learning Rate: 2.15e-05\n",
      "Step 3391, Train Loss: 2.3435, Test Loss: 2.3550, Learning Rate: 2.15e-05\n",
      "Step 3392, Train Loss: 2.2925, Test Loss: 1.9171, Learning Rate: 2.15e-05\n",
      "Step 3393, Train Loss: 2.3163, Test Loss: 2.2696, Learning Rate: 2.15e-05\n",
      "Step 3394, Train Loss: 2.1971, Test Loss: 2.3244, Learning Rate: 2.15e-05\n",
      "Step 3395, Train Loss: 2.3803, Test Loss: 2.4007, Learning Rate: 2.15e-05\n",
      "Step 3396, Train Loss: 2.4276, Test Loss: 2.0667, Learning Rate: 2.15e-05\n",
      "Step 3397, Train Loss: 2.3975, Test Loss: 2.2136, Learning Rate: 2.15e-05\n",
      "Step 3398, Train Loss: 2.2064, Test Loss: 2.4169, Learning Rate: 2.15e-05\n",
      "Step 3399, Train Loss: 2.1693, Test Loss: 2.4230, Learning Rate: 2.15e-05\n",
      "Step 3400, Train Loss: 2.3897, Test Loss: 2.3873, Learning Rate: 2.15e-05\n",
      "Step 3401, Train Loss: 2.3205, Test Loss: 2.3330, Learning Rate: 2.15e-05\n",
      "Step 3402, Train Loss: 2.2141, Test Loss: 2.3519, Learning Rate: 2.15e-05\n",
      "Step 3403, Train Loss: 2.2895, Test Loss: 2.3775, Learning Rate: 2.15e-05\n",
      "Step 3404, Train Loss: 2.5027, Test Loss: 2.3952, Learning Rate: 2.15e-05\n",
      "Step 3405, Train Loss: 2.3970, Test Loss: 2.2986, Learning Rate: 2.15e-05\n",
      "Step 3406, Train Loss: 2.2480, Test Loss: 2.4998, Learning Rate: 2.15e-05\n",
      "Step 3407, Train Loss: 2.2492, Test Loss: 2.2451, Learning Rate: 2.15e-05\n",
      "Step 3408, Train Loss: 2.3711, Test Loss: 2.1676, Learning Rate: 2.15e-05\n",
      "Step 3409, Train Loss: 2.2925, Test Loss: 2.3318, Learning Rate: 2.15e-05\n",
      "Step 3410, Train Loss: 2.4160, Test Loss: 2.4810, Learning Rate: 2.15e-05\n",
      "Step 3411, Train Loss: 2.3464, Test Loss: 2.3420, Learning Rate: 2.15e-05\n",
      "Step 3412, Train Loss: 2.3998, Test Loss: 2.4367, Learning Rate: 2.15e-05\n",
      "Step 3413, Train Loss: 2.3023, Test Loss: 2.3190, Learning Rate: 2.15e-05\n",
      "Step 3414, Train Loss: 2.3778, Test Loss: 2.0117, Learning Rate: 2.15e-05\n",
      "Step 3415, Train Loss: 2.3107, Test Loss: 2.2747, Learning Rate: 2.15e-05\n",
      "Step 3416, Train Loss: 2.3552, Test Loss: 2.4581, Learning Rate: 2.15e-05\n",
      "Step 3417, Train Loss: 2.3231, Test Loss: 2.3738, Learning Rate: 2.15e-05\n",
      "Step 3418, Train Loss: 2.3610, Test Loss: 2.3624, Learning Rate: 2.15e-05\n",
      "Step 3419, Train Loss: 2.2301, Test Loss: 2.1745, Learning Rate: 2.15e-05\n",
      "Step 3420, Train Loss: 2.3659, Test Loss: 2.1831, Learning Rate: 2.14e-05\n",
      "Step 3421, Train Loss: 2.3771, Test Loss: 2.4414, Learning Rate: 2.14e-05\n",
      "Step 3422, Train Loss: 2.2632, Test Loss: 2.1482, Learning Rate: 2.14e-05\n",
      "Step 3423, Train Loss: 2.3910, Test Loss: 2.4947, Learning Rate: 2.14e-05\n",
      "Step 3424, Train Loss: 2.4619, Test Loss: 2.4449, Learning Rate: 2.14e-05\n",
      "Step 3425, Train Loss: 2.2494, Test Loss: 2.4789, Learning Rate: 2.14e-05\n",
      "Step 3426, Train Loss: 2.3520, Test Loss: 2.4140, Learning Rate: 2.14e-05\n",
      "Step 3427, Train Loss: 2.4159, Test Loss: 2.5314, Learning Rate: 2.14e-05\n",
      "Step 3428, Train Loss: 2.4264, Test Loss: 2.0724, Learning Rate: 2.14e-05\n",
      "Step 3429, Train Loss: 2.2934, Test Loss: 2.2542, Learning Rate: 2.14e-05\n",
      "Step 3430, Train Loss: 2.2694, Test Loss: 2.3163, Learning Rate: 2.14e-05\n",
      "Step 3431, Train Loss: 2.3532, Test Loss: 2.2836, Learning Rate: 2.14e-05\n",
      "Step 3432, Train Loss: 2.2808, Test Loss: 2.2508, Learning Rate: 2.14e-05\n",
      "Step 3433, Train Loss: 2.2476, Test Loss: 2.2290, Learning Rate: 2.14e-05\n",
      "Step 3434, Train Loss: 2.3760, Test Loss: 2.5220, Learning Rate: 2.14e-05\n",
      "Step 3435, Train Loss: 2.3460, Test Loss: 2.3969, Learning Rate: 2.14e-05\n",
      "Step 3436, Train Loss: 2.3714, Test Loss: 2.4070, Learning Rate: 2.14e-05\n",
      "Step 3437, Train Loss: 2.2656, Test Loss: 2.1728, Learning Rate: 2.14e-05\n",
      "Step 3438, Train Loss: 2.3187, Test Loss: 2.3075, Learning Rate: 2.14e-05\n",
      "Step 3439, Train Loss: 2.3570, Test Loss: 2.4151, Learning Rate: 2.14e-05\n",
      "Step 3440, Train Loss: 2.4403, Test Loss: 2.2250, Learning Rate: 2.14e-05\n",
      "Step 3441, Train Loss: 2.3694, Test Loss: 2.4146, Learning Rate: 2.14e-05\n",
      "Step 3442, Train Loss: 2.3078, Test Loss: 2.3312, Learning Rate: 2.14e-05\n",
      "Step 3443, Train Loss: 2.4006, Test Loss: 2.3759, Learning Rate: 2.14e-05\n",
      "Step 3444, Train Loss: 2.3623, Test Loss: 2.3516, Learning Rate: 2.14e-05\n",
      "Step 3445, Train Loss: 2.2743, Test Loss: 2.5513, Learning Rate: 2.14e-05\n",
      "Step 3446, Train Loss: 2.3187, Test Loss: 2.1639, Learning Rate: 2.14e-05\n",
      "Step 3447, Train Loss: 2.3701, Test Loss: 2.1659, Learning Rate: 2.14e-05\n",
      "Step 3448, Train Loss: 2.4406, Test Loss: 2.3428, Learning Rate: 2.14e-05\n",
      "Step 3449, Train Loss: 2.3349, Test Loss: 2.3427, Learning Rate: 2.14e-05\n",
      "Step 3450, Train Loss: 2.1745, Test Loss: 2.4307, Learning Rate: 2.14e-05\n",
      "Step 3451, Train Loss: 2.3125, Test Loss: 2.2020, Learning Rate: 2.14e-05\n",
      "Step 3452, Train Loss: 2.2819, Test Loss: 2.2293, Learning Rate: 2.14e-05\n",
      "Step 3453, Train Loss: 2.2724, Test Loss: 2.2401, Learning Rate: 2.14e-05\n",
      "Step 3454, Train Loss: 2.4260, Test Loss: 2.0827, Learning Rate: 2.14e-05\n",
      "Step 3455, Train Loss: 2.3061, Test Loss: 2.1630, Learning Rate: 2.14e-05\n",
      "Step 3456, Train Loss: 2.4747, Test Loss: 2.3925, Learning Rate: 2.14e-05\n",
      "Step 3457, Train Loss: 2.3335, Test Loss: 2.3506, Learning Rate: 2.14e-05\n",
      "Step 3458, Train Loss: 2.3803, Test Loss: 2.2226, Learning Rate: 2.14e-05\n",
      "Step 3459, Train Loss: 2.3730, Test Loss: 2.2051, Learning Rate: 2.14e-05\n",
      "Step 3460, Train Loss: 2.4166, Test Loss: 2.2973, Learning Rate: 2.14e-05\n",
      "Step 3461, Train Loss: 2.3005, Test Loss: 2.2835, Learning Rate: 2.14e-05\n",
      "Step 3462, Train Loss: 2.5381, Test Loss: 2.3031, Learning Rate: 2.14e-05\n",
      "Step 3463, Train Loss: 2.3316, Test Loss: 2.1972, Learning Rate: 2.14e-05\n",
      "Step 3464, Train Loss: 2.3183, Test Loss: 2.2879, Learning Rate: 2.14e-05\n",
      "Step 3465, Train Loss: 2.3608, Test Loss: 2.3068, Learning Rate: 2.14e-05\n",
      "Step 3466, Train Loss: 2.2166, Test Loss: 2.5353, Learning Rate: 2.14e-05\n",
      "Step 3467, Train Loss: 2.3457, Test Loss: 2.6884, Learning Rate: 2.14e-05\n",
      "Step 3468, Train Loss: 2.2705, Test Loss: 2.3251, Learning Rate: 2.14e-05\n",
      "Step 3469, Train Loss: 2.2887, Test Loss: 2.1850, Learning Rate: 2.14e-05\n",
      "Step 3470, Train Loss: 2.2526, Test Loss: 2.4454, Learning Rate: 2.14e-05\n",
      "Step 3471, Train Loss: 2.3506, Test Loss: 2.3795, Learning Rate: 2.14e-05\n",
      "Step 3472, Train Loss: 2.2391, Test Loss: 2.3705, Learning Rate: 2.14e-05\n",
      "Step 3473, Train Loss: 2.5397, Test Loss: 2.1318, Learning Rate: 2.14e-05\n",
      "Step 3474, Train Loss: 2.3208, Test Loss: 2.2797, Learning Rate: 2.14e-05\n",
      "Step 3475, Train Loss: 2.3344, Test Loss: 2.5047, Learning Rate: 2.14e-05\n",
      "Step 3476, Train Loss: 2.2972, Test Loss: 2.2020, Learning Rate: 2.14e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (batch_train, batch_test) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(ds_train, ds_test))):\n\u001b[1;32m----> 3\u001b[0m     loss_train \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m      4\u001b[0m     losses_train\u001b[38;5;241m.\u001b[39mappend(loss_train)\n\u001b[0;32m      6\u001b[0m     loss_test \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(batch_test)\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, (batch_train, batch_test) in tqdm(enumerate(zip(ds_train, ds_test))):\n",
    "\n",
    "    loss_train = model.train_step(batch_train).numpy()\n",
    "    losses_train.append(loss_train)\n",
    "    \n",
    "    loss_test = model.evaluate(batch_test).numpy()\n",
    "    losses_test.append(loss_test)\n",
    "\n",
    "    if (i+1) % 1000 == 0:\n",
    "        ckpt_manager.save()\n",
    "        pkl.dump([losses_train, losses_test], open(\"checkpoints/losses_\" + name + \".pkl\", 'wb'))\n",
    "\n",
    "\n",
    "    lr = model.opt.inner_optimizer._decayed_lr(tf.float32).numpy()\n",
    "    \n",
    "    print(f\"Step {i+1}, Train Loss: {loss_train:.4f}, Test Loss: {loss_test:.4f}, Learning Rate: {lr:.2e}\")\n",
    "    \"\"\"\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # prepare x-axis for the last 400 steps\n",
    "    start = max(0, len(losses_train) - 1000)\n",
    "    x_zoom = np.arange(start, len(losses_train))\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 8), sharex=False)\n",
    "\n",
    "    # Top subplot: zoom on last 400 steps\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(x_zoom, losses_test[-1000:], label=\"Test Loss\")\n",
    "    ax1.plot(x_zoom, losses_train[-1000:], label=\"Train Loss\")\n",
    "\n",
    "    _min = min(losses_train[-1000:] + losses_test[-1000:])\n",
    "    _max = max(losses_train[-1000:] + losses_test[-1000:])\n",
    "    delta = _max - _min\n",
    "    #ax1.set_ylim(_min - 0.1 * delta, _max + 0.1 * delta)\n",
    "\n",
    "    ax1.set_title(\"Training Loss (Last 1000 Steps)\")\n",
    "    ax1.set_xlabel(\"Step\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Bottom subplot: full series\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(losses_test[10:], label=\"Test Loss\")\n",
    "    ax2.plot(losses_train[10:], label=\"Train Loss, lr = {:.2e}\".format(lr))\n",
    "\n",
    "    ax2.set_title(\"Training Loss (Full Series)\")\n",
    "    ax2.set_xlabel(\"Step\")\n",
    "    ax2.set_ylabel(\"Loss\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0eeef46d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(losses_test[\u001b[38;5;241m100\u001b[39m:], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(losses_train[\u001b[38;5;241m100\u001b[39m:], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss, lr = \u001b[39m\u001b[38;5;132;01m{:.2e}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[43mlr\u001b[49m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lr' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses_test[100:], label=\"Test Loss\")\n",
    "plt.plot(losses_train[100:], label=\"Train Loss, lr = {:.2e}\".format(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e216ce01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[16070     9   253    10     1    14    14     1 10094]], shape=(1, 9), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "text = \"(cnn) -- trump\"\n",
    "text = text.lower()\n",
    "SOS = tf.convert_to_tensor([[tokenizer.token_to_idx[\"<s>\"]]])\n",
    "indices = tf.cast(tokenizer.encode(text), tf.int32)\n",
    "indices = tf.concat([SOS, indices], axis=1)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "74a017b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6edaded9356406db83bb27188824180",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', disabled=True, layout=Layout(height='20em', width='80ch'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000027D30F0F0D0>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2761, in while_loop\n",
      "    while cond(*loop_vars):  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2753, in <lambda>\n",
      "    body = lambda i, lv: (i + 1, orig_body(*lv))  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\", line 655, in compute\n",
      "    return (next_i, flat_a_out, tas)  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\", line 650, in <listcomp>\n",
      "    tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_a_out)]  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 243, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n",
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x0000027DCDB9F670>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2761, in while_loop\n",
      "    while cond(*loop_vars):  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py\", line 2753, in <lambda>\n",
      "    body = lambda i, lv: (i + 1, orig_body(*lv))  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\", line 655, in compute\n",
      "    return (next_i, flat_a_out, tas)  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\functional_ops.py\", line 650, in <listcomp>\n",
      "    tas = [ta.write(i, value) for (ta, value) in zip(tas, flat_a_out)]  File \"c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py\", line 243, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs),\n",
      "==================================\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "T = 0.5\n",
    "tf.random.set_seed(42)\n",
    "wrapper = textwrap.TextWrapper(width=80)\n",
    "\n",
    "# create a read-only text area\n",
    "ta = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    layout=widgets.Layout(width='80ch', height='20em'),\n",
    "    disabled=True\n",
    ")\n",
    "display(ta)\n",
    "\n",
    "for i in range(512):\n",
    "    logits = model.call(indices)[0, -1:]\n",
    "    idx = tf.cast(\n",
    "        tf.random.categorical(logits / T, num_samples=1),\n",
    "        tf.int32\n",
    "    )\n",
    "    indices = tf.concat([indices, idx], axis=1)\n",
    "\n",
    "    text_pred = (\n",
    "        tokenizer\n",
    "        .decode(indices)\n",
    "        .numpy()[0]\n",
    "        .decode('utf-8')\n",
    "        .replace(\"\\n\", \" \")\n",
    "    )\n",
    "    ta.value = wrapper.fill(text_pred)  # this updates in-place\n",
    "\n",
    "    if idx[0, 0] == tokenizer.token_to_idx[\"</s>\"]:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680d4eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[553]], shape=(1, 1), dtype=int32)\n",
      "obama are a of the the the of the a of the of the the of with the u by international the first\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5588\\3758968849.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m43\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0midx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mtext_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\krist\\Documents\\llm-basics\\src\\transformer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, x, training, logits)\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_blocks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munembed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\krist\\Documents\\llm-basics\\src\\transformer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, x_embeds, training)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_embeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mx_embeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_embeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m         \u001b[0mx_embeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mffnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_embeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx_embeds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\krist\\Documents\\llm-basics\\src\\transformer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, x_embeds, training)\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_up\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_embeds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_down\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdol3\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mln2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mout\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mx_embeds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1093\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1094\u001b[0m                 with autocast_variable.enable_auto_cast_variables(\n\u001b[0;32m   1095\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_compute_dtype_object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1096\u001b[0m                 ):\n\u001b[1;32m-> 1097\u001b[1;33m                     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1098\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1100\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[0mnew_e\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mnew_e\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m             \u001b[1;32mdel\u001b[0m \u001b[0mbound_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\keras\\layers\\normalization\\layer_normalization.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    283\u001b[0m                 \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"float32\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m             ):\n\u001b[0;32m    285\u001b[0m                 \u001b[1;31m# If mixed precision is used, cast inputs to float32 so that\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    286\u001b[0m                 \u001b[1;31m# this is at least as numerically stable as the fused version.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 287\u001b[1;33m                 \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"float32\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m             \u001b[1;31m# Calculate the moments on the last axis (layer activations).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m             \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoments\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1174\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, dtype, name)\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[1;31m# allows some conversions that cast() can't do, e.g. casting numbers to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1000\u001b[0m       \u001b[1;31m# strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"x\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1002\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1003\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1004\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_complex\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mbase_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_floating\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Casting complex to real discards imaginary part.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, DstT, Truncate, name)\u001b[0m\n\u001b[0;32m   1997\u001b[0m         _ctx, \"Cast\", name, x, \"DstT\", DstT, \"Truncate\", Truncate)\n\u001b[0;32m   1998\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1999\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2000\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2001\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2002\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2003\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2004\u001b[0m       return cast_eager_fallback(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text = \"obama\"\n",
    "text = text.lower()\n",
    "\n",
    "indices = tf.cast(tokenizer.tokenize(text), tf.int32)\n",
    "print(indices)\n",
    "\n",
    "T = 0.5\n",
    "tf.random.set_seed(43)\n",
    "for i in range(128):\n",
    "    logits = model.call(indices)[0,-1:]\n",
    "    idx = tf.cast(tf.random.categorical(logits/T, num_samples=1), tf.int32)\n",
    "    indices = tf.concat([indices, idx], axis=1)\n",
    "    text_pred = tokenizer.detokenize(indices)\n",
    "    text_pred = text_pred.numpy()[0].decode('utf-8').replace(\"\\n\", \" \")\n",
    "    print(text_pred, end='\\r', flush=True)\n",
    "    #time.sleep(0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77fb01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def cosine_similarity(embed_a, embed_b):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two vectors.\n",
    "    \"\"\"\n",
    "    embed_b_T = tf.transpose(embed_b)\n",
    "    dot_product = embed_a@embed_b_T\n",
    "    \n",
    "    norm_a = tf.linalg.norm(embed_a, axis=1, keepdims=True)\n",
    "    norm_b = tf.linalg.norm(embed_b_T, axis=0, keepdims=True)\n",
    "\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "\n",
    "def cluster(X, n_clusters, normalize=True):\n",
    "    if normalize:\n",
    "        X = X/np.linalg.norm(X, axis=1, keepdims=True)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n",
    "    inertia = kmeans.inertia_\n",
    "    labels = kmeans.labels_\n",
    "    clusters = kmeans.cluster_centers_\n",
    "\n",
    "    return inertia, labels, clusters\n",
    "\n",
    "\n",
    "class EmbeddingClustering:\n",
    "    def __init__(self, tokenizer, n_clusters=10):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_clusters = n_clusters\n",
    "\n",
    "    def fit(self, word_embed, normalize=True):\n",
    "        inertia, labels, clusters = cluster(word_embed, self.n_clusters, normalize)\n",
    "        self.word_embed = word_embed\n",
    "        self.inertia = inertia\n",
    "        self.labels = labels\n",
    "        self.clusters = tf.convert_to_tensor(clusters, dtype=tf.float32)\n",
    "\n",
    "        cos_sim = cosine_similarity(self.clusters, word_embed, normalize)\n",
    "        self.idx_list =  tf.argsort(cos_sim, axis=-1, direction='DESCENDING', stable=False, name=None)\n",
    "\n",
    "    def print_clusters(self, n_words=10):\n",
    "        for idx in self.idx_list:\n",
    "            for i in idx[:n_words]:\n",
    "                word = self.tokenizer.detokenize(tf.expand_dims(tf.cast(i, tf.int32), axis=0))\n",
    "                word = word.numpy().decode('utf-8')\n",
    "                print(word)\n",
    "            print(\"\\n\")\n",
    "\n",
    "\n",
    "def cosine_similarity(embed_a, embed_b, normalize=True):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two vectors.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        embed_a = tf.nn.l2_normalize(embed_a, axis=1)\n",
    "        embed_b = tf.nn.l2_normalize(embed_b, axis=1)\n",
    "    dot_product = embed_a@tf.transpose(embed_b)\n",
    "\n",
    "\n",
    "    return dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48583bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debbie\n",
      "denise\n",
      "katherine\n",
      "stephanie\n",
      "randy\n",
      "jason\n",
      "patricia\n",
      "kathy\n",
      "jesse\n",
      "sergio\n",
      "\n",
      "\n",
      "deficits\n",
      "revenues\n",
      "reductions\n",
      "subsidies\n",
      "incentives\n",
      "loans\n",
      "exports\n",
      "budgets\n",
      "salaries\n",
      "bonuses\n",
      "\n",
      "\n",
      "attem\n",
      "theless\n",
      "occas\n",
      "ieval\n",
      "theastern\n",
      "enthusia\n",
      "negot\n",
      "lesterol\n",
      "subsequ\n",
      "fahren\n",
      "\n",
      "\n",
      "occas\n",
      "afgh\n",
      "negot\n",
      "includ\n",
      "delaw\n",
      "glary\n",
      "ifics\n",
      "theast\n",
      "salad\n",
      "patro\n",
      "\n",
      "\n",
      "theastern\n",
      "experi\n",
      "delaw\n",
      "theast\n",
      "lesterol\n",
      "dort\n",
      "semif\n",
      "attem\n",
      "surpris\n",
      "compon\n",
      "\n",
      "\n",
      "ffed\n",
      "strugg\n",
      "portra\n",
      "athle\n",
      "massachu\n",
      "ifics\n",
      "pered\n",
      "experi\n",
      "arct\n",
      "thered\n",
      "\n",
      "\n",
      "subsequ\n",
      "theast\n",
      "prede\n",
      "occas\n",
      "theastern\n",
      "ieval\n",
      "lesterol\n",
      "experi\n",
      "arct\n",
      "ailand\n",
      "\n",
      "\n",
      "pretty\n",
      "fairly\n",
      "reasonably\n",
      "utterly\n",
      "remarkably\n",
      "substantially\n",
      "fundamentally\n",
      "relatively\n",
      "totally\n",
      "unusually\n",
      "\n",
      "\n",
      "deal\n",
      "challenge\n",
      "talk\n",
      "question\n",
      "move\n",
      "answer\n",
      "appeal\n",
      "attempt\n",
      "approach\n",
      "decision\n",
      "\n",
      "\n",
      "sending\n",
      "helping\n",
      "letting\n",
      "handing\n",
      "putting\n",
      "taking\n",
      "giving\n",
      "bringing\n",
      "pulling\n",
      "throwing\n",
      "\n",
      "\n",
      "tens\n",
      "hundreds\n",
      "thousands\n",
      "dozens\n",
      "millions\n",
      "billions\n",
      "plenty\n",
      "sort\n",
      "there\n",
      "none\n",
      "\n",
      "\n",
      "uguay\n",
      "theid\n",
      "lesterol\n",
      "negot\n",
      "theastern\n",
      "delaw\n",
      "ailand\n",
      "occas\n",
      "guate\n",
      "arct\n",
      "\n",
      "\n",
      "convince\n",
      "impose\n",
      "abide\n",
      "eliminate\n",
      "undermine\n",
      "prohibit\n",
      "tolerate\n",
      "educate\n",
      "satisfy\n",
      "violate\n",
      "\n",
      "\n",
      "two\n",
      "three\n",
      "four\n",
      "the\n",
      "six\n",
      "a\n",
      "several\n",
      "some\n",
      "no\n",
      "more\n",
      "\n",
      "\n",
      "christop\n",
      "djok\n",
      "berdy\n",
      "tember\n",
      "benjam\n",
      "whate\n",
      "aragu\n",
      "petrole\n",
      "dono\n",
      "bundes\n",
      "\n",
      "\n",
      "gives\n",
      "helps\n",
      "provides\n",
      "encourages\n",
      "relies\n",
      "deserves\n",
      "refuses\n",
      "enjoys\n",
      "seeks\n",
      "tends\n",
      "\n",
      "\n",
      "renewable\n",
      "greenhouse\n",
      "geological\n",
      "tropical\n",
      "mountainous\n",
      "carbon\n",
      "saharan\n",
      "rugged\n",
      "atomic\n",
      "fukushima\n",
      "\n",
      "\n",
      "encouraged\n",
      "instructed\n",
      "persuaded\n",
      "refused\n",
      "urged\n",
      "subjected\n",
      "managed\n",
      "referred\n",
      "helped\n",
      "unable\n",
      "\n",
      "\n",
      "glary\n",
      "gbag\n",
      "strugg\n",
      "espion\n",
      "risings\n",
      "beliefs\n",
      "guate\n",
      "transparen\n",
      "athle\n",
      "usalem\n",
      "\n",
      "\n",
      "behavioral\n",
      "psychiatric\n",
      "reproductive\n",
      "cognitive\n",
      "bodily\n",
      "preventive\n",
      "genetic\n",
      "spinal\n",
      "traumatic\n",
      "infectious\n",
      "\n",
      "\n",
      "1972\n",
      "2002\n",
      "1992\n",
      "2001\n",
      "1993\n",
      "2003\n",
      "1969\n",
      "1995\n",
      "1990\n",
      "2005\n",
      "\n",
      "\n",
      "glary\n",
      "burglary\n",
      "interrogation\n",
      "gbag\n",
      "portra\n",
      "prede\n",
      "afgh\n",
      "fahren\n",
      "environ\n",
      "lesterol\n",
      "\n",
      "\n",
      "bolivia\n",
      "tunisia\n",
      "belarus\n",
      "croatia\n",
      "kazakhstan\n",
      "portugal\n",
      "serbia\n",
      "uzbek\n",
      "paraguay\n",
      "algeria\n",
      "\n",
      "\n",
      "indicted\n",
      "accuses\n",
      "sentenced\n",
      "pleaded\n",
      "criticized\n",
      "denounced\n",
      "denied\n",
      "hailed\n",
      "dismissed\n",
      "acquitted\n",
      "\n",
      "\n",
      "delicious\n",
      "magical\n",
      "exciting\n",
      "fascinating\n",
      "vibrant\n",
      "formidable\n",
      "thoughtful\n",
      "lovely\n",
      "beneficial\n",
      "profound\n",
      "\n",
      "\n",
      "nostal\n",
      "norwe\n",
      "twel\n",
      "archite\n",
      "aero\n",
      "nove\n",
      "reci\n",
      "engul\n",
      "whate\n",
      "anthro\n",
      "\n",
      "\n",
      "oldest\n",
      "longest\n",
      "lowest\n",
      "fastest\n",
      "tallest\n",
      "busiest\n",
      "hottest\n",
      "12th\n",
      "youngest\n",
      "16th\n",
      "\n",
      "\n",
      "unsure\n",
      "wondering\n",
      "convinced\n",
      "insisting\n",
      "wondered\n",
      "reminded\n",
      "complaining\n",
      "excited\n",
      "arguing\n",
      "hoping\n",
      "\n",
      "\n",
      "ag\n",
      " \n",
      "\n",
      "\n",
      "leng\n",
      "scand\n",
      "ig\n",
      "uc\n",
      "em\n",
      "ud\n",
      "mar\n",
      "\n",
      "\n",
      "inno\n",
      "spon\n",
      "mber\n",
      "lene\n",
      "resu\n",
      "mediterran\n",
      "uke\n",
      "tember\n",
      "bam\n",
      "kyr\n",
      "\n",
      "\n",
      "mo\n",
      "lo\n",
      "con\n",
      "mu\n",
      "de\n",
      "sha\n",
      "du\n",
      "su\n",
      "tu\n",
      "hu\n",
      "\n",
      "\n",
      "ailand\n",
      "zimbab\n",
      "occas\n",
      "immen\n",
      "includ\n",
      "delaw\n",
      "athle\n",
      "experi\n",
      "negot\n",
      "hrir\n",
      "\n",
      "\n",
      "grandmother\n",
      "roommate\n",
      "boyfriend\n",
      "girlfriend\n",
      "fiance\n",
      "aunt\n",
      "cousin\n",
      "fiancee\n",
      "nephew\n",
      "counselor\n",
      "\n",
      "\n",
      "ised\n",
      "ipped\n",
      "aded\n",
      "ered\n",
      "uted\n",
      "ized\n",
      "ilized\n",
      "oned\n",
      "ished\n",
      "aled\n",
      "\n",
      "\n",
      "28\n",
      "15\n",
      "36\n",
      "29\n",
      "25\n",
      "23\n",
      "39\n",
      "40\n",
      "27\n",
      "38\n",
      "\n",
      "\n",
      "popu\n",
      "magist\n",
      "nove\n",
      "theat\n",
      "injun\n",
      "frust\n",
      "juris\n",
      "convin\n",
      "immig\n",
      "anthro\n",
      "\n",
      "\n",
      "spacecraft\n",
      "telescope\n",
      "airliner\n",
      "dreamliner\n",
      "tanker\n",
      "delaw\n",
      "submarine\n",
      "airbus\n",
      "occas\n",
      "plane's\n",
      "\n",
      "\n",
      "theless\n",
      "lesterol\n",
      "occas\n",
      "subsequ\n",
      "assage\n",
      "fahren\n",
      "ailand\n",
      "secutive\n",
      "luscon\n",
      "theid\n",
      "\n",
      "\n",
      "atp\n",
      "paralympic\n",
      "tennis\n",
      "wta\n",
      "basketball\n",
      "rugby\n",
      "athletics\n",
      "wimbledon\n",
      "jazz\n",
      "players'\n",
      "\n",
      "\n",
      "civil\n",
      "foreign\n",
      "political\n",
      "supreme\n",
      "financial\n",
      "constitutional\n",
      "legislative\n",
      "social\n",
      "economic\n",
      "immigration\n",
      "\n",
      "\n",
      "i'd\n",
      "we'd\n",
      "you'd\n",
      "we'll\n",
      "we've\n",
      "i'll\n",
      "you've\n",
      "i've\n",
      "hasn't\n",
      "they'd\n",
      "\n",
      "\n",
      "investigators\n",
      "officials\n",
      "residents\n",
      "doctors\n",
      "firefighters\n",
      "authorities\n",
      "students\n",
      "experts\n",
      "activists\n",
      "attorneys\n",
      "\n",
      "\n",
      "uguay\n",
      "negot\n",
      "rouhani\n",
      "maduro\n",
      "athle\n",
      "occas\n",
      "guate\n",
      "gbag\n",
      "afgh\n",
      "theastern\n",
      "\n",
      "\n",
      "injunction\n",
      "rulings\n",
      "athle\n",
      "experi\n",
      "tribunal\n",
      "guate\n",
      "arct\n",
      "theastern\n",
      "itored\n",
      "gbag\n",
      "\n",
      "\n",
      "scrut\n",
      "inflam\n",
      "nutr\n",
      "reim\n",
      "symp\n",
      "sophist\n",
      "inef\n",
      "frust\n",
      "erad\n",
      "scand\n",
      "\n",
      "\n",
      "enthusia\n",
      "immen\n",
      "usalem\n",
      "surpris\n",
      "fahren\n",
      "delaw\n",
      "assage\n",
      "theless\n",
      "subsequ\n",
      "theid\n",
      "\n",
      "\n",
      "strategist\n",
      "coordinator\n",
      "commentator\n",
      "columnist\n",
      "chairwoman\n",
      "mogul\n",
      "grapher\n",
      "adviser\n",
      "contributor\n",
      "historian\n",
      "\n",
      "\n",
      "hosni\n",
      "christiane\n",
      "jethro\n",
      "cristiano\n",
      "elise\n",
      "kanye\n",
      "saad\n",
      "udad\n",
      "rory\n",
      "charac\n",
      "\n",
      "\n",
      "concerns\n",
      "doubts\n",
      "commitment\n",
      "distinction\n",
      "sympathy\n",
      "concern\n",
      "efforts\n",
      "frustration\n",
      "connections\n",
      "questions\n",
      "\n",
      "\n",
      "teed\n",
      "descended\n",
      "poured\n",
      "climbed\n",
      "bounced\n",
      "slipped\n",
      "tossed\n",
      "wiped\n",
      "ripped\n",
      "edged\n",
      "\n",
      "\n",
      "fbi's\n",
      "singer's\n",
      "tour's\n",
      "organization's\n",
      "military's\n",
      "couple's\n",
      "agency's\n",
      "minister's\n",
      "army's\n",
      "show's\n",
      "\n",
      "\n",
      "175\n",
      "450\n",
      "550\n",
      "650\n",
      "120\n",
      "250\n",
      "750\n",
      "260\n",
      "270\n",
      "240\n",
      "\n",
      "\n",
      "hampered\n",
      "governed\n",
      "enriched\n",
      "administered\n",
      "transmitted\n",
      "traced\n",
      "regulated\n",
      "renewable\n",
      "populated\n",
      "resistant\n",
      "\n",
      "\n",
      "involuntary\n",
      "impending\n",
      "lucrative\n",
      "immediate\n",
      "deepwater\n",
      "prolonged\n",
      "unnamed\n",
      "extensive\n",
      "broader\n",
      "lengthy\n",
      "\n",
      "\n",
      "photograp\n",
      "moroc\n",
      "leng\n",
      "reim\n",
      "resur\n",
      "refres\n",
      "phis\n",
      "inflam\n",
      "detro\n",
      "resor\n",
      "\n",
      "\n",
      "generated\n",
      "influenced\n",
      "benefited\n",
      "initiated\n",
      "supported\n",
      "supplied\n",
      "aided\n",
      "attracted\n",
      "reviewed\n",
      "acquired\n",
      "\n",
      "\n",
      "theless\n",
      "negot\n",
      "compon\n",
      "ivid\n",
      "enthusia\n",
      "assage\n",
      "immen\n",
      "usalem\n",
      "dort\n",
      "itored\n",
      "\n",
      "\n",
      "scrut\n",
      "fict\n",
      "confis\n",
      "insurg\n",
      "proxim\n",
      "accompan\n",
      "obst\n",
      "o'ne\n",
      "itored\n",
      "dys\n",
      "\n",
      "\n",
      "profound\n",
      "dense\n",
      "immense\n",
      "substantial\n",
      "vague\n",
      "fierce\n",
      "fragile\n",
      "tremendous\n",
      "neat\n",
      "enormous\n",
      "\n",
      "\n",
      "kansas\n",
      "connecticut\n",
      "illinois\n",
      "pennsylvania\n",
      "arkansas\n",
      "louisiana\n",
      "maryland\n",
      "missouri\n",
      "tampa\n",
      "wisconsin\n",
      "\n",
      "\n",
      "ley's\n",
      "er's\n",
      "i's\n",
      "e's\n",
      "an's\n",
      "ton's\n",
      "on's\n",
      "ie's\n",
      "es'\n",
      "ler's\n",
      "\n",
      "\n",
      "espion\n",
      "explos\n",
      "unbeliev\n",
      "mclaugh\n",
      "inevit\n",
      "gbag\n",
      "ieval\n",
      "strugg\n",
      "itored\n",
      "glary\n",
      "\n",
      "\n",
      "athle\n",
      "drivers'\n",
      "quarterfinals\n",
      "standings\n",
      "occas\n",
      "espion\n",
      "liga\n",
      "couver\n",
      "glary\n",
      "bundesliga\n",
      "\n",
      "\n",
      "higher\n",
      "better\n",
      "harder\n",
      "faster\n",
      "bigger\n",
      "deeper\n",
      "worse\n",
      "greater\n",
      "cheaper\n",
      "stronger\n",
      "\n",
      "\n",
      "whate\n",
      "cuis\n",
      "manh\n",
      "accompan\n",
      "espion\n",
      "demp\n",
      "delaw\n",
      "o'ne\n",
      "enthusia\n",
      "theid\n",
      "\n",
      "\n",
      "anbar\n",
      "fallu\n",
      "occas\n",
      "homs\n",
      "environ\n",
      "mosul\n",
      "daraa\n",
      "lesterol\n",
      "ttp\n",
      "ifics\n",
      "\n",
      "\n",
      "confis\n",
      "dort\n",
      "compon\n",
      "attem\n",
      "ieval\n",
      "enrich\n",
      "incre\n",
      "surpris\n",
      "includ\n",
      "subsequ\n",
      "\n",
      "\n",
      "bullied\n",
      "depressed\n",
      "ashamed\n",
      "raped\n",
      "saddened\n",
      "gbag\n",
      "charac\n",
      "obese\n",
      "terrified\n",
      "ifics\n",
      "\n",
      "\n",
      "rehear\n",
      "engul\n",
      "resur\n",
      "eclip\n",
      "reim\n",
      "glimp\n",
      "moroc\n",
      "suc\n",
      "spear\n",
      "popu\n",
      "\n",
      "\n",
      "year\n",
      "friday\n",
      "week\n",
      "month\n",
      "sunday\n",
      "thursday\n",
      "monday\n",
      "saturday\n",
      "tuesday\n",
      "wednesday\n",
      "\n",
      "\n",
      "rampage\n",
      "bombings\n",
      "clashes\n",
      "shootings\n",
      "killings\n",
      "massacre\n",
      "slayings\n",
      "explosions\n",
      "altercation\n",
      "siege\n",
      "\n",
      "\n",
      "requiring\n",
      "letting\n",
      "violating\n",
      "distributing\n",
      "implementing\n",
      "introducing\n",
      "reducing\n",
      "ordering\n",
      "enforcing\n",
      "eliminating\n",
      "\n",
      "\n",
      "odox\n",
      "athle\n",
      "ifics\n",
      "massachu\n",
      "secutive\n",
      "charac\n",
      "mbley\n",
      "anmen\n",
      "tournam\n",
      "hrir\n",
      "\n",
      "\n",
      "museum\n",
      "institution\n",
      "facility\n",
      "library\n",
      "institute\n",
      "corporation\n",
      "organization\n",
      "council\n",
      "foundation\n",
      "ministry\n",
      "\n",
      "\n",
      "dys\n",
      "distr\n",
      "retr\n",
      "desc\n",
      "contr\n",
      "cont\n",
      "extr\n",
      "appreh\n",
      "videot\n",
      "transc\n",
      "\n",
      "\n",
      "insisted\n",
      "explained\n",
      "argued\n",
      "cautioned\n",
      "replied\n",
      "joked\n",
      "testified\n",
      "acknowledged\n",
      "wondered\n",
      "vowed\n",
      "\n",
      "\n",
      "independents\n",
      "hispanics\n",
      "colleges\n",
      "economists\n",
      "liberals\n",
      "households\n",
      "respondents\n",
      "entrepreneurs\n",
      "conservatives\n",
      "governors\n",
      "\n",
      "\n",
      "scand\n",
      "traged\n",
      "dort\n",
      "strug\n",
      "yose\n",
      "jere\n",
      "compon\n",
      "surpris\n",
      "califor\n",
      "theless\n",
      "\n",
      "\n",
      "detonated\n",
      "stormed\n",
      "grabbed\n",
      "engulfed\n",
      "transported\n",
      "raided\n",
      "chased\n",
      "entered\n",
      "collided\n",
      "invaded\n",
      "\n",
      "\n",
      "albums\n",
      "singers\n",
      "festivals\n",
      "musicians\n",
      "novels\n",
      "uguay\n",
      "espion\n",
      "glary\n",
      "strugg\n",
      "oscars\n",
      "\n",
      "\n",
      "in\n",
      "at\n",
      "from\n",
      "by\n",
      "when\n",
      "during\n",
      "after\n",
      "for\n",
      "on\n",
      "with\n",
      "\n",
      "\n",
      "certainly\n",
      "probably\n",
      "definitely\n",
      "never\n",
      "obviously\n",
      "hardly\n",
      "surely\n",
      "actually\n",
      "always\n",
      "usually\n",
      "\n",
      "\n",
      "suites\n",
      "floors\n",
      "shelters\n",
      "beaches\n",
      "trees\n",
      "pools\n",
      "bottles\n",
      "shops\n",
      "delaw\n",
      "glary\n",
      "\n",
      "\n",
      "boko\n",
      "yemeni\n",
      "somali\n",
      "somalia's\n",
      "saudi\n",
      "lebanese\n",
      "bosnian\n",
      "sri\n",
      "sudanese\n",
      "transitional\n",
      "\n",
      "\n",
      "responses\n",
      "attempts\n",
      "lawsuits\n",
      "decisions\n",
      "statements\n",
      "inquiries\n",
      "agreements\n",
      "proposals\n",
      "accusations\n",
      "discussions\n",
      "\n",
      "\n",
      "islamists\n",
      "militias\n",
      "shiites\n",
      "gunmen\n",
      "kurds\n",
      "militants\n",
      "egyptians\n",
      "insurgents\n",
      "houthis\n",
      "sunnis\n",
      "\n",
      "\n",
      "couver\n",
      "ailand\n",
      "inals\n",
      "glary\n",
      "ences\n",
      "ivid\n",
      "espion\n",
      "hrir\n",
      "theid\n",
      "negot\n",
      "\n",
      "\n",
      "mashable\n",
      "android\n",
      "verizon\n",
      "itunes\n",
      "playstation\n",
      "samsung\n",
      "nintend\n",
      "nintendo\n",
      "silicon\n",
      "netflix\n",
      "\n",
      "\n",
      "rainfall\n",
      "floods\n",
      "diarrhea\n",
      "devastation\n",
      "outages\n",
      "rains\n",
      "earthquakes\n",
      "vomiting\n",
      "espion\n",
      "snowfall\n",
      "\n",
      "\n",
      "scrut\n",
      "testim\n",
      "rele\n",
      "nutr\n",
      "proto\n",
      "leng\n",
      "scand\n",
      "obst\n",
      "accompan\n",
      "whate\n",
      "\n",
      "\n",
      "f\n",
      "l\n",
      "g\n",
      "d\n",
      "r\n",
      "t\n",
      " \n",
      "v\n",
      "b\n",
      "k\n",
      "\n",
      "\n",
      "lebanon's\n",
      "london's\n",
      "yemen's\n",
      "football's\n",
      "ukraine's\n",
      "egypt's\n",
      "afghanistan's\n",
      "libya's\n",
      "somalia's\n",
      "greece's\n",
      "\n",
      "\n",
      "sectors\n",
      "technologies\n",
      "sensors\n",
      "industries\n",
      "providers\n",
      "environments\n",
      "techniques\n",
      "entities\n",
      "installations\n",
      "tasks\n",
      "\n",
      "\n",
      "hrir\n",
      "lesterol\n",
      "glary\n",
      "icting\n",
      "charac\n",
      "fahren\n",
      "mbley\n",
      "negot\n",
      "couver\n",
      "oking\n",
      "\n",
      "\n",
      "assumption\n",
      "acknowled\n",
      "reminder\n",
      "notion\n",
      "mechanism\n",
      "incentive\n",
      "espion\n",
      "strugg\n",
      "obstacle\n",
      "inevit\n",
      "\n",
      "\n",
      "promptly\n",
      "swiftly\n",
      "safely\n",
      "adequately\n",
      "broadly\n",
      "properly\n",
      "readily\n",
      "thoroughly\n",
      "instantly\n",
      "voluntarily\n",
      "\n",
      "\n",
      "give\n",
      "make\n",
      "bring\n",
      "get\n",
      "want\n",
      "take\n",
      "tell\n",
      "subscribe\n",
      "learn\n",
      "keep\n",
      "\n",
      "\n",
      "assage\n",
      "enthusia\n",
      "theid\n",
      "uguay\n",
      "usalem\n",
      "ieval\n",
      "negot\n",
      "espion\n",
      "moil\n",
      "performan\n",
      "\n",
      "\n",
      "confis\n",
      "compon\n",
      "surpris\n",
      "notor\n",
      "dort\n",
      "attem\n",
      "scrut\n",
      "insurg\n",
      "refres\n",
      "accompan\n",
      "\n",
      "\n",
      "went\n",
      "took\n",
      "brought\n",
      "came\n",
      "pulled\n",
      "turned\n",
      "moved\n",
      "gave\n",
      "jumped\n",
      "walked\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_embed = model.word_embed\n",
    "embedding_clustering = EmbeddingClustering(tokenizer, n_clusters=100)\n",
    "embedding_clustering.fit(word_embed, normalize=True)\n",
    "embedding_clustering.print_clusters(n_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc968e2d",
   "metadata": {},
   "source": [
    "# Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de0e293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1602]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([[3512]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([[5393]], shape=(1, 1), dtype=int32)\n",
      "netanyahu\n",
      "russia\n",
      "israel\n",
      "hamas\n",
      "israelis\n",
      "jerusalem\n",
      "tehran\n",
      "kiev\n",
      "gaza\n",
      "palestinians\n",
      "democr\n",
      "beirut\n",
      "azer\n",
      "syria\n",
      "egypt\n",
      "idf\n",
      "iran\n",
      "britain\n",
      "palestinian\n",
      "abbas\n",
      "tunisia\n",
      "alger\n",
      "lebanon\n",
      "perpe\n",
      "israeli\n",
      "davos\n",
      "brahim\n",
      "controver\n",
      "hezbollah\n",
      "hagel\n",
      "jevich\n",
      "norway\n",
      "fah\n",
      "guinea\n",
      "khamenei\n",
      "cuba\n",
      "anbar\n",
      "utt\n",
      "khamene\n",
      "hezbol\n",
      "weren\n",
      "canada\n",
      "sunnis\n",
      "dipl\n",
      "stoke\n",
      "lavrov\n",
      "aviv\n",
      "karzai\n",
      "israel's\n",
      "arct\n",
      "cambodia\n",
      "zuckerberg\n",
      "yanukov\n",
      "yad\n",
      "ukraine\n",
      "pakistan\n",
      "carney\n",
      "netherlands\n",
      "cairo\n",
      "lieberman\n",
      "panetta\n",
      "homs\n",
      "zawah\n",
      "austria\n",
      "espion\n",
      "wawrink\n",
      "vinc\n",
      "libertar\n",
      "libya\n",
      "poland\n",
      "indonesia\n",
      "liby\n",
      "merkel\n",
      "pyongyang\n",
      "tik\n",
      "airstrikes\n",
      "ibrahimovic\n",
      "abe\n",
      "iran's\n",
      "yugo\n",
      "kass\n",
      "mosul\n",
      "galax\n",
      "yemen\n",
      "scotland\n",
      "settlements\n",
      "sudan\n",
      "nuri\n",
      "niger\n",
      "palestine\n",
      "tsvangira\n",
      "ieval\n",
      "iaea\n",
      "denmark\n",
      "hmer\n",
      "tahrir\n",
      "nusra\n",
      "sarkoz\n",
      "ukrain\n",
      "bolivia\n"
     ]
    }
   ],
   "source": [
    "word_embed = model.word_embed\n",
    "\n",
    "text = \"russia\"\n",
    "text = text.lower()\n",
    "\n",
    "idx = tf.cast(tokenizer.tokenizer.tokenize(text), tf.int32)\n",
    "idx = tokenize(idx, tokenizer.merge_list)\n",
    "print(idx)\n",
    "embed1 = tf.expand_dims(word_embed[idx[0][0]], axis=0)\n",
    "\n",
    "\n",
    "text = \"putin\"\n",
    "text = text.lower()\n",
    "\n",
    "idx = tf.cast(tokenizer.tokenizer.tokenize(text), tf.int32)\n",
    "idx = tokenize(idx, tokenizer.merge_list)\n",
    "print(idx)\n",
    "embed2 = tf.expand_dims(word_embed[idx[0][0]], axis=0)\n",
    "\n",
    "text = \"netanyahu\"\n",
    "text = text.lower()\n",
    "\n",
    "idx = tf.cast(tokenizer.tokenizer.tokenize(text), tf.int32)\n",
    "idx = tokenize(idx, tokenizer.merge_list)\n",
    "print(idx)\n",
    "embed3 = tf.expand_dims(word_embed[idx[0][0]], axis=0)\n",
    "\n",
    "embed = embed1 - embed2 + embed3\n",
    "\n",
    "cosine_sim = cosine_similarity(embed, word_embed, normalize=False)\n",
    "idx = tf.argsort(cosine_sim, axis=-1, \n",
    "                 direction='DESCENDING',\n",
    "                 #direction='ASCENDING', \n",
    "                 stable=False, name=None)[0]\n",
    "\n",
    "for i in idx[:100]:\n",
    "    i = tf.expand_dims(i, axis=0)\n",
    "    print(tokenizer.detokenize(i).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54734624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[553]], shape=(1, 1), dtype=int32)\n",
      "obama\n",
      "obama's\n",
      "clinton\n",
      "romney\n",
      "republicans\n",
      "bush\n",
      "boehner\n",
      "sen\n",
      "reagan\n",
      "democrats\n",
      "barack\n",
      "mccain\n",
      "congressional\n",
      "sarkozy\n",
      "pentagon\n",
      "putin\n",
      "u\n",
      "assad\n",
      "liberals\n",
      "afghans\n",
      "calderon\n",
      "washington\n",
      "bush's\n",
      "conservatives\n",
      "president\n",
      "obamacare\n",
      "iraqis\n",
      "panetta\n",
      "snowden\n",
      "mcconnell\n",
      "clinton's\n",
      "chavez\n",
      "gop\n",
      "palin\n",
      "americans\n",
      "senate\n",
      "christie\n",
      "isis\n",
      "veterans\n",
      "he\n",
      "voters\n",
      "petraeus\n",
      "pelosi\n",
      "secretary\n",
      "jindal\n",
      "george\n",
      "mandela\n",
      "republican\n",
      "kerry\n",
      "karzai\n",
      "biden\n",
      "gop's\n",
      "francis\n",
      "economists\n",
      "lawmakers\n",
      "jeb\n",
      "we've\n",
      "congressman\n",
      "rouhani\n",
      "navarrette\n",
      "congress\n",
      "netanyahu\n",
      "latinos\n",
      "nixon\n",
      "aides\n",
      "iraqi\n",
      "nra\n",
      "pelos\n",
      "richard\n",
      "white\n",
      "clint\n",
      "gov\n",
      "lincoln\n",
      "romney's\n",
      "gingrich\n",
      "taxpayers\n",
      "nato\n",
      "presidents\n",
      "vietnam\n",
      "nieto\n",
      "analysts\n",
      "president's\n",
      "ryan\n",
      "gupta\n",
      "senators\n",
      "cdc\n",
      "shinse\n",
      "brennan\n",
      "next\n",
      "capitol\n",
      "legislators\n",
      "ahmadinejad\n",
      "elect\n",
      "erdogan\n",
      "afghanistan\n",
      "iraq\n",
      "gadhafi\n",
      "roosevelt\n",
      "cheney\n",
      "santorum\n"
     ]
    }
   ],
   "source": [
    "word_embed = model.word_embed\n",
    "\n",
    "text = \"obama\"\n",
    "text = text.lower()\n",
    "\n",
    "idx = tf.cast(tokenizer.tokenize(text), tf.int32)\n",
    "print(idx)\n",
    "embed = tf.expand_dims(word_embed[idx[0][0]], axis=0)\n",
    "\n",
    "cosine_sim = embed@tf.transpose(word_embed)\n",
    "idx = tf.argsort(cosine_sim, axis=-1, \n",
    "                 direction='DESCENDING',\n",
    "                 #direction='ASCENDING', \n",
    "                 stable=False, name=None)[0]\n",
    "\n",
    "for i in idx[:100]:\n",
    "    i = tf.expand_dims(i, axis=0)\n",
    "    print(tokenizer.detokenize(i).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff23330f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[5393]], shape=(1, 1), dtype=int32)\n",
      "netanyahu\n",
      "abulary\n",
      "hagel\n",
      "maduro\n",
      "espion\n",
      "yingluck\n",
      "onsored\n",
      "nandez\n",
      "saleh\n",
      "natur\n",
      "ailand\n",
      "hezbol\n",
      "panetta\n",
      "biden\n",
      "shinse\n",
      "kerry\n",
      "gibbs\n",
      "sarkozy\n",
      "fundam\n",
      "hift\n",
      "patro\n",
      "signific\n",
      "anonymity\n",
      "putin\n",
      "mugabe\n",
      "lades\n",
      "boehner\n",
      "pelosi\n",
      "medvedev\n",
      "ahmadinejad\n",
      "warri\n",
      "thaksin\n",
      "landrieu\n",
      "shaba\n",
      "gbag\n",
      "accust\n",
      "charac\n",
      "fahren\n",
      "liby\n",
      "peninsu\n",
      "helicop\n",
      "zuma\n",
      "traged\n",
      "portugu\n",
      "morsy\n",
      "publ\n",
      "enjo\n",
      "ilight\n",
      "abbas\n",
      "erdogan\n",
      "ieval\n",
      "bachmann\n",
      "yanukovych\n",
      "leep\n",
      "confir\n",
      "rodrigue\n",
      "secutive\n",
      "provin\n",
      "mccain's\n",
      "moil\n",
      "subsequ\n",
      "abled\n",
      "juvent\n",
      "o'ne\n",
      "guardiola\n",
      "lieberman\n",
      "karzai\n",
      "catastro\n",
      "ouatt\n",
      "zardari\n",
      "possib\n",
      "toug\n",
      "theless\n",
      "burma\n",
      "carney\n",
      "ricul\n",
      "zhok\n",
      "barcelon\n",
      "dort\n",
      "sunnis\n",
      "lomb\n",
      "snowden\n",
      "avez\n",
      "diffic\n",
      "sess\n",
      "khamenei\n",
      "exer\n",
      "golese\n",
      "copen\n",
      "rouhani\n",
      "ipal\n",
      "transparen\n",
      "ultane\n",
      "mccain\n",
      "boeh\n",
      "diox\n",
      "citiz\n",
      "adjac\n",
      "nieto\n",
      "lavrov\n"
     ]
    }
   ],
   "source": [
    "word_embed = model.word_embed\n",
    "\n",
    "text = \"netanyahu\"\n",
    "text = text.lower()\n",
    "\n",
    "idx = tf.cast(tokenizer.tokenizer.tokenize(text), tf.int32)\n",
    "idx = tokenize(idx, tokenizer.merge_list)\n",
    "print(idx)\n",
    "embed = tf.expand_dims(word_embed[idx[0][0]], axis=0)\n",
    "\n",
    "cosine_sim = embed@tf.transpose(word_embed)\n",
    "idx = tf.argsort(cosine_sim, axis=-1, \n",
    "                 direction='DESCENDING',\n",
    "                 #direction='ASCENDING', \n",
    "                 stable=False, name=None)[0]\n",
    "\n",
    "for i in idx[:100]:\n",
    "    i = tf.expand_dims(i, axis=0)\n",
    "    print(tokenizer.detokenize(i).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ff28a5",
   "metadata": {},
   "source": [
    "## Mean Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71e4c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 62)\n",
      " \n",
      ".\n",
      "-\n",
      "\"\n",
      ",\n",
      "a\n",
      "in\n",
      "\n",
      "\n",
      "and\n",
      "the\n",
      "on\n",
      "to\n",
      "at\n",
      "'\n",
      "an\n",
      "or\n",
      "u\n",
      "by\n",
      "that\n",
      ":\n",
      "as\n",
      "'s\n",
      "s\n",
      "al\n",
      "of\n",
      "it\n",
      "he\n",
      "for\n",
      "un\n",
      "over\n",
      "e\n",
      "about\n",
      "is\n",
      "with\n",
      "after\n",
      "up\n",
      "not\n",
      "last\n",
      "more\n",
      "may\n",
      "?\n",
      "re\n",
      "from\n",
      "ad\n",
      "(\n",
      "state\n",
      "be\n",
      "just\n",
      "so\n",
      "was\n",
      "one\n",
      "/\n",
      "ed\n",
      "no\n",
      "war\n",
      "while\n",
      "security\n",
      ";\n",
      "but\n",
      "1\n",
      "en\n",
      "n\n",
      "man\n",
      "house\n",
      "i\n",
      "north\n",
      "m\n",
      "first\n",
      "ar\n",
      "l\n",
      "f\n",
      "c\n",
      "er\n",
      "there\n",
      "out\n",
      "o\n",
      "do\n",
      "two\n",
      "when\n",
      "less\n",
      "had\n",
      "air\n",
      "k\n",
      "v\n",
      "h\n",
      "near\n",
      "they\n",
      "2\n",
      "his\n",
      "some\n",
      "de\n",
      "back\n",
      "we\n",
      "field\n",
      "fire\n",
      "if\n",
      "this\n",
      "under\n",
      "p\n",
      "right\n"
     ]
    }
   ],
   "source": [
    "word_embed = model.word_embed\n",
    "\n",
    "text = \"Obama's remarks came shortly after U.N. inspectors left Syria, carrying evidence that will determine whether chemical weapons were used in an attack early last week in a Damascus suburb.\"\n",
    "text = text.lower()\n",
    "\n",
    "idx = tf.cast(tokenizer.tokenizer.tokenize(text), tf.int32)\n",
    "idx = tokenize(idx, tokenizer.merge_list)\n",
    "print(idx.shape)\n",
    "embed_final = model.call(idx, logits=False)\n",
    "#embed_mean = embed_final[:,-1,:]\n",
    "embed_mean = tf.reduce_mean(embed_final, axis=1)\n",
    "embed_mean = tf.cast(embed_mean, dtype=tf.float32) \n",
    "\n",
    "cosine_sim = cosine_similarity(embed_mean, word_embed, normalize=False)\n",
    "#cosine_sim = cosine_similarity(embed_mean, word_embed, normalize=True)\n",
    "\n",
    "idx = tf.argsort(cosine_sim, axis=-1, \n",
    "                 direction='DESCENDING',\n",
    "                 #direction='ASCENDING', \n",
    "                 stable=False, name=None)[0]\n",
    "\n",
    "for i in idx[:100]:\n",
    "    i = tf.expand_dims(i, axis=0)\n",
    "    print(tokenizer.detokenize(i).numpy().decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d6f1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[553]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor(0.08745351, shape=(), dtype=float32)\n",
      "tf.Tensor([[    1    13    15 ... 15466  9736 15505]], shape=(1, 16070), dtype=int32)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m i \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(i, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "word_embed = model.word_embed\n",
    "\n",
    "text = \"Obama\"\n",
    "text = text.lower()\n",
    "\n",
    "idx = tf.cast(tokenizer.tokenizer.tokenize(text), tf.int32)\n",
    "idx = tokenize(idx, tokenizer.merge_list)\n",
    "print(idx)\n",
    "b = model.unembed_b[idx[0][0]]\n",
    "print(b)\n",
    "logits = model.call(idx, logits=True) \n",
    "#embed_mean = embed_final[:,-1,:]\n",
    "embed_mean = tf.reduce_mean(embed_final, axis=1)\n",
    "embed_mean = tf.cast(embed_mean, dtype=tf.float32)\n",
    "\n",
    "cosine_sim = cosine_similarity(embed_mean, word_embed, normalize=False)\n",
    "#cosine_sim = cosine_similarity(embed_mean, word_embed, normalize=True)\n",
    "\n",
    "idx = tf.argsort(logits, axis=-1, \n",
    "                 direction='DESCENDING',\n",
    "                 #direction='ASCENDING', \n",
    "                 stable=False, name=None)[0]\n",
    "\n",
    "for i in idx[:100]:\n",
    "    i = tf.expand_dims(i, axis=0)\n",
    "    print(i)\n",
    "    print(tokenizer.detokenize(i).numpy().decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f09e0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthis is a text to find out how to make a stop mask\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mlower()\n\u001b[1;32m----> 3\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mencode(text)\n\u001b[0;32m      4\u001b[0m tokens\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, tokenizer\u001b[38;5;241m.\u001b[39mtoken_to_idx[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<s>\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      5\u001b[0m tokens\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m10\u001b[39m, tokenizer\u001b[38;5;241m.\u001b[39mtoken_to_idx[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</s>\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "text = \"this is a text to find out how to make a stop mask\"\n",
    "text = text.lower()\n",
    "tokens = tokenizer.encode(text)\n",
    "tokens.insert(0, tokenizer.token_to_idx[\"<s>\"])\n",
    "tokens.insert(10, tokenizer.token_to_idx[\"</s>\"])\n",
    "tokens.insert(10, tokenizer.token_to_idx[\"<s>\"])\n",
    "tokens.insert(15, tokenizer.token_to_idx[\"</s>\"])\n",
    "tokens.insert(15, tokenizer.token_to_idx[\"<s>\"])\n",
    "tokens.append(tokenizer.token_to_idx[\"</s>\"])\n",
    "\n",
    "print(tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a5f20b",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
