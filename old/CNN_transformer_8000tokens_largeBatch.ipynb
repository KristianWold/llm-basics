{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04010a08",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa3d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from src.tokenizer import TokenizerBPE, word_split, normalize_to_ascii\n",
    "\n",
    "import os\n",
    "import time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm.notebook import tqdm\n",
    "from src.transformer import *\n",
    "from src.data_handling import read_first_n, sample_batch\n",
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b59898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7afd4e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = pkl.load(open(\"tokenizers/tokenizer_CNN8000_lowercase.pkl\", 'rb'))\n",
    "tokenizer.create_hash()\n",
    "\n",
    "random.seed(42)\n",
    "corpus_indicies = pkl.load(open('corpus/CNN_tokenized8000_lowercase.pkl', 'rb'))\n",
    "random.shuffle(corpus_indicies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4020ef6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_indicies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f830d881",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a564402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmUpThenDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self,\n",
    "                 initial_learning_rate: float,\n",
    "                 warmup_steps: int,\n",
    "                 decay_schedule_fn: tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "        \"\"\"\n",
    "        initial_learning_rate: peak LR reached at end of warmup\n",
    "        warmup_steps:      # of steps to ramp from 0 â†’ initial_learning_rate\n",
    "        decay_schedule_fn: a tf.keras schedule to apply *after* warmup\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.initial_lr = initial_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.decay_schedule_fn = decay_schedule_fn\n",
    "\n",
    "    def __call__(self, step):\n",
    "        # Cast to float32 for safety in graph mode\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
    "\n",
    "        # compute linear warmup: lr = initial_lr * (step / warmup_steps)\n",
    "        warmup_lr = self.initial_lr * (step / warmup_steps)\n",
    "\n",
    "        # after warmup_steps, switch to decay schedule (shift step count)\n",
    "        decay_step = step - warmup_steps\n",
    "        decay_lr = self.decay_schedule_fn(decay_step)\n",
    "\n",
    "        # if step < warmup_steps, pick warmup_lr, else decay_lr\n",
    "        return tf.cond(step < warmup_steps,\n",
    "                       lambda: warmup_lr,\n",
    "                       lambda: decay_lr)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a056790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DenseLayer(tf.keras.Model):\n",
    "    def __init__(self, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        d_xavier = np.sqrt(6.0 / (input_dim + output_dim))\n",
    "        self.W = tf.Variable(tf.random.uniform([input_dim, output_dim], -1 / d_xavier , 1 / d_xavier))\n",
    "        self.b = tf.Variable(tf.zeros([output_dim]))\n",
    "\n",
    "        self.parameter_list = [self.W, self.b]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return tf.linalg.matmul(x, self.W) + self.b\n",
    "\n",
    "\n",
    "class TransformerBlock(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        max_seq_len,\n",
    "        heads,\n",
    "        embed_dim,\n",
    "        ff_dim,\n",
    "        dropout,\n",
    "        **kwargs\n",
    "\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.heads = heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.ff_dim = ff_dim\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.head_dim = embed_dim // heads\n",
    "        self.dol1 = tf.keras.layers.Dropout(dropout)\n",
    "        self.dol2 = tf.keras.layers.Dropout(dropout)\n",
    "        self.dol3 = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        d = tf.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "\n",
    "        self.KQV = tf.Variable(\n",
    "            tf.random.uniform([embed_dim, 3*embed_dim], -1 / d, 1 / d), name=\"KQV\"\n",
    "        )\n",
    "        self.WO = tf.Variable(\n",
    "            tf.random.uniform([embed_dim, embed_dim], -1 / d, 1 / d), name=\"WO\"\n",
    "        )\n",
    "\n",
    "        self.layer_up = DenseLayer(embed_dim, ff_dim)\n",
    "        self.layer_down = DenseLayer(ff_dim, embed_dim)\n",
    "\n",
    "        self.ln1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ln1.build((None, None, embed_dim))\n",
    "\n",
    "        self.ln2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.ln2.build((None, None, embed_dim))\n",
    "\n",
    "        self.parameter_list = [\n",
    "            self.KQV,\n",
    "            self.WO,\n",
    "        ]\n",
    "        self.parameter_list += self.layer_up.parameter_list\n",
    "        self.parameter_list += self.layer_down.parameter_list\n",
    "        self.parameter_list += self.ln1.trainable_variables\n",
    "        self.parameter_list += self.ln2.trainable_variables\n",
    "\n",
    "        self.parameter_decay = [\n",
    "            self.KQV,\n",
    "            self.WO,\n",
    "            self.layer_up.W,\n",
    "            self.layer_down.W,\n",
    "        ]\n",
    "\n",
    "    def attention(self, x_embeds, training=False):\n",
    "        batch = tf.shape(x_embeds)[0]\n",
    "        seq = tf.shape(x_embeds)[1]\n",
    "\n",
    "        x_kqv = tf.matmul(x_embeds, self.KQV)\n",
    "        x_kqv = tf.reshape(x_kqv, [batch, seq, self.heads, 3, self.head_dim])\n",
    "        x_kqv = tf.transpose(x_kqv, [0, 2, 3, 1, 4])\n",
    "        x_k = x_kqv[:, :, 0, :, :]\n",
    "        x_q = x_kqv[:, :, 1, :, :]\n",
    "        x_v = x_kqv[:, :, 2, :, :]\n",
    "\n",
    "\n",
    "        inner = tf.einsum(\"bijl, bikl -> bijk\", x_q, x_k)\n",
    "        mask = tf.linalg.band_part(tf.ones((1, 1, seq, seq), dtype=tf.bool), -1, 0)\n",
    "\n",
    "        inner_masked = tf.where(mask, inner, tf.constant(-np.inf))\n",
    "        \n",
    "\n",
    "        dk = tf.sqrt(tf.cast(self.head_dim, tf.float32))\n",
    "        WA = tf.nn.softmax(inner_masked / dk, axis=-1)\n",
    "        WA = self.dol1(WA, training)\n",
    "\n",
    "        head_outs = WA @ x_v\n",
    "        concat = tf.transpose(head_outs, [0, 2, 1, 3])  # [batch, seq, heads, head_dim]\n",
    "        out = tf.reshape(concat, [batch, seq, self.embed_dim])\n",
    "        out = tf.einsum(\"ijk,kl -> ijl\", out, self.WO)\n",
    "\n",
    "        # pre-norm to keep gradients alive\n",
    "        out = self.dol2(out)\n",
    "        out = self.ln1(out)\n",
    "        out = out + x_embeds\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def ffnn(self, x_embeds, training=False):\n",
    "        out = self.layer_up(x_embeds)\n",
    "        out = tf.nn.gelu(out)\n",
    "        out = self.layer_down(out)\n",
    "        out = self.dol3(out, training)\n",
    "        out = self.ln2(out)\n",
    "        out = out + x_embeds\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def call(self, x_embeds, training=False):\n",
    "        x_embeds = self.attention(x_embeds, training)\n",
    "        x_embeds = self.ffnn(x_embeds, training)\n",
    "\n",
    "        return x_embeds\n",
    "\n",
    "\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size,\n",
    "        max_seq_len,\n",
    "        tf_blocks,\n",
    "        embed_dim,\n",
    "        heads,\n",
    "        ff_dim,\n",
    "        unembed_dims,\n",
    "        lr,\n",
    "        wd=None,\n",
    "        dropout=0.1,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.heads = heads\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.tf_blocks = tf_blocks\n",
    "        self.ff_dim = ff_dim\n",
    "        self.unembed_dims = unembed_dims\n",
    "        self.wd = wd\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.head_dim = embed_dim // heads\n",
    "        self.dol = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "\n",
    "        d = tf.sqrt(tf.cast(self.embed_dim, tf.float32))\n",
    "\n",
    "        self.word_embed = tf.Variable(\n",
    "            tf.random.uniform([vocab_size, embed_dim], -1 / d, 1 / d), name=\"W_embed\"\n",
    "        )\n",
    "        self.pos_embed = tf.Variable(\n",
    "            tf.random.uniform([max_seq_len, embed_dim], -1 / d, 1 / d), name=\"W_pos_embed\"\n",
    "        )\n",
    "\n",
    "        self.tf_blocks = []\n",
    "        for i in range(tf_blocks):\n",
    "            self.tf_blocks.append(TransformerBlock(vocab_size, max_seq_len, heads, embed_dim, ff_dim, dropout))\n",
    "    \n",
    "\n",
    "        self.unembed_b = tf.Variable(tf.zeros([vocab_size]))\n",
    "        self.parameter_list = [\n",
    "            self.word_embed,\n",
    "            self.pos_embed,\n",
    "        ]\n",
    "\n",
    "        for block in self.tf_blocks:\n",
    "            self.parameter_list += block.parameter_list\n",
    "        \n",
    "        self.parameter_list.append(self.unembed_b)\n",
    "\n",
    "        self.parameter_decay = [\n",
    "            self.word_embed,\n",
    "            self.pos_embed,]\n",
    "        \n",
    "        for block in self.tf_blocks:\n",
    "            self.parameter_decay += block.parameter_decay\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "    def call(self, x, training=False, logits=True, ):\n",
    "\n",
    "        x = self.embed(x, training)\n",
    "        for block in self.tf_blocks:\n",
    "            x = block.call(x, training)\n",
    "        \n",
    "        if logits:\n",
    "            x = self.unembed(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def embed(self, x, training=False):\n",
    "        seq = tf.shape(x)[1]\n",
    "        if seq > self.max_seq_len:\n",
    "            x = x[:, -self.max_seq_len :]\n",
    "            seq = self.max_seq_len\n",
    "        x_embeds = tf.nn.embedding_lookup(self.word_embed, x)\n",
    "        x_embeds = x_embeds + tf.expand_dims(self.pos_embed[:seq], axis=0)\n",
    "        x_embeds = self.dol(x_embeds, training)\n",
    "\n",
    "        return x_embeds\n",
    "\n",
    "    def unembed(self, x_embeds):\n",
    "        logits = x_embeds @ tf.transpose(self.word_embed) + self.unembed_b\n",
    "\n",
    "        return logits\n",
    "\n",
    "    @tf.function()\n",
    "    def train_step(self, indices, y_true):\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self.evaluate(indices, y_true, training=True)\n",
    "\n",
    "        grads = tape.gradient(loss, self.parameter_list)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.parameter_list))\n",
    "\n",
    "        get_lr = self.optimizer._decayed_lr(tf.float32)\n",
    "        \n",
    "        if self.wd is not None:\n",
    "            for param in self.parameter_decay:\n",
    "                param.assign_sub(get_lr*self.wd * param)\n",
    "                \n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, indices, y_true, training = False):\n",
    "        y_true = y_true[:, 1:]\n",
    "        \n",
    "        y_pred = self.call(indices, training)[:, :-1]\n",
    "        loss = tf.math.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0a5a33ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lr = 1e-4\n",
    "decay_steps = 10000\n",
    "decay_rate = 0.5\n",
    "decay_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_lr,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=False)\n",
    "\n",
    "warmup_steps = 5000\n",
    "lr_schedule = WarmUpThenDecay(\n",
    "    initial_learning_rate=initial_lr,\n",
    "    warmup_steps=warmup_steps,\n",
    "    decay_schedule_fn=decay_schedule)\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "max_seq_len = 128\n",
    "embed_dim = 768\n",
    "tf_blocks = 6\n",
    "heads = 6\n",
    "ff_dim = 4*embed_dim\n",
    "weight_decay = 0.01\n",
    "dropout = 0.05\n",
    "\n",
    "unembed_dims = []\n",
    "\n",
    "model = Transformer(vocab_size=tokenizer.vocab_size,\n",
    "                    max_seq_len=max_seq_len,\n",
    "                    embed_dim=embed_dim,\n",
    "                    tf_blocks=tf_blocks,\n",
    "                    heads=heads,\n",
    "                    ff_dim = ff_dim,\n",
    "                    unembed_dims=unembed_dims,\n",
    "                    lr=lr_schedule,\n",
    "                    wd = weight_decay,\n",
    "                    dropout=dropout,\n",
    "                    )\n",
    "\n",
    "losses_train = []\n",
    "losses_test = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7970a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"model_8k_tokens_largeBatch\"\n",
    "\n",
    "\n",
    "ckpt = tf.train.Checkpoint(\n",
    "    optimizer=model.optimizer,\n",
    "    model=model\n",
    ")\n",
    "ckpt_manager = tf.train.CheckpointManager(\n",
    "    ckpt, \n",
    "    directory=\"checkpoints/\" + name,      # folder where ckpts are saved\n",
    "    max_to_keep=5                         # only keep 5 latest checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "88b34765",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "losses_train, losses_test = pkl.load(open(\"checkpoints/losses_\" + name + \".pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a6527620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 48811396\n"
     ]
    }
   ],
   "source": [
    "total_params = 0\n",
    "for var in model.parameter_list:\n",
    "    shape = var.get_shape()\n",
    "    num_params = 1\n",
    "    for dim in shape:\n",
    "        num_params *= dim\n",
    "    total_params += num_params\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "93977cd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 64\u001b[0m\n\u001b[0;32m     61\u001b[0m ax2\u001b[38;5;241m.\u001b[39mgrid(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     63\u001b[0m plt\u001b[38;5;241m.\u001b[39mtight_layout()\n\u001b[1;32m---> 64\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\pyplot.py:614\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;124;03mDisplay all open figures.\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[38;5;124;03mexplicitly there.\u001b[39;00m\n\u001b[0;32m    612\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    613\u001b[0m _warn_if_gui_out_of_main_thread()\n\u001b[1;32m--> 614\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_backend_mod()\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib_inline\\backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(close, block)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m figure_manager \u001b[38;5;129;01min\u001b[39;00m Gcf\u001b[38;5;241m.\u001b[39mget_all_fig_managers():\n\u001b[1;32m---> 90\u001b[0m         \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fetch_figure_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     show\u001b[38;5;241m.\u001b[39m_to_draw \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\IPython\\core\\display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[1;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m     publish_display_data(data\u001b[38;5;241m=\u001b[39mobj, metadata\u001b[38;5;241m=\u001b[39mmetadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 298\u001b[0m     format_dict, md_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\IPython\\core\\formatters.py:238\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[1;34m(self, obj, include, exclude)\u001b[0m\n\u001b[0;32m    236\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 238\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\IPython\\core\\formatters.py:282\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[1;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     r \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\IPython\\core\\formatters.py:402\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[0;32m    404\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\IPython\\core\\pylabtools.py:170\u001b[0m, in \u001b[0;36mprint_figure\u001b[1;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[0;32m    168\u001b[0m     FigureCanvasBase(fig)\n\u001b[1;32m--> 170\u001b[0m fig\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mprint_figure(bytes_io, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    171\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\backend_bases.py:2184\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2181\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[0;32m   2182\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[0;32m   2183\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[1;32m-> 2184\u001b[0m         result \u001b[38;5;241m=\u001b[39m print_method(\n\u001b[0;32m   2185\u001b[0m             filename,\n\u001b[0;32m   2186\u001b[0m             facecolor\u001b[38;5;241m=\u001b[39mfacecolor,\n\u001b[0;32m   2187\u001b[0m             edgecolor\u001b[38;5;241m=\u001b[39medgecolor,\n\u001b[0;32m   2188\u001b[0m             orientation\u001b[38;5;241m=\u001b[39morientation,\n\u001b[0;32m   2189\u001b[0m             bbox_inches_restore\u001b[38;5;241m=\u001b[39m_bbox_inches_restore,\n\u001b[0;32m   2190\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2191\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   2192\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\backend_bases.py:2040\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2036\u001b[0m     optional_kws \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[0;32m   2037\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2038\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_inches_restore\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m   2039\u001b[0m     skip \u001b[38;5;241m=\u001b[39m optional_kws \u001b[38;5;241m-\u001b[39m {\u001b[38;5;241m*\u001b[39minspect\u001b[38;5;241m.\u001b[39msignature(meth)\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[1;32m-> 2040\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: meth(\n\u001b[0;32m   2041\u001b[0m         \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m skip}))\n\u001b[0;32m   2042\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[0;32m   2043\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m meth\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:481\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[1;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    435\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 481\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_print_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:429\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[1;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_print_pil\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, fmt, pil_kwargs, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    425\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;124;03m    Draw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;124;03m    *pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 429\u001b[0m     \u001b[43mFigureCanvasAgg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    430\u001b[0m     mpl\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimsave(\n\u001b[0;32m    431\u001b[0m         filename_or_obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_rgba(), \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mfmt, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    432\u001b[0m         dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdpi, metadata\u001b[38;5;241m=\u001b[39mmetadata, pil_kwargs\u001b[38;5;241m=\u001b[39mpil_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:382\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\u001b[38;5;241m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\n\u001b[0;32m    381\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m nullcontext()):\n\u001b[1;32m--> 382\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;66;03m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# don't forget to call the superclass.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdraw()\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\artist.py:94\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 94\u001b[0m     result \u001b[38;5;241m=\u001b[39m draw(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[0;32m     96\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\artist.py:71\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\figure.py:3257\u001b[0m, in \u001b[0;36mFigure.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   3254\u001b[0m             \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[0;32m   3256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[1;32m-> 3257\u001b[0m     \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3260\u001b[0m     renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigure\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   3261\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\image.py:134\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[1;32m--> 134\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\artist.py:71\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\axes\\_base.py:3210\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   3207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m artists_rasterized:\n\u001b[0;32m   3208\u001b[0m     _draw_rasterized(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_figure(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), artists_rasterized, renderer)\n\u001b[1;32m-> 3210\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3213\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   3214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\image.py:134\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[1;32m--> 134\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\artist.py:71\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\lines.py:807\u001b[0m, in \u001b[0;36mLine2D.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m    804\u001b[0m         gc\u001b[38;5;241m.\u001b[39mset_foreground(lc_rgba, isRGBA\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    806\u001b[0m         gc\u001b[38;5;241m.\u001b[39mset_dashes(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dash_pattern)\n\u001b[1;32m--> 807\u001b[0m         \u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maffine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrozen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    808\u001b[0m         gc\u001b[38;5;241m.\u001b[39mrestore()\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_marker \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_markersize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for i in tqdm(range(100000)):\n",
    "    indices, y_true = sample_batch(corpus_indicies[:20000], \n",
    "                                   batch_size, \n",
    "                                   tokenizer, \n",
    "                                   max_seq_len)\n",
    "    \n",
    "    \n",
    "    loss_train = model.train_step(indices, y_true).numpy()\n",
    "    losses_train.append(loss_train)\n",
    "\n",
    "\n",
    "    indices, y_true = sample_batch(corpus_indicies[20000:], \n",
    "                                batch_size//4, \n",
    "                                tokenizer, \n",
    "                                max_seq_len)\n",
    "    \n",
    "    loss_test = model.evaluate(indices, y_true).numpy()\n",
    "        \n",
    "    losses_test.append(loss_test)\n",
    "\n",
    "    if (i+1) % 1000 == 0:\n",
    "        ckpt_manager.save()\n",
    "        pkl.dump([losses_train, losses_test], open(\"checkpoints/losses_\" + name + \".pkl\", 'wb'))\n",
    "\n",
    "\n",
    "    lr = model.optimizer._decayed_lr(tf.float32).numpy()\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # prepare x-axis for the last 400 steps\n",
    "    start = max(0, len(losses_train) - 1000)\n",
    "    x_zoom = np.arange(start, len(losses_train))\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 8), sharex=False)\n",
    "\n",
    "    # Top subplot: zoom on last 400 steps\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(x_zoom, losses_test[-1000:], label=\"Test Loss\")\n",
    "    ax1.plot(x_zoom, losses_train[-1000:], label=\"Train Loss\")\n",
    "\n",
    "    _min = min(losses_train[-1000:] + losses_test[-1000:])\n",
    "    _max = max(losses_train[-1000:] + losses_test[-1000:])\n",
    "    delta = _max - _min\n",
    "    #ax1.set_ylim(_min - 0.1 * delta, _max + 0.1 * delta)\n",
    "\n",
    "    ax1.set_title(\"Training Loss (Last 1000 Steps)\")\n",
    "    ax1.set_xlabel(\"Step\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Bottom subplot: full series\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(losses_test[500:], label=\"Test Loss\")\n",
    "    ax2.plot(losses_train[500:], label=\"Train Loss, lr = {:.2e}\".format(lr))\n",
    "\n",
    "    ax2.set_title(\"Training Loss (Full Series)\")\n",
    "    ax2.set_xlabel(\"Step\")\n",
    "    ax2.set_ylabel(\"Loss\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "58649737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(indices, merge_list):\n",
    "    indices = np.array(indices)\n",
    "    for pair, new_idx in merge_list:\n",
    "        slice = np.where(np.logical_and(indices[:-1] == pair[0],  indices[1:] == pair[1]))\n",
    "        if len(slice[0]) > 0:\n",
    "            indices[:-1][slice] = new_idx\n",
    "            indices = np.delete(indices, (slice[0]+1))\n",
    "\n",
    "    return tf.expand_dims(tf.convert_to_tensor(indices, dtype=tf.int32), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e216ce01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[2318]], shape=(1, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "text = \"obama's\"\n",
    "text = text.lower()\n",
    "\n",
    "indices = tf.cast(tokenizer.tokenizer.tokenize(text), tf.int32)\n",
    "indices = tokenize(indices, tokenizer.merge_list)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "74a017b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obama's speech to hear its speech that \"we have been to a call to speak to the middle east of the country,\" said in a statement.  \"i think it was a very very successful and i don't want to have to live on the internet and we have to do this time.\"  in fact, the company has been on the national tour television efforts to be the long warmed in the world of the meeting of the united states\r"
     ]
    }
   ],
   "source": [
    "T = 0.5\n",
    "tf.random.set_seed(43)\n",
    "for i in range(128):\n",
    "    logits = model.call(indices)[0,-1:]\n",
    "    idx = tf.cast(tf.random.categorical(logits/T, num_samples=1), tf.int32)\n",
    "    indices = tf.concat([indices, idx], axis=1)\n",
    "    text_pred = tokenizer.detokenize(indices)\n",
    "    text_pred = text_pred.numpy()[0].decode('utf-8').replace(\"\\n\", \" \")\n",
    "    print(text_pred, end='\\r', flush=True)\n",
    "    #time.sleep(0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e77fb01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def cosine_similarity(embed_a, embed_b):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two vectors.\n",
    "    \"\"\"\n",
    "    embed_b_T = tf.transpose(embed_b)\n",
    "    dot_product = embed_a@embed_b_T\n",
    "    \n",
    "    norm_a = tf.linalg.norm(embed_a, axis=1, keepdims=True)\n",
    "    norm_b = tf.linalg.norm(embed_b_T, axis=0, keepdims=True)\n",
    "\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "\n",
    "def cluster(X, n_clusters):\n",
    "    X = X/np.linalg.norm(X, axis=1, keepdims=True)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n",
    "    inertia = kmeans.inertia_\n",
    "    labels = kmeans.labels_\n",
    "    clusters = kmeans.cluster_centers_\n",
    "\n",
    "    return inertia, labels, clusters\n",
    "\n",
    "\n",
    "class EmbeddingClustering:\n",
    "    def __init__(self, tokenizer, n_clusters=10):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_clusters = n_clusters\n",
    "\n",
    "    def fit(self, word_embed):\n",
    "        inertia, labels, clusters = cluster(word_embed, self.n_clusters)\n",
    "        self.word_embed = word_embed\n",
    "        self.inertia = inertia\n",
    "        self.labels = labels\n",
    "        self.clusters = tf.convert_to_tensor(clusters, dtype=tf.float32)\n",
    "\n",
    "        cos_sim = cosine_similarity(self.clusters, word_embed)\n",
    "        self.idx_list =  tf.argsort(cos_sim, axis=-1, direction='DESCENDING', stable=False, name=None)\n",
    "\n",
    "    def print_clusters(self, n_words=10):\n",
    "        for idx in self.idx_list:\n",
    "            for i in idx[:n_words]:\n",
    "                word = self.tokenizer.detokenize(tf.expand_dims(tf.cast(i, tf.int32), axis=0))\n",
    "                word = word.numpy().decode('utf-8')\n",
    "                print(word)\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "48583bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punish\n",
      "restrain\n",
      "assum\n",
      "accomplish\n",
      "interven\n",
      "disagree\n",
      "commit\n",
      "adjust\n",
      "embarrass\n",
      "proceed\n",
      "\n",
      "\n",
      "born\n",
      "connected\n",
      "speaking\n",
      "packed\n",
      "talking\n",
      "traveling\n",
      "gathered\n",
      "working\n",
      "deployed\n",
      "struggled\n",
      "\n",
      "\n",
      "legislative\n",
      "legal\n",
      "federal\n",
      "criminal\n",
      "formal\n",
      "constitutional\n",
      "current\n",
      "judicial\n",
      "supreme\n",
      "professional\n",
      "\n",
      "\n",
      "oklahom\n",
      "minnes\n",
      "matthe\n",
      "princi\n",
      "trium\n",
      "thomp\n",
      "cele\n",
      "wim\n",
      "franch\n",
      "unte\n",
      "\n",
      "\n",
      "iet\n",
      "ork\n",
      "inois\n",
      "aste\n",
      "ool\n",
      "aff\n",
      "helicop\n",
      "uck\n",
      "ld\n",
      "igh\n",
      "\n",
      "\n",
      "vehicles\n",
      "homes\n",
      "buildings\n",
      "stations\n",
      "airports\n",
      "boats\n",
      "towns\n",
      "equipment\n",
      "spots\n",
      "animals\n",
      "\n",
      "\n",
      "absolutely\n",
      "really\n",
      "pretty\n",
      "always\n",
      "simply\n",
      "probably\n",
      "totally\n",
      "usually\n",
      "rarely\n",
      "essentially\n",
      "\n",
      "\n",
      "liverpool\n",
      "midfielder\n",
      "portugal\n",
      "england\n",
      "argentina\n",
      "barcelona\n",
      "goalkeeper\n",
      "soccer\n",
      "ferrari\n",
      "striker\n",
      "\n",
      "\n",
      "genu\n",
      "^\n",
      "includ\n",
      "lished\n",
      "bollah\n",
      "apore\n",
      "estab\n",
      "massachu\n",
      "issip\n",
      "inclu\n",
      "\n",
      "\n",
      "tun\n",
      "hurrican\n",
      "nutr\n",
      "kne\n",
      "exhib\n",
      "var\n",
      "gg\n",
      "conven\n",
      "hay\n",
      "nam\n",
      "\n",
      "\n",
      "republicans\n",
      "governments\n",
      "observers\n",
      "leaders\n",
      "lawmakers\n",
      "conservatives\n",
      "candidates\n",
      "analysts\n",
      "lawyers\n",
      "critics\n",
      "\n",
      "\n",
      "republican\n",
      "conservative\n",
      "democratic\n",
      "hispanic\n",
      "liberal\n",
      "communist\n",
      "former\n",
      "independent\n",
      "elected\n",
      "senior\n",
      "\n",
      "\n",
      "quiet\n",
      "beautiful\n",
      "perfect\n",
      "genuine\n",
      "unique\n",
      "wonderful\n",
      "great\n",
      "huge\n",
      "stunning\n",
      "exciting\n",
      "\n",
      "\n",
      "trying\n",
      "failed\n",
      "tried\n",
      "wanted\n",
      "refused\n",
      "continues\n",
      "intended\n",
      "wanting\n",
      "planned\n",
      "wants\n",
      "\n",
      "\n",
      "susp\n",
      "shr\n",
      "ext\n",
      "adm\n",
      "dec\n",
      "lic\n",
      "exp\n",
      "kr\n",
      "197\n",
      "prov\n",
      "\n",
      "\n",
      "georg\n",
      "austral\n",
      "hait\n",
      "indones\n",
      "phoen\n",
      "canad\n",
      "olymp\n",
      "malays\n",
      "tunis\n",
      "gill\n",
      "\n",
      "\n",
      "proposals\n",
      "programs\n",
      "cuts\n",
      "policies\n",
      "systems\n",
      "payments\n",
      "benefits\n",
      "guidelines\n",
      "measures\n",
      "sector\n",
      "\n",
      "\n",
      "pract\n",
      "dynam\n",
      "tact\n",
      "skept\n",
      "rhetor\n",
      "academ\n",
      "trag\n",
      "clin\n",
      "genet\n",
      "strateg\n",
      "\n",
      "\n",
      "emot\n",
      "convers\n",
      "suspic\n",
      "aggress\n",
      "relig\n",
      "explos\n",
      "occas\n",
      "abort\n",
      "compan\n",
      "consc\n",
      "\n",
      "\n",
      "complaints\n",
      "rumors\n",
      "relationship\n",
      "software\n",
      "attacking\n",
      "complained\n",
      "strategy\n",
      "ambassador\n",
      "ances\n",
      "conversation\n",
      "\n",
      "\n",
      "owner\n",
      "lawyer\n",
      "correspondent\n",
      "director\n",
      "prosecutor\n",
      "association\n",
      "scientist\n",
      "headquarters\n",
      "committee\n",
      "candidate\n",
      "\n",
      "\n",
      "miami\n",
      "cleveland\n",
      "lake\n",
      "cape\n",
      "orange\n",
      "gray\n",
      "colorado\n",
      "ville\n",
      "seattle\n",
      "santa\n",
      "\n",
      "\n",
      "le\n",
      "co\n",
      "ra\n",
      "pal\n",
      "mo\n",
      "ha\n",
      "sa\n",
      "ro\n",
      "li\n",
      "bi\n",
      "\n",
      "\n",
      "pursu\n",
      "assess\n",
      "sweep\n",
      "jok\n",
      "polic\n",
      "inspir\n",
      "possess\n",
      "choos\n",
      "organiz\n",
      "shout\n",
      "\n",
      "\n",
      "moder\n",
      "propag\n",
      "immedi\n",
      "parli\n",
      "conserv\n",
      "intimid\n",
      "restaur\n",
      "advoc\n",
      "domin\n",
      "adequ\n",
      "\n",
      "\n",
      "game\n",
      "finals\n",
      "feat\n",
      "games\n",
      "matches\n",
      "season\n",
      "races\n",
      "seconds\n",
      "minutes\n",
      "tournament\n",
      "\n",
      "\n",
      "palin\n",
      "moore\n",
      "mccain\n",
      "boehner\n",
      "perry\n",
      "stevens\n",
      "graham\n",
      "zimmerman\n",
      "christie\n",
      "bergdahl\n",
      "\n",
      "\n",
      "ashed\n",
      "oked\n",
      "enced\n",
      "iving\n",
      "acked\n",
      "ched\n",
      "aced\n",
      "ulated\n",
      "ned\n",
      "pping\n",
      "\n",
      "\n",
      "heavy\n",
      "cease\n",
      "wild\n",
      "18\n",
      "mini\n",
      "remote\n",
      "multi\n",
      "50\n",
      "high\n",
      "extra\n",
      "\n",
      "\n",
      "mike\n",
      "richard\n",
      "jim\n",
      "brian\n",
      "paul\n",
      "robert\n",
      "sean\n",
      "james\n",
      "steven\n",
      "jennifer\n",
      "\n",
      "\n",
      "praised\n",
      "defended\n",
      "denied\n",
      "requested\n",
      "argued\n",
      "ordered\n",
      "announcing\n",
      "arguing\n",
      "addressed\n",
      "challenged\n",
      "\n",
      "\n",
      "solution\n",
      "value\n",
      "benefit\n",
      "idea\n",
      "challenge\n",
      "conclusion\n",
      "perspective\n",
      "experience\n",
      "loss\n",
      "decision\n",
      "\n",
      "\n",
      "germany\n",
      "thailand\n",
      "nigeria\n",
      "tunisia\n",
      "spain\n",
      "japan\n",
      "yemen\n",
      "indonesia\n",
      "bangladesh\n",
      "libya\n",
      "\n",
      "\n",
      "biggest\n",
      "third\n",
      "ultimate\n",
      "fifth\n",
      "youngest\n",
      "second\n",
      "upcoming\n",
      "worst\n",
      "first\n",
      "greatest\n",
      "\n",
      "\n",
      "into\n",
      "during\n",
      "from\n",
      "with\n",
      "while\n",
      "within\n",
      "in\n",
      "by\n",
      "toward\n",
      "of\n",
      "\n",
      "\n",
      "films\n",
      "movie\n",
      "movies\n",
      "musical\n",
      "comedy\n",
      "drama\n",
      "documentary\n",
      "hollywood\n",
      "characters\n",
      "songs\n",
      "\n",
      "\n",
      "confront\n",
      "transl\n",
      "veget\n",
      "transform\n",
      "prepar\n",
      "isol\n",
      "interrog\n",
      "devast\n",
      "explor\n",
      "intimid\n",
      "\n",
      "\n",
      "ises\n",
      "ents\n",
      "ards\n",
      "nels\n",
      "ots\n",
      "ils\n",
      "cers\n",
      "lers\n",
      "naev\n",
      "iles\n",
      "\n",
      "\n",
      "distin\n",
      "condu\n",
      "dete\n",
      "prote\n",
      "refle\n",
      "instru\n",
      "inspe\n",
      "conne\n",
      "archite\n",
      "sele\n",
      "\n",
      "\n",
      "scar\n",
      "du\n",
      "sli\n",
      "cham\n",
      "glo\n",
      "ho\n",
      "cree\n",
      "tre\n",
      "stri\n",
      "pu\n",
      "\n",
      "\n",
      "2005\n",
      "1996\n",
      "2012\n",
      "february\n",
      "2008\n",
      "2002\n",
      "june\n",
      "2004\n",
      "2003\n",
      "1997\n",
      "\n",
      "\n",
      "explained\n",
      "writes\n",
      "announced\n",
      "said\n",
      "added\n",
      "acknowledged\n",
      "argued\n",
      "tweeted\n",
      "concluded\n",
      "recalled\n",
      "\n",
      "\n",
      "tears\n",
      "herself\n",
      "shoes\n",
      "experiences\n",
      "therapy\n",
      "classes\n",
      "memories\n",
      "characters\n",
      "universe\n",
      "foods\n",
      "\n",
      "\n",
      "digital\n",
      "global\n",
      "technical\n",
      "massive\n",
      "yellow\n",
      "tourist\n",
      "economic\n",
      "natural\n",
      "electronic\n",
      "big\n",
      "\n",
      "\n",
      "i\n",
      "we\n",
      "you\n",
      "he\n",
      "they\n",
      "she\n",
      "it\n",
      "me\n",
      "never\n",
      "there\n",
      "\n",
      "\n",
      "preced\n",
      "fundam\n",
      "perman\n",
      "persist\n",
      "consist\n",
      "promin\n",
      "embarrass\n",
      "excell\n",
      "myster\n",
      "subsequ\n",
      "\n",
      "\n",
      "it's\n",
      "she's\n",
      "i'd\n",
      "we'd\n",
      "i'll\n",
      "he's\n",
      "i'm\n",
      "i've\n",
      "he'll\n",
      "he'd\n",
      "\n",
      "\n",
      "lod\n",
      "spar\n",
      "collap\n",
      "emer\n",
      "privile\n",
      "sie\n",
      "reven\n",
      "jud\n",
      "gor\n",
      "dun\n",
      "\n",
      "\n",
      "bat\n",
      "te\n",
      "ve\n",
      "ge\n",
      "e\n",
      "ce\n",
      "de\n",
      "ash\n",
      "bi\n",
      "ble\n",
      "\n",
      "\n",
      "putting\n",
      "providing\n",
      "leaving\n",
      "bringing\n",
      "pushing\n",
      "creating\n",
      "paying\n",
      "seeing\n",
      "promoting\n",
      "receiving\n",
      "\n",
      "\n",
      "complaint\n",
      "conviction\n",
      "indictment\n",
      "probation\n",
      "verdict\n",
      "charges\n",
      "lawsuit\n",
      "proceedings\n",
      "filing\n",
      "execution\n",
      "\n",
      "\n",
      "ations\n",
      "^\n",
      "includ\n",
      "genu\n",
      "ulation\n",
      "lished\n",
      "ation\n",
      "naev\n",
      "estab\n",
      "inclu\n",
      "\n",
      "\n",
      "attack\n",
      "bomber\n",
      "bombing\n",
      "insurgents\n",
      "airstrikes\n",
      "militants\n",
      "taliban\n",
      "explosives\n",
      "shootings\n",
      "attackers\n",
      "\n",
      "\n",
      "painting\n",
      "wheel\n",
      "trail\n",
      "sweep\n",
      "roof\n",
      "feed\n",
      "boom\n",
      "catch\n",
      "draw\n",
      "brand\n",
      "\n",
      "\n",
      "anything\n",
      "everything\n",
      "everywhere\n",
      "alive\n",
      "somebody\n",
      "anywhere\n",
      "anymore\n",
      "something\n",
      "nothing\n",
      "here\n",
      "\n",
      "\n",
      "wife\n",
      "girl\n",
      "father\n",
      "daughter\n",
      "mother\n",
      "brother\n",
      "grandmother\n",
      "husband\n",
      "girlfriend\n",
      "uncle\n",
      "\n",
      "\n",
      "ouri\n",
      "ikh\n",
      "za\n",
      "aya\n",
      "ara\n",
      "ano\n",
      "ino\n",
      "ina\n",
      "aba\n",
      "nandez\n",
      "\n",
      "\n",
      "also\n",
      "already\n",
      "never\n",
      "eventually\n",
      "initially\n",
      "allegedly\n",
      "originally\n",
      "previously\n",
      "being\n",
      "widely\n",
      "\n",
      "\n",
      "keeps\n",
      "gives\n",
      "provides\n",
      "involves\n",
      "takes\n",
      "gave\n",
      "enjoyed\n",
      "sees\n",
      "requires\n",
      "prompted\n",
      "\n",
      "\n",
      "people\n",
      "personnel\n",
      "employees\n",
      "families\n",
      "workers\n",
      "customers\n",
      "officers\n",
      "civilians\n",
      "children\n",
      "suspects\n",
      "\n",
      "\n",
      "spec\n",
      "intens\n",
      "pred\n",
      "ex\n",
      "magn\n",
      "pict\n",
      "gall\n",
      "accommo\n",
      "ident\n",
      "fut\n",
      "\n",
      "\n",
      "elections\n",
      "crimes\n",
      "muslims\n",
      "democracy\n",
      "regime\n",
      "violence\n",
      "enemies\n",
      "racism\n",
      "conflict\n",
      "principles\n",
      "\n",
      "\n",
      "came\n",
      "crashed\n",
      "walked\n",
      "went\n",
      "struck\n",
      "moved\n",
      "arrived\n",
      "happened\n",
      "occurred\n",
      "stayed\n",
      "\n",
      "\n",
      "barri\n",
      "manufactur\n",
      "recip\n",
      "challeng\n",
      "passeng\n",
      "soldi\n",
      "sist\n",
      "ione\n",
      "regist\n",
      "scor\n",
      "\n",
      "\n",
      "acare\n",
      "wiscons\n",
      "cording\n",
      "setts\n",
      "termine\n",
      "accompan\n",
      "^\n",
      "usalem\n",
      "genu\n",
      "estab\n",
      "\n",
      "\n",
      "marath\n",
      "gall\n",
      "kir\n",
      "chap\n",
      "jen\n",
      "oklahom\n",
      "sof\n",
      "volunte\n",
      "dak\n",
      "gaz\n",
      "\n",
      "\n",
      "could\n",
      "would\n",
      "can\n",
      "can't\n",
      "should\n",
      "won't\n",
      "might\n",
      "cannot\n",
      "must\n",
      "wouldn't\n",
      "\n",
      "\n",
      "introdu\n",
      "rele\n",
      "purch\n",
      "incredi\n",
      "produ\n",
      "decre\n",
      "commer\n",
      "clim\n",
      "bom\n",
      "dou\n",
      "\n",
      "\n",
      "attacked\n",
      "struck\n",
      "pulled\n",
      "deployed\n",
      "escaped\n",
      "entered\n",
      "spotted\n",
      "approached\n",
      "evacuated\n",
      "surrounded\n",
      "\n",
      "\n",
      "br\n",
      "gr\n",
      "v\n",
      "cr\n",
      "p\n",
      "g\n",
      "sl\n",
      "bl\n",
      "sh\n",
      "h\n",
      "\n",
      "\n",
      "texas\n",
      "florida\n",
      "arizona\n",
      "louisiana\n",
      "california\n",
      "maryland\n",
      "tennessee\n",
      "illinois\n",
      "kentucky\n",
      "alabama\n",
      "\n",
      "\n",
      "quake\n",
      "debris\n",
      "earthquake\n",
      "storm\n",
      "outbreak\n",
      "winds\n",
      "flooding\n",
      "volcano\n",
      "storms\n",
      "disaster\n",
      "\n",
      "\n",
      "challenged\n",
      "provided\n",
      "backed\n",
      "joined\n",
      "raised\n",
      "used\n",
      "earned\n",
      "imposed\n",
      "played\n",
      "built\n",
      "\n",
      "\n",
      "interviews\n",
      "remarks\n",
      "tweets\n",
      "messages\n",
      "roles\n",
      "views\n",
      "concerns\n",
      "details\n",
      "statements\n",
      "comments\n",
      "\n",
      "\n",
      "thousands\n",
      "millions\n",
      "hundreds\n",
      "dozens\n",
      "examples\n",
      "plenty\n",
      "lots\n",
      "sort\n",
      "kinds\n",
      "variety\n",
      "\n",
      "\n",
      "intimid\n",
      "moder\n",
      "elev\n",
      "coordin\n",
      "negoti\n",
      "interrog\n",
      "demonstr\n",
      "isol\n",
      "deliber\n",
      "regul\n",
      "\n",
      "\n",
      "appreciate\n",
      "remember\n",
      "know\n",
      "demonstrate\n",
      "tell\n",
      "discover\n",
      "imagine\n",
      "knowing\n",
      "confirm\n",
      "understand\n",
      "\n",
      "\n",
      "northern\n",
      "southern\n",
      "coastal\n",
      "eastern\n",
      "southeast\n",
      "central\n",
      "gulf\n",
      "western\n",
      "northwest\n",
      "south\n",
      "\n",
      "\n",
      "ugal\n",
      "on's\n",
      "ational\n",
      "ious\n",
      "yester\n",
      "itive\n",
      "genu\n",
      "massachu\n",
      "infrast\n",
      "inary\n",
      "\n",
      "\n",
      "his\n",
      "america's\n",
      "saturday's\n",
      "iraq's\n",
      "a's\n",
      "china's\n",
      "ley's\n",
      "er's\n",
      "thursday's\n",
      "son's\n",
      "\n",
      "\n",
      "ref\n",
      "disc\n",
      "pract\n",
      "pun\n",
      "explo\n",
      "stit\n",
      "constit\n",
      "instit\n",
      "pover\n",
      "imin\n",
      "\n",
      "\n",
      "sexual\n",
      "spiritual\n",
      "islamist\n",
      "extremist\n",
      "muslim\n",
      "gender\n",
      "jewish\n",
      "religious\n",
      "brutal\n",
      "militant\n",
      "\n",
      "\n",
      "apps\n",
      "google\n",
      "products\n",
      "technology\n",
      "devices\n",
      "smartphone\n",
      "amazon\n",
      "ipad\n",
      "android\n",
      "iphone\n",
      "\n",
      "\n",
      "17\n",
      "15\n",
      "25\n",
      "14\n",
      "35\n",
      "33\n",
      "50\n",
      "28\n",
      "46\n",
      "16\n",
      "\n",
      "\n",
      "aled\n",
      "pting\n",
      "tained\n",
      "izing\n",
      "ified\n",
      "ifies\n",
      "ured\n",
      "apped\n",
      "aling\n",
      "ounced\n",
      "\n",
      "\n",
      "yemeni\n",
      "japanese\n",
      "kenyan\n",
      "french\n",
      "turkish\n",
      "egyptian\n",
      "iraqi\n",
      "thai\n",
      "spanish\n",
      "russian\n",
      "\n",
      "\n",
      "overwhel\n",
      "alco\n",
      "onso\n",
      "scar\n",
      "insu\n",
      "amne\n",
      "boeh\n",
      "agre\n",
      "consu\n",
      "buil\n",
      "\n",
      "\n",
      "exhib\n",
      "nutr\n",
      "prohib\n",
      "recru\n",
      "reven\n",
      "plat\n",
      "circu\n",
      "exped\n",
      "cred\n",
      "jud\n",
      "\n",
      "\n",
      "happ\n",
      "individ\n",
      "advant\n",
      "encour\n",
      "prene\n",
      "pack\n",
      "langu\n",
      "manh\n",
      "eval\n",
      "rav\n",
      "\n",
      "\n",
      "water\n",
      "brain\n",
      "food\n",
      "wine\n",
      "neck\n",
      "blue\n",
      "radiation\n",
      "toxic\n",
      "nose\n",
      "metal\n",
      "\n",
      "\n",
      "3\n",
      "7\n",
      "8\n",
      "5\n",
      "2\n",
      "6\n",
      "1\n",
      "4\n",
      "9\n",
      "10\n",
      "\n",
      "\n",
      "replace\n",
      "achieve\n",
      "accomplish\n",
      "adjust\n",
      "incre\n",
      "environ\n",
      "depart\n",
      "invest\n",
      "arrange\n",
      "improve\n",
      "\n",
      "\n",
      "greater\n",
      "significant\n",
      "humanitarian\n",
      "negative\n",
      "rapid\n",
      "financial\n",
      "substantial\n",
      "immediate\n",
      "huge\n",
      "political\n",
      "\n",
      "\n",
      "pow\n",
      "bau\n",
      "wil\n",
      "scar\n",
      "buil\n",
      "cor\n",
      "kat\n",
      "rac\n",
      "sto\n",
      "pal\n",
      "\n",
      "\n",
      "credi\n",
      "satis\n",
      "responsi\n",
      "compe\n",
      "absol\n",
      "impro\n",
      "circum\n",
      "horri\n",
      "possi\n",
      "suc\n",
      "\n",
      "\n",
      "trium\n",
      "zam\n",
      "tai\n",
      "syl\n",
      "moham\n",
      "shang\n",
      "ku\n",
      "bou\n",
      "minnes\n",
      "mak\n",
      "\n",
      "\n",
      "shif\n",
      "moun\n",
      "tex\n",
      "prin\n",
      "secre\n",
      "rup\n",
      "aler\n",
      "confron\n",
      "erup\n",
      "adop\n",
      "\n",
      "\n",
      "congressional\n",
      "transportation\n",
      "legislative\n",
      "council\n",
      "resolution\n",
      "voting\n",
      "convention\n",
      "financial\n",
      "budget\n",
      "political\n",
      "\n",
      "\n",
      "get\n",
      "give\n",
      "find\n",
      "keep\n",
      "share\n",
      "pull\n",
      "take\n",
      "come\n",
      "stand\n",
      "put\n",
      "\n",
      "\n",
      "bring\n",
      "remove\n",
      "pursue\n",
      "reduce\n",
      "extend\n",
      "operate\n",
      "create\n",
      "raise\n",
      "appreciate\n",
      "contribute\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_embed = model.word_embed\n",
    "embedding_clustering = EmbeddingClustering(tokenizer, n_clusters=100)\n",
    "embedding_clustering.fit(word_embed)\n",
    "embedding_clustering.print_clusters(n_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc968e2d",
   "metadata": {},
   "source": [
    "# Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0de0e293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[4225]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([[964]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([[187]], shape=(1, 1), dtype=int32)\n",
      "queen\n",
      "man\n",
      "richard\n",
      "mar\n",
      "nic\n",
      "prince\n",
      "des\n",
      "ca\n",
      "van\n",
      "friend\n",
      "la\n",
      "yan\n",
      "pri\n",
      "mon\n",
      "fro\n",
      "hop\n",
      "ju\n",
      "wife\n",
      "lie\n",
      "dest\n",
      "epis\n",
      "te\n",
      "star\n",
      "pa\n",
      "bra\n",
      "hi\n",
      "daniel\n",
      "bi\n",
      "mic\n",
      "jane\n",
      "p\n",
      "simp\n",
      "ar\n",
      "nor\n",
      "den\n",
      "adm\n",
      "hua\n",
      "198\n",
      "lu\n",
      "fa\n",
      "miami\n",
      "ham\n",
      "bur\n",
      "u\n",
      "stron\n",
      "fol\n",
      "stat\n",
      "clark\n",
      "george\n",
      "m\n",
      "ra\n",
      "legendary\n",
      "ch\n",
      "ja\n",
      "phe\n",
      "sla\n",
      "stone\n",
      "cy\n",
      "hoff\n",
      "bron\n",
      "sha\n",
      "year's\n",
      "canad\n",
      "cop\n",
      "son\n",
      "d\n",
      "cape\n",
      "hu\n",
      "glas\n",
      "isa\n",
      "smu\n",
      "rev\n",
      "dy\n",
      "(\n",
      "vo\n",
      "tan\n",
      "mu\n",
      "louis\n",
      "pic\n",
      "ti\n",
      "pra\n",
      "former\n",
      "ta\n",
      "da\n",
      "bureau\n",
      "aven\n",
      "tom\n",
      "l\n",
      "rw\n",
      "de\n",
      "lou\n",
      "mary\n",
      "an\n",
      "mum\n",
      "kes\n",
      "li\n",
      "leon\n",
      "photograp\n",
      "dem\n",
      "christ\n"
     ]
    }
   ],
   "source": [
    "word_embed = model.word_embed\n",
    "\n",
    "text = \"queen\"\n",
    "text = text.lower()\n",
    "\n",
    "idx = tf.cast(tokenizer.tokenizer.tokenize(text), tf.int32)\n",
    "idx = tokenize(idx, tokenizer.merge_list)\n",
    "print(idx)\n",
    "embed1 = tf.expand_dims(word_embed[idx[0][0]], axis=0)\n",
    "\n",
    "\n",
    "text = \"woman\"\n",
    "text = text.lower()\n",
    "\n",
    "idx = tf.cast(tokenizer.tokenizer.tokenize(text), tf.int32)\n",
    "idx = tokenize(idx, tokenizer.merge_list)\n",
    "print(idx)\n",
    "embed2 = tf.expand_dims(word_embed[idx[0][0]], axis=0)\n",
    "\n",
    "text = \"man\"\n",
    "text = text.lower()\n",
    "\n",
    "idx = tf.cast(tokenizer.tokenizer.tokenize(text), tf.int32)\n",
    "idx = tokenize(idx, tokenizer.merge_list)\n",
    "print(idx)\n",
    "embed3 = tf.expand_dims(word_embed[idx[0][0]], axis=0)\n",
    "\n",
    "embed = embed1/tf.norm(embed1) - embed2/tf.norm(embed2) + embed3/tf.norm(embed3)\n",
    "\n",
    "cosine_sim = embed@tf.transpose(word_embed)\n",
    "idx = tf.argsort(cosine_sim, axis=-1, \n",
    "                 direction='DESCENDING',\n",
    "                 #direction='ASCENDING', \n",
    "                 stable=False, name=None)[0]\n",
    "\n",
    "for i in idx[:100]:\n",
    "    i = tf.expand_dims(i, axis=0)\n",
    "    print(tokenizer.detokenize(i).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a5f20b",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
