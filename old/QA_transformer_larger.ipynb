{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04010a08",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa3d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 4080, compute capability 8.9\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from src.tokenizer import TokenizerBPE, fuse_tokenized_corpus, chunk_corpus\n",
    "\n",
    "import os\n",
    "import time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm.notebook import tqdm\n",
    "from src.transformer import *\n",
    "from src.data_handling import split_on_value\n",
    "\n",
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8751dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = pkl.load(open(\"tokenizers/tokenizer_superQA_24k.pkl\", \"rb\"))\n",
    "tokenizer.create_hash()\n",
    "tokenizer.add_special_tokens([\"<s>\", \"</s>\", \"<q>\", \"<a>\",\"<pad>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73ef7665",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(corpus, batch_size=32):\n",
    "    length = corpus.shape[1]\n",
    "\n",
    "    batches = length // batch_size\n",
    "\n",
    "    corpus = corpus[:, :batches * batch_size]\n",
    "\n",
    "    corpus = tf.reshape(corpus, [-1, batch_size])\n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0868e180",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_coqa_sqa = pkl.load(open('corpus/corpus_coqa_sqa_24k_padded', 'rb'))\n",
    "corpus_squad_sqa = pkl.load(open(\"corpus/corpus_squad_sqa_24k_padded\", 'rb'))\n",
    "\n",
    "corpus_web_qa = pkl.load(open(\"corpus/corpus_web_qa_24k\", 'rb'))\n",
    "corpus_web_qa = batch(corpus_web_qa, batch_size=768)\n",
    "\n",
    "corpus_web_article = pkl.load(open(\"corpus/corpus_web_article_24k\", 'rb'))\n",
    "corpus_web_article = batch(corpus_web_article, batch_size=768)\n",
    "\n",
    "corpus_wiki_qa = pkl.load(open(\"corpus/corpus_wiki_qa_24k\", 'rb'))\n",
    "corpus_wiki_qa = batch(corpus_wiki_qa, batch_size=768)\n",
    "\n",
    "corpus_wiki_article = pkl.load(open(\"corpus/corpus_wiki_article_24k\", 'rb'))\n",
    "corpus_wiki_article = batch(corpus_wiki_article, batch_size=768)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36cedb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7199, 768)\n",
      "(19035, 768)\n",
      "(2495, 768)\n",
      "(21710, 768)\n",
      "(2026, 768)\n",
      "(26420, 768)\n"
     ]
    }
   ],
   "source": [
    "print(corpus_coqa_sqa.shape)\n",
    "print(corpus_squad_sqa.shape)\n",
    "print(corpus_web_qa.shape)\n",
    "print(corpus_web_article.shape)\n",
    "print(corpus_wiki_qa.shape)\n",
    "print(corpus_wiki_article.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dbd56bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(corpus_coqa_sqa[:100]==-1))\n",
    "print(np.sum(corpus_squad_sqa[:100]==-1))\n",
    "print(np.sum(corpus_web_qa[:100]==-1))\n",
    "print(np.sum(corpus_web_article[:100]==-1))\n",
    "print(np.sum(corpus_wiki_qa[:100]==-1))\n",
    "print(np.sum(corpus_wiki_article[:100]==-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07a40f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60583680\n"
     ]
    }
   ],
   "source": [
    "corpus = tf.concat([corpus_coqa_sqa, corpus_squad_sqa, corpus_web_qa, corpus_web_article, corpus_wiki_qa, corpus_wiki_article], axis=0)\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "corpus = tf.random.shuffle(corpus)\n",
    "length = corpus.shape[0]\n",
    "ratio = int(length * 0.95)\n",
    "\n",
    "corpus_train = corpus[:ratio] \n",
    "corpus_test = corpus[ratio:]\n",
    "\n",
    "print(length*768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8d39e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pipeline(corpus, batch_size=32):\n",
    "    samples = corpus.shape[0]\n",
    "\n",
    "    steps_per_epoch = samples // batch_size\n",
    "    \n",
    "    ds = tf.data.Dataset.from_tensor_slices(corpus)\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.shuffle(buffer_size=100*batch_size, reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return ds, steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37844241",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, steps_per_epoch = data_pipeline(corpus_train, batch_size=5)\n",
    "ds_test,_ = data_pipeline(corpus_test, batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f830d881",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a5a33ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lr = 1e-4\n",
    "decay_steps = 20000\n",
    "decay_rate = 0.5\n",
    "decay_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_lr,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=False)\n",
    "\n",
    "warmup_steps = 1000\n",
    "lr_schedule = WarmUpThenDecay(\n",
    "    initial_learning_rate=initial_lr,\n",
    "    warmup_steps=warmup_steps,\n",
    "    decay_schedule_fn=decay_schedule)\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "max_seq_len = 768\n",
    "embed_dim = 896\n",
    "tf_blocks = 14\n",
    "heads = 14\n",
    "ff_dim = 4*embed_dim\n",
    "weight_decay = 0.01\n",
    "dropout = 0.1\n",
    "\n",
    "unembed_dims = []\n",
    "accum_steps = 10\n",
    "\n",
    "model = Transformer(vocab_size=tokenizer.vocab_size,\n",
    "                    max_seq_len=max_seq_len,\n",
    "                    embed_dim=embed_dim,\n",
    "                    tf_blocks=tf_blocks,\n",
    "                    heads=heads,\n",
    "                    ff_dim = ff_dim,\n",
    "                    unembed_dims=unembed_dims,\n",
    "                    tokenizer=tokenizer,\n",
    "                    lr=lr_schedule,\n",
    "                    wd = weight_decay,\n",
    "                    dropout=dropout,\n",
    "                    accum_steps=accum_steps,\n",
    "                    )\n",
    "\n",
    "losses_train = []\n",
    "losses_test = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7970a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"model_super_qa_larger\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(\n",
    "    optimizer=model.opt,\n",
    "    model=model\n",
    ")\n",
    "ckpt_manager = tf.train.CheckpointManager(\n",
    "    ckpt, \n",
    "    directory=\"checkpoints/\" + name,      # folder where ckpts are saved\n",
    "    max_to_keep=5                         # only keep 5 latest checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88b34765",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "losses_train, losses_test = pkl.load(open(\"checkpoints/losses_\" + name + \".pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6527620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 157298091\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "total_params = 0\n",
    "for var in model.parameter_list:\n",
    "    shape = var.get_shape()\n",
    "    num_params = 1\n",
    "    for dim in shape:\n",
    "        num_params *= dim\n",
    "    total_params += num_params\n",
    "print(f\"Total number of parameters: {total_params}\")\n",
    "print(model.accum_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93977cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(iter_train, iter_test, steps_per_epoch, epochs):\n",
    "    for i in tqdm(range(steps_per_epoch//accum_steps*epochs)):\n",
    "        loss_train_temp = 0\n",
    "        loss_test_temp = 0\n",
    "        for _ in range(accum_steps):\n",
    "            batch_train = next(iter_train)\n",
    "            batch_test = next(iter_test)\n",
    "            \n",
    "            loss_train_temp += model.train_step(batch_train).numpy()\n",
    "            \n",
    "        loss_test_temp = model.evaluate(batch_test).numpy()\n",
    "            \n",
    "        loss_train_temp /= accum_steps\n",
    "        losses_train.append(loss_train_temp)\n",
    "        losses_test.append(loss_test_temp)\n",
    "\n",
    "        if (i+1) % 1000 == 0:\n",
    "            ckpt_manager.save()\n",
    "            pkl.dump([losses_train, losses_test], open(\"checkpoints/losses_\" + name + \".pkl\", 'wb'))\n",
    "\n",
    "    \n",
    "        lr = model.opt.inner_optimizer._decayed_lr(tf.float32).numpy()\n",
    "        print(f\"Step {i+1}, Loss Train: {loss_train_temp:.4f}, Loss Test: {loss_test_temp:.4f}, LR: {lr:.6f}\")\n",
    "    ckpt_manager.save()\n",
    "    pkl.dump([losses_train, losses_test], open(\"checkpoints/losses_\" + name + \".pkl\", 'wb'))\n",
    "    return losses_train, losses_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c652c842",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_train = iter(ds_train)\n",
    "iter_test = iter(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 986,
   "id": "28e04f4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13fb0455e31746af8fde97faa49af50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5992 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1, Loss Train: 3.0538, Loss Test: 3.9885, LR: 0.000031\n",
      "Step 2, Loss Train: 2.9546, Loss Test: 3.4908, LR: 0.000031\n",
      "Step 3, Loss Train: 3.0938, Loss Test: 3.9962, LR: 0.000031\n",
      "Step 4, Loss Train: 3.0038, Loss Test: 4.1693, LR: 0.000031\n",
      "Step 5, Loss Train: 2.9154, Loss Test: 2.5450, LR: 0.000031\n",
      "Step 6, Loss Train: 2.9208, Loss Test: 4.0887, LR: 0.000031\n",
      "Step 7, Loss Train: 3.0034, Loss Test: 3.8552, LR: 0.000031\n",
      "Step 8, Loss Train: 2.8440, Loss Test: 4.3759, LR: 0.000031\n",
      "Step 9, Loss Train: 3.1054, Loss Test: 3.8763, LR: 0.000031\n",
      "Step 10, Loss Train: 2.7316, Loss Test: 4.4927, LR: 0.000031\n",
      "Step 11, Loss Train: 2.9994, Loss Test: 3.1807, LR: 0.000031\n",
      "Step 12, Loss Train: 3.1585, Loss Test: 3.9829, LR: 0.000031\n",
      "Step 13, Loss Train: 2.9909, Loss Test: 4.1398, LR: 0.000031\n",
      "Step 14, Loss Train: 3.1708, Loss Test: 3.9126, LR: 0.000031\n",
      "Step 15, Loss Train: 3.0113, Loss Test: 4.5229, LR: 0.000031\n",
      "Step 16, Loss Train: 2.9962, Loss Test: 3.8865, LR: 0.000031\n",
      "Step 17, Loss Train: 2.9627, Loss Test: 4.3713, LR: 0.000031\n",
      "Step 18, Loss Train: 3.0191, Loss Test: 3.7820, LR: 0.000031\n",
      "Step 19, Loss Train: 2.9235, Loss Test: 3.7746, LR: 0.000031\n",
      "Step 20, Loss Train: 2.9524, Loss Test: 3.9020, LR: 0.000031\n",
      "Step 21, Loss Train: 3.1316, Loss Test: 2.8402, LR: 0.000031\n",
      "Step 22, Loss Train: 2.9126, Loss Test: 4.4409, LR: 0.000031\n",
      "Step 23, Loss Train: 2.9970, Loss Test: 2.1028, LR: 0.000031\n",
      "Step 24, Loss Train: 2.7248, Loss Test: 4.0264, LR: 0.000031\n",
      "Step 25, Loss Train: 2.8181, Loss Test: 4.0119, LR: 0.000031\n",
      "Step 26, Loss Train: 2.9888, Loss Test: 3.1863, LR: 0.000031\n",
      "Step 27, Loss Train: 2.8606, Loss Test: 3.8132, LR: 0.000031\n",
      "Step 28, Loss Train: 3.0674, Loss Test: 4.2723, LR: 0.000031\n",
      "Step 29, Loss Train: 2.9586, Loss Test: 2.3525, LR: 0.000031\n",
      "Step 30, Loss Train: 2.9062, Loss Test: 3.6153, LR: 0.000031\n",
      "Step 31, Loss Train: 2.9834, Loss Test: 4.1828, LR: 0.000031\n",
      "Step 32, Loss Train: 2.9676, Loss Test: 3.8016, LR: 0.000031\n",
      "Step 33, Loss Train: 2.9783, Loss Test: 4.2566, LR: 0.000031\n",
      "Step 34, Loss Train: 3.0613, Loss Test: 4.1280, LR: 0.000031\n",
      "Step 35, Loss Train: 2.9709, Loss Test: 4.2112, LR: 0.000031\n",
      "Step 36, Loss Train: 2.9182, Loss Test: 4.0585, LR: 0.000031\n",
      "Step 37, Loss Train: 2.8605, Loss Test: 3.5803, LR: 0.000031\n",
      "Step 38, Loss Train: 3.0238, Loss Test: 4.0727, LR: 0.000031\n",
      "Step 39, Loss Train: 2.8951, Loss Test: 2.7678, LR: 0.000031\n",
      "Step 40, Loss Train: 2.9562, Loss Test: 4.1403, LR: 0.000031\n",
      "Step 41, Loss Train: 3.0689, Loss Test: 4.3079, LR: 0.000031\n",
      "Step 42, Loss Train: 3.0451, Loss Test: 3.9729, LR: 0.000031\n",
      "Step 43, Loss Train: 3.0745, Loss Test: 3.9260, LR: 0.000031\n",
      "Step 44, Loss Train: 2.9723, Loss Test: 3.6078, LR: 0.000031\n",
      "Step 45, Loss Train: 3.0420, Loss Test: 4.5875, LR: 0.000031\n",
      "Step 46, Loss Train: 3.0112, Loss Test: 2.8800, LR: 0.000031\n",
      "Step 47, Loss Train: 3.0016, Loss Test: 2.1138, LR: 0.000031\n",
      "Step 48, Loss Train: 3.0820, Loss Test: 3.8939, LR: 0.000031\n",
      "Step 49, Loss Train: 3.1267, Loss Test: 3.7363, LR: 0.000031\n",
      "Step 50, Loss Train: 3.0771, Loss Test: 4.5637, LR: 0.000031\n",
      "Step 51, Loss Train: 2.9329, Loss Test: 2.6377, LR: 0.000031\n",
      "Step 52, Loss Train: 3.0407, Loss Test: 4.2030, LR: 0.000031\n",
      "Step 53, Loss Train: 2.8141, Loss Test: 3.6571, LR: 0.000031\n",
      "Step 54, Loss Train: 3.1639, Loss Test: 3.7130, LR: 0.000031\n",
      "Step 55, Loss Train: 2.8927, Loss Test: 3.2003, LR: 0.000031\n",
      "Step 56, Loss Train: 3.0917, Loss Test: 4.3605, LR: 0.000031\n",
      "Step 57, Loss Train: 3.0876, Loss Test: 3.3869, LR: 0.000031\n",
      "Step 58, Loss Train: 2.9641, Loss Test: 3.7825, LR: 0.000031\n",
      "Step 59, Loss Train: 3.1206, Loss Test: 2.4648, LR: 0.000031\n",
      "Step 60, Loss Train: 3.1295, Loss Test: 3.9841, LR: 0.000031\n",
      "Step 61, Loss Train: 3.1020, Loss Test: 4.0976, LR: 0.000031\n",
      "Step 62, Loss Train: 2.9628, Loss Test: 3.9460, LR: 0.000031\n",
      "Step 63, Loss Train: 3.1063, Loss Test: 3.0815, LR: 0.000031\n",
      "Step 64, Loss Train: 3.0112, Loss Test: 3.3277, LR: 0.000031\n",
      "Step 65, Loss Train: 3.0400, Loss Test: 3.0786, LR: 0.000031\n",
      "Step 66, Loss Train: 2.8539, Loss Test: 3.9139, LR: 0.000031\n",
      "Step 67, Loss Train: 2.9959, Loss Test: 3.9270, LR: 0.000031\n",
      "Step 68, Loss Train: 3.0882, Loss Test: 4.3550, LR: 0.000031\n",
      "Step 69, Loss Train: 2.8180, Loss Test: 4.0833, LR: 0.000031\n",
      "Step 70, Loss Train: 3.1316, Loss Test: 3.9014, LR: 0.000031\n",
      "Step 71, Loss Train: 2.8677, Loss Test: 3.3481, LR: 0.000031\n",
      "Step 72, Loss Train: 2.9565, Loss Test: 4.2019, LR: 0.000031\n",
      "Step 73, Loss Train: 2.8982, Loss Test: 3.0622, LR: 0.000031\n",
      "Step 74, Loss Train: 3.0281, Loss Test: 4.2976, LR: 0.000031\n",
      "Step 75, Loss Train: 3.0567, Loss Test: 2.2677, LR: 0.000031\n",
      "Step 76, Loss Train: 2.9854, Loss Test: 4.1903, LR: 0.000031\n",
      "Step 77, Loss Train: 2.8702, Loss Test: 3.6400, LR: 0.000031\n",
      "Step 78, Loss Train: 2.9916, Loss Test: 3.4661, LR: 0.000031\n",
      "Step 79, Loss Train: 3.0021, Loss Test: 3.5730, LR: 0.000031\n",
      "Step 80, Loss Train: 3.0746, Loss Test: 4.0483, LR: 0.000031\n",
      "Step 81, Loss Train: 2.8928, Loss Test: 4.2852, LR: 0.000031\n",
      "Step 82, Loss Train: 3.0568, Loss Test: 3.6473, LR: 0.000031\n",
      "Step 83, Loss Train: 2.7651, Loss Test: 3.6992, LR: 0.000031\n",
      "Step 84, Loss Train: 2.8897, Loss Test: 3.7778, LR: 0.000031\n",
      "Step 85, Loss Train: 3.0007, Loss Test: 3.5445, LR: 0.000031\n",
      "Step 86, Loss Train: 2.9824, Loss Test: 3.6784, LR: 0.000031\n",
      "Step 87, Loss Train: 3.2110, Loss Test: 3.1064, LR: 0.000031\n",
      "Step 88, Loss Train: 3.0043, Loss Test: 4.4506, LR: 0.000031\n",
      "Step 89, Loss Train: 2.9980, Loss Test: 3.9963, LR: 0.000031\n",
      "Step 90, Loss Train: 2.9553, Loss Test: 4.4110, LR: 0.000031\n",
      "Step 91, Loss Train: 3.0639, Loss Test: 3.9114, LR: 0.000031\n",
      "Step 92, Loss Train: 2.8521, Loss Test: 4.1155, LR: 0.000031\n",
      "Step 93, Loss Train: 3.0154, Loss Test: 3.9361, LR: 0.000031\n",
      "Step 94, Loss Train: 2.9493, Loss Test: 3.7099, LR: 0.000031\n",
      "Step 95, Loss Train: 2.9884, Loss Test: 4.0354, LR: 0.000031\n",
      "Step 96, Loss Train: 2.9911, Loss Test: 2.2680, LR: 0.000031\n",
      "Step 97, Loss Train: 2.8016, Loss Test: 3.7285, LR: 0.000031\n",
      "Step 98, Loss Train: 2.8605, Loss Test: 4.0737, LR: 0.000031\n",
      "Step 99, Loss Train: 3.0109, Loss Test: 3.8302, LR: 0.000031\n",
      "Step 100, Loss Train: 3.1705, Loss Test: 4.3608, LR: 0.000031\n",
      "Step 101, Loss Train: 3.1011, Loss Test: 3.4632, LR: 0.000031\n",
      "Step 102, Loss Train: 2.9664, Loss Test: 4.0511, LR: 0.000031\n",
      "Step 103, Loss Train: 3.0366, Loss Test: 3.7002, LR: 0.000031\n",
      "Step 104, Loss Train: 3.0009, Loss Test: 4.2988, LR: 0.000031\n",
      "Step 105, Loss Train: 2.9419, Loss Test: 3.9125, LR: 0.000031\n",
      "Step 106, Loss Train: 3.1954, Loss Test: 4.0013, LR: 0.000031\n",
      "Step 107, Loss Train: 2.9619, Loss Test: 3.4927, LR: 0.000031\n",
      "Step 108, Loss Train: 3.0176, Loss Test: 3.5175, LR: 0.000031\n",
      "Step 109, Loss Train: 2.9045, Loss Test: 3.5897, LR: 0.000031\n",
      "Step 110, Loss Train: 2.9977, Loss Test: 3.2026, LR: 0.000031\n",
      "Step 111, Loss Train: 2.9471, Loss Test: 4.4026, LR: 0.000031\n",
      "Step 112, Loss Train: 3.1206, Loss Test: 3.6663, LR: 0.000031\n",
      "Step 113, Loss Train: 3.2198, Loss Test: 2.8413, LR: 0.000031\n",
      "Step 114, Loss Train: 2.9507, Loss Test: 4.0048, LR: 0.000031\n",
      "Step 115, Loss Train: 2.9107, Loss Test: 3.6393, LR: 0.000031\n",
      "Step 116, Loss Train: 3.1120, Loss Test: 4.1371, LR: 0.000031\n",
      "Step 117, Loss Train: 2.9334, Loss Test: 3.0498, LR: 0.000031\n",
      "Step 118, Loss Train: 3.1299, Loss Test: 3.8687, LR: 0.000031\n",
      "Step 119, Loss Train: 3.0306, Loss Test: 4.1851, LR: 0.000031\n",
      "Step 120, Loss Train: 2.9705, Loss Test: 3.8457, LR: 0.000031\n",
      "Step 121, Loss Train: 2.9612, Loss Test: 3.5517, LR: 0.000031\n",
      "Step 122, Loss Train: 2.8954, Loss Test: 3.6416, LR: 0.000031\n",
      "Step 123, Loss Train: 3.0140, Loss Test: 4.5497, LR: 0.000031\n",
      "Step 124, Loss Train: 2.8898, Loss Test: 3.6521, LR: 0.000031\n",
      "Step 125, Loss Train: 2.7955, Loss Test: 2.8569, LR: 0.000031\n",
      "Step 126, Loss Train: 2.9689, Loss Test: 3.9987, LR: 0.000031\n",
      "Step 127, Loss Train: 3.1900, Loss Test: 3.5360, LR: 0.000031\n",
      "Step 128, Loss Train: 3.0251, Loss Test: 4.2134, LR: 0.000031\n",
      "Step 129, Loss Train: 3.0772, Loss Test: 3.9978, LR: 0.000031\n",
      "Step 130, Loss Train: 3.0244, Loss Test: 3.6807, LR: 0.000031\n",
      "Step 131, Loss Train: 2.9545, Loss Test: 3.6860, LR: 0.000031\n",
      "Step 132, Loss Train: 2.9527, Loss Test: 3.1909, LR: 0.000031\n",
      "Step 133, Loss Train: 2.9423, Loss Test: 3.4385, LR: 0.000031\n",
      "Step 134, Loss Train: 3.0893, Loss Test: 2.6678, LR: 0.000031\n",
      "Step 135, Loss Train: 2.9819, Loss Test: 4.1833, LR: 0.000031\n",
      "Step 136, Loss Train: 2.8484, Loss Test: 4.0352, LR: 0.000031\n",
      "Step 137, Loss Train: 2.8936, Loss Test: 2.5746, LR: 0.000031\n",
      "Step 138, Loss Train: 3.0564, Loss Test: 3.9576, LR: 0.000031\n",
      "Step 139, Loss Train: 3.0635, Loss Test: 3.5148, LR: 0.000031\n",
      "Step 140, Loss Train: 2.8309, Loss Test: 3.8238, LR: 0.000031\n",
      "Step 141, Loss Train: 3.0632, Loss Test: 4.4248, LR: 0.000031\n",
      "Step 142, Loss Train: 2.9507, Loss Test: 3.5271, LR: 0.000031\n",
      "Step 143, Loss Train: 3.0585, Loss Test: 3.4141, LR: 0.000031\n",
      "Step 144, Loss Train: 3.0330, Loss Test: 3.4666, LR: 0.000031\n",
      "Step 145, Loss Train: 2.9268, Loss Test: 4.0059, LR: 0.000031\n",
      "Step 146, Loss Train: 3.0418, Loss Test: 4.0527, LR: 0.000031\n",
      "Step 147, Loss Train: 2.9663, Loss Test: 3.2340, LR: 0.000031\n",
      "Step 148, Loss Train: 3.0357, Loss Test: 3.2778, LR: 0.000031\n",
      "Step 149, Loss Train: 2.9575, Loss Test: 3.5501, LR: 0.000031\n",
      "Step 150, Loss Train: 2.9208, Loss Test: 4.1016, LR: 0.000031\n",
      "Step 151, Loss Train: 3.0148, Loss Test: 3.6964, LR: 0.000031\n",
      "Step 152, Loss Train: 3.0852, Loss Test: 3.5226, LR: 0.000031\n",
      "Step 153, Loss Train: 2.9575, Loss Test: 3.2098, LR: 0.000031\n",
      "Step 154, Loss Train: 3.0423, Loss Test: 2.4898, LR: 0.000031\n",
      "Step 155, Loss Train: 3.1166, Loss Test: 4.0734, LR: 0.000031\n",
      "Step 156, Loss Train: 2.8542, Loss Test: 2.3902, LR: 0.000031\n",
      "Step 157, Loss Train: 3.0269, Loss Test: 4.3368, LR: 0.000031\n",
      "Step 158, Loss Train: 3.0048, Loss Test: 3.8599, LR: 0.000031\n",
      "Step 159, Loss Train: 2.8909, Loss Test: 4.1251, LR: 0.000031\n",
      "Step 160, Loss Train: 2.7171, Loss Test: 4.1052, LR: 0.000031\n",
      "Step 161, Loss Train: 3.0172, Loss Test: 3.5707, LR: 0.000031\n",
      "Step 162, Loss Train: 3.0047, Loss Test: 4.2550, LR: 0.000031\n",
      "Step 163, Loss Train: 3.1031, Loss Test: 3.7590, LR: 0.000031\n",
      "Step 164, Loss Train: 3.0776, Loss Test: 4.4227, LR: 0.000031\n",
      "Step 165, Loss Train: 3.1446, Loss Test: 3.6885, LR: 0.000031\n",
      "Step 166, Loss Train: 3.1199, Loss Test: 4.6491, LR: 0.000031\n",
      "Step 167, Loss Train: 2.8567, Loss Test: 3.3175, LR: 0.000031\n",
      "Step 168, Loss Train: 2.8684, Loss Test: 3.5953, LR: 0.000031\n",
      "Step 169, Loss Train: 2.8209, Loss Test: 3.9182, LR: 0.000031\n",
      "Step 170, Loss Train: 2.9535, Loss Test: 3.6161, LR: 0.000031\n",
      "Step 171, Loss Train: 2.9108, Loss Test: 4.1096, LR: 0.000031\n",
      "Step 172, Loss Train: 3.1308, Loss Test: 2.9793, LR: 0.000031\n",
      "Step 173, Loss Train: 3.0054, Loss Test: 4.1286, LR: 0.000031\n",
      "Step 174, Loss Train: 2.8726, Loss Test: 3.3578, LR: 0.000031\n",
      "Step 175, Loss Train: 3.0665, Loss Test: 3.8428, LR: 0.000031\n",
      "Step 176, Loss Train: 3.0166, Loss Test: 4.1830, LR: 0.000031\n",
      "Step 177, Loss Train: 3.0115, Loss Test: 3.0002, LR: 0.000031\n",
      "Step 178, Loss Train: 3.0495, Loss Test: 4.3539, LR: 0.000031\n",
      "Step 179, Loss Train: 3.0281, Loss Test: 4.3484, LR: 0.000031\n",
      "Step 180, Loss Train: 2.8633, Loss Test: 3.7893, LR: 0.000031\n",
      "Step 181, Loss Train: 3.1403, Loss Test: 4.1955, LR: 0.000031\n",
      "Step 182, Loss Train: 2.8060, Loss Test: 3.9678, LR: 0.000031\n",
      "Step 183, Loss Train: 2.9234, Loss Test: 4.0900, LR: 0.000031\n",
      "Step 184, Loss Train: 2.9882, Loss Test: 4.2050, LR: 0.000031\n",
      "Step 185, Loss Train: 3.0182, Loss Test: 3.8100, LR: 0.000031\n",
      "Step 186, Loss Train: 3.0882, Loss Test: 3.0309, LR: 0.000031\n",
      "Step 187, Loss Train: 3.0812, Loss Test: 3.2075, LR: 0.000031\n",
      "Step 188, Loss Train: 2.8471, Loss Test: 3.9562, LR: 0.000031\n",
      "Step 189, Loss Train: 3.0006, Loss Test: 3.6815, LR: 0.000031\n",
      "Step 190, Loss Train: 2.8934, Loss Test: 4.3729, LR: 0.000031\n",
      "Step 191, Loss Train: 2.9829, Loss Test: 4.4874, LR: 0.000031\n",
      "Step 192, Loss Train: 2.9887, Loss Test: 3.4344, LR: 0.000031\n",
      "Step 193, Loss Train: 2.9331, Loss Test: 4.3026, LR: 0.000031\n",
      "Step 194, Loss Train: 3.0164, Loss Test: 3.5730, LR: 0.000031\n",
      "Step 195, Loss Train: 3.1342, Loss Test: 3.7561, LR: 0.000031\n",
      "Step 196, Loss Train: 3.0249, Loss Test: 0.9258, LR: 0.000031\n",
      "Step 197, Loss Train: 3.0302, Loss Test: 4.0373, LR: 0.000031\n",
      "Step 198, Loss Train: 3.1132, Loss Test: 4.1189, LR: 0.000031\n",
      "Step 199, Loss Train: 3.0872, Loss Test: 3.6741, LR: 0.000031\n",
      "Step 200, Loss Train: 3.0129, Loss Test: 4.0945, LR: 0.000031\n",
      "Step 201, Loss Train: 2.8102, Loss Test: 3.6240, LR: 0.000031\n",
      "Step 202, Loss Train: 3.0281, Loss Test: 3.7590, LR: 0.000031\n",
      "Step 203, Loss Train: 2.9017, Loss Test: 4.2470, LR: 0.000031\n",
      "Step 204, Loss Train: 2.8536, Loss Test: 3.1596, LR: 0.000031\n",
      "Step 205, Loss Train: 3.1353, Loss Test: 2.9253, LR: 0.000031\n",
      "Step 206, Loss Train: 3.0806, Loss Test: 3.4319, LR: 0.000031\n",
      "Step 207, Loss Train: 2.8989, Loss Test: 4.4148, LR: 0.000031\n",
      "Step 208, Loss Train: 2.8409, Loss Test: 3.0107, LR: 0.000031\n",
      "Step 209, Loss Train: 3.0356, Loss Test: 1.8760, LR: 0.000031\n",
      "Step 210, Loss Train: 2.8989, Loss Test: 3.5579, LR: 0.000031\n",
      "Step 211, Loss Train: 3.0478, Loss Test: 3.8893, LR: 0.000031\n",
      "Step 212, Loss Train: 3.0601, Loss Test: 3.7922, LR: 0.000031\n",
      "Step 213, Loss Train: 2.9586, Loss Test: 4.3454, LR: 0.000031\n",
      "Step 214, Loss Train: 2.8448, Loss Test: 3.9774, LR: 0.000031\n",
      "Step 215, Loss Train: 3.0106, Loss Test: 4.1320, LR: 0.000031\n",
      "Step 216, Loss Train: 2.8614, Loss Test: 3.5461, LR: 0.000031\n",
      "Step 217, Loss Train: 2.9060, Loss Test: 2.7535, LR: 0.000031\n",
      "Step 218, Loss Train: 2.9700, Loss Test: 3.5627, LR: 0.000031\n",
      "Step 219, Loss Train: 2.9651, Loss Test: 3.1323, LR: 0.000031\n",
      "Step 220, Loss Train: 3.0522, Loss Test: 4.2289, LR: 0.000031\n",
      "Step 221, Loss Train: 2.9706, Loss Test: 3.6883, LR: 0.000031\n",
      "Step 222, Loss Train: 2.8850, Loss Test: 4.2201, LR: 0.000031\n",
      "Step 223, Loss Train: 3.0535, Loss Test: 3.1596, LR: 0.000031\n",
      "Step 224, Loss Train: 3.0681, Loss Test: 2.1920, LR: 0.000031\n",
      "Step 225, Loss Train: 3.1390, Loss Test: 4.1466, LR: 0.000031\n",
      "Step 226, Loss Train: 2.9360, Loss Test: 2.8956, LR: 0.000031\n",
      "Step 227, Loss Train: 2.8793, Loss Test: 4.1520, LR: 0.000031\n",
      "Step 228, Loss Train: 2.8484, Loss Test: 3.9751, LR: 0.000031\n",
      "Step 229, Loss Train: 3.0638, Loss Test: 3.8874, LR: 0.000031\n",
      "Step 230, Loss Train: 2.9229, Loss Test: 3.6894, LR: 0.000031\n",
      "Step 231, Loss Train: 3.0106, Loss Test: 3.1942, LR: 0.000031\n",
      "Step 232, Loss Train: 2.9313, Loss Test: 3.9257, LR: 0.000031\n",
      "Step 233, Loss Train: 2.8808, Loss Test: 3.2525, LR: 0.000031\n",
      "Step 234, Loss Train: 2.8897, Loss Test: 3.5726, LR: 0.000031\n",
      "Step 235, Loss Train: 2.9152, Loss Test: 4.5982, LR: 0.000031\n",
      "Step 236, Loss Train: 3.0378, Loss Test: 4.2365, LR: 0.000031\n",
      "Step 237, Loss Train: 2.7381, Loss Test: 3.5590, LR: 0.000031\n",
      "Step 238, Loss Train: 3.0378, Loss Test: 4.2427, LR: 0.000031\n",
      "Step 239, Loss Train: 3.0111, Loss Test: 1.4111, LR: 0.000031\n",
      "Step 240, Loss Train: 2.8616, Loss Test: 3.1266, LR: 0.000031\n",
      "Step 241, Loss Train: 3.0948, Loss Test: 3.9844, LR: 0.000031\n",
      "Step 242, Loss Train: 3.0217, Loss Test: 4.2856, LR: 0.000031\n",
      "Step 243, Loss Train: 2.9443, Loss Test: 3.2639, LR: 0.000031\n",
      "Step 244, Loss Train: 2.8217, Loss Test: 4.4188, LR: 0.000031\n",
      "Step 245, Loss Train: 2.9422, Loss Test: 3.8664, LR: 0.000031\n",
      "Step 246, Loss Train: 3.0488, Loss Test: 3.1194, LR: 0.000031\n",
      "Step 247, Loss Train: 2.9242, Loss Test: 3.3180, LR: 0.000031\n",
      "Step 248, Loss Train: 2.9822, Loss Test: 4.0668, LR: 0.000031\n",
      "Step 249, Loss Train: 2.8511, Loss Test: 3.4476, LR: 0.000031\n",
      "Step 250, Loss Train: 2.9617, Loss Test: 3.5678, LR: 0.000031\n",
      "Step 251, Loss Train: 2.9688, Loss Test: 4.1336, LR: 0.000031\n",
      "Step 252, Loss Train: 3.0534, Loss Test: 4.0658, LR: 0.000031\n",
      "Step 253, Loss Train: 3.0821, Loss Test: 3.2153, LR: 0.000031\n",
      "Step 254, Loss Train: 2.9974, Loss Test: 2.1235, LR: 0.000031\n",
      "Step 255, Loss Train: 3.0863, Loss Test: 3.9740, LR: 0.000031\n",
      "Step 256, Loss Train: 3.0369, Loss Test: 3.7111, LR: 0.000031\n",
      "Step 257, Loss Train: 2.7301, Loss Test: 3.7932, LR: 0.000031\n",
      "Step 258, Loss Train: 2.8444, Loss Test: 3.4411, LR: 0.000031\n",
      "Step 259, Loss Train: 3.1211, Loss Test: 2.9233, LR: 0.000031\n",
      "Step 260, Loss Train: 3.0875, Loss Test: 3.9639, LR: 0.000031\n",
      "Step 261, Loss Train: 3.0230, Loss Test: 2.8412, LR: 0.000031\n",
      "Step 262, Loss Train: 3.0291, Loss Test: 3.9354, LR: 0.000031\n",
      "Step 263, Loss Train: 3.0268, Loss Test: 3.7465, LR: 0.000031\n",
      "Step 264, Loss Train: 2.9524, Loss Test: 4.2993, LR: 0.000031\n",
      "Step 265, Loss Train: 2.9648, Loss Test: 3.7314, LR: 0.000031\n",
      "Step 266, Loss Train: 2.8522, Loss Test: 3.9610, LR: 0.000031\n",
      "Step 267, Loss Train: 3.1486, Loss Test: 4.2201, LR: 0.000031\n",
      "Step 268, Loss Train: 2.9990, Loss Test: 3.9026, LR: 0.000031\n",
      "Step 269, Loss Train: 3.2152, Loss Test: 4.2297, LR: 0.000031\n",
      "Step 270, Loss Train: 3.0133, Loss Test: 4.5128, LR: 0.000031\n",
      "Step 271, Loss Train: 2.9882, Loss Test: 3.3347, LR: 0.000031\n",
      "Step 272, Loss Train: 3.1047, Loss Test: 3.2734, LR: 0.000031\n",
      "Step 273, Loss Train: 2.9913, Loss Test: 3.7381, LR: 0.000031\n",
      "Step 274, Loss Train: 2.9771, Loss Test: 4.6181, LR: 0.000031\n",
      "Step 275, Loss Train: 3.0372, Loss Test: 3.9177, LR: 0.000031\n",
      "Step 276, Loss Train: 3.0570, Loss Test: 4.0705, LR: 0.000031\n",
      "Step 277, Loss Train: 3.0544, Loss Test: 4.0242, LR: 0.000031\n",
      "Step 278, Loss Train: 3.0265, Loss Test: 4.3950, LR: 0.000031\n",
      "Step 279, Loss Train: 2.8820, Loss Test: 4.2049, LR: 0.000031\n",
      "Step 280, Loss Train: 2.8526, Loss Test: 3.9805, LR: 0.000031\n",
      "Step 281, Loss Train: 3.0847, Loss Test: 4.0949, LR: 0.000031\n",
      "Step 282, Loss Train: 2.8171, Loss Test: 3.7588, LR: 0.000031\n",
      "Step 283, Loss Train: 3.0482, Loss Test: 2.4315, LR: 0.000031\n",
      "Step 284, Loss Train: 2.9865, Loss Test: 3.2088, LR: 0.000031\n",
      "Step 285, Loss Train: 2.9948, Loss Test: 2.4786, LR: 0.000031\n",
      "Step 286, Loss Train: 2.9562, Loss Test: 4.2114, LR: 0.000031\n",
      "Step 287, Loss Train: 3.0649, Loss Test: 4.3833, LR: 0.000031\n",
      "Step 288, Loss Train: 2.8897, Loss Test: 4.1430, LR: 0.000031\n",
      "Step 289, Loss Train: 3.0419, Loss Test: 3.8759, LR: 0.000031\n",
      "Step 290, Loss Train: 3.0943, Loss Test: 4.3544, LR: 0.000031\n",
      "Step 291, Loss Train: 2.9645, Loss Test: 3.9841, LR: 0.000031\n",
      "Step 292, Loss Train: 2.7937, Loss Test: 3.8024, LR: 0.000031\n",
      "Step 293, Loss Train: 2.9192, Loss Test: 3.3311, LR: 0.000031\n",
      "Step 294, Loss Train: 3.0821, Loss Test: 3.4240, LR: 0.000031\n",
      "Step 295, Loss Train: 2.7233, Loss Test: 4.3410, LR: 0.000031\n",
      "Step 296, Loss Train: 2.6747, Loss Test: 3.8738, LR: 0.000031\n",
      "Step 297, Loss Train: 2.8888, Loss Test: 4.0601, LR: 0.000031\n",
      "Step 298, Loss Train: 2.8855, Loss Test: 4.0046, LR: 0.000031\n",
      "Step 299, Loss Train: 2.8256, Loss Test: 3.8252, LR: 0.000031\n",
      "Step 300, Loss Train: 2.9151, Loss Test: 3.0980, LR: 0.000031\n",
      "Step 301, Loss Train: 2.9414, Loss Test: 4.1327, LR: 0.000031\n",
      "Step 302, Loss Train: 3.0269, Loss Test: 4.0774, LR: 0.000031\n",
      "Step 303, Loss Train: 2.9441, Loss Test: 4.1695, LR: 0.000031\n",
      "Step 304, Loss Train: 2.9805, Loss Test: 2.7665, LR: 0.000031\n",
      "Step 305, Loss Train: 2.9566, Loss Test: 3.8641, LR: 0.000031\n",
      "Step 306, Loss Train: 2.8186, Loss Test: 4.4551, LR: 0.000031\n",
      "Step 307, Loss Train: 2.8438, Loss Test: 3.8322, LR: 0.000031\n",
      "Step 308, Loss Train: 2.9936, Loss Test: 3.6255, LR: 0.000031\n",
      "Step 309, Loss Train: 2.8563, Loss Test: 3.3341, LR: 0.000031\n",
      "Step 310, Loss Train: 2.9881, Loss Test: 3.0411, LR: 0.000031\n",
      "Step 311, Loss Train: 2.8366, Loss Test: 3.8221, LR: 0.000031\n",
      "Step 312, Loss Train: 2.8996, Loss Test: 3.3037, LR: 0.000031\n",
      "Step 313, Loss Train: 2.9257, Loss Test: 3.4231, LR: 0.000031\n",
      "Step 314, Loss Train: 2.9314, Loss Test: 3.5284, LR: 0.000031\n",
      "Step 315, Loss Train: 3.0226, Loss Test: 3.9601, LR: 0.000031\n",
      "Step 316, Loss Train: 3.0990, Loss Test: 4.0277, LR: 0.000031\n",
      "Step 317, Loss Train: 3.0959, Loss Test: 3.9482, LR: 0.000031\n",
      "Step 318, Loss Train: 3.0495, Loss Test: 3.4743, LR: 0.000031\n",
      "Step 319, Loss Train: 2.8805, Loss Test: 4.0884, LR: 0.000031\n",
      "Step 320, Loss Train: 3.0315, Loss Test: 2.8410, LR: 0.000031\n",
      "Step 321, Loss Train: 2.9001, Loss Test: 3.2510, LR: 0.000031\n",
      "Step 322, Loss Train: 3.0065, Loss Test: 3.7791, LR: 0.000031\n",
      "Step 323, Loss Train: 2.9321, Loss Test: 4.4106, LR: 0.000031\n",
      "Step 324, Loss Train: 2.9988, Loss Test: 3.4141, LR: 0.000031\n",
      "Step 325, Loss Train: 2.8044, Loss Test: 3.2812, LR: 0.000031\n",
      "Step 326, Loss Train: 2.8885, Loss Test: 2.9642, LR: 0.000031\n",
      "Step 327, Loss Train: 2.9286, Loss Test: 3.5543, LR: 0.000031\n",
      "Step 328, Loss Train: 3.0019, Loss Test: 3.8190, LR: 0.000031\n",
      "Step 329, Loss Train: 2.7985, Loss Test: 2.9494, LR: 0.000031\n",
      "Step 330, Loss Train: 2.8855, Loss Test: 3.2622, LR: 0.000031\n",
      "Step 331, Loss Train: 3.0463, Loss Test: 3.5065, LR: 0.000031\n",
      "Step 332, Loss Train: 2.9441, Loss Test: 3.7880, LR: 0.000031\n",
      "Step 333, Loss Train: 3.1234, Loss Test: 3.7019, LR: 0.000031\n",
      "Step 334, Loss Train: 3.0740, Loss Test: 3.6962, LR: 0.000031\n",
      "Step 335, Loss Train: 2.8840, Loss Test: 3.9352, LR: 0.000031\n",
      "Step 336, Loss Train: 2.8280, Loss Test: 4.2819, LR: 0.000031\n",
      "Step 337, Loss Train: 2.8928, Loss Test: 3.5088, LR: 0.000031\n",
      "Step 338, Loss Train: 2.7805, Loss Test: 4.2474, LR: 0.000031\n",
      "Step 339, Loss Train: 2.9180, Loss Test: 3.2010, LR: 0.000031\n",
      "Step 340, Loss Train: 2.8487, Loss Test: 4.3747, LR: 0.000031\n",
      "Step 341, Loss Train: 3.0569, Loss Test: 3.6348, LR: 0.000031\n",
      "Step 342, Loss Train: 3.0831, Loss Test: 3.5166, LR: 0.000031\n",
      "Step 343, Loss Train: 2.8524, Loss Test: 3.0149, LR: 0.000031\n",
      "Step 344, Loss Train: 2.9196, Loss Test: 3.5842, LR: 0.000031\n",
      "Step 345, Loss Train: 2.9899, Loss Test: 4.3527, LR: 0.000031\n",
      "Step 346, Loss Train: 2.9702, Loss Test: 3.8894, LR: 0.000031\n",
      "Step 347, Loss Train: 3.0922, Loss Test: 4.1215, LR: 0.000031\n",
      "Step 348, Loss Train: 2.9437, Loss Test: 4.4891, LR: 0.000031\n",
      "Step 349, Loss Train: 2.8814, Loss Test: 3.9116, LR: 0.000031\n",
      "Step 350, Loss Train: 2.8401, Loss Test: 3.1208, LR: 0.000031\n",
      "Step 351, Loss Train: 2.9683, Loss Test: 2.4006, LR: 0.000031\n",
      "Step 352, Loss Train: 2.8788, Loss Test: 3.5769, LR: 0.000031\n",
      "Step 353, Loss Train: 2.9318, Loss Test: 3.9949, LR: 0.000031\n",
      "Step 354, Loss Train: 2.8211, Loss Test: 2.3084, LR: 0.000031\n",
      "Step 355, Loss Train: 2.8867, Loss Test: 4.0813, LR: 0.000031\n",
      "Step 356, Loss Train: 2.8852, Loss Test: 3.5475, LR: 0.000031\n",
      "Step 357, Loss Train: 2.9805, Loss Test: 3.5744, LR: 0.000031\n",
      "Step 358, Loss Train: 2.9239, Loss Test: 2.7403, LR: 0.000031\n",
      "Step 359, Loss Train: 2.8742, Loss Test: 2.6209, LR: 0.000031\n",
      "Step 360, Loss Train: 3.0064, Loss Test: 3.5122, LR: 0.000031\n",
      "Step 361, Loss Train: 2.8993, Loss Test: 4.4797, LR: 0.000031\n",
      "Step 362, Loss Train: 2.9067, Loss Test: 3.8978, LR: 0.000031\n",
      "Step 363, Loss Train: 2.7417, Loss Test: 4.1337, LR: 0.000031\n",
      "Step 364, Loss Train: 2.9369, Loss Test: 4.2631, LR: 0.000031\n",
      "Step 365, Loss Train: 2.9274, Loss Test: 3.9409, LR: 0.000031\n",
      "Step 366, Loss Train: 3.1518, Loss Test: 3.8967, LR: 0.000031\n",
      "Step 367, Loss Train: 2.9941, Loss Test: 4.1142, LR: 0.000031\n",
      "Step 368, Loss Train: 3.1113, Loss Test: 4.2021, LR: 0.000031\n",
      "Step 369, Loss Train: 2.8361, Loss Test: 4.2399, LR: 0.000031\n",
      "Step 370, Loss Train: 2.9239, Loss Test: 4.0193, LR: 0.000031\n",
      "Step 371, Loss Train: 2.9215, Loss Test: 3.9462, LR: 0.000031\n",
      "Step 372, Loss Train: 2.8094, Loss Test: 3.6415, LR: 0.000031\n",
      "Step 373, Loss Train: 2.9312, Loss Test: 3.8079, LR: 0.000031\n",
      "Step 374, Loss Train: 2.9388, Loss Test: 3.5246, LR: 0.000031\n",
      "Step 375, Loss Train: 2.9273, Loss Test: 4.5172, LR: 0.000031\n",
      "Step 376, Loss Train: 3.0876, Loss Test: 3.7580, LR: 0.000031\n",
      "Step 377, Loss Train: 2.9713, Loss Test: 3.6322, LR: 0.000031\n",
      "Step 378, Loss Train: 2.9122, Loss Test: 3.4595, LR: 0.000031\n",
      "Step 379, Loss Train: 2.9196, Loss Test: 3.8578, LR: 0.000031\n",
      "Step 380, Loss Train: 3.0073, Loss Test: 4.4761, LR: 0.000031\n",
      "Step 381, Loss Train: 2.8603, Loss Test: 4.1730, LR: 0.000031\n",
      "Step 382, Loss Train: 3.0398, Loss Test: 4.0615, LR: 0.000031\n",
      "Step 383, Loss Train: 2.9791, Loss Test: 3.9612, LR: 0.000031\n",
      "Step 384, Loss Train: 2.9758, Loss Test: 3.4879, LR: 0.000031\n",
      "Step 385, Loss Train: 3.0255, Loss Test: 3.1318, LR: 0.000031\n",
      "Step 386, Loss Train: 2.9345, Loss Test: 3.9723, LR: 0.000031\n",
      "Step 387, Loss Train: 3.0499, Loss Test: 3.2720, LR: 0.000031\n",
      "Step 388, Loss Train: 2.8861, Loss Test: 3.9089, LR: 0.000031\n",
      "Step 389, Loss Train: 3.0947, Loss Test: 4.1892, LR: 0.000031\n",
      "Step 390, Loss Train: 3.1509, Loss Test: 3.9785, LR: 0.000031\n",
      "Step 391, Loss Train: 2.9489, Loss Test: 2.4619, LR: 0.000031\n",
      "Step 392, Loss Train: 3.0235, Loss Test: 2.1563, LR: 0.000031\n",
      "Step 393, Loss Train: 2.9558, Loss Test: 3.3486, LR: 0.000031\n",
      "Step 394, Loss Train: 2.9415, Loss Test: 2.6439, LR: 0.000031\n",
      "Step 395, Loss Train: 3.1202, Loss Test: 3.8896, LR: 0.000031\n",
      "Step 396, Loss Train: 2.9220, Loss Test: 2.9351, LR: 0.000031\n",
      "Step 397, Loss Train: 3.0272, Loss Test: 4.7534, LR: 0.000031\n",
      "Step 398, Loss Train: 2.9531, Loss Test: 4.4507, LR: 0.000031\n",
      "Step 399, Loss Train: 3.1245, Loss Test: 4.0160, LR: 0.000031\n",
      "Step 400, Loss Train: 2.9635, Loss Test: 3.1947, LR: 0.000031\n",
      "Step 401, Loss Train: 2.9372, Loss Test: 3.6083, LR: 0.000031\n",
      "Step 402, Loss Train: 3.1151, Loss Test: 3.5617, LR: 0.000031\n",
      "Step 403, Loss Train: 3.0629, Loss Test: 3.8685, LR: 0.000031\n",
      "Step 404, Loss Train: 3.0488, Loss Test: 3.7331, LR: 0.000031\n",
      "Step 405, Loss Train: 2.9341, Loss Test: 3.4423, LR: 0.000031\n",
      "Step 406, Loss Train: 2.8943, Loss Test: 4.1314, LR: 0.000031\n",
      "Step 407, Loss Train: 3.0986, Loss Test: 2.9536, LR: 0.000031\n",
      "Step 408, Loss Train: 2.7591, Loss Test: 3.3106, LR: 0.000031\n",
      "Step 409, Loss Train: 2.9887, Loss Test: 4.3465, LR: 0.000031\n",
      "Step 410, Loss Train: 2.9562, Loss Test: 2.8717, LR: 0.000031\n",
      "Step 411, Loss Train: 3.1028, Loss Test: 4.4805, LR: 0.000031\n",
      "Step 412, Loss Train: 3.0151, Loss Test: 4.2083, LR: 0.000031\n",
      "Step 413, Loss Train: 2.8736, Loss Test: 3.0471, LR: 0.000031\n",
      "Step 414, Loss Train: 2.9548, Loss Test: 2.1017, LR: 0.000031\n",
      "Step 415, Loss Train: 2.9132, Loss Test: 3.8828, LR: 0.000031\n",
      "Step 416, Loss Train: 3.0456, Loss Test: 4.0583, LR: 0.000031\n",
      "Step 417, Loss Train: 2.9723, Loss Test: 4.2640, LR: 0.000031\n",
      "Step 418, Loss Train: 2.8910, Loss Test: 3.0435, LR: 0.000031\n",
      "Step 419, Loss Train: 3.0355, Loss Test: 4.1045, LR: 0.000031\n",
      "Step 420, Loss Train: 2.9099, Loss Test: 3.9326, LR: 0.000031\n",
      "Step 421, Loss Train: 3.0615, Loss Test: 2.9111, LR: 0.000031\n",
      "Step 422, Loss Train: 3.2184, Loss Test: 3.4541, LR: 0.000031\n",
      "Step 423, Loss Train: 2.9751, Loss Test: 1.9169, LR: 0.000031\n",
      "Step 424, Loss Train: 2.9652, Loss Test: 3.9127, LR: 0.000031\n",
      "Step 425, Loss Train: 3.0018, Loss Test: 4.2315, LR: 0.000031\n",
      "Step 426, Loss Train: 2.8098, Loss Test: 3.6471, LR: 0.000031\n",
      "Step 427, Loss Train: 3.1576, Loss Test: 4.2686, LR: 0.000031\n",
      "Step 428, Loss Train: 3.0054, Loss Test: 2.2639, LR: 0.000031\n",
      "Step 429, Loss Train: 3.0093, Loss Test: 2.8015, LR: 0.000031\n",
      "Step 430, Loss Train: 2.8487, Loss Test: 2.4868, LR: 0.000031\n",
      "Step 431, Loss Train: 2.9750, Loss Test: 4.4317, LR: 0.000031\n",
      "Step 432, Loss Train: 2.9464, Loss Test: 3.9440, LR: 0.000031\n",
      "Step 433, Loss Train: 3.0183, Loss Test: 3.4824, LR: 0.000031\n",
      "Step 434, Loss Train: 2.6795, Loss Test: 3.5680, LR: 0.000031\n",
      "Step 435, Loss Train: 2.9528, Loss Test: 4.2584, LR: 0.000031\n",
      "Step 436, Loss Train: 2.8194, Loss Test: 4.0646, LR: 0.000031\n",
      "Step 437, Loss Train: 2.9486, Loss Test: 3.2450, LR: 0.000031\n",
      "Step 438, Loss Train: 2.9019, Loss Test: 3.2217, LR: 0.000031\n",
      "Step 439, Loss Train: 3.1319, Loss Test: 3.4773, LR: 0.000031\n",
      "Step 440, Loss Train: 3.0695, Loss Test: 3.5518, LR: 0.000031\n",
      "Step 441, Loss Train: 2.8640, Loss Test: 2.9264, LR: 0.000031\n",
      "Step 442, Loss Train: 3.1399, Loss Test: 4.1477, LR: 0.000031\n",
      "Step 443, Loss Train: 2.9446, Loss Test: 3.3767, LR: 0.000031\n",
      "Step 444, Loss Train: 2.8474, Loss Test: 4.1392, LR: 0.000031\n",
      "Step 445, Loss Train: 3.1042, Loss Test: 3.8177, LR: 0.000031\n",
      "Step 446, Loss Train: 2.9191, Loss Test: 3.8824, LR: 0.000031\n",
      "Step 447, Loss Train: 3.0436, Loss Test: 2.2523, LR: 0.000031\n",
      "Step 448, Loss Train: 2.9117, Loss Test: 4.0733, LR: 0.000031\n",
      "Step 449, Loss Train: 2.9909, Loss Test: 4.5441, LR: 0.000031\n",
      "Step 450, Loss Train: 2.8677, Loss Test: 2.8050, LR: 0.000031\n",
      "Step 451, Loss Train: 3.0485, Loss Test: 2.4786, LR: 0.000031\n",
      "Step 452, Loss Train: 2.9146, Loss Test: 3.9779, LR: 0.000031\n",
      "Step 453, Loss Train: 3.0426, Loss Test: 3.9926, LR: 0.000031\n",
      "Step 454, Loss Train: 2.7021, Loss Test: 3.6328, LR: 0.000031\n",
      "Step 455, Loss Train: 2.8898, Loss Test: 2.6453, LR: 0.000031\n",
      "Step 456, Loss Train: 2.9267, Loss Test: 4.0415, LR: 0.000031\n",
      "Step 457, Loss Train: 3.0982, Loss Test: 4.2270, LR: 0.000031\n",
      "Step 458, Loss Train: 2.9725, Loss Test: 4.4409, LR: 0.000031\n",
      "Step 459, Loss Train: 2.9537, Loss Test: 1.4547, LR: 0.000031\n",
      "Step 460, Loss Train: 3.0319, Loss Test: 2.4368, LR: 0.000031\n",
      "Step 461, Loss Train: 3.0351, Loss Test: 3.6449, LR: 0.000031\n",
      "Step 462, Loss Train: 3.0116, Loss Test: 4.2166, LR: 0.000031\n",
      "Step 463, Loss Train: 2.8942, Loss Test: 4.2211, LR: 0.000031\n",
      "Step 464, Loss Train: 3.1072, Loss Test: 4.2258, LR: 0.000031\n",
      "Step 465, Loss Train: 2.7881, Loss Test: 4.1261, LR: 0.000031\n",
      "Step 466, Loss Train: 3.0915, Loss Test: 3.8408, LR: 0.000031\n",
      "Step 467, Loss Train: 2.9452, Loss Test: 4.0626, LR: 0.000031\n",
      "Step 468, Loss Train: 2.9294, Loss Test: 4.0340, LR: 0.000031\n",
      "Step 469, Loss Train: 3.0967, Loss Test: 4.2079, LR: 0.000031\n",
      "Step 470, Loss Train: 3.0956, Loss Test: 4.5042, LR: 0.000031\n",
      "Step 471, Loss Train: 3.0792, Loss Test: 3.6021, LR: 0.000031\n",
      "Step 472, Loss Train: 2.9051, Loss Test: 3.7196, LR: 0.000031\n",
      "Step 473, Loss Train: 2.8638, Loss Test: 3.8420, LR: 0.000031\n",
      "Step 474, Loss Train: 2.8970, Loss Test: 4.0219, LR: 0.000031\n",
      "Step 475, Loss Train: 3.0523, Loss Test: 4.2541, LR: 0.000031\n",
      "Step 476, Loss Train: 2.9386, Loss Test: 4.2703, LR: 0.000031\n",
      "Step 477, Loss Train: 3.0673, Loss Test: 4.4554, LR: 0.000031\n",
      "Step 478, Loss Train: 3.1428, Loss Test: 3.3126, LR: 0.000031\n",
      "Step 479, Loss Train: 2.9949, Loss Test: 3.9135, LR: 0.000031\n",
      "Step 480, Loss Train: 3.0727, Loss Test: 2.1520, LR: 0.000031\n",
      "Step 481, Loss Train: 3.1305, Loss Test: 3.7436, LR: 0.000031\n",
      "Step 482, Loss Train: 2.9455, Loss Test: 3.7195, LR: 0.000031\n",
      "Step 483, Loss Train: 2.9214, Loss Test: 3.2490, LR: 0.000031\n",
      "Step 484, Loss Train: 3.0853, Loss Test: 3.5443, LR: 0.000031\n",
      "Step 485, Loss Train: 2.8814, Loss Test: 4.4448, LR: 0.000031\n",
      "Step 486, Loss Train: 2.9133, Loss Test: 3.8342, LR: 0.000031\n",
      "Step 487, Loss Train: 3.0176, Loss Test: 3.7940, LR: 0.000031\n",
      "Step 488, Loss Train: 2.9094, Loss Test: 3.2582, LR: 0.000031\n",
      "Step 489, Loss Train: 2.9401, Loss Test: 2.7786, LR: 0.000031\n",
      "Step 490, Loss Train: 3.0857, Loss Test: 4.0292, LR: 0.000031\n",
      "Step 491, Loss Train: 2.9101, Loss Test: 3.4662, LR: 0.000031\n",
      "Step 492, Loss Train: 3.0995, Loss Test: 3.9333, LR: 0.000031\n",
      "Step 493, Loss Train: 3.1268, Loss Test: 2.4595, LR: 0.000031\n",
      "Step 494, Loss Train: 2.9596, Loss Test: 3.3583, LR: 0.000031\n",
      "Step 495, Loss Train: 3.0706, Loss Test: 3.4773, LR: 0.000031\n",
      "Step 496, Loss Train: 3.1382, Loss Test: 4.1316, LR: 0.000031\n",
      "Step 497, Loss Train: 3.0821, Loss Test: 3.8233, LR: 0.000031\n",
      "Step 498, Loss Train: 2.9626, Loss Test: 3.9121, LR: 0.000031\n",
      "Step 499, Loss Train: 2.8689, Loss Test: 2.4313, LR: 0.000031\n",
      "Step 500, Loss Train: 3.0707, Loss Test: 4.1118, LR: 0.000031\n",
      "Step 501, Loss Train: 3.0184, Loss Test: 4.2169, LR: 0.000031\n",
      "Step 502, Loss Train: 3.0619, Loss Test: 4.0308, LR: 0.000031\n",
      "Step 503, Loss Train: 2.8591, Loss Test: 4.0240, LR: 0.000031\n",
      "Step 504, Loss Train: 3.1435, Loss Test: 2.8148, LR: 0.000031\n",
      "Step 505, Loss Train: 2.8326, Loss Test: 4.1100, LR: 0.000031\n",
      "Step 506, Loss Train: 3.0392, Loss Test: 4.0869, LR: 0.000031\n",
      "Step 507, Loss Train: 2.9726, Loss Test: 4.0539, LR: 0.000031\n",
      "Step 508, Loss Train: 2.8763, Loss Test: 3.6923, LR: 0.000031\n",
      "Step 509, Loss Train: 2.9625, Loss Test: 2.8301, LR: 0.000031\n",
      "Step 510, Loss Train: 2.9803, Loss Test: 2.8715, LR: 0.000031\n",
      "Step 511, Loss Train: 3.1058, Loss Test: 3.6692, LR: 0.000031\n",
      "Step 512, Loss Train: 3.0285, Loss Test: 4.2072, LR: 0.000031\n",
      "Step 513, Loss Train: 3.0896, Loss Test: 4.4362, LR: 0.000031\n",
      "Step 514, Loss Train: 3.2205, Loss Test: 2.9772, LR: 0.000031\n",
      "Step 515, Loss Train: 3.0389, Loss Test: 3.9773, LR: 0.000031\n",
      "Step 516, Loss Train: 3.1276, Loss Test: 4.1221, LR: 0.000031\n",
      "Step 517, Loss Train: 3.0592, Loss Test: 2.9103, LR: 0.000031\n",
      "Step 518, Loss Train: 2.8642, Loss Test: 4.2793, LR: 0.000031\n",
      "Step 519, Loss Train: 2.9847, Loss Test: 3.7415, LR: 0.000031\n",
      "Step 520, Loss Train: 3.0348, Loss Test: 3.5193, LR: 0.000031\n",
      "Step 521, Loss Train: 2.8789, Loss Test: 3.9460, LR: 0.000031\n",
      "Step 522, Loss Train: 2.9818, Loss Test: 3.5696, LR: 0.000031\n",
      "Step 523, Loss Train: 2.9650, Loss Test: 3.6352, LR: 0.000031\n",
      "Step 524, Loss Train: 3.0800, Loss Test: 2.7371, LR: 0.000031\n",
      "Step 525, Loss Train: 3.0888, Loss Test: 2.8539, LR: 0.000031\n",
      "Step 526, Loss Train: 3.0011, Loss Test: 3.5968, LR: 0.000031\n",
      "Step 527, Loss Train: 3.0147, Loss Test: 4.1099, LR: 0.000031\n",
      "Step 528, Loss Train: 2.9009, Loss Test: 3.8255, LR: 0.000031\n",
      "Step 529, Loss Train: 2.9466, Loss Test: 3.8206, LR: 0.000031\n",
      "Step 530, Loss Train: 2.8805, Loss Test: 2.6098, LR: 0.000031\n",
      "Step 531, Loss Train: 2.9209, Loss Test: 3.8563, LR: 0.000031\n",
      "Step 532, Loss Train: 2.8976, Loss Test: 3.9070, LR: 0.000031\n",
      "Step 533, Loss Train: 3.0886, Loss Test: 4.1678, LR: 0.000031\n",
      "Step 534, Loss Train: 2.9234, Loss Test: 3.7293, LR: 0.000031\n",
      "Step 535, Loss Train: 2.9163, Loss Test: 4.1193, LR: 0.000031\n",
      "Step 536, Loss Train: 2.9578, Loss Test: 4.0815, LR: 0.000031\n",
      "Step 537, Loss Train: 2.9844, Loss Test: 3.7928, LR: 0.000031\n",
      "Step 538, Loss Train: 2.9188, Loss Test: 1.6228, LR: 0.000031\n",
      "Step 539, Loss Train: 3.0300, Loss Test: 4.0983, LR: 0.000031\n",
      "Step 540, Loss Train: 3.0885, Loss Test: 4.4760, LR: 0.000031\n",
      "Step 541, Loss Train: 2.7666, Loss Test: 3.1358, LR: 0.000031\n",
      "Step 542, Loss Train: 3.2331, Loss Test: 4.1525, LR: 0.000031\n",
      "Step 543, Loss Train: 2.7984, Loss Test: 4.4985, LR: 0.000031\n",
      "Step 544, Loss Train: 2.8953, Loss Test: 3.5853, LR: 0.000031\n",
      "Step 545, Loss Train: 2.8701, Loss Test: 3.2869, LR: 0.000031\n",
      "Step 546, Loss Train: 3.1101, Loss Test: 3.3038, LR: 0.000031\n",
      "Step 547, Loss Train: 2.9879, Loss Test: 3.7780, LR: 0.000031\n",
      "Step 548, Loss Train: 2.9185, Loss Test: 3.6861, LR: 0.000031\n",
      "Step 549, Loss Train: 2.9506, Loss Test: 4.0588, LR: 0.000031\n",
      "Step 550, Loss Train: 3.0868, Loss Test: 4.3480, LR: 0.000031\n",
      "Step 551, Loss Train: 3.0989, Loss Test: 3.9716, LR: 0.000031\n",
      "Step 552, Loss Train: 3.0609, Loss Test: 2.5627, LR: 0.000031\n",
      "Step 553, Loss Train: 3.0713, Loss Test: 4.0269, LR: 0.000031\n",
      "Step 554, Loss Train: 2.7496, Loss Test: 4.6620, LR: 0.000031\n",
      "Step 555, Loss Train: 2.7714, Loss Test: 4.1296, LR: 0.000031\n",
      "Step 556, Loss Train: 2.9144, Loss Test: 3.8047, LR: 0.000031\n",
      "Step 557, Loss Train: 3.0412, Loss Test: 4.4313, LR: 0.000031\n",
      "Step 558, Loss Train: 2.9081, Loss Test: 4.0096, LR: 0.000031\n",
      "Step 559, Loss Train: 3.0073, Loss Test: 2.9708, LR: 0.000031\n",
      "Step 560, Loss Train: 2.9842, Loss Test: 3.9980, LR: 0.000031\n",
      "Step 561, Loss Train: 3.0124, Loss Test: 3.9504, LR: 0.000031\n",
      "Step 562, Loss Train: 2.8895, Loss Test: 4.1603, LR: 0.000031\n",
      "Step 563, Loss Train: 3.0323, Loss Test: 4.0461, LR: 0.000031\n",
      "Step 564, Loss Train: 2.9197, Loss Test: 3.6460, LR: 0.000031\n",
      "Step 565, Loss Train: 3.0577, Loss Test: 4.2708, LR: 0.000031\n",
      "Step 566, Loss Train: 2.9951, Loss Test: 3.5373, LR: 0.000031\n",
      "Step 567, Loss Train: 2.9193, Loss Test: 4.3538, LR: 0.000031\n",
      "Step 568, Loss Train: 3.0417, Loss Test: 3.6231, LR: 0.000031\n",
      "Step 569, Loss Train: 3.0221, Loss Test: 3.1496, LR: 0.000031\n",
      "Step 570, Loss Train: 3.1095, Loss Test: 4.3631, LR: 0.000031\n",
      "Step 571, Loss Train: 3.1782, Loss Test: 3.4478, LR: 0.000031\n",
      "Step 572, Loss Train: 2.9143, Loss Test: 3.6086, LR: 0.000031\n",
      "Step 573, Loss Train: 2.9160, Loss Test: 4.0217, LR: 0.000031\n",
      "Step 574, Loss Train: 2.8441, Loss Test: 3.7090, LR: 0.000031\n",
      "Step 575, Loss Train: 3.0092, Loss Test: 3.9247, LR: 0.000031\n",
      "Step 576, Loss Train: 3.0193, Loss Test: 3.9143, LR: 0.000031\n",
      "Step 577, Loss Train: 3.2457, Loss Test: 3.4406, LR: 0.000031\n",
      "Step 578, Loss Train: 2.9957, Loss Test: 4.2085, LR: 0.000031\n",
      "Step 579, Loss Train: 3.1030, Loss Test: 3.9451, LR: 0.000031\n",
      "Step 580, Loss Train: 2.9547, Loss Test: 3.8493, LR: 0.000031\n",
      "Step 581, Loss Train: 3.1346, Loss Test: 4.3410, LR: 0.000031\n",
      "Step 582, Loss Train: 2.9983, Loss Test: 3.9919, LR: 0.000031\n",
      "Step 583, Loss Train: 2.8922, Loss Test: 3.6235, LR: 0.000031\n",
      "Step 584, Loss Train: 3.0356, Loss Test: 3.7169, LR: 0.000031\n",
      "Step 585, Loss Train: 2.9256, Loss Test: 4.0724, LR: 0.000031\n",
      "Step 586, Loss Train: 2.9239, Loss Test: 3.5022, LR: 0.000031\n",
      "Step 587, Loss Train: 3.0006, Loss Test: 4.8503, LR: 0.000031\n",
      "Step 588, Loss Train: 2.9457, Loss Test: 3.7903, LR: 0.000031\n",
      "Step 589, Loss Train: 2.9876, Loss Test: 3.8711, LR: 0.000031\n",
      "Step 590, Loss Train: 2.9404, Loss Test: 4.2095, LR: 0.000031\n",
      "Step 591, Loss Train: 2.9406, Loss Test: 3.3479, LR: 0.000031\n",
      "Step 592, Loss Train: 3.0453, Loss Test: 3.9582, LR: 0.000031\n",
      "Step 593, Loss Train: 2.8756, Loss Test: 3.0567, LR: 0.000031\n",
      "Step 594, Loss Train: 2.9044, Loss Test: 2.2887, LR: 0.000031\n",
      "Step 595, Loss Train: 3.0421, Loss Test: 4.0393, LR: 0.000031\n",
      "Step 596, Loss Train: 3.1071, Loss Test: 4.3710, LR: 0.000031\n",
      "Step 597, Loss Train: 2.9332, Loss Test: 3.3810, LR: 0.000031\n",
      "Step 598, Loss Train: 3.0265, Loss Test: 4.2007, LR: 0.000031\n",
      "Step 599, Loss Train: 3.0048, Loss Test: 3.0793, LR: 0.000031\n",
      "Step 600, Loss Train: 2.9079, Loss Test: 2.7563, LR: 0.000031\n",
      "Step 601, Loss Train: 2.9803, Loss Test: 3.5804, LR: 0.000031\n",
      "Step 602, Loss Train: 2.8381, Loss Test: 4.6105, LR: 0.000031\n",
      "Step 603, Loss Train: 3.0680, Loss Test: 2.2083, LR: 0.000031\n",
      "Step 604, Loss Train: 2.8348, Loss Test: 4.0141, LR: 0.000031\n",
      "Step 605, Loss Train: 2.8974, Loss Test: 4.4960, LR: 0.000031\n",
      "Step 606, Loss Train: 3.1258, Loss Test: 2.8666, LR: 0.000031\n",
      "Step 607, Loss Train: 2.9542, Loss Test: 3.8409, LR: 0.000031\n",
      "Step 608, Loss Train: 2.9938, Loss Test: 4.1951, LR: 0.000031\n",
      "Step 609, Loss Train: 2.9825, Loss Test: 3.3880, LR: 0.000031\n",
      "Step 610, Loss Train: 2.9005, Loss Test: 4.1830, LR: 0.000031\n",
      "Step 611, Loss Train: 3.0072, Loss Test: 3.9601, LR: 0.000031\n",
      "Step 612, Loss Train: 2.9560, Loss Test: 4.0613, LR: 0.000031\n",
      "Step 613, Loss Train: 2.9493, Loss Test: 4.4457, LR: 0.000031\n",
      "Step 614, Loss Train: 2.9447, Loss Test: 3.4842, LR: 0.000031\n",
      "Step 615, Loss Train: 2.9534, Loss Test: 3.1615, LR: 0.000031\n",
      "Step 616, Loss Train: 2.7499, Loss Test: 4.3312, LR: 0.000031\n",
      "Step 617, Loss Train: 2.8871, Loss Test: 4.3439, LR: 0.000031\n",
      "Step 618, Loss Train: 2.9610, Loss Test: 3.9236, LR: 0.000031\n",
      "Step 619, Loss Train: 2.9261, Loss Test: 3.6680, LR: 0.000031\n",
      "Step 620, Loss Train: 2.9491, Loss Test: 3.6133, LR: 0.000031\n",
      "Step 621, Loss Train: 3.0571, Loss Test: 3.4983, LR: 0.000031\n",
      "Step 622, Loss Train: 2.9626, Loss Test: 3.6887, LR: 0.000031\n",
      "Step 623, Loss Train: 3.2496, Loss Test: 3.3023, LR: 0.000031\n",
      "Step 624, Loss Train: 2.8979, Loss Test: 3.8304, LR: 0.000031\n",
      "Step 625, Loss Train: 3.0691, Loss Test: 2.4825, LR: 0.000031\n",
      "Step 626, Loss Train: 3.0500, Loss Test: 3.9878, LR: 0.000031\n",
      "Step 627, Loss Train: 2.8918, Loss Test: 3.6100, LR: 0.000031\n",
      "Step 628, Loss Train: 3.0788, Loss Test: 4.0846, LR: 0.000031\n",
      "Step 629, Loss Train: 2.9618, Loss Test: 3.7836, LR: 0.000031\n",
      "Step 630, Loss Train: 3.0022, Loss Test: 4.2112, LR: 0.000031\n",
      "Step 631, Loss Train: 2.8925, Loss Test: 3.4082, LR: 0.000031\n",
      "Step 632, Loss Train: 3.0400, Loss Test: 2.6177, LR: 0.000031\n",
      "Step 633, Loss Train: 2.9682, Loss Test: 4.2330, LR: 0.000031\n",
      "Step 634, Loss Train: 2.9503, Loss Test: 3.3892, LR: 0.000031\n",
      "Step 635, Loss Train: 3.1979, Loss Test: 4.0834, LR: 0.000031\n",
      "Step 636, Loss Train: 2.8703, Loss Test: 2.9903, LR: 0.000031\n",
      "Step 637, Loss Train: 3.1363, Loss Test: 4.6754, LR: 0.000031\n",
      "Step 638, Loss Train: 2.9113, Loss Test: 4.1348, LR: 0.000031\n",
      "Step 639, Loss Train: 2.9985, Loss Test: 4.5371, LR: 0.000031\n",
      "Step 640, Loss Train: 3.1041, Loss Test: 3.3770, LR: 0.000031\n",
      "Step 641, Loss Train: 3.1246, Loss Test: 3.1651, LR: 0.000031\n",
      "Step 642, Loss Train: 2.8675, Loss Test: 3.8487, LR: 0.000031\n",
      "Step 643, Loss Train: 2.9744, Loss Test: 3.9670, LR: 0.000031\n",
      "Step 644, Loss Train: 3.0996, Loss Test: 3.8717, LR: 0.000031\n",
      "Step 645, Loss Train: 2.9214, Loss Test: 3.2835, LR: 0.000031\n",
      "Step 646, Loss Train: 2.8221, Loss Test: 4.0115, LR: 0.000031\n",
      "Step 647, Loss Train: 2.9660, Loss Test: 3.9786, LR: 0.000031\n",
      "Step 648, Loss Train: 2.8551, Loss Test: 2.4860, LR: 0.000031\n",
      "Step 649, Loss Train: 3.1184, Loss Test: 3.6802, LR: 0.000031\n",
      "Step 650, Loss Train: 2.8366, Loss Test: 3.8465, LR: 0.000031\n",
      "Step 651, Loss Train: 2.9257, Loss Test: 4.0948, LR: 0.000031\n",
      "Step 652, Loss Train: 3.1611, Loss Test: 3.6321, LR: 0.000031\n",
      "Step 653, Loss Train: 2.9267, Loss Test: 2.8883, LR: 0.000031\n",
      "Step 654, Loss Train: 2.9768, Loss Test: 3.3910, LR: 0.000031\n",
      "Step 655, Loss Train: 3.0426, Loss Test: 4.4259, LR: 0.000031\n",
      "Step 656, Loss Train: 2.9194, Loss Test: 3.6431, LR: 0.000031\n",
      "Step 657, Loss Train: 2.9849, Loss Test: 4.0949, LR: 0.000031\n",
      "Step 658, Loss Train: 3.1076, Loss Test: 4.2223, LR: 0.000031\n",
      "Step 659, Loss Train: 3.1265, Loss Test: 4.3519, LR: 0.000031\n",
      "Step 660, Loss Train: 3.0019, Loss Test: 3.8474, LR: 0.000031\n",
      "Step 661, Loss Train: 2.9940, Loss Test: 4.2202, LR: 0.000031\n",
      "Step 662, Loss Train: 3.2472, Loss Test: 3.9144, LR: 0.000031\n",
      "Step 663, Loss Train: 2.9669, Loss Test: 3.2994, LR: 0.000031\n",
      "Step 664, Loss Train: 3.1206, Loss Test: 4.0880, LR: 0.000031\n",
      "Step 665, Loss Train: 2.7981, Loss Test: 3.5835, LR: 0.000031\n",
      "Step 666, Loss Train: 3.0323, Loss Test: 3.0132, LR: 0.000031\n",
      "Step 667, Loss Train: 3.0034, Loss Test: 3.9850, LR: 0.000031\n",
      "Step 668, Loss Train: 2.8680, Loss Test: 2.8212, LR: 0.000031\n",
      "Step 669, Loss Train: 2.8067, Loss Test: 4.2524, LR: 0.000031\n",
      "Step 670, Loss Train: 2.9154, Loss Test: 3.1816, LR: 0.000031\n",
      "Step 671, Loss Train: 2.9997, Loss Test: 3.8711, LR: 0.000031\n",
      "Step 672, Loss Train: 2.9527, Loss Test: 4.2890, LR: 0.000031\n",
      "Step 673, Loss Train: 2.9597, Loss Test: 3.8243, LR: 0.000031\n",
      "Step 674, Loss Train: 2.7491, Loss Test: 3.5472, LR: 0.000031\n",
      "Step 675, Loss Train: 2.8706, Loss Test: 4.2015, LR: 0.000031\n",
      "Step 676, Loss Train: 2.9724, Loss Test: 4.2973, LR: 0.000031\n",
      "Step 677, Loss Train: 2.8948, Loss Test: 4.3748, LR: 0.000031\n",
      "Step 678, Loss Train: 2.7572, Loss Test: 3.4061, LR: 0.000031\n",
      "Step 679, Loss Train: 2.8045, Loss Test: 3.2315, LR: 0.000031\n",
      "Step 680, Loss Train: 3.0280, Loss Test: 3.2486, LR: 0.000031\n",
      "Step 681, Loss Train: 3.1359, Loss Test: 3.8605, LR: 0.000031\n",
      "Step 682, Loss Train: 3.0658, Loss Test: 3.4556, LR: 0.000031\n",
      "Step 683, Loss Train: 3.0425, Loss Test: 4.2868, LR: 0.000031\n",
      "Step 684, Loss Train: 3.0493, Loss Test: 4.1701, LR: 0.000031\n",
      "Step 685, Loss Train: 3.0969, Loss Test: 4.5377, LR: 0.000031\n",
      "Step 686, Loss Train: 2.8662, Loss Test: 4.4090, LR: 0.000031\n",
      "Step 687, Loss Train: 3.0322, Loss Test: 3.9027, LR: 0.000031\n",
      "Step 688, Loss Train: 2.9738, Loss Test: 4.0326, LR: 0.000031\n",
      "Step 689, Loss Train: 2.9853, Loss Test: 3.3564, LR: 0.000031\n",
      "Step 690, Loss Train: 2.9311, Loss Test: 3.6358, LR: 0.000031\n",
      "Step 691, Loss Train: 3.0724, Loss Test: 4.2565, LR: 0.000031\n",
      "Step 692, Loss Train: 2.8430, Loss Test: 4.0311, LR: 0.000031\n",
      "Step 693, Loss Train: 2.8199, Loss Test: 4.1439, LR: 0.000031\n",
      "Step 694, Loss Train: 2.9487, Loss Test: 3.8761, LR: 0.000031\n",
      "Step 695, Loss Train: 3.0794, Loss Test: 4.0055, LR: 0.000031\n",
      "Step 696, Loss Train: 3.0121, Loss Test: 4.0593, LR: 0.000031\n",
      "Step 697, Loss Train: 2.9666, Loss Test: 4.0778, LR: 0.000031\n",
      "Step 698, Loss Train: 2.9136, Loss Test: 3.7779, LR: 0.000031\n",
      "Step 699, Loss Train: 3.1416, Loss Test: 2.5663, LR: 0.000031\n",
      "Step 700, Loss Train: 2.9360, Loss Test: 1.1830, LR: 0.000031\n",
      "Step 701, Loss Train: 2.9801, Loss Test: 3.3112, LR: 0.000031\n",
      "Step 702, Loss Train: 3.0010, Loss Test: 3.3797, LR: 0.000031\n",
      "Step 703, Loss Train: 3.0173, Loss Test: 4.2041, LR: 0.000031\n",
      "Step 704, Loss Train: 2.9286, Loss Test: 4.0337, LR: 0.000031\n",
      "Step 705, Loss Train: 3.0969, Loss Test: 3.5853, LR: 0.000031\n",
      "Step 706, Loss Train: 2.8695, Loss Test: 1.7419, LR: 0.000031\n",
      "Step 707, Loss Train: 2.8993, Loss Test: 3.7461, LR: 0.000031\n",
      "Step 708, Loss Train: 2.8939, Loss Test: 3.5071, LR: 0.000031\n",
      "Step 709, Loss Train: 2.9217, Loss Test: 3.6607, LR: 0.000031\n",
      "Step 710, Loss Train: 3.0711, Loss Test: 4.4698, LR: 0.000031\n",
      "Step 711, Loss Train: 2.9929, Loss Test: 2.9846, LR: 0.000031\n",
      "Step 712, Loss Train: 2.7997, Loss Test: 3.7186, LR: 0.000031\n",
      "Step 713, Loss Train: 3.0542, Loss Test: 4.1579, LR: 0.000031\n",
      "Step 714, Loss Train: 2.8561, Loss Test: 4.1100, LR: 0.000031\n",
      "Step 715, Loss Train: 2.8773, Loss Test: 3.4391, LR: 0.000031\n",
      "Step 716, Loss Train: 3.1871, Loss Test: 3.7952, LR: 0.000031\n",
      "Step 717, Loss Train: 3.0883, Loss Test: 3.4913, LR: 0.000031\n",
      "Step 718, Loss Train: 3.0054, Loss Test: 3.9591, LR: 0.000031\n",
      "Step 719, Loss Train: 2.7784, Loss Test: 3.9716, LR: 0.000031\n",
      "Step 720, Loss Train: 2.8750, Loss Test: 3.9492, LR: 0.000031\n",
      "Step 721, Loss Train: 3.1944, Loss Test: 3.5068, LR: 0.000031\n",
      "Step 722, Loss Train: 3.1387, Loss Test: 3.9085, LR: 0.000031\n",
      "Step 723, Loss Train: 2.9960, Loss Test: 3.4894, LR: 0.000031\n",
      "Step 724, Loss Train: 3.0310, Loss Test: 2.6664, LR: 0.000031\n",
      "Step 725, Loss Train: 2.9396, Loss Test: 4.0946, LR: 0.000031\n",
      "Step 726, Loss Train: 2.8560, Loss Test: 3.6343, LR: 0.000031\n",
      "Step 727, Loss Train: 3.1330, Loss Test: 4.4561, LR: 0.000031\n",
      "Step 728, Loss Train: 3.0751, Loss Test: 4.0018, LR: 0.000031\n",
      "Step 729, Loss Train: 2.8651, Loss Test: 3.3185, LR: 0.000031\n",
      "Step 730, Loss Train: 3.0694, Loss Test: 3.6860, LR: 0.000031\n",
      "Step 731, Loss Train: 2.9805, Loss Test: 3.1575, LR: 0.000031\n",
      "Step 732, Loss Train: 3.2547, Loss Test: 3.4603, LR: 0.000031\n",
      "Step 733, Loss Train: 3.1748, Loss Test: 4.5523, LR: 0.000031\n",
      "Step 734, Loss Train: 3.0153, Loss Test: 2.8608, LR: 0.000031\n",
      "Step 735, Loss Train: 3.0171, Loss Test: 3.6023, LR: 0.000031\n",
      "Step 736, Loss Train: 3.0421, Loss Test: 4.0672, LR: 0.000031\n",
      "Step 737, Loss Train: 2.9213, Loss Test: 3.9635, LR: 0.000031\n",
      "Step 738, Loss Train: 2.8950, Loss Test: 2.4532, LR: 0.000031\n",
      "Step 739, Loss Train: 3.0631, Loss Test: 4.0123, LR: 0.000031\n",
      "Step 740, Loss Train: 3.1821, Loss Test: 3.7834, LR: 0.000031\n",
      "Step 741, Loss Train: 2.8391, Loss Test: 4.0145, LR: 0.000031\n",
      "Step 742, Loss Train: 3.0155, Loss Test: 4.0814, LR: 0.000031\n",
      "Step 743, Loss Train: 3.0262, Loss Test: 3.6408, LR: 0.000031\n",
      "Step 744, Loss Train: 2.8074, Loss Test: 2.3007, LR: 0.000031\n",
      "Step 745, Loss Train: 2.7284, Loss Test: 3.7676, LR: 0.000031\n",
      "Step 746, Loss Train: 2.9118, Loss Test: 3.9798, LR: 0.000031\n",
      "Step 747, Loss Train: 2.9731, Loss Test: 3.9491, LR: 0.000031\n",
      "Step 748, Loss Train: 2.9590, Loss Test: 4.0753, LR: 0.000031\n",
      "Step 749, Loss Train: 2.8126, Loss Test: 1.9966, LR: 0.000031\n",
      "Step 750, Loss Train: 2.9598, Loss Test: 4.1442, LR: 0.000031\n",
      "Step 751, Loss Train: 3.0607, Loss Test: 3.8187, LR: 0.000031\n",
      "Step 752, Loss Train: 3.1160, Loss Test: 4.1301, LR: 0.000031\n",
      "Step 753, Loss Train: 3.1033, Loss Test: 3.0807, LR: 0.000031\n",
      "Step 754, Loss Train: 3.0061, Loss Test: 3.4391, LR: 0.000031\n",
      "Step 755, Loss Train: 2.8980, Loss Test: 3.8411, LR: 0.000031\n",
      "Step 756, Loss Train: 2.7962, Loss Test: 4.3748, LR: 0.000031\n",
      "Step 757, Loss Train: 2.9271, Loss Test: 4.1591, LR: 0.000031\n",
      "Step 758, Loss Train: 3.1286, Loss Test: 4.0959, LR: 0.000031\n",
      "Step 759, Loss Train: 2.9397, Loss Test: 3.9276, LR: 0.000031\n",
      "Step 760, Loss Train: 3.0085, Loss Test: 3.8091, LR: 0.000031\n",
      "Step 761, Loss Train: 2.9247, Loss Test: 4.3095, LR: 0.000031\n",
      "Step 762, Loss Train: 3.1259, Loss Test: 4.0552, LR: 0.000031\n",
      "Step 763, Loss Train: 2.9573, Loss Test: 3.8021, LR: 0.000031\n",
      "Step 764, Loss Train: 3.0828, Loss Test: 3.7883, LR: 0.000031\n",
      "Step 765, Loss Train: 3.0767, Loss Test: 3.4613, LR: 0.000031\n",
      "Step 766, Loss Train: 2.8653, Loss Test: 2.8178, LR: 0.000031\n",
      "Step 767, Loss Train: 2.9831, Loss Test: 3.5282, LR: 0.000031\n",
      "Step 768, Loss Train: 3.0200, Loss Test: 2.0594, LR: 0.000031\n",
      "Step 769, Loss Train: 3.1064, Loss Test: 3.6334, LR: 0.000031\n",
      "Step 770, Loss Train: 3.0890, Loss Test: 3.3685, LR: 0.000031\n",
      "Step 771, Loss Train: 2.7876, Loss Test: 3.5423, LR: 0.000031\n",
      "Step 772, Loss Train: 2.9810, Loss Test: 3.5930, LR: 0.000031\n",
      "Step 773, Loss Train: 2.8138, Loss Test: 2.9324, LR: 0.000031\n",
      "Step 774, Loss Train: 2.8549, Loss Test: 3.6406, LR: 0.000031\n",
      "Step 775, Loss Train: 2.9452, Loss Test: 4.3655, LR: 0.000031\n",
      "Step 776, Loss Train: 3.0276, Loss Test: 4.3885, LR: 0.000031\n",
      "Step 777, Loss Train: 3.0405, Loss Test: 3.8358, LR: 0.000031\n",
      "Step 778, Loss Train: 2.9216, Loss Test: 4.1707, LR: 0.000031\n",
      "Step 779, Loss Train: 3.0640, Loss Test: 4.4238, LR: 0.000031\n",
      "Step 780, Loss Train: 2.9661, Loss Test: 3.9390, LR: 0.000031\n",
      "Step 781, Loss Train: 3.0199, Loss Test: 3.5744, LR: 0.000031\n",
      "Step 782, Loss Train: 2.8774, Loss Test: 3.5677, LR: 0.000031\n",
      "Step 783, Loss Train: 2.8931, Loss Test: 3.7302, LR: 0.000031\n",
      "Step 784, Loss Train: 2.9936, Loss Test: 3.8161, LR: 0.000031\n",
      "Step 785, Loss Train: 2.8971, Loss Test: 3.5809, LR: 0.000031\n",
      "Step 786, Loss Train: 3.0155, Loss Test: 3.8423, LR: 0.000031\n",
      "Step 787, Loss Train: 2.9788, Loss Test: 3.2742, LR: 0.000031\n",
      "Step 788, Loss Train: 2.8613, Loss Test: 4.2203, LR: 0.000031\n",
      "Step 789, Loss Train: 3.1285, Loss Test: 3.9888, LR: 0.000031\n",
      "Step 790, Loss Train: 2.7947, Loss Test: 3.5116, LR: 0.000031\n",
      "Step 791, Loss Train: 3.0362, Loss Test: 3.9140, LR: 0.000031\n",
      "Step 792, Loss Train: 3.0042, Loss Test: 4.4075, LR: 0.000031\n",
      "Step 793, Loss Train: 2.9614, Loss Test: 4.2403, LR: 0.000031\n",
      "Step 794, Loss Train: 3.0184, Loss Test: 3.7791, LR: 0.000031\n",
      "Step 795, Loss Train: 2.9306, Loss Test: 2.8489, LR: 0.000031\n",
      "Step 796, Loss Train: 3.0553, Loss Test: 3.8770, LR: 0.000031\n",
      "Step 797, Loss Train: 3.0428, Loss Test: 3.9419, LR: 0.000031\n",
      "Step 798, Loss Train: 2.8883, Loss Test: 3.2981, LR: 0.000031\n",
      "Step 799, Loss Train: 2.9359, Loss Test: 2.9921, LR: 0.000031\n",
      "Step 800, Loss Train: 2.8844, Loss Test: 3.6829, LR: 0.000031\n",
      "Step 801, Loss Train: 3.0818, Loss Test: 4.2351, LR: 0.000031\n",
      "Step 802, Loss Train: 3.0385, Loss Test: 2.7495, LR: 0.000031\n",
      "Step 803, Loss Train: 2.8394, Loss Test: 4.7277, LR: 0.000031\n",
      "Step 804, Loss Train: 3.0702, Loss Test: 4.0450, LR: 0.000031\n",
      "Step 805, Loss Train: 2.7651, Loss Test: 3.3449, LR: 0.000031\n",
      "Step 806, Loss Train: 2.9139, Loss Test: 3.9969, LR: 0.000031\n",
      "Step 807, Loss Train: 2.9409, Loss Test: 4.4324, LR: 0.000031\n",
      "Step 808, Loss Train: 3.0225, Loss Test: 2.4540, LR: 0.000031\n",
      "Step 809, Loss Train: 2.9306, Loss Test: 4.1565, LR: 0.000031\n",
      "Step 810, Loss Train: 2.8880, Loss Test: 2.1477, LR: 0.000031\n",
      "Step 811, Loss Train: 2.9456, Loss Test: 3.4825, LR: 0.000031\n",
      "Step 812, Loss Train: 2.9336, Loss Test: 3.5555, LR: 0.000031\n",
      "Step 813, Loss Train: 3.1150, Loss Test: 3.8008, LR: 0.000031\n",
      "Step 814, Loss Train: 3.0579, Loss Test: 4.1257, LR: 0.000031\n",
      "Step 815, Loss Train: 2.9022, Loss Test: 4.5163, LR: 0.000031\n",
      "Step 816, Loss Train: 3.0847, Loss Test: 3.9678, LR: 0.000031\n",
      "Step 817, Loss Train: 2.8101, Loss Test: 3.0492, LR: 0.000031\n",
      "Step 818, Loss Train: 2.7686, Loss Test: 4.0445, LR: 0.000031\n",
      "Step 819, Loss Train: 2.9156, Loss Test: 4.1263, LR: 0.000031\n",
      "Step 820, Loss Train: 2.9122, Loss Test: 3.8235, LR: 0.000031\n",
      "Step 821, Loss Train: 2.9060, Loss Test: 3.6039, LR: 0.000031\n",
      "Step 822, Loss Train: 3.1062, Loss Test: 4.1585, LR: 0.000031\n",
      "Step 823, Loss Train: 3.1080, Loss Test: 3.1747, LR: 0.000031\n",
      "Step 824, Loss Train: 2.9848, Loss Test: 3.6867, LR: 0.000031\n",
      "Step 825, Loss Train: 2.8901, Loss Test: 3.5516, LR: 0.000031\n",
      "Step 826, Loss Train: 2.9588, Loss Test: 3.9486, LR: 0.000031\n",
      "Step 827, Loss Train: 2.8125, Loss Test: 3.4764, LR: 0.000031\n",
      "Step 828, Loss Train: 2.9615, Loss Test: 3.9367, LR: 0.000031\n",
      "Step 829, Loss Train: 2.8884, Loss Test: 4.1785, LR: 0.000031\n",
      "Step 830, Loss Train: 3.0751, Loss Test: 3.3829, LR: 0.000031\n",
      "Step 831, Loss Train: 2.8987, Loss Test: 3.2836, LR: 0.000031\n",
      "Step 832, Loss Train: 3.0171, Loss Test: 3.0870, LR: 0.000031\n",
      "Step 833, Loss Train: 2.9379, Loss Test: 3.4794, LR: 0.000031\n",
      "Step 834, Loss Train: 3.0859, Loss Test: 4.3110, LR: 0.000031\n",
      "Step 835, Loss Train: 2.9599, Loss Test: 4.0282, LR: 0.000031\n",
      "Step 836, Loss Train: 2.9727, Loss Test: 3.0651, LR: 0.000031\n",
      "Step 837, Loss Train: 2.8899, Loss Test: 3.0525, LR: 0.000031\n",
      "Step 838, Loss Train: 2.8986, Loss Test: 4.2524, LR: 0.000031\n",
      "Step 839, Loss Train: 3.0260, Loss Test: 4.0517, LR: 0.000031\n",
      "Step 840, Loss Train: 2.9145, Loss Test: 3.7280, LR: 0.000031\n",
      "Step 841, Loss Train: 2.8830, Loss Test: 3.4863, LR: 0.000031\n",
      "Step 842, Loss Train: 2.8948, Loss Test: 2.7150, LR: 0.000031\n",
      "Step 843, Loss Train: 2.9745, Loss Test: 4.1803, LR: 0.000031\n",
      "Step 844, Loss Train: 2.9725, Loss Test: 4.0470, LR: 0.000031\n",
      "Step 845, Loss Train: 2.7912, Loss Test: 3.7447, LR: 0.000030\n",
      "Step 846, Loss Train: 2.8156, Loss Test: 4.1490, LR: 0.000030\n",
      "Step 847, Loss Train: 3.1267, Loss Test: 4.0905, LR: 0.000030\n",
      "Step 848, Loss Train: 2.9536, Loss Test: 4.6292, LR: 0.000030\n",
      "Step 849, Loss Train: 3.0377, Loss Test: 3.8563, LR: 0.000030\n",
      "Step 850, Loss Train: 3.0549, Loss Test: 4.1666, LR: 0.000030\n",
      "Step 851, Loss Train: 3.0339, Loss Test: 3.5830, LR: 0.000030\n",
      "Step 852, Loss Train: 2.9264, Loss Test: 3.1481, LR: 0.000030\n",
      "Step 853, Loss Train: 3.0923, Loss Test: 3.1921, LR: 0.000030\n",
      "Step 854, Loss Train: 3.0912, Loss Test: 4.3186, LR: 0.000030\n",
      "Step 855, Loss Train: 3.1599, Loss Test: 4.2971, LR: 0.000030\n",
      "Step 856, Loss Train: 3.0429, Loss Test: 3.2364, LR: 0.000030\n",
      "Step 857, Loss Train: 2.9729, Loss Test: 4.2967, LR: 0.000030\n",
      "Step 858, Loss Train: 2.9303, Loss Test: 4.4180, LR: 0.000030\n",
      "Step 859, Loss Train: 3.0282, Loss Test: 4.3313, LR: 0.000030\n",
      "Step 860, Loss Train: 3.1824, Loss Test: 3.7370, LR: 0.000030\n",
      "Step 861, Loss Train: 3.0642, Loss Test: 4.0158, LR: 0.000030\n",
      "Step 862, Loss Train: 2.9236, Loss Test: 4.0713, LR: 0.000030\n",
      "Step 863, Loss Train: 2.7534, Loss Test: 3.8673, LR: 0.000030\n",
      "Step 864, Loss Train: 2.9175, Loss Test: 4.3070, LR: 0.000030\n",
      "Step 865, Loss Train: 2.9381, Loss Test: 3.2850, LR: 0.000030\n",
      "Step 866, Loss Train: 2.8898, Loss Test: 3.4105, LR: 0.000030\n",
      "Step 867, Loss Train: 2.9448, Loss Test: 3.6379, LR: 0.000030\n",
      "Step 868, Loss Train: 2.9088, Loss Test: 3.6658, LR: 0.000030\n",
      "Step 869, Loss Train: 2.9570, Loss Test: 4.0466, LR: 0.000030\n",
      "Step 870, Loss Train: 2.9415, Loss Test: 3.3955, LR: 0.000030\n",
      "Step 871, Loss Train: 2.7248, Loss Test: 3.7220, LR: 0.000030\n",
      "Step 872, Loss Train: 3.0786, Loss Test: 3.7302, LR: 0.000030\n",
      "Step 873, Loss Train: 2.9510, Loss Test: 3.9752, LR: 0.000030\n",
      "Step 874, Loss Train: 2.9675, Loss Test: 3.4514, LR: 0.000030\n",
      "Step 875, Loss Train: 2.9735, Loss Test: 3.9861, LR: 0.000030\n",
      "Step 876, Loss Train: 3.0771, Loss Test: 4.3701, LR: 0.000030\n",
      "Step 877, Loss Train: 2.7881, Loss Test: 4.3976, LR: 0.000030\n",
      "Step 878, Loss Train: 3.0096, Loss Test: 3.4044, LR: 0.000030\n",
      "Step 879, Loss Train: 2.9956, Loss Test: 3.3923, LR: 0.000030\n",
      "Step 880, Loss Train: 2.9348, Loss Test: 4.3772, LR: 0.000030\n",
      "Step 881, Loss Train: 2.9764, Loss Test: 3.4440, LR: 0.000030\n",
      "Step 882, Loss Train: 3.0395, Loss Test: 3.6596, LR: 0.000030\n",
      "Step 883, Loss Train: 3.1596, Loss Test: 4.0461, LR: 0.000030\n",
      "Step 884, Loss Train: 3.0230, Loss Test: 3.9791, LR: 0.000030\n",
      "Step 885, Loss Train: 2.7388, Loss Test: 4.1985, LR: 0.000030\n",
      "Step 886, Loss Train: 2.7732, Loss Test: 3.6115, LR: 0.000030\n",
      "Step 887, Loss Train: 3.0023, Loss Test: 4.0223, LR: 0.000030\n",
      "Step 888, Loss Train: 2.9876, Loss Test: 2.7943, LR: 0.000030\n",
      "Step 889, Loss Train: 2.9680, Loss Test: 4.3253, LR: 0.000030\n",
      "Step 890, Loss Train: 3.0736, Loss Test: 3.9441, LR: 0.000030\n",
      "Step 891, Loss Train: 2.8968, Loss Test: 3.6870, LR: 0.000030\n",
      "Step 892, Loss Train: 2.8841, Loss Test: 3.9549, LR: 0.000030\n",
      "Step 893, Loss Train: 3.0851, Loss Test: 3.8814, LR: 0.000030\n",
      "Step 894, Loss Train: 2.8414, Loss Test: 3.1372, LR: 0.000030\n",
      "Step 895, Loss Train: 2.9304, Loss Test: 4.2647, LR: 0.000030\n",
      "Step 896, Loss Train: 2.9787, Loss Test: 3.7138, LR: 0.000030\n",
      "Step 897, Loss Train: 2.8969, Loss Test: 3.9275, LR: 0.000030\n",
      "Step 898, Loss Train: 2.9847, Loss Test: 4.1319, LR: 0.000030\n",
      "Step 899, Loss Train: 2.8863, Loss Test: 4.5266, LR: 0.000030\n",
      "Step 900, Loss Train: 3.1184, Loss Test: 4.3514, LR: 0.000030\n",
      "Step 901, Loss Train: 3.1131, Loss Test: 3.8726, LR: 0.000030\n",
      "Step 902, Loss Train: 3.0133, Loss Test: 3.9683, LR: 0.000030\n",
      "Step 903, Loss Train: 3.0329, Loss Test: 3.2731, LR: 0.000030\n",
      "Step 904, Loss Train: 3.0302, Loss Test: 2.4681, LR: 0.000030\n",
      "Step 905, Loss Train: 2.7327, Loss Test: 3.9704, LR: 0.000030\n",
      "Step 906, Loss Train: 2.8538, Loss Test: 3.6375, LR: 0.000030\n",
      "Step 907, Loss Train: 2.8947, Loss Test: 3.9023, LR: 0.000030\n",
      "Step 908, Loss Train: 2.9474, Loss Test: 3.7494, LR: 0.000030\n",
      "Step 909, Loss Train: 3.0290, Loss Test: 4.0789, LR: 0.000030\n",
      "Step 910, Loss Train: 2.9103, Loss Test: 2.7086, LR: 0.000030\n",
      "Step 911, Loss Train: 2.9907, Loss Test: 3.4786, LR: 0.000030\n",
      "Step 912, Loss Train: 3.0545, Loss Test: 4.1345, LR: 0.000030\n",
      "Step 913, Loss Train: 3.0259, Loss Test: 4.0146, LR: 0.000030\n",
      "Step 914, Loss Train: 3.1219, Loss Test: 3.8335, LR: 0.000030\n",
      "Step 915, Loss Train: 3.0599, Loss Test: 4.6000, LR: 0.000030\n",
      "Step 916, Loss Train: 2.8888, Loss Test: 3.1340, LR: 0.000030\n",
      "Step 917, Loss Train: 2.7674, Loss Test: 3.6255, LR: 0.000030\n",
      "Step 918, Loss Train: 2.9204, Loss Test: 4.3642, LR: 0.000030\n",
      "Step 919, Loss Train: 3.1413, Loss Test: 2.7204, LR: 0.000030\n",
      "Step 920, Loss Train: 3.0198, Loss Test: 3.4977, LR: 0.000030\n",
      "Step 921, Loss Train: 2.9991, Loss Test: 2.6616, LR: 0.000030\n",
      "Step 922, Loss Train: 2.9726, Loss Test: 2.6747, LR: 0.000030\n",
      "Step 923, Loss Train: 2.9472, Loss Test: 3.1773, LR: 0.000030\n",
      "Step 924, Loss Train: 2.8864, Loss Test: 3.9152, LR: 0.000030\n",
      "Step 925, Loss Train: 2.8591, Loss Test: 4.2057, LR: 0.000030\n",
      "Step 926, Loss Train: 3.0357, Loss Test: 4.0261, LR: 0.000030\n",
      "Step 927, Loss Train: 2.8669, Loss Test: 4.0927, LR: 0.000030\n",
      "Step 928, Loss Train: 3.1323, Loss Test: 3.4678, LR: 0.000030\n",
      "Step 929, Loss Train: 2.8539, Loss Test: 3.7234, LR: 0.000030\n",
      "Step 930, Loss Train: 2.9951, Loss Test: 3.7684, LR: 0.000030\n",
      "Step 931, Loss Train: 2.8102, Loss Test: 3.6431, LR: 0.000030\n",
      "Step 932, Loss Train: 3.1207, Loss Test: 3.6552, LR: 0.000030\n",
      "Step 933, Loss Train: 2.9630, Loss Test: 4.0906, LR: 0.000030\n",
      "Step 934, Loss Train: 2.9640, Loss Test: 3.5097, LR: 0.000030\n",
      "Step 935, Loss Train: 2.8602, Loss Test: 3.1991, LR: 0.000030\n",
      "Step 936, Loss Train: 2.9490, Loss Test: 4.0025, LR: 0.000030\n",
      "Step 937, Loss Train: 2.9276, Loss Test: 1.2148, LR: 0.000030\n",
      "Step 938, Loss Train: 2.9305, Loss Test: 3.6649, LR: 0.000030\n",
      "Step 939, Loss Train: 3.1677, Loss Test: 2.3809, LR: 0.000030\n",
      "Step 940, Loss Train: 2.9328, Loss Test: 4.4036, LR: 0.000030\n",
      "Step 941, Loss Train: 2.9537, Loss Test: 4.0585, LR: 0.000030\n",
      "Step 942, Loss Train: 2.9169, Loss Test: 3.8283, LR: 0.000030\n",
      "Step 943, Loss Train: 3.0325, Loss Test: 3.8855, LR: 0.000030\n",
      "Step 944, Loss Train: 2.9073, Loss Test: 3.1878, LR: 0.000030\n",
      "Step 945, Loss Train: 3.0400, Loss Test: 2.9576, LR: 0.000030\n",
      "Step 946, Loss Train: 2.9526, Loss Test: 3.9044, LR: 0.000030\n",
      "Step 947, Loss Train: 2.8392, Loss Test: 4.5514, LR: 0.000030\n",
      "Step 948, Loss Train: 2.9152, Loss Test: 4.1025, LR: 0.000030\n",
      "Step 949, Loss Train: 2.9083, Loss Test: 4.2034, LR: 0.000030\n",
      "Step 950, Loss Train: 3.0548, Loss Test: 3.5540, LR: 0.000030\n",
      "Step 951, Loss Train: 2.9655, Loss Test: 3.9686, LR: 0.000030\n",
      "Step 952, Loss Train: 3.0351, Loss Test: 3.4986, LR: 0.000030\n",
      "Step 953, Loss Train: 2.8966, Loss Test: 3.0431, LR: 0.000030\n",
      "Step 954, Loss Train: 3.0448, Loss Test: 4.0270, LR: 0.000030\n",
      "Step 955, Loss Train: 2.9417, Loss Test: 3.5988, LR: 0.000030\n",
      "Step 956, Loss Train: 2.8454, Loss Test: 4.1275, LR: 0.000030\n",
      "Step 957, Loss Train: 2.9504, Loss Test: 3.8940, LR: 0.000030\n",
      "Step 958, Loss Train: 2.9706, Loss Test: 1.8769, LR: 0.000030\n",
      "Step 959, Loss Train: 3.0714, Loss Test: 2.6948, LR: 0.000030\n",
      "Step 960, Loss Train: 2.9044, Loss Test: 3.5190, LR: 0.000030\n",
      "Step 961, Loss Train: 2.8493, Loss Test: 3.4937, LR: 0.000030\n",
      "Step 962, Loss Train: 3.0093, Loss Test: 3.8865, LR: 0.000030\n",
      "Step 963, Loss Train: 2.7509, Loss Test: 4.2586, LR: 0.000030\n",
      "Step 964, Loss Train: 2.9089, Loss Test: 3.5506, LR: 0.000030\n",
      "Step 965, Loss Train: 3.0215, Loss Test: 3.4196, LR: 0.000030\n",
      "Step 966, Loss Train: 2.8226, Loss Test: 4.1229, LR: 0.000030\n",
      "Step 967, Loss Train: 2.8220, Loss Test: 3.4296, LR: 0.000030\n",
      "Step 968, Loss Train: 3.0253, Loss Test: 3.9749, LR: 0.000030\n",
      "Step 969, Loss Train: 3.1884, Loss Test: 4.2562, LR: 0.000030\n",
      "Step 970, Loss Train: 2.9196, Loss Test: 3.7619, LR: 0.000030\n",
      "Step 971, Loss Train: 2.7954, Loss Test: 3.6967, LR: 0.000030\n",
      "Step 972, Loss Train: 3.0299, Loss Test: 3.9652, LR: 0.000030\n",
      "Step 973, Loss Train: 2.9591, Loss Test: 3.9864, LR: 0.000030\n",
      "Step 974, Loss Train: 2.8414, Loss Test: 3.6287, LR: 0.000030\n",
      "Step 975, Loss Train: 2.7889, Loss Test: 4.0976, LR: 0.000030\n",
      "Step 976, Loss Train: 3.1540, Loss Test: 4.4297, LR: 0.000030\n",
      "Step 977, Loss Train: 2.9159, Loss Test: 4.0370, LR: 0.000030\n",
      "Step 978, Loss Train: 3.1229, Loss Test: 3.8272, LR: 0.000030\n",
      "Step 979, Loss Train: 2.9480, Loss Test: 4.7462, LR: 0.000030\n",
      "Step 980, Loss Train: 2.9268, Loss Test: 3.1369, LR: 0.000030\n",
      "Step 981, Loss Train: 2.9424, Loss Test: 2.0810, LR: 0.000030\n",
      "Step 982, Loss Train: 2.9947, Loss Test: 3.9905, LR: 0.000030\n",
      "Step 983, Loss Train: 2.7203, Loss Test: 4.1113, LR: 0.000030\n",
      "Step 984, Loss Train: 3.1860, Loss Test: 3.5069, LR: 0.000030\n",
      "Step 985, Loss Train: 2.9824, Loss Test: 3.4138, LR: 0.000030\n",
      "Step 986, Loss Train: 2.8757, Loss Test: 3.6527, LR: 0.000030\n",
      "Step 987, Loss Train: 3.0376, Loss Test: 3.8249, LR: 0.000030\n",
      "Step 988, Loss Train: 3.1283, Loss Test: 2.8041, LR: 0.000030\n",
      "Step 989, Loss Train: 2.9981, Loss Test: 4.3160, LR: 0.000030\n",
      "Step 990, Loss Train: 2.9082, Loss Test: 2.4366, LR: 0.000030\n",
      "Step 991, Loss Train: 2.9204, Loss Test: 4.5591, LR: 0.000030\n",
      "Step 992, Loss Train: 3.1715, Loss Test: 3.5934, LR: 0.000030\n",
      "Step 993, Loss Train: 3.0183, Loss Test: 3.9450, LR: 0.000030\n",
      "Step 994, Loss Train: 2.8561, Loss Test: 4.3644, LR: 0.000030\n",
      "Step 995, Loss Train: 2.6540, Loss Test: 3.5712, LR: 0.000030\n",
      "Step 996, Loss Train: 2.8246, Loss Test: 4.1818, LR: 0.000030\n",
      "Step 997, Loss Train: 3.0020, Loss Test: 4.1118, LR: 0.000030\n",
      "Step 998, Loss Train: 3.0815, Loss Test: 3.3869, LR: 0.000030\n",
      "Step 999, Loss Train: 2.8510, Loss Test: 4.2231, LR: 0.000030\n",
      "Step 1000, Loss Train: 3.0085, Loss Test: 4.0831, LR: 0.000030\n",
      "Step 1001, Loss Train: 3.0660, Loss Test: 2.9312, LR: 0.000030\n",
      "Step 1002, Loss Train: 3.1628, Loss Test: 2.3622, LR: 0.000030\n",
      "Step 1003, Loss Train: 2.9337, Loss Test: 4.1891, LR: 0.000030\n",
      "Step 1004, Loss Train: 3.0792, Loss Test: 4.0281, LR: 0.000030\n",
      "Step 1005, Loss Train: 2.6696, Loss Test: 4.3186, LR: 0.000030\n",
      "Step 1006, Loss Train: 3.2476, Loss Test: 3.4111, LR: 0.000030\n",
      "Step 1007, Loss Train: 2.9763, Loss Test: 3.3664, LR: 0.000030\n",
      "Step 1008, Loss Train: 3.0074, Loss Test: 4.4605, LR: 0.000030\n",
      "Step 1009, Loss Train: 2.9608, Loss Test: 3.0071, LR: 0.000030\n",
      "Step 1010, Loss Train: 2.8712, Loss Test: 3.7255, LR: 0.000030\n",
      "Step 1011, Loss Train: 3.0556, Loss Test: 3.6324, LR: 0.000030\n",
      "Step 1012, Loss Train: 2.9438, Loss Test: 3.1709, LR: 0.000030\n",
      "Step 1013, Loss Train: 2.9321, Loss Test: 2.9047, LR: 0.000030\n",
      "Step 1014, Loss Train: 2.8445, Loss Test: 4.1696, LR: 0.000030\n",
      "Step 1015, Loss Train: 2.9599, Loss Test: 4.3435, LR: 0.000030\n",
      "Step 1016, Loss Train: 3.1371, Loss Test: 3.4737, LR: 0.000030\n",
      "Step 1017, Loss Train: 2.9396, Loss Test: 3.5983, LR: 0.000030\n",
      "Step 1018, Loss Train: 2.8667, Loss Test: 3.9340, LR: 0.000030\n",
      "Step 1019, Loss Train: 3.1125, Loss Test: 4.3923, LR: 0.000030\n",
      "Step 1020, Loss Train: 2.8359, Loss Test: 3.6359, LR: 0.000030\n",
      "Step 1021, Loss Train: 2.9312, Loss Test: 4.0233, LR: 0.000030\n",
      "Step 1022, Loss Train: 3.0747, Loss Test: 3.3557, LR: 0.000030\n",
      "Step 1023, Loss Train: 2.9144, Loss Test: 4.2349, LR: 0.000030\n",
      "Step 1024, Loss Train: 2.9721, Loss Test: 4.0597, LR: 0.000030\n",
      "Step 1025, Loss Train: 2.9614, Loss Test: 4.3994, LR: 0.000030\n",
      "Step 1026, Loss Train: 2.8386, Loss Test: 3.8207, LR: 0.000030\n",
      "Step 1027, Loss Train: 3.0969, Loss Test: 3.8821, LR: 0.000030\n",
      "Step 1028, Loss Train: 3.0196, Loss Test: 2.9662, LR: 0.000030\n",
      "Step 1029, Loss Train: 2.9192, Loss Test: 3.0840, LR: 0.000030\n",
      "Step 1030, Loss Train: 2.8381, Loss Test: 4.0107, LR: 0.000030\n",
      "Step 1031, Loss Train: 2.9778, Loss Test: 3.0270, LR: 0.000030\n",
      "Step 1032, Loss Train: 2.6433, Loss Test: 4.1814, LR: 0.000030\n",
      "Step 1033, Loss Train: 2.8827, Loss Test: 3.2679, LR: 0.000030\n",
      "Step 1034, Loss Train: 3.0391, Loss Test: 3.1117, LR: 0.000030\n",
      "Step 1035, Loss Train: 3.0623, Loss Test: 3.8380, LR: 0.000030\n",
      "Step 1036, Loss Train: 2.9147, Loss Test: 4.0435, LR: 0.000030\n",
      "Step 1037, Loss Train: 3.0632, Loss Test: 4.1983, LR: 0.000030\n",
      "Step 1038, Loss Train: 3.0034, Loss Test: 3.6812, LR: 0.000030\n",
      "Step 1039, Loss Train: 3.1522, Loss Test: 3.7423, LR: 0.000030\n",
      "Step 1040, Loss Train: 2.8629, Loss Test: 2.9356, LR: 0.000030\n",
      "Step 1041, Loss Train: 2.9543, Loss Test: 3.2442, LR: 0.000030\n",
      "Step 1042, Loss Train: 2.9432, Loss Test: 3.9310, LR: 0.000030\n",
      "Step 1043, Loss Train: 2.9990, Loss Test: 4.2730, LR: 0.000030\n",
      "Step 1044, Loss Train: 3.0259, Loss Test: 4.1810, LR: 0.000030\n",
      "Step 1045, Loss Train: 2.9247, Loss Test: 3.3145, LR: 0.000030\n",
      "Step 1046, Loss Train: 2.8707, Loss Test: 2.6955, LR: 0.000030\n",
      "Step 1047, Loss Train: 2.8964, Loss Test: 3.8214, LR: 0.000030\n",
      "Step 1048, Loss Train: 3.0847, Loss Test: 3.8395, LR: 0.000030\n",
      "Step 1049, Loss Train: 2.8431, Loss Test: 4.3182, LR: 0.000030\n",
      "Step 1050, Loss Train: 2.8930, Loss Test: 4.1689, LR: 0.000030\n",
      "Step 1051, Loss Train: 2.8098, Loss Test: 3.9993, LR: 0.000030\n",
      "Step 1052, Loss Train: 3.0681, Loss Test: 1.9409, LR: 0.000030\n",
      "Step 1053, Loss Train: 2.9810, Loss Test: 2.4533, LR: 0.000030\n",
      "Step 1054, Loss Train: 2.9587, Loss Test: 3.6600, LR: 0.000030\n",
      "Step 1055, Loss Train: 3.0740, Loss Test: 4.0704, LR: 0.000030\n",
      "Step 1056, Loss Train: 3.0194, Loss Test: 3.7507, LR: 0.000030\n",
      "Step 1057, Loss Train: 3.0287, Loss Test: 4.4864, LR: 0.000030\n",
      "Step 1058, Loss Train: 2.8726, Loss Test: 3.1046, LR: 0.000030\n",
      "Step 1059, Loss Train: 2.9497, Loss Test: 3.6972, LR: 0.000030\n",
      "Step 1060, Loss Train: 2.8798, Loss Test: 2.3784, LR: 0.000030\n",
      "Step 1061, Loss Train: 3.0199, Loss Test: 3.3414, LR: 0.000030\n",
      "Step 1062, Loss Train: 3.0606, Loss Test: 3.9758, LR: 0.000030\n",
      "Step 1063, Loss Train: 3.0818, Loss Test: 3.1384, LR: 0.000030\n",
      "Step 1064, Loss Train: 2.9315, Loss Test: 4.3151, LR: 0.000030\n",
      "Step 1065, Loss Train: 2.9331, Loss Test: 3.6105, LR: 0.000030\n",
      "Step 1066, Loss Train: 2.8442, Loss Test: 3.4006, LR: 0.000030\n",
      "Step 1067, Loss Train: 2.9687, Loss Test: 4.0446, LR: 0.000030\n",
      "Step 1068, Loss Train: 2.8861, Loss Test: 4.2398, LR: 0.000030\n",
      "Step 1069, Loss Train: 2.8593, Loss Test: 3.6312, LR: 0.000030\n",
      "Step 1070, Loss Train: 2.9676, Loss Test: 4.5405, LR: 0.000030\n",
      "Step 1071, Loss Train: 2.9895, Loss Test: 3.2848, LR: 0.000030\n",
      "Step 1072, Loss Train: 2.9287, Loss Test: 3.6018, LR: 0.000030\n",
      "Step 1073, Loss Train: 2.8475, Loss Test: 1.6140, LR: 0.000030\n",
      "Step 1074, Loss Train: 3.0737, Loss Test: 3.9140, LR: 0.000030\n",
      "Step 1075, Loss Train: 2.7024, Loss Test: 2.2862, LR: 0.000030\n",
      "Step 1076, Loss Train: 2.9955, Loss Test: 3.7118, LR: 0.000030\n",
      "Step 1077, Loss Train: 2.8402, Loss Test: 4.1156, LR: 0.000030\n",
      "Step 1078, Loss Train: 2.9606, Loss Test: 4.0689, LR: 0.000030\n",
      "Step 1079, Loss Train: 2.8689, Loss Test: 3.9413, LR: 0.000030\n",
      "Step 1080, Loss Train: 3.0386, Loss Test: 3.6937, LR: 0.000030\n",
      "Step 1081, Loss Train: 2.9646, Loss Test: 3.7006, LR: 0.000030\n",
      "Step 1082, Loss Train: 2.9571, Loss Test: 3.1326, LR: 0.000030\n",
      "Step 1083, Loss Train: 2.9528, Loss Test: 3.2984, LR: 0.000030\n",
      "Step 1084, Loss Train: 2.8650, Loss Test: 3.8810, LR: 0.000030\n",
      "Step 1085, Loss Train: 2.9500, Loss Test: 2.9612, LR: 0.000030\n",
      "Step 1086, Loss Train: 2.7911, Loss Test: 2.3091, LR: 0.000030\n",
      "Step 1087, Loss Train: 2.9857, Loss Test: 2.9656, LR: 0.000030\n",
      "Step 1088, Loss Train: 3.0036, Loss Test: 3.6456, LR: 0.000030\n",
      "Step 1089, Loss Train: 3.0808, Loss Test: 3.5230, LR: 0.000030\n",
      "Step 1090, Loss Train: 2.7106, Loss Test: 4.4412, LR: 0.000030\n",
      "Step 1091, Loss Train: 2.9109, Loss Test: 3.9552, LR: 0.000030\n",
      "Step 1092, Loss Train: 3.1125, Loss Test: 3.8416, LR: 0.000030\n",
      "Step 1093, Loss Train: 3.0303, Loss Test: 3.4697, LR: 0.000030\n",
      "Step 1094, Loss Train: 3.0639, Loss Test: 4.0840, LR: 0.000030\n",
      "Step 1095, Loss Train: 2.9431, Loss Test: 3.1794, LR: 0.000030\n",
      "Step 1096, Loss Train: 2.8842, Loss Test: 3.9614, LR: 0.000030\n",
      "Step 1097, Loss Train: 2.8375, Loss Test: 3.5556, LR: 0.000030\n",
      "Step 1098, Loss Train: 3.0992, Loss Test: 3.8382, LR: 0.000030\n",
      "Step 1099, Loss Train: 3.1701, Loss Test: 2.0140, LR: 0.000030\n",
      "Step 1100, Loss Train: 3.1588, Loss Test: 4.3689, LR: 0.000030\n",
      "Step 1101, Loss Train: 2.9517, Loss Test: 3.8604, LR: 0.000030\n",
      "Step 1102, Loss Train: 2.7876, Loss Test: 4.1893, LR: 0.000030\n",
      "Step 1103, Loss Train: 2.8899, Loss Test: 3.0678, LR: 0.000030\n",
      "Step 1104, Loss Train: 2.8678, Loss Test: 3.9550, LR: 0.000030\n",
      "Step 1105, Loss Train: 2.9818, Loss Test: 3.2044, LR: 0.000030\n",
      "Step 1106, Loss Train: 3.2188, Loss Test: 3.9423, LR: 0.000030\n",
      "Step 1107, Loss Train: 2.8461, Loss Test: 3.7545, LR: 0.000030\n",
      "Step 1108, Loss Train: 2.9891, Loss Test: 3.3959, LR: 0.000030\n",
      "Step 1109, Loss Train: 3.0383, Loss Test: 3.4571, LR: 0.000030\n",
      "Step 1110, Loss Train: 2.9966, Loss Test: 3.7862, LR: 0.000030\n",
      "Step 1111, Loss Train: 2.7805, Loss Test: 3.5624, LR: 0.000030\n",
      "Step 1112, Loss Train: 2.8780, Loss Test: 3.4724, LR: 0.000030\n",
      "Step 1113, Loss Train: 3.1425, Loss Test: 3.8212, LR: 0.000030\n",
      "Step 1114, Loss Train: 2.9752, Loss Test: 2.2548, LR: 0.000030\n",
      "Step 1115, Loss Train: 2.9898, Loss Test: 4.0226, LR: 0.000030\n",
      "Step 1116, Loss Train: 2.9154, Loss Test: 3.5388, LR: 0.000030\n",
      "Step 1117, Loss Train: 2.9572, Loss Test: 4.1687, LR: 0.000030\n",
      "Step 1118, Loss Train: 3.0253, Loss Test: 3.9170, LR: 0.000030\n",
      "Step 1119, Loss Train: 2.9431, Loss Test: 3.7164, LR: 0.000030\n",
      "Step 1120, Loss Train: 2.9982, Loss Test: 3.5479, LR: 0.000030\n",
      "Step 1121, Loss Train: 2.9040, Loss Test: 3.7622, LR: 0.000030\n",
      "Step 1122, Loss Train: 2.9162, Loss Test: 4.2181, LR: 0.000030\n",
      "Step 1123, Loss Train: 3.0751, Loss Test: 3.6931, LR: 0.000030\n",
      "Step 1124, Loss Train: 2.9528, Loss Test: 3.6574, LR: 0.000030\n",
      "Step 1125, Loss Train: 2.8792, Loss Test: 3.5678, LR: 0.000030\n",
      "Step 1126, Loss Train: 3.0952, Loss Test: 3.6980, LR: 0.000030\n",
      "Step 1127, Loss Train: 3.0668, Loss Test: 4.0774, LR: 0.000030\n",
      "Step 1128, Loss Train: 2.8460, Loss Test: 3.5378, LR: 0.000030\n",
      "Step 1129, Loss Train: 2.9958, Loss Test: 3.0712, LR: 0.000030\n",
      "Step 1130, Loss Train: 2.9967, Loss Test: 3.9065, LR: 0.000030\n",
      "Step 1131, Loss Train: 2.9827, Loss Test: 3.4960, LR: 0.000030\n",
      "Step 1132, Loss Train: 2.9390, Loss Test: 3.9739, LR: 0.000030\n",
      "Step 1133, Loss Train: 3.0632, Loss Test: 2.9307, LR: 0.000030\n",
      "Step 1134, Loss Train: 2.9308, Loss Test: 3.4959, LR: 0.000030\n",
      "Step 1135, Loss Train: 3.0255, Loss Test: 4.2775, LR: 0.000030\n",
      "Step 1136, Loss Train: 2.8789, Loss Test: 3.4575, LR: 0.000030\n",
      "Step 1137, Loss Train: 3.0351, Loss Test: 3.8979, LR: 0.000030\n",
      "Step 1138, Loss Train: 2.7718, Loss Test: 3.7309, LR: 0.000030\n",
      "Step 1139, Loss Train: 2.9966, Loss Test: 3.6841, LR: 0.000030\n",
      "Step 1140, Loss Train: 2.8641, Loss Test: 3.6598, LR: 0.000030\n",
      "Step 1141, Loss Train: 3.0922, Loss Test: 3.8704, LR: 0.000030\n",
      "Step 1142, Loss Train: 2.8069, Loss Test: 4.1487, LR: 0.000030\n",
      "Step 1143, Loss Train: 3.2048, Loss Test: 3.5869, LR: 0.000030\n",
      "Step 1144, Loss Train: 2.9258, Loss Test: 3.4768, LR: 0.000030\n",
      "Step 1145, Loss Train: 2.8541, Loss Test: 2.9674, LR: 0.000030\n",
      "Step 1146, Loss Train: 3.0551, Loss Test: 3.4773, LR: 0.000030\n",
      "Step 1147, Loss Train: 3.0629, Loss Test: 3.6030, LR: 0.000030\n",
      "Step 1148, Loss Train: 2.8845, Loss Test: 2.7889, LR: 0.000030\n",
      "Step 1149, Loss Train: 2.9181, Loss Test: 4.1462, LR: 0.000030\n",
      "Step 1150, Loss Train: 3.0705, Loss Test: 3.6177, LR: 0.000030\n",
      "Step 1151, Loss Train: 2.9813, Loss Test: 3.4360, LR: 0.000030\n",
      "Step 1152, Loss Train: 2.8402, Loss Test: 4.4445, LR: 0.000030\n",
      "Step 1153, Loss Train: 2.8864, Loss Test: 3.9495, LR: 0.000030\n",
      "Step 1154, Loss Train: 3.0234, Loss Test: 2.7921, LR: 0.000030\n",
      "Step 1155, Loss Train: 3.0431, Loss Test: 3.8009, LR: 0.000030\n",
      "Step 1156, Loss Train: 2.9757, Loss Test: 4.7607, LR: 0.000030\n",
      "Step 1157, Loss Train: 2.7434, Loss Test: 4.0079, LR: 0.000030\n",
      "Step 1158, Loss Train: 2.7965, Loss Test: 4.2100, LR: 0.000030\n",
      "Step 1159, Loss Train: 3.0016, Loss Test: 3.9489, LR: 0.000030\n",
      "Step 1160, Loss Train: 2.9657, Loss Test: 3.8984, LR: 0.000030\n",
      "Step 1161, Loss Train: 3.0140, Loss Test: 4.3347, LR: 0.000030\n",
      "Step 1162, Loss Train: 2.9515, Loss Test: 4.3125, LR: 0.000030\n",
      "Step 1163, Loss Train: 3.0659, Loss Test: 4.2957, LR: 0.000030\n",
      "Step 1164, Loss Train: 2.8768, Loss Test: 2.9698, LR: 0.000030\n",
      "Step 1165, Loss Train: 2.9313, Loss Test: 3.9545, LR: 0.000030\n",
      "Step 1166, Loss Train: 3.0768, Loss Test: 4.0097, LR: 0.000030\n",
      "Step 1167, Loss Train: 3.0577, Loss Test: 4.2494, LR: 0.000030\n",
      "Step 1168, Loss Train: 2.9210, Loss Test: 4.2202, LR: 0.000030\n",
      "Step 1169, Loss Train: 3.0698, Loss Test: 3.9559, LR: 0.000030\n",
      "Step 1170, Loss Train: 2.8606, Loss Test: 3.8687, LR: 0.000030\n",
      "Step 1171, Loss Train: 2.9607, Loss Test: 3.5379, LR: 0.000030\n",
      "Step 1172, Loss Train: 3.0698, Loss Test: 3.9142, LR: 0.000030\n",
      "Step 1173, Loss Train: 2.9673, Loss Test: 4.3260, LR: 0.000030\n",
      "Step 1174, Loss Train: 2.9626, Loss Test: 3.9318, LR: 0.000030\n",
      "Step 1175, Loss Train: 2.6538, Loss Test: 2.9606, LR: 0.000030\n",
      "Step 1176, Loss Train: 2.7640, Loss Test: 4.1873, LR: 0.000030\n",
      "Step 1177, Loss Train: 3.0118, Loss Test: 2.3546, LR: 0.000030\n",
      "Step 1178, Loss Train: 3.0339, Loss Test: 3.9604, LR: 0.000030\n",
      "Step 1179, Loss Train: 3.0230, Loss Test: 3.1793, LR: 0.000030\n",
      "Step 1180, Loss Train: 3.0163, Loss Test: 4.1509, LR: 0.000030\n",
      "Step 1181, Loss Train: 2.9981, Loss Test: 2.4786, LR: 0.000030\n",
      "Step 1182, Loss Train: 2.9772, Loss Test: 2.7147, LR: 0.000030\n",
      "Step 1183, Loss Train: 2.9507, Loss Test: 2.7722, LR: 0.000030\n",
      "Step 1184, Loss Train: 2.9384, Loss Test: 3.8169, LR: 0.000030\n",
      "Step 1185, Loss Train: 2.9435, Loss Test: 3.5488, LR: 0.000030\n",
      "Step 1186, Loss Train: 3.1423, Loss Test: 3.0904, LR: 0.000030\n",
      "Step 1187, Loss Train: 2.9863, Loss Test: 3.6388, LR: 0.000030\n",
      "Step 1188, Loss Train: 2.8929, Loss Test: 2.0258, LR: 0.000030\n",
      "Step 1189, Loss Train: 3.1438, Loss Test: 3.8753, LR: 0.000030\n",
      "Step 1190, Loss Train: 2.7501, Loss Test: 3.9630, LR: 0.000030\n",
      "Step 1191, Loss Train: 2.8322, Loss Test: 4.4949, LR: 0.000030\n",
      "Step 1192, Loss Train: 3.0643, Loss Test: 3.5082, LR: 0.000030\n",
      "Step 1193, Loss Train: 2.8556, Loss Test: 3.3250, LR: 0.000030\n",
      "Step 1194, Loss Train: 3.1046, Loss Test: 4.5320, LR: 0.000030\n",
      "Step 1195, Loss Train: 2.9745, Loss Test: 4.4041, LR: 0.000030\n",
      "Step 1196, Loss Train: 2.9104, Loss Test: 3.9914, LR: 0.000030\n",
      "Step 1197, Loss Train: 3.1729, Loss Test: 4.0615, LR: 0.000030\n",
      "Step 1198, Loss Train: 2.9438, Loss Test: 2.7775, LR: 0.000030\n",
      "Step 1199, Loss Train: 2.8223, Loss Test: 3.3199, LR: 0.000030\n",
      "Step 1200, Loss Train: 2.9712, Loss Test: 2.2673, LR: 0.000030\n",
      "Step 1201, Loss Train: 2.8701, Loss Test: 2.6792, LR: 0.000030\n",
      "Step 1202, Loss Train: 2.8850, Loss Test: 3.8187, LR: 0.000030\n",
      "Step 1203, Loss Train: 2.8082, Loss Test: 4.1876, LR: 0.000030\n",
      "Step 1204, Loss Train: 2.8615, Loss Test: 3.7335, LR: 0.000030\n",
      "Step 1205, Loss Train: 2.9537, Loss Test: 4.1751, LR: 0.000030\n",
      "Step 1206, Loss Train: 2.9408, Loss Test: 3.9438, LR: 0.000030\n",
      "Step 1207, Loss Train: 3.0318, Loss Test: 3.5003, LR: 0.000030\n",
      "Step 1208, Loss Train: 3.0347, Loss Test: 3.2107, LR: 0.000030\n",
      "Step 1209, Loss Train: 2.9056, Loss Test: 3.6075, LR: 0.000030\n",
      "Step 1210, Loss Train: 2.9439, Loss Test: 2.7026, LR: 0.000030\n",
      "Step 1211, Loss Train: 3.0424, Loss Test: 4.2631, LR: 0.000030\n",
      "Step 1212, Loss Train: 2.9821, Loss Test: 3.3987, LR: 0.000030\n",
      "Step 1213, Loss Train: 3.1090, Loss Test: 4.0262, LR: 0.000030\n",
      "Step 1214, Loss Train: 3.2417, Loss Test: 3.9814, LR: 0.000030\n",
      "Step 1215, Loss Train: 3.0756, Loss Test: 3.9444, LR: 0.000030\n",
      "Step 1216, Loss Train: 2.8197, Loss Test: 3.5857, LR: 0.000030\n",
      "Step 1217, Loss Train: 2.9531, Loss Test: 3.9042, LR: 0.000030\n",
      "Step 1218, Loss Train: 3.1045, Loss Test: 3.8328, LR: 0.000030\n",
      "Step 1219, Loss Train: 2.9985, Loss Test: 4.0351, LR: 0.000030\n",
      "Step 1220, Loss Train: 3.0202, Loss Test: 3.7788, LR: 0.000030\n",
      "Step 1221, Loss Train: 3.0442, Loss Test: 3.6890, LR: 0.000030\n",
      "Step 1222, Loss Train: 2.9542, Loss Test: 4.3284, LR: 0.000030\n",
      "Step 1223, Loss Train: 2.9057, Loss Test: 4.0882, LR: 0.000030\n",
      "Step 1224, Loss Train: 3.0073, Loss Test: 3.9669, LR: 0.000030\n",
      "Step 1225, Loss Train: 2.9525, Loss Test: 3.7192, LR: 0.000030\n",
      "Step 1226, Loss Train: 3.0080, Loss Test: 4.0686, LR: 0.000030\n",
      "Step 1227, Loss Train: 2.8763, Loss Test: 3.6583, LR: 0.000030\n",
      "Step 1228, Loss Train: 3.1112, Loss Test: 3.6887, LR: 0.000030\n",
      "Step 1229, Loss Train: 2.9346, Loss Test: 4.3337, LR: 0.000030\n",
      "Step 1230, Loss Train: 2.8930, Loss Test: 4.0005, LR: 0.000030\n",
      "Step 1231, Loss Train: 2.8155, Loss Test: 3.5087, LR: 0.000030\n",
      "Step 1232, Loss Train: 2.9034, Loss Test: 3.5684, LR: 0.000030\n",
      "Step 1233, Loss Train: 2.9182, Loss Test: 4.0078, LR: 0.000030\n",
      "Step 1234, Loss Train: 2.9501, Loss Test: 3.6560, LR: 0.000030\n",
      "Step 1235, Loss Train: 2.8953, Loss Test: 3.4419, LR: 0.000030\n",
      "Step 1236, Loss Train: 3.0106, Loss Test: 3.3951, LR: 0.000030\n",
      "Step 1237, Loss Train: 3.0114, Loss Test: 3.7931, LR: 0.000030\n",
      "Step 1238, Loss Train: 2.8983, Loss Test: 3.9115, LR: 0.000030\n",
      "Step 1239, Loss Train: 3.1015, Loss Test: 3.8015, LR: 0.000030\n",
      "Step 1240, Loss Train: 2.8385, Loss Test: 3.8336, LR: 0.000030\n",
      "Step 1241, Loss Train: 3.1557, Loss Test: 3.3380, LR: 0.000030\n",
      "Step 1242, Loss Train: 2.9806, Loss Test: 3.9672, LR: 0.000030\n",
      "Step 1243, Loss Train: 3.0750, Loss Test: 3.2083, LR: 0.000030\n",
      "Step 1244, Loss Train: 3.0554, Loss Test: 3.7919, LR: 0.000030\n",
      "Step 1245, Loss Train: 2.8495, Loss Test: 4.0385, LR: 0.000030\n",
      "Step 1246, Loss Train: 2.8740, Loss Test: 4.3332, LR: 0.000030\n",
      "Step 1247, Loss Train: 2.9576, Loss Test: 3.9009, LR: 0.000030\n",
      "Step 1248, Loss Train: 3.0622, Loss Test: 3.2610, LR: 0.000030\n",
      "Step 1249, Loss Train: 3.0799, Loss Test: 3.8812, LR: 0.000030\n",
      "Step 1250, Loss Train: 2.9415, Loss Test: 2.8172, LR: 0.000030\n",
      "Step 1251, Loss Train: 3.0597, Loss Test: 3.2887, LR: 0.000030\n",
      "Step 1252, Loss Train: 3.1138, Loss Test: 3.6367, LR: 0.000030\n",
      "Step 1253, Loss Train: 3.0446, Loss Test: 4.1523, LR: 0.000030\n",
      "Step 1254, Loss Train: 2.9244, Loss Test: 3.5455, LR: 0.000030\n",
      "Step 1255, Loss Train: 2.8305, Loss Test: 3.9922, LR: 0.000030\n",
      "Step 1256, Loss Train: 2.9966, Loss Test: 3.1092, LR: 0.000030\n",
      "Step 1257, Loss Train: 2.8475, Loss Test: 3.3225, LR: 0.000030\n",
      "Step 1258, Loss Train: 2.8906, Loss Test: 3.9239, LR: 0.000030\n",
      "Step 1259, Loss Train: 2.9557, Loss Test: 3.8850, LR: 0.000030\n",
      "Step 1260, Loss Train: 2.9778, Loss Test: 4.1674, LR: 0.000030\n",
      "Step 1261, Loss Train: 2.9649, Loss Test: 3.7488, LR: 0.000030\n",
      "Step 1262, Loss Train: 3.0026, Loss Test: 3.6180, LR: 0.000030\n",
      "Step 1263, Loss Train: 2.8771, Loss Test: 4.4511, LR: 0.000030\n",
      "Step 1264, Loss Train: 3.0090, Loss Test: 2.7925, LR: 0.000030\n",
      "Step 1265, Loss Train: 2.9932, Loss Test: 3.3598, LR: 0.000030\n",
      "Step 1266, Loss Train: 2.8478, Loss Test: 3.7727, LR: 0.000030\n",
      "Step 1267, Loss Train: 2.8103, Loss Test: 2.3828, LR: 0.000030\n",
      "Step 1268, Loss Train: 2.6578, Loss Test: 3.1826, LR: 0.000030\n",
      "Step 1269, Loss Train: 2.9520, Loss Test: 3.3016, LR: 0.000030\n",
      "Step 1270, Loss Train: 2.8891, Loss Test: 4.6697, LR: 0.000030\n",
      "Step 1271, Loss Train: 3.0593, Loss Test: 3.5451, LR: 0.000030\n",
      "Step 1272, Loss Train: 3.0187, Loss Test: 4.0817, LR: 0.000030\n",
      "Step 1273, Loss Train: 3.0144, Loss Test: 3.1463, LR: 0.000030\n",
      "Step 1274, Loss Train: 2.9993, Loss Test: 4.1154, LR: 0.000030\n",
      "Step 1275, Loss Train: 3.0391, Loss Test: 4.1091, LR: 0.000030\n",
      "Step 1276, Loss Train: 2.7786, Loss Test: 3.0579, LR: 0.000030\n",
      "Step 1277, Loss Train: 2.9401, Loss Test: 4.0042, LR: 0.000030\n",
      "Step 1278, Loss Train: 2.8857, Loss Test: 2.4266, LR: 0.000030\n",
      "Step 1279, Loss Train: 3.0199, Loss Test: 3.5259, LR: 0.000030\n",
      "Step 1280, Loss Train: 2.7989, Loss Test: 3.8100, LR: 0.000030\n",
      "Step 1281, Loss Train: 3.0442, Loss Test: 4.2700, LR: 0.000030\n",
      "Step 1282, Loss Train: 3.0222, Loss Test: 1.4054, LR: 0.000030\n",
      "Step 1283, Loss Train: 2.9897, Loss Test: 4.0325, LR: 0.000030\n",
      "Step 1284, Loss Train: 2.7805, Loss Test: 3.6804, LR: 0.000030\n",
      "Step 1285, Loss Train: 2.8271, Loss Test: 2.8407, LR: 0.000030\n",
      "Step 1286, Loss Train: 3.0621, Loss Test: 3.3256, LR: 0.000030\n",
      "Step 1287, Loss Train: 2.8885, Loss Test: 3.1200, LR: 0.000030\n",
      "Step 1288, Loss Train: 2.7969, Loss Test: 3.8584, LR: 0.000030\n",
      "Step 1289, Loss Train: 2.9811, Loss Test: 3.9446, LR: 0.000030\n",
      "Step 1290, Loss Train: 2.9591, Loss Test: 3.4032, LR: 0.000030\n",
      "Step 1291, Loss Train: 2.8099, Loss Test: 3.5362, LR: 0.000030\n",
      "Step 1292, Loss Train: 2.9877, Loss Test: 4.1232, LR: 0.000030\n",
      "Step 1293, Loss Train: 3.1184, Loss Test: 3.5458, LR: 0.000030\n",
      "Step 1294, Loss Train: 2.9705, Loss Test: 3.4541, LR: 0.000030\n",
      "Step 1295, Loss Train: 3.0137, Loss Test: 3.5891, LR: 0.000030\n",
      "Step 1296, Loss Train: 2.9732, Loss Test: 3.8724, LR: 0.000030\n",
      "Step 1297, Loss Train: 2.8936, Loss Test: 2.6286, LR: 0.000030\n",
      "Step 1298, Loss Train: 2.8471, Loss Test: 4.0981, LR: 0.000030\n",
      "Step 1299, Loss Train: 3.0817, Loss Test: 2.9188, LR: 0.000030\n",
      "Step 1300, Loss Train: 3.0575, Loss Test: 3.4328, LR: 0.000030\n",
      "Step 1301, Loss Train: 2.8522, Loss Test: 3.7760, LR: 0.000030\n",
      "Step 1302, Loss Train: 2.8988, Loss Test: 3.9842, LR: 0.000030\n",
      "Step 1303, Loss Train: 3.0304, Loss Test: 4.4096, LR: 0.000030\n",
      "Step 1304, Loss Train: 2.9648, Loss Test: 3.9079, LR: 0.000030\n",
      "Step 1305, Loss Train: 3.0394, Loss Test: 3.7788, LR: 0.000030\n",
      "Step 1306, Loss Train: 2.9540, Loss Test: 2.9244, LR: 0.000030\n",
      "Step 1307, Loss Train: 2.7250, Loss Test: 3.4695, LR: 0.000030\n",
      "Step 1308, Loss Train: 2.7123, Loss Test: 3.9459, LR: 0.000030\n",
      "Step 1309, Loss Train: 2.9143, Loss Test: 2.6225, LR: 0.000030\n",
      "Step 1310, Loss Train: 3.0443, Loss Test: 3.5236, LR: 0.000030\n",
      "Step 1311, Loss Train: 2.9203, Loss Test: 3.6797, LR: 0.000030\n",
      "Step 1312, Loss Train: 3.0781, Loss Test: 3.8852, LR: 0.000030\n",
      "Step 1313, Loss Train: 2.9347, Loss Test: 4.4157, LR: 0.000030\n",
      "Step 1314, Loss Train: 3.1585, Loss Test: 4.1553, LR: 0.000030\n",
      "Step 1315, Loss Train: 2.9898, Loss Test: 3.9826, LR: 0.000030\n",
      "Step 1316, Loss Train: 2.8212, Loss Test: 3.0318, LR: 0.000030\n",
      "Step 1317, Loss Train: 3.0113, Loss Test: 3.6946, LR: 0.000030\n",
      "Step 1318, Loss Train: 3.0403, Loss Test: 3.9238, LR: 0.000030\n",
      "Step 1319, Loss Train: 2.8038, Loss Test: 4.2401, LR: 0.000030\n",
      "Step 1320, Loss Train: 2.9796, Loss Test: 2.5057, LR: 0.000030\n",
      "Step 1321, Loss Train: 2.8059, Loss Test: 3.8235, LR: 0.000030\n",
      "Step 1322, Loss Train: 3.1301, Loss Test: 3.8505, LR: 0.000030\n",
      "Step 1323, Loss Train: 2.8400, Loss Test: 3.5438, LR: 0.000030\n",
      "Step 1324, Loss Train: 2.9995, Loss Test: 3.6391, LR: 0.000030\n",
      "Step 1325, Loss Train: 3.0096, Loss Test: 3.7166, LR: 0.000030\n",
      "Step 1326, Loss Train: 2.9624, Loss Test: 4.4294, LR: 0.000030\n",
      "Step 1327, Loss Train: 3.0131, Loss Test: 3.4265, LR: 0.000030\n",
      "Step 1328, Loss Train: 2.9692, Loss Test: 3.7918, LR: 0.000030\n",
      "Step 1329, Loss Train: 2.9114, Loss Test: 3.3952, LR: 0.000030\n",
      "Step 1330, Loss Train: 2.9986, Loss Test: 3.1225, LR: 0.000030\n",
      "Step 1331, Loss Train: 2.7082, Loss Test: 4.3482, LR: 0.000030\n",
      "Step 1332, Loss Train: 2.8028, Loss Test: 3.8388, LR: 0.000030\n",
      "Step 1333, Loss Train: 3.1136, Loss Test: 3.8353, LR: 0.000030\n",
      "Step 1334, Loss Train: 2.9686, Loss Test: 4.0905, LR: 0.000030\n",
      "Step 1335, Loss Train: 3.0692, Loss Test: 4.1199, LR: 0.000030\n",
      "Step 1336, Loss Train: 2.9708, Loss Test: 4.2291, LR: 0.000030\n",
      "Step 1337, Loss Train: 2.8997, Loss Test: 3.5617, LR: 0.000030\n",
      "Step 1338, Loss Train: 2.9348, Loss Test: 3.2646, LR: 0.000030\n",
      "Step 1339, Loss Train: 2.8514, Loss Test: 3.8212, LR: 0.000030\n",
      "Step 1340, Loss Train: 2.9116, Loss Test: 4.2516, LR: 0.000030\n",
      "Step 1341, Loss Train: 3.1004, Loss Test: 3.9181, LR: 0.000030\n",
      "Step 1342, Loss Train: 3.0038, Loss Test: 2.1068, LR: 0.000030\n",
      "Step 1343, Loss Train: 3.0386, Loss Test: 4.0307, LR: 0.000030\n",
      "Step 1344, Loss Train: 2.6844, Loss Test: 3.1790, LR: 0.000030\n",
      "Step 1345, Loss Train: 2.9004, Loss Test: 2.7774, LR: 0.000030\n",
      "Step 1346, Loss Train: 2.9210, Loss Test: 4.0936, LR: 0.000030\n",
      "Step 1347, Loss Train: 2.8937, Loss Test: 3.9905, LR: 0.000030\n",
      "Step 1348, Loss Train: 3.1573, Loss Test: 3.5884, LR: 0.000030\n",
      "Step 1349, Loss Train: 2.8927, Loss Test: 3.5456, LR: 0.000030\n",
      "Step 1350, Loss Train: 2.9375, Loss Test: 3.8835, LR: 0.000030\n",
      "Step 1351, Loss Train: 3.0078, Loss Test: 3.8972, LR: 0.000030\n",
      "Step 1352, Loss Train: 2.8091, Loss Test: 4.4382, LR: 0.000030\n",
      "Step 1353, Loss Train: 3.0446, Loss Test: 3.8949, LR: 0.000030\n",
      "Step 1354, Loss Train: 3.0449, Loss Test: 3.9129, LR: 0.000030\n",
      "Step 1355, Loss Train: 3.1005, Loss Test: 3.9956, LR: 0.000030\n",
      "Step 1356, Loss Train: 2.9160, Loss Test: 3.5542, LR: 0.000030\n",
      "Step 1357, Loss Train: 3.0795, Loss Test: 3.3533, LR: 0.000030\n",
      "Step 1358, Loss Train: 2.8743, Loss Test: 3.9614, LR: 0.000030\n",
      "Step 1359, Loss Train: 2.8874, Loss Test: 3.8374, LR: 0.000030\n",
      "Step 1360, Loss Train: 3.0409, Loss Test: 3.1192, LR: 0.000030\n",
      "Step 1361, Loss Train: 3.0742, Loss Test: 3.7704, LR: 0.000030\n",
      "Step 1362, Loss Train: 3.1124, Loss Test: 3.9210, LR: 0.000030\n",
      "Step 1363, Loss Train: 2.9263, Loss Test: 4.1285, LR: 0.000030\n",
      "Step 1364, Loss Train: 2.8418, Loss Test: 3.6480, LR: 0.000030\n",
      "Step 1365, Loss Train: 2.9339, Loss Test: 4.0685, LR: 0.000030\n",
      "Step 1366, Loss Train: 3.2056, Loss Test: 4.0042, LR: 0.000030\n",
      "Step 1367, Loss Train: 3.0654, Loss Test: 3.6846, LR: 0.000030\n",
      "Step 1368, Loss Train: 2.9700, Loss Test: 3.4343, LR: 0.000030\n",
      "Step 1369, Loss Train: 3.0514, Loss Test: 3.3138, LR: 0.000030\n",
      "Step 1370, Loss Train: 3.0635, Loss Test: 4.2396, LR: 0.000030\n",
      "Step 1371, Loss Train: 3.0521, Loss Test: 3.7257, LR: 0.000030\n",
      "Step 1372, Loss Train: 2.9097, Loss Test: 3.6730, LR: 0.000030\n",
      "Step 1373, Loss Train: 2.9709, Loss Test: 4.2092, LR: 0.000030\n",
      "Step 1374, Loss Train: 2.8732, Loss Test: 4.1312, LR: 0.000030\n",
      "Step 1375, Loss Train: 2.9919, Loss Test: 4.0908, LR: 0.000030\n",
      "Step 1376, Loss Train: 3.0695, Loss Test: 3.9273, LR: 0.000030\n",
      "Step 1377, Loss Train: 2.9704, Loss Test: 3.6400, LR: 0.000030\n",
      "Step 1378, Loss Train: 3.1369, Loss Test: 2.4322, LR: 0.000030\n",
      "Step 1379, Loss Train: 3.1140, Loss Test: 4.0126, LR: 0.000030\n",
      "Step 1380, Loss Train: 3.0376, Loss Test: 4.0656, LR: 0.000030\n",
      "Step 1381, Loss Train: 2.9135, Loss Test: 3.5909, LR: 0.000030\n",
      "Step 1382, Loss Train: 2.9430, Loss Test: 3.4586, LR: 0.000030\n",
      "Step 1383, Loss Train: 2.8995, Loss Test: 3.2825, LR: 0.000030\n",
      "Step 1384, Loss Train: 2.8290, Loss Test: 3.5517, LR: 0.000030\n",
      "Step 1385, Loss Train: 2.8168, Loss Test: 3.7457, LR: 0.000030\n",
      "Step 1386, Loss Train: 2.9038, Loss Test: 4.0549, LR: 0.000030\n",
      "Step 1387, Loss Train: 3.0299, Loss Test: 4.1797, LR: 0.000030\n",
      "Step 1388, Loss Train: 2.8638, Loss Test: 4.5642, LR: 0.000030\n",
      "Step 1389, Loss Train: 3.0888, Loss Test: 3.4629, LR: 0.000030\n",
      "Step 1390, Loss Train: 2.9843, Loss Test: 4.0036, LR: 0.000030\n",
      "Step 1391, Loss Train: 3.0527, Loss Test: 4.7297, LR: 0.000030\n",
      "Step 1392, Loss Train: 2.8174, Loss Test: 3.1184, LR: 0.000030\n",
      "Step 1393, Loss Train: 2.6781, Loss Test: 3.2627, LR: 0.000030\n",
      "Step 1394, Loss Train: 2.9105, Loss Test: 3.5680, LR: 0.000030\n",
      "Step 1395, Loss Train: 2.6939, Loss Test: 3.9250, LR: 0.000030\n",
      "Step 1396, Loss Train: 2.9342, Loss Test: 3.6081, LR: 0.000030\n",
      "Step 1397, Loss Train: 3.1276, Loss Test: 3.9680, LR: 0.000030\n",
      "Step 1398, Loss Train: 2.8446, Loss Test: 3.8582, LR: 0.000030\n",
      "Step 1399, Loss Train: 2.8006, Loss Test: 2.9248, LR: 0.000030\n",
      "Step 1400, Loss Train: 2.8887, Loss Test: 3.7376, LR: 0.000030\n",
      "Step 1401, Loss Train: 3.0696, Loss Test: 4.1072, LR: 0.000030\n",
      "Step 1402, Loss Train: 3.1524, Loss Test: 3.6142, LR: 0.000030\n",
      "Step 1403, Loss Train: 2.8839, Loss Test: 3.8528, LR: 0.000030\n",
      "Step 1404, Loss Train: 2.9785, Loss Test: 4.1007, LR: 0.000030\n",
      "Step 1405, Loss Train: 3.0579, Loss Test: 4.4711, LR: 0.000030\n",
      "Step 1406, Loss Train: 2.8987, Loss Test: 4.1331, LR: 0.000030\n",
      "Step 1407, Loss Train: 2.9397, Loss Test: 3.7501, LR: 0.000030\n",
      "Step 1408, Loss Train: 2.9197, Loss Test: 4.5430, LR: 0.000030\n",
      "Step 1409, Loss Train: 2.9514, Loss Test: 3.0000, LR: 0.000030\n",
      "Step 1410, Loss Train: 3.0010, Loss Test: 4.1884, LR: 0.000030\n",
      "Step 1411, Loss Train: 2.7910, Loss Test: 3.2047, LR: 0.000030\n",
      "Step 1412, Loss Train: 2.8107, Loss Test: 3.8471, LR: 0.000030\n",
      "Step 1413, Loss Train: 2.9078, Loss Test: 3.6047, LR: 0.000030\n",
      "Step 1414, Loss Train: 3.1042, Loss Test: 4.2171, LR: 0.000030\n",
      "Step 1415, Loss Train: 2.9864, Loss Test: 4.5387, LR: 0.000030\n",
      "Step 1416, Loss Train: 3.0474, Loss Test: 4.1311, LR: 0.000030\n",
      "Step 1417, Loss Train: 3.0298, Loss Test: 4.1391, LR: 0.000030\n",
      "Step 1418, Loss Train: 2.9004, Loss Test: 3.9439, LR: 0.000030\n",
      "Step 1419, Loss Train: 2.8756, Loss Test: 3.8808, LR: 0.000030\n",
      "Step 1420, Loss Train: 2.9310, Loss Test: 3.4473, LR: 0.000030\n",
      "Step 1421, Loss Train: 2.7846, Loss Test: 2.4834, LR: 0.000030\n",
      "Step 1422, Loss Train: 3.1052, Loss Test: 4.4081, LR: 0.000030\n",
      "Step 1423, Loss Train: 2.8995, Loss Test: 4.0999, LR: 0.000030\n",
      "Step 1424, Loss Train: 3.1892, Loss Test: 3.5955, LR: 0.000030\n",
      "Step 1425, Loss Train: 2.9923, Loss Test: 4.3395, LR: 0.000030\n",
      "Step 1426, Loss Train: 3.0975, Loss Test: 3.5871, LR: 0.000030\n",
      "Step 1427, Loss Train: 2.9959, Loss Test: 3.5969, LR: 0.000030\n",
      "Step 1428, Loss Train: 3.0201, Loss Test: 3.0157, LR: 0.000030\n",
      "Step 1429, Loss Train: 2.9220, Loss Test: 3.9741, LR: 0.000030\n",
      "Step 1430, Loss Train: 2.7968, Loss Test: 3.9610, LR: 0.000030\n",
      "Step 1431, Loss Train: 3.0129, Loss Test: 4.2471, LR: 0.000030\n",
      "Step 1432, Loss Train: 3.0499, Loss Test: 2.9896, LR: 0.000030\n",
      "Step 1433, Loss Train: 2.8970, Loss Test: 4.1883, LR: 0.000030\n",
      "Step 1434, Loss Train: 2.9042, Loss Test: 3.2864, LR: 0.000030\n",
      "Step 1435, Loss Train: 2.9199, Loss Test: 2.9405, LR: 0.000030\n",
      "Step 1436, Loss Train: 2.9675, Loss Test: 2.8220, LR: 0.000030\n",
      "Step 1437, Loss Train: 2.7913, Loss Test: 3.9689, LR: 0.000030\n",
      "Step 1438, Loss Train: 2.8728, Loss Test: 3.5918, LR: 0.000030\n",
      "Step 1439, Loss Train: 2.9612, Loss Test: 4.2741, LR: 0.000030\n",
      "Step 1440, Loss Train: 2.8260, Loss Test: 2.3939, LR: 0.000030\n",
      "Step 1441, Loss Train: 2.8293, Loss Test: 2.7222, LR: 0.000030\n",
      "Step 1442, Loss Train: 2.8878, Loss Test: 2.7588, LR: 0.000030\n",
      "Step 1443, Loss Train: 2.9162, Loss Test: 3.3212, LR: 0.000030\n",
      "Step 1444, Loss Train: 2.9619, Loss Test: 4.2053, LR: 0.000030\n",
      "Step 1445, Loss Train: 2.8995, Loss Test: 4.0322, LR: 0.000030\n",
      "Step 1446, Loss Train: 2.9527, Loss Test: 3.5395, LR: 0.000030\n",
      "Step 1447, Loss Train: 3.0230, Loss Test: 3.0992, LR: 0.000030\n",
      "Step 1448, Loss Train: 3.0964, Loss Test: 3.2421, LR: 0.000030\n",
      "Step 1449, Loss Train: 2.8634, Loss Test: 3.5161, LR: 0.000030\n",
      "Step 1450, Loss Train: 2.9824, Loss Test: 3.8674, LR: 0.000030\n",
      "Step 1451, Loss Train: 2.8692, Loss Test: 3.8542, LR: 0.000030\n",
      "Step 1452, Loss Train: 3.2402, Loss Test: 3.1381, LR: 0.000030\n",
      "Step 1453, Loss Train: 3.2096, Loss Test: 4.0173, LR: 0.000030\n",
      "Step 1454, Loss Train: 3.0332, Loss Test: 3.6499, LR: 0.000030\n",
      "Step 1455, Loss Train: 3.0712, Loss Test: 3.2789, LR: 0.000030\n",
      "Step 1456, Loss Train: 2.9597, Loss Test: 4.0782, LR: 0.000030\n",
      "Step 1457, Loss Train: 2.9964, Loss Test: 2.2744, LR: 0.000030\n",
      "Step 1458, Loss Train: 3.0827, Loss Test: 4.0513, LR: 0.000030\n",
      "Step 1459, Loss Train: 2.9995, Loss Test: 3.2907, LR: 0.000030\n",
      "Step 1460, Loss Train: 3.1489, Loss Test: 2.8702, LR: 0.000030\n",
      "Step 1461, Loss Train: 2.9510, Loss Test: 4.2950, LR: 0.000030\n",
      "Step 1462, Loss Train: 2.9637, Loss Test: 4.3196, LR: 0.000030\n",
      "Step 1463, Loss Train: 3.0305, Loss Test: 2.3518, LR: 0.000030\n",
      "Step 1464, Loss Train: 3.1055, Loss Test: 3.8992, LR: 0.000030\n",
      "Step 1465, Loss Train: 2.9265, Loss Test: 3.9005, LR: 0.000030\n",
      "Step 1466, Loss Train: 3.0335, Loss Test: 4.2885, LR: 0.000030\n",
      "Step 1467, Loss Train: 2.9846, Loss Test: 2.9441, LR: 0.000030\n",
      "Step 1468, Loss Train: 2.9549, Loss Test: 3.5719, LR: 0.000030\n",
      "Step 1469, Loss Train: 2.9812, Loss Test: 3.8279, LR: 0.000030\n",
      "Step 1470, Loss Train: 2.8532, Loss Test: 4.2166, LR: 0.000030\n",
      "Step 1471, Loss Train: 2.9847, Loss Test: 3.9764, LR: 0.000030\n",
      "Step 1472, Loss Train: 2.8718, Loss Test: 3.4386, LR: 0.000030\n",
      "Step 1473, Loss Train: 2.8669, Loss Test: 3.6565, LR: 0.000030\n",
      "Step 1474, Loss Train: 3.0039, Loss Test: 3.8540, LR: 0.000030\n",
      "Step 1475, Loss Train: 3.0047, Loss Test: 3.9161, LR: 0.000030\n",
      "Step 1476, Loss Train: 2.8776, Loss Test: 3.7062, LR: 0.000030\n",
      "Step 1477, Loss Train: 2.9236, Loss Test: 4.1657, LR: 0.000030\n",
      "Step 1478, Loss Train: 2.9908, Loss Test: 4.1454, LR: 0.000030\n",
      "Step 1479, Loss Train: 2.9241, Loss Test: 3.9606, LR: 0.000030\n",
      "Step 1480, Loss Train: 2.8726, Loss Test: 3.9134, LR: 0.000030\n",
      "Step 1481, Loss Train: 3.1268, Loss Test: 3.5021, LR: 0.000030\n",
      "Step 1482, Loss Train: 2.9140, Loss Test: 3.7794, LR: 0.000030\n",
      "Step 1483, Loss Train: 2.8707, Loss Test: 4.4027, LR: 0.000030\n",
      "Step 1484, Loss Train: 2.9502, Loss Test: 4.1762, LR: 0.000030\n",
      "Step 1485, Loss Train: 3.0986, Loss Test: 3.8766, LR: 0.000030\n",
      "Step 1486, Loss Train: 3.0528, Loss Test: 3.9088, LR: 0.000030\n",
      "Step 1487, Loss Train: 3.0029, Loss Test: 3.3440, LR: 0.000030\n",
      "Step 1488, Loss Train: 2.8664, Loss Test: 4.2839, LR: 0.000030\n",
      "Step 1489, Loss Train: 2.9533, Loss Test: 3.8277, LR: 0.000030\n",
      "Step 1490, Loss Train: 2.9803, Loss Test: 4.2418, LR: 0.000030\n",
      "Step 1491, Loss Train: 2.9405, Loss Test: 3.4896, LR: 0.000030\n",
      "Step 1492, Loss Train: 2.9109, Loss Test: 3.4610, LR: 0.000030\n",
      "Step 1493, Loss Train: 3.0360, Loss Test: 3.9589, LR: 0.000030\n",
      "Step 1494, Loss Train: 2.7928, Loss Test: 4.3691, LR: 0.000030\n",
      "Step 1495, Loss Train: 2.9424, Loss Test: 3.7599, LR: 0.000030\n",
      "Step 1496, Loss Train: 2.9917, Loss Test: 2.8201, LR: 0.000030\n",
      "Step 1497, Loss Train: 2.8472, Loss Test: 3.7930, LR: 0.000030\n",
      "Step 1498, Loss Train: 2.9688, Loss Test: 3.9638, LR: 0.000030\n",
      "Step 1499, Loss Train: 2.9345, Loss Test: 3.8820, LR: 0.000030\n",
      "Step 1500, Loss Train: 3.1285, Loss Test: 3.7350, LR: 0.000030\n",
      "Step 1501, Loss Train: 3.0920, Loss Test: 3.7447, LR: 0.000030\n",
      "Step 1502, Loss Train: 2.9395, Loss Test: 4.1526, LR: 0.000030\n",
      "Step 1503, Loss Train: 2.8360, Loss Test: 4.1821, LR: 0.000030\n",
      "Step 1504, Loss Train: 2.9962, Loss Test: 3.6156, LR: 0.000030\n",
      "Step 1505, Loss Train: 3.1055, Loss Test: 3.7937, LR: 0.000030\n",
      "Step 1506, Loss Train: 2.9673, Loss Test: 3.8045, LR: 0.000030\n",
      "Step 1507, Loss Train: 3.0325, Loss Test: 3.7987, LR: 0.000030\n",
      "Step 1508, Loss Train: 3.0314, Loss Test: 3.5195, LR: 0.000030\n",
      "Step 1509, Loss Train: 3.0551, Loss Test: 4.4568, LR: 0.000030\n",
      "Step 1510, Loss Train: 3.0538, Loss Test: 4.2003, LR: 0.000030\n",
      "Step 1511, Loss Train: 2.9398, Loss Test: 4.0221, LR: 0.000030\n",
      "Step 1512, Loss Train: 2.9970, Loss Test: 3.7769, LR: 0.000030\n",
      "Step 1513, Loss Train: 3.1093, Loss Test: 3.7600, LR: 0.000030\n",
      "Step 1514, Loss Train: 2.9506, Loss Test: 3.2451, LR: 0.000030\n",
      "Step 1515, Loss Train: 2.8030, Loss Test: 2.8680, LR: 0.000030\n",
      "Step 1516, Loss Train: 3.0934, Loss Test: 4.5252, LR: 0.000030\n",
      "Step 1517, Loss Train: 2.9395, Loss Test: 3.8061, LR: 0.000030\n",
      "Step 1518, Loss Train: 3.0328, Loss Test: 4.2536, LR: 0.000030\n",
      "Step 1519, Loss Train: 3.0350, Loss Test: 3.9961, LR: 0.000030\n",
      "Step 1520, Loss Train: 2.9691, Loss Test: 3.1107, LR: 0.000030\n",
      "Step 1521, Loss Train: 2.8182, Loss Test: 3.1677, LR: 0.000030\n",
      "Step 1522, Loss Train: 2.8835, Loss Test: 4.1946, LR: 0.000030\n",
      "Step 1523, Loss Train: 3.1036, Loss Test: 4.2770, LR: 0.000030\n",
      "Step 1524, Loss Train: 2.7993, Loss Test: 2.5864, LR: 0.000030\n",
      "Step 1525, Loss Train: 3.0053, Loss Test: 3.2431, LR: 0.000030\n",
      "Step 1526, Loss Train: 2.9802, Loss Test: 4.4892, LR: 0.000030\n",
      "Step 1527, Loss Train: 2.9182, Loss Test: 3.9554, LR: 0.000030\n",
      "Step 1528, Loss Train: 2.9417, Loss Test: 4.0806, LR: 0.000030\n",
      "Step 1529, Loss Train: 2.8736, Loss Test: 3.8095, LR: 0.000030\n",
      "Step 1530, Loss Train: 2.9285, Loss Test: 3.4823, LR: 0.000030\n",
      "Step 1531, Loss Train: 2.9320, Loss Test: 4.0982, LR: 0.000030\n",
      "Step 1532, Loss Train: 2.8930, Loss Test: 4.0147, LR: 0.000030\n",
      "Step 1533, Loss Train: 2.8435, Loss Test: 3.6311, LR: 0.000030\n",
      "Step 1534, Loss Train: 2.8536, Loss Test: 4.0780, LR: 0.000030\n",
      "Step 1535, Loss Train: 2.8703, Loss Test: 0.7225, LR: 0.000030\n",
      "Step 1536, Loss Train: 3.0148, Loss Test: 2.8938, LR: 0.000030\n",
      "Step 1537, Loss Train: 3.0642, Loss Test: 3.2823, LR: 0.000030\n",
      "Step 1538, Loss Train: 2.9171, Loss Test: 3.6917, LR: 0.000030\n",
      "Step 1539, Loss Train: 2.8805, Loss Test: 4.0385, LR: 0.000030\n",
      "Step 1540, Loss Train: 2.8544, Loss Test: 3.9396, LR: 0.000030\n",
      "Step 1541, Loss Train: 3.0694, Loss Test: 4.0965, LR: 0.000030\n",
      "Step 1542, Loss Train: 2.8939, Loss Test: 4.2704, LR: 0.000030\n",
      "Step 1543, Loss Train: 2.9315, Loss Test: 4.3887, LR: 0.000030\n",
      "Step 1544, Loss Train: 3.2107, Loss Test: 2.2192, LR: 0.000030\n",
      "Step 1545, Loss Train: 2.9576, Loss Test: 2.2071, LR: 0.000030\n",
      "Step 1546, Loss Train: 3.0270, Loss Test: 3.5713, LR: 0.000030\n",
      "Step 1547, Loss Train: 2.9098, Loss Test: 3.3715, LR: 0.000030\n",
      "Step 1548, Loss Train: 3.0507, Loss Test: 3.8470, LR: 0.000030\n",
      "Step 1549, Loss Train: 3.0597, Loss Test: 4.3615, LR: 0.000030\n",
      "Step 1550, Loss Train: 2.9194, Loss Test: 3.6528, LR: 0.000030\n",
      "Step 1551, Loss Train: 3.1151, Loss Test: 4.1705, LR: 0.000030\n",
      "Step 1552, Loss Train: 2.9718, Loss Test: 3.9363, LR: 0.000030\n",
      "Step 1553, Loss Train: 2.9658, Loss Test: 3.9285, LR: 0.000030\n",
      "Step 1554, Loss Train: 3.0099, Loss Test: 4.3555, LR: 0.000030\n",
      "Step 1555, Loss Train: 3.1107, Loss Test: 3.8349, LR: 0.000030\n",
      "Step 1556, Loss Train: 3.0344, Loss Test: 3.9934, LR: 0.000030\n",
      "Step 1557, Loss Train: 3.1308, Loss Test: 4.2553, LR: 0.000030\n",
      "Step 1558, Loss Train: 3.0203, Loss Test: 4.5837, LR: 0.000030\n",
      "Step 1559, Loss Train: 2.8876, Loss Test: 4.4279, LR: 0.000030\n",
      "Step 1560, Loss Train: 3.0565, Loss Test: 3.8711, LR: 0.000030\n",
      "Step 1561, Loss Train: 3.1221, Loss Test: 4.2047, LR: 0.000030\n",
      "Step 1562, Loss Train: 2.9747, Loss Test: 3.7891, LR: 0.000030\n",
      "Step 1563, Loss Train: 3.0032, Loss Test: 3.8832, LR: 0.000030\n",
      "Step 1564, Loss Train: 3.0350, Loss Test: 2.0271, LR: 0.000030\n",
      "Step 1565, Loss Train: 2.8687, Loss Test: 3.7292, LR: 0.000030\n",
      "Step 1566, Loss Train: 2.9953, Loss Test: 4.0741, LR: 0.000030\n",
      "Step 1567, Loss Train: 2.7385, Loss Test: 4.2015, LR: 0.000030\n",
      "Step 1568, Loss Train: 3.1169, Loss Test: 3.8378, LR: 0.000030\n",
      "Step 1569, Loss Train: 2.9783, Loss Test: 4.0240, LR: 0.000030\n",
      "Step 1570, Loss Train: 2.7492, Loss Test: 2.8201, LR: 0.000030\n",
      "Step 1571, Loss Train: 2.9812, Loss Test: 4.1072, LR: 0.000030\n",
      "Step 1572, Loss Train: 2.9497, Loss Test: 4.3886, LR: 0.000030\n",
      "Step 1573, Loss Train: 2.8681, Loss Test: 2.3547, LR: 0.000030\n",
      "Step 1574, Loss Train: 3.0402, Loss Test: 4.0699, LR: 0.000030\n",
      "Step 1575, Loss Train: 2.9431, Loss Test: 4.3498, LR: 0.000030\n",
      "Step 1576, Loss Train: 2.9409, Loss Test: 3.9756, LR: 0.000030\n",
      "Step 1577, Loss Train: 2.8827, Loss Test: 3.5151, LR: 0.000030\n",
      "Step 1578, Loss Train: 3.0437, Loss Test: 3.7459, LR: 0.000030\n",
      "Step 1579, Loss Train: 2.9260, Loss Test: 3.5482, LR: 0.000030\n",
      "Step 1580, Loss Train: 3.0580, Loss Test: 3.5995, LR: 0.000030\n",
      "Step 1581, Loss Train: 2.8544, Loss Test: 3.9876, LR: 0.000030\n",
      "Step 1582, Loss Train: 2.6966, Loss Test: 2.9561, LR: 0.000030\n",
      "Step 1583, Loss Train: 2.9446, Loss Test: 2.4775, LR: 0.000030\n",
      "Step 1584, Loss Train: 2.7536, Loss Test: 4.2592, LR: 0.000030\n",
      "Step 1585, Loss Train: 3.1239, Loss Test: 3.6034, LR: 0.000030\n",
      "Step 1586, Loss Train: 2.9683, Loss Test: 3.1065, LR: 0.000030\n",
      "Step 1587, Loss Train: 3.0156, Loss Test: 3.4782, LR: 0.000030\n",
      "Step 1588, Loss Train: 3.0413, Loss Test: 4.1239, LR: 0.000030\n",
      "Step 1589, Loss Train: 2.8623, Loss Test: 4.1255, LR: 0.000030\n",
      "Step 1590, Loss Train: 3.0510, Loss Test: 4.3528, LR: 0.000030\n",
      "Step 1591, Loss Train: 2.9333, Loss Test: 4.2965, LR: 0.000030\n",
      "Step 1592, Loss Train: 2.9215, Loss Test: 3.8600, LR: 0.000030\n",
      "Step 1593, Loss Train: 2.8876, Loss Test: 4.0686, LR: 0.000030\n",
      "Step 1594, Loss Train: 2.8473, Loss Test: 4.1494, LR: 0.000030\n",
      "Step 1595, Loss Train: 2.8064, Loss Test: 3.9521, LR: 0.000030\n",
      "Step 1596, Loss Train: 3.1558, Loss Test: 3.9994, LR: 0.000030\n",
      "Step 1597, Loss Train: 2.6731, Loss Test: 3.6479, LR: 0.000030\n",
      "Step 1598, Loss Train: 3.0605, Loss Test: 3.1621, LR: 0.000030\n",
      "Step 1599, Loss Train: 3.0034, Loss Test: 4.6065, LR: 0.000030\n",
      "Step 1600, Loss Train: 2.9587, Loss Test: 3.6730, LR: 0.000030\n",
      "Step 1601, Loss Train: 3.1143, Loss Test: 3.7589, LR: 0.000030\n",
      "Step 1602, Loss Train: 2.8679, Loss Test: 2.5658, LR: 0.000030\n",
      "Step 1603, Loss Train: 3.0635, Loss Test: 3.9659, LR: 0.000030\n",
      "Step 1604, Loss Train: 2.9218, Loss Test: 3.7637, LR: 0.000030\n",
      "Step 1605, Loss Train: 2.7979, Loss Test: 4.2421, LR: 0.000030\n",
      "Step 1606, Loss Train: 3.0011, Loss Test: 4.4979, LR: 0.000030\n",
      "Step 1607, Loss Train: 3.0944, Loss Test: 4.2631, LR: 0.000030\n",
      "Step 1608, Loss Train: 3.0343, Loss Test: 3.9433, LR: 0.000030\n",
      "Step 1609, Loss Train: 3.1369, Loss Test: 3.4280, LR: 0.000030\n",
      "Step 1610, Loss Train: 2.9728, Loss Test: 4.4633, LR: 0.000030\n",
      "Step 1611, Loss Train: 2.9618, Loss Test: 3.6190, LR: 0.000030\n",
      "Step 1612, Loss Train: 3.0746, Loss Test: 3.5839, LR: 0.000030\n",
      "Step 1613, Loss Train: 3.0476, Loss Test: 4.4308, LR: 0.000030\n",
      "Step 1614, Loss Train: 3.1119, Loss Test: 4.3797, LR: 0.000030\n",
      "Step 1615, Loss Train: 2.7668, Loss Test: 3.8369, LR: 0.000030\n",
      "Step 1616, Loss Train: 2.9916, Loss Test: 4.1059, LR: 0.000030\n",
      "Step 1617, Loss Train: 2.9196, Loss Test: 4.1351, LR: 0.000030\n",
      "Step 1618, Loss Train: 2.9737, Loss Test: 2.3452, LR: 0.000030\n",
      "Step 1619, Loss Train: 2.9284, Loss Test: 3.6094, LR: 0.000030\n",
      "Step 1620, Loss Train: 2.8266, Loss Test: 4.0226, LR: 0.000030\n",
      "Step 1621, Loss Train: 2.9990, Loss Test: 3.7804, LR: 0.000030\n",
      "Step 1622, Loss Train: 2.8428, Loss Test: 1.2643, LR: 0.000030\n",
      "Step 1623, Loss Train: 2.9775, Loss Test: 3.8721, LR: 0.000030\n",
      "Step 1624, Loss Train: 2.8443, Loss Test: 3.6335, LR: 0.000030\n",
      "Step 1625, Loss Train: 2.9815, Loss Test: 3.8900, LR: 0.000030\n",
      "Step 1626, Loss Train: 2.9212, Loss Test: 3.2801, LR: 0.000030\n",
      "Step 1627, Loss Train: 2.9349, Loss Test: 3.7038, LR: 0.000030\n",
      "Step 1628, Loss Train: 3.0366, Loss Test: 4.1107, LR: 0.000030\n",
      "Step 1629, Loss Train: 2.9953, Loss Test: 3.0960, LR: 0.000030\n",
      "Step 1630, Loss Train: 3.0057, Loss Test: 4.3902, LR: 0.000030\n",
      "Step 1631, Loss Train: 2.9755, Loss Test: 2.1171, LR: 0.000030\n",
      "Step 1632, Loss Train: 2.9806, Loss Test: 2.4858, LR: 0.000030\n",
      "Step 1633, Loss Train: 2.9039, Loss Test: 3.2586, LR: 0.000030\n",
      "Step 1634, Loss Train: 3.1450, Loss Test: 3.8634, LR: 0.000030\n",
      "Step 1635, Loss Train: 2.9042, Loss Test: 3.0091, LR: 0.000030\n",
      "Step 1636, Loss Train: 2.9481, Loss Test: 3.8595, LR: 0.000030\n",
      "Step 1637, Loss Train: 3.1626, Loss Test: 3.7893, LR: 0.000030\n",
      "Step 1638, Loss Train: 2.8781, Loss Test: 2.7421, LR: 0.000030\n",
      "Step 1639, Loss Train: 3.0655, Loss Test: 3.9908, LR: 0.000030\n",
      "Step 1640, Loss Train: 2.8453, Loss Test: 4.0174, LR: 0.000030\n",
      "Step 1641, Loss Train: 3.1544, Loss Test: 4.1137, LR: 0.000030\n",
      "Step 1642, Loss Train: 2.8449, Loss Test: 3.6878, LR: 0.000030\n",
      "Step 1643, Loss Train: 2.9707, Loss Test: 3.9074, LR: 0.000030\n",
      "Step 1644, Loss Train: 3.0980, Loss Test: 3.7769, LR: 0.000030\n",
      "Step 1645, Loss Train: 2.9972, Loss Test: 3.0402, LR: 0.000030\n",
      "Step 1646, Loss Train: 2.9017, Loss Test: 3.9577, LR: 0.000030\n",
      "Step 1647, Loss Train: 3.0306, Loss Test: 3.3471, LR: 0.000030\n",
      "Step 1648, Loss Train: 2.8666, Loss Test: 3.6219, LR: 0.000030\n",
      "Step 1649, Loss Train: 2.9201, Loss Test: 4.1428, LR: 0.000030\n",
      "Step 1650, Loss Train: 3.1320, Loss Test: 4.5934, LR: 0.000030\n",
      "Step 1651, Loss Train: 2.9038, Loss Test: 3.6542, LR: 0.000030\n",
      "Step 1652, Loss Train: 2.8859, Loss Test: 4.0796, LR: 0.000030\n",
      "Step 1653, Loss Train: 2.7885, Loss Test: 2.5189, LR: 0.000030\n",
      "Step 1654, Loss Train: 3.0210, Loss Test: 3.1267, LR: 0.000030\n",
      "Step 1655, Loss Train: 3.0414, Loss Test: 3.6798, LR: 0.000030\n",
      "Step 1656, Loss Train: 3.0324, Loss Test: 3.9687, LR: 0.000030\n",
      "Step 1657, Loss Train: 2.8714, Loss Test: 4.2756, LR: 0.000030\n",
      "Step 1658, Loss Train: 2.9932, Loss Test: 2.7042, LR: 0.000030\n",
      "Step 1659, Loss Train: 3.1127, Loss Test: 4.2455, LR: 0.000030\n",
      "Step 1660, Loss Train: 3.0475, Loss Test: 3.9085, LR: 0.000030\n",
      "Step 1661, Loss Train: 3.0103, Loss Test: 3.7591, LR: 0.000030\n",
      "Step 1662, Loss Train: 2.9053, Loss Test: 3.5793, LR: 0.000030\n",
      "Step 1663, Loss Train: 3.0986, Loss Test: 4.2529, LR: 0.000030\n",
      "Step 1664, Loss Train: 3.0304, Loss Test: 3.1727, LR: 0.000030\n",
      "Step 1665, Loss Train: 2.9083, Loss Test: 3.4766, LR: 0.000030\n",
      "Step 1666, Loss Train: 2.9403, Loss Test: 3.3687, LR: 0.000030\n",
      "Step 1667, Loss Train: 3.0218, Loss Test: 3.6099, LR: 0.000030\n",
      "Step 1668, Loss Train: 2.8313, Loss Test: 3.7313, LR: 0.000030\n",
      "Step 1669, Loss Train: 2.9025, Loss Test: 3.6275, LR: 0.000030\n",
      "Step 1670, Loss Train: 2.7288, Loss Test: 3.3916, LR: 0.000030\n",
      "Step 1671, Loss Train: 2.8974, Loss Test: 4.0260, LR: 0.000030\n",
      "Step 1672, Loss Train: 2.8672, Loss Test: 3.7257, LR: 0.000030\n",
      "Step 1673, Loss Train: 2.9199, Loss Test: 3.8544, LR: 0.000030\n",
      "Step 1674, Loss Train: 2.8672, Loss Test: 2.5419, LR: 0.000030\n",
      "Step 1675, Loss Train: 2.8111, Loss Test: 3.1111, LR: 0.000030\n",
      "Step 1676, Loss Train: 3.0035, Loss Test: 3.8510, LR: 0.000030\n",
      "Step 1677, Loss Train: 3.0806, Loss Test: 4.1866, LR: 0.000030\n",
      "Step 1678, Loss Train: 3.0364, Loss Test: 3.3533, LR: 0.000030\n",
      "Step 1679, Loss Train: 2.8028, Loss Test: 4.1602, LR: 0.000030\n",
      "Step 1680, Loss Train: 2.9450, Loss Test: 3.1835, LR: 0.000030\n",
      "Step 1681, Loss Train: 2.9823, Loss Test: 3.4248, LR: 0.000030\n",
      "Step 1682, Loss Train: 2.8913, Loss Test: 3.9269, LR: 0.000030\n",
      "Step 1683, Loss Train: 3.0704, Loss Test: 4.2636, LR: 0.000030\n",
      "Step 1684, Loss Train: 3.0662, Loss Test: 3.9074, LR: 0.000030\n",
      "Step 1685, Loss Train: 2.8577, Loss Test: 3.7176, LR: 0.000030\n",
      "Step 1686, Loss Train: 2.9849, Loss Test: 4.2130, LR: 0.000030\n",
      "Step 1687, Loss Train: 3.0864, Loss Test: 2.3101, LR: 0.000030\n",
      "Step 1688, Loss Train: 3.0636, Loss Test: 3.7874, LR: 0.000030\n",
      "Step 1689, Loss Train: 3.0240, Loss Test: 4.9630, LR: 0.000030\n",
      "Step 1690, Loss Train: 3.0212, Loss Test: 4.2435, LR: 0.000030\n",
      "Step 1691, Loss Train: 3.1496, Loss Test: 3.9493, LR: 0.000030\n",
      "Step 1692, Loss Train: 2.9061, Loss Test: 3.9979, LR: 0.000030\n",
      "Step 1693, Loss Train: 2.9190, Loss Test: 3.4803, LR: 0.000030\n",
      "Step 1694, Loss Train: 2.9972, Loss Test: 3.7142, LR: 0.000030\n",
      "Step 1695, Loss Train: 2.6517, Loss Test: 3.9814, LR: 0.000030\n",
      "Step 1696, Loss Train: 2.8418, Loss Test: 3.7388, LR: 0.000030\n",
      "Step 1697, Loss Train: 2.9127, Loss Test: 4.0981, LR: 0.000030\n",
      "Step 1698, Loss Train: 2.8256, Loss Test: 3.2444, LR: 0.000030\n",
      "Step 1699, Loss Train: 3.1132, Loss Test: 3.9276, LR: 0.000030\n",
      "Step 1700, Loss Train: 3.1730, Loss Test: 2.8962, LR: 0.000030\n",
      "Step 1701, Loss Train: 2.9328, Loss Test: 3.0204, LR: 0.000030\n",
      "Step 1702, Loss Train: 2.9804, Loss Test: 3.5232, LR: 0.000030\n",
      "Step 1703, Loss Train: 3.0559, Loss Test: 3.1309, LR: 0.000030\n",
      "Step 1704, Loss Train: 2.8470, Loss Test: 3.9755, LR: 0.000030\n",
      "Step 1705, Loss Train: 3.0564, Loss Test: 3.7085, LR: 0.000030\n",
      "Step 1706, Loss Train: 3.0140, Loss Test: 3.6937, LR: 0.000030\n",
      "Step 1707, Loss Train: 3.1004, Loss Test: 3.7220, LR: 0.000030\n",
      "Step 1708, Loss Train: 2.9829, Loss Test: 3.4930, LR: 0.000030\n",
      "Step 1709, Loss Train: 3.0650, Loss Test: 4.2848, LR: 0.000030\n",
      "Step 1710, Loss Train: 2.9447, Loss Test: 3.9978, LR: 0.000030\n",
      "Step 1711, Loss Train: 2.9712, Loss Test: 3.9197, LR: 0.000030\n",
      "Step 1712, Loss Train: 3.1136, Loss Test: 3.8640, LR: 0.000030\n",
      "Step 1713, Loss Train: 2.9132, Loss Test: 3.9752, LR: 0.000030\n",
      "Step 1714, Loss Train: 2.7417, Loss Test: 1.7433, LR: 0.000030\n",
      "Step 1715, Loss Train: 2.8614, Loss Test: 3.9656, LR: 0.000030\n",
      "Step 1716, Loss Train: 2.9605, Loss Test: 3.5560, LR: 0.000030\n",
      "Step 1717, Loss Train: 2.7829, Loss Test: 4.2217, LR: 0.000030\n",
      "Step 1718, Loss Train: 2.9883, Loss Test: 3.5484, LR: 0.000030\n",
      "Step 1719, Loss Train: 2.8358, Loss Test: 3.9479, LR: 0.000030\n",
      "Step 1720, Loss Train: 2.9381, Loss Test: 4.0840, LR: 0.000030\n",
      "Step 1721, Loss Train: 2.9875, Loss Test: 3.8242, LR: 0.000030\n",
      "Step 1722, Loss Train: 2.9638, Loss Test: 3.8459, LR: 0.000030\n",
      "Step 1723, Loss Train: 3.0065, Loss Test: 3.8886, LR: 0.000030\n",
      "Step 1724, Loss Train: 2.9834, Loss Test: 4.0305, LR: 0.000030\n",
      "Step 1725, Loss Train: 2.6751, Loss Test: 4.4457, LR: 0.000030\n",
      "Step 1726, Loss Train: 2.8999, Loss Test: 3.8409, LR: 0.000030\n",
      "Step 1727, Loss Train: 2.8132, Loss Test: 3.0598, LR: 0.000030\n",
      "Step 1728, Loss Train: 2.7951, Loss Test: 4.2581, LR: 0.000030\n",
      "Step 1729, Loss Train: 3.0474, Loss Test: 4.0412, LR: 0.000030\n",
      "Step 1730, Loss Train: 2.8558, Loss Test: 2.7734, LR: 0.000030\n",
      "Step 1731, Loss Train: 2.9733, Loss Test: 2.9432, LR: 0.000030\n",
      "Step 1732, Loss Train: 2.9170, Loss Test: 2.5617, LR: 0.000030\n",
      "Step 1733, Loss Train: 2.7146, Loss Test: 3.7872, LR: 0.000030\n",
      "Step 1734, Loss Train: 2.8917, Loss Test: 3.0929, LR: 0.000030\n",
      "Step 1735, Loss Train: 2.9424, Loss Test: 3.9076, LR: 0.000030\n",
      "Step 1736, Loss Train: 2.9118, Loss Test: 4.2802, LR: 0.000030\n",
      "Step 1737, Loss Train: 3.0499, Loss Test: 3.3834, LR: 0.000030\n",
      "Step 1738, Loss Train: 3.0115, Loss Test: 3.6985, LR: 0.000030\n",
      "Step 1739, Loss Train: 3.1578, Loss Test: 3.4321, LR: 0.000030\n",
      "Step 1740, Loss Train: 2.9798, Loss Test: 4.0214, LR: 0.000030\n",
      "Step 1741, Loss Train: 2.9478, Loss Test: 4.1258, LR: 0.000030\n",
      "Step 1742, Loss Train: 2.8631, Loss Test: 3.4356, LR: 0.000030\n",
      "Step 1743, Loss Train: 2.9018, Loss Test: 3.8809, LR: 0.000030\n",
      "Step 1744, Loss Train: 2.9483, Loss Test: 3.9857, LR: 0.000030\n",
      "Step 1745, Loss Train: 3.0035, Loss Test: 4.2726, LR: 0.000030\n",
      "Step 1746, Loss Train: 2.8091, Loss Test: 2.6936, LR: 0.000030\n",
      "Step 1747, Loss Train: 2.9368, Loss Test: 3.9196, LR: 0.000030\n",
      "Step 1748, Loss Train: 2.9783, Loss Test: 3.3980, LR: 0.000030\n",
      "Step 1749, Loss Train: 2.9661, Loss Test: 3.6610, LR: 0.000030\n",
      "Step 1750, Loss Train: 3.1154, Loss Test: 2.6303, LR: 0.000030\n",
      "Step 1751, Loss Train: 3.0956, Loss Test: 4.0733, LR: 0.000030\n",
      "Step 1752, Loss Train: 2.9604, Loss Test: 4.1894, LR: 0.000030\n",
      "Step 1753, Loss Train: 2.8920, Loss Test: 3.1248, LR: 0.000030\n",
      "Step 1754, Loss Train: 3.2083, Loss Test: 3.5335, LR: 0.000030\n",
      "Step 1755, Loss Train: 3.0017, Loss Test: 2.3874, LR: 0.000030\n",
      "Step 1756, Loss Train: 2.7669, Loss Test: 4.5867, LR: 0.000030\n",
      "Step 1757, Loss Train: 2.8967, Loss Test: 3.9681, LR: 0.000030\n",
      "Step 1758, Loss Train: 3.0567, Loss Test: 4.0577, LR: 0.000030\n",
      "Step 1759, Loss Train: 3.0397, Loss Test: 4.0063, LR: 0.000030\n",
      "Step 1760, Loss Train: 3.0897, Loss Test: 3.6488, LR: 0.000030\n",
      "Step 1761, Loss Train: 2.8882, Loss Test: 3.1097, LR: 0.000030\n",
      "Step 1762, Loss Train: 2.9561, Loss Test: 3.1215, LR: 0.000030\n",
      "Step 1763, Loss Train: 2.9254, Loss Test: 3.4282, LR: 0.000030\n",
      "Step 1764, Loss Train: 2.9646, Loss Test: 3.4784, LR: 0.000030\n",
      "Step 1765, Loss Train: 2.8981, Loss Test: 2.6907, LR: 0.000030\n",
      "Step 1766, Loss Train: 2.9758, Loss Test: 3.7908, LR: 0.000030\n",
      "Step 1767, Loss Train: 2.9312, Loss Test: 3.9589, LR: 0.000030\n",
      "Step 1768, Loss Train: 2.9527, Loss Test: 4.3034, LR: 0.000030\n",
      "Step 1769, Loss Train: 3.1701, Loss Test: 3.6918, LR: 0.000030\n",
      "Step 1770, Loss Train: 2.9375, Loss Test: 3.4554, LR: 0.000030\n",
      "Step 1771, Loss Train: 2.9320, Loss Test: 1.9925, LR: 0.000030\n",
      "Step 1772, Loss Train: 2.8393, Loss Test: 4.1694, LR: 0.000030\n",
      "Step 1773, Loss Train: 2.9411, Loss Test: 4.1565, LR: 0.000030\n",
      "Step 1774, Loss Train: 2.8694, Loss Test: 4.0532, LR: 0.000030\n",
      "Step 1775, Loss Train: 2.9998, Loss Test: 3.3960, LR: 0.000030\n",
      "Step 1776, Loss Train: 3.1191, Loss Test: 3.8070, LR: 0.000030\n",
      "Step 1777, Loss Train: 2.8611, Loss Test: 4.2315, LR: 0.000030\n",
      "Step 1778, Loss Train: 3.0474, Loss Test: 3.4240, LR: 0.000030\n",
      "Step 1779, Loss Train: 2.9306, Loss Test: 3.5862, LR: 0.000030\n",
      "Step 1780, Loss Train: 3.0137, Loss Test: 4.0761, LR: 0.000030\n",
      "Step 1781, Loss Train: 2.9317, Loss Test: 3.0516, LR: 0.000030\n",
      "Step 1782, Loss Train: 3.0089, Loss Test: 2.0729, LR: 0.000030\n",
      "Step 1783, Loss Train: 3.0925, Loss Test: 4.1295, LR: 0.000030\n",
      "Step 1784, Loss Train: 3.0925, Loss Test: 2.4865, LR: 0.000030\n",
      "Step 1785, Loss Train: 2.7209, Loss Test: 3.4945, LR: 0.000030\n",
      "Step 1786, Loss Train: 3.0142, Loss Test: 2.8237, LR: 0.000030\n",
      "Step 1787, Loss Train: 2.9428, Loss Test: 4.2207, LR: 0.000030\n",
      "Step 1788, Loss Train: 2.9684, Loss Test: 4.4007, LR: 0.000030\n",
      "Step 1789, Loss Train: 2.9707, Loss Test: 4.2690, LR: 0.000030\n",
      "Step 1790, Loss Train: 2.9740, Loss Test: 3.7083, LR: 0.000030\n",
      "Step 1791, Loss Train: 2.8299, Loss Test: 4.0313, LR: 0.000030\n",
      "Step 1792, Loss Train: 2.9105, Loss Test: 3.6249, LR: 0.000030\n",
      "Step 1793, Loss Train: 2.8649, Loss Test: 3.9210, LR: 0.000030\n",
      "Step 1794, Loss Train: 3.0064, Loss Test: 3.4189, LR: 0.000030\n",
      "Step 1795, Loss Train: 2.9311, Loss Test: 4.0623, LR: 0.000030\n",
      "Step 1796, Loss Train: 2.6749, Loss Test: 4.0154, LR: 0.000030\n",
      "Step 1797, Loss Train: 2.8542, Loss Test: 4.3728, LR: 0.000030\n",
      "Step 1798, Loss Train: 2.8033, Loss Test: 4.3879, LR: 0.000030\n",
      "Step 1799, Loss Train: 2.8492, Loss Test: 4.3947, LR: 0.000030\n",
      "Step 1800, Loss Train: 3.0038, Loss Test: 2.8793, LR: 0.000030\n",
      "Step 1801, Loss Train: 2.8644, Loss Test: 3.9190, LR: 0.000030\n",
      "Step 1802, Loss Train: 2.8375, Loss Test: 2.7072, LR: 0.000030\n",
      "Step 1803, Loss Train: 2.8635, Loss Test: 2.6843, LR: 0.000030\n",
      "Step 1804, Loss Train: 2.8716, Loss Test: 4.1860, LR: 0.000030\n",
      "Step 1805, Loss Train: 2.9073, Loss Test: 4.4515, LR: 0.000030\n",
      "Step 1806, Loss Train: 2.8747, Loss Test: 2.9529, LR: 0.000030\n",
      "Step 1807, Loss Train: 2.9333, Loss Test: 2.7297, LR: 0.000029\n",
      "Step 1808, Loss Train: 2.9112, Loss Test: 3.5756, LR: 0.000029\n",
      "Step 1809, Loss Train: 2.9364, Loss Test: 3.2293, LR: 0.000029\n",
      "Step 1810, Loss Train: 2.9281, Loss Test: 3.0753, LR: 0.000029\n",
      "Step 1811, Loss Train: 2.9682, Loss Test: 2.6110, LR: 0.000029\n",
      "Step 1812, Loss Train: 3.0953, Loss Test: 4.5125, LR: 0.000029\n",
      "Step 1813, Loss Train: 2.8070, Loss Test: 2.5024, LR: 0.000029\n",
      "Step 1814, Loss Train: 3.0583, Loss Test: 3.8228, LR: 0.000029\n",
      "Step 1815, Loss Train: 3.1048, Loss Test: 3.8396, LR: 0.000029\n",
      "Step 1816, Loss Train: 2.9478, Loss Test: 3.2884, LR: 0.000029\n",
      "Step 1817, Loss Train: 3.0912, Loss Test: 3.2627, LR: 0.000029\n",
      "Step 1818, Loss Train: 2.8528, Loss Test: 3.6857, LR: 0.000029\n",
      "Step 1819, Loss Train: 2.8047, Loss Test: 3.8943, LR: 0.000029\n",
      "Step 1820, Loss Train: 2.8512, Loss Test: 3.5193, LR: 0.000029\n",
      "Step 1821, Loss Train: 2.8320, Loss Test: 4.1049, LR: 0.000029\n",
      "Step 1822, Loss Train: 2.8508, Loss Test: 3.9230, LR: 0.000029\n",
      "Step 1823, Loss Train: 3.0947, Loss Test: 3.5402, LR: 0.000029\n",
      "Step 1824, Loss Train: 2.9196, Loss Test: 4.2494, LR: 0.000029\n",
      "Step 1825, Loss Train: 2.8909, Loss Test: 3.8287, LR: 0.000029\n",
      "Step 1826, Loss Train: 2.7938, Loss Test: 3.1644, LR: 0.000029\n",
      "Step 1827, Loss Train: 2.9997, Loss Test: 3.9704, LR: 0.000029\n",
      "Step 1828, Loss Train: 2.8681, Loss Test: 3.3307, LR: 0.000029\n",
      "Step 1829, Loss Train: 2.7459, Loss Test: 3.7000, LR: 0.000029\n",
      "Step 1830, Loss Train: 3.0126, Loss Test: 2.9334, LR: 0.000029\n",
      "Step 1831, Loss Train: 2.8459, Loss Test: 3.6102, LR: 0.000029\n",
      "Step 1832, Loss Train: 2.9288, Loss Test: 3.7140, LR: 0.000029\n",
      "Step 1833, Loss Train: 2.9507, Loss Test: 3.8578, LR: 0.000029\n",
      "Step 1834, Loss Train: 3.0416, Loss Test: 3.7835, LR: 0.000029\n",
      "Step 1835, Loss Train: 3.0677, Loss Test: 3.8041, LR: 0.000029\n",
      "Step 1836, Loss Train: 3.0259, Loss Test: 3.5961, LR: 0.000029\n",
      "Step 1837, Loss Train: 2.8421, Loss Test: 3.7179, LR: 0.000029\n",
      "Step 1838, Loss Train: 2.9030, Loss Test: 4.8957, LR: 0.000029\n",
      "Step 1839, Loss Train: 2.8244, Loss Test: 3.5716, LR: 0.000029\n",
      "Step 1840, Loss Train: 2.7913, Loss Test: 3.7255, LR: 0.000029\n",
      "Step 1841, Loss Train: 2.8013, Loss Test: 3.6445, LR: 0.000029\n",
      "Step 1842, Loss Train: 2.9252, Loss Test: 3.4102, LR: 0.000029\n",
      "Step 1843, Loss Train: 2.9383, Loss Test: 3.7331, LR: 0.000029\n",
      "Step 1844, Loss Train: 2.9124, Loss Test: 4.2157, LR: 0.000029\n",
      "Step 1845, Loss Train: 2.9108, Loss Test: 3.6196, LR: 0.000029\n",
      "Step 1846, Loss Train: 2.8329, Loss Test: 3.9695, LR: 0.000029\n",
      "Step 1847, Loss Train: 2.7890, Loss Test: 3.4746, LR: 0.000029\n",
      "Step 1848, Loss Train: 2.9873, Loss Test: 3.2846, LR: 0.000029\n",
      "Step 1849, Loss Train: 2.7601, Loss Test: 4.1709, LR: 0.000029\n",
      "Step 1850, Loss Train: 2.9847, Loss Test: 4.0997, LR: 0.000029\n",
      "Step 1851, Loss Train: 3.2161, Loss Test: 3.7568, LR: 0.000029\n",
      "Step 1852, Loss Train: 2.9394, Loss Test: 2.0290, LR: 0.000029\n",
      "Step 1853, Loss Train: 3.0030, Loss Test: 3.0126, LR: 0.000029\n",
      "Step 1854, Loss Train: 2.9222, Loss Test: 3.9977, LR: 0.000029\n",
      "Step 1855, Loss Train: 2.9644, Loss Test: 3.7634, LR: 0.000029\n",
      "Step 1856, Loss Train: 2.9618, Loss Test: 4.3234, LR: 0.000029\n",
      "Step 1857, Loss Train: 2.9543, Loss Test: 3.5643, LR: 0.000029\n",
      "Step 1858, Loss Train: 2.7268, Loss Test: 3.5520, LR: 0.000029\n",
      "Step 1859, Loss Train: 3.0484, Loss Test: 4.3721, LR: 0.000029\n",
      "Step 1860, Loss Train: 2.9219, Loss Test: 3.9082, LR: 0.000029\n",
      "Step 1861, Loss Train: 2.9367, Loss Test: 3.4301, LR: 0.000029\n",
      "Step 1862, Loss Train: 2.8476, Loss Test: 3.7332, LR: 0.000029\n",
      "Step 1863, Loss Train: 2.9257, Loss Test: 3.9023, LR: 0.000029\n",
      "Step 1864, Loss Train: 3.0279, Loss Test: 3.4326, LR: 0.000029\n",
      "Step 1865, Loss Train: 2.8638, Loss Test: 3.0301, LR: 0.000029\n",
      "Step 1866, Loss Train: 2.9546, Loss Test: 2.7319, LR: 0.000029\n",
      "Step 1867, Loss Train: 2.8181, Loss Test: 4.1230, LR: 0.000029\n",
      "Step 1868, Loss Train: 2.8286, Loss Test: 2.7666, LR: 0.000029\n",
      "Step 1869, Loss Train: 2.9110, Loss Test: 4.1901, LR: 0.000029\n",
      "Step 1870, Loss Train: 2.7394, Loss Test: 3.9613, LR: 0.000029\n",
      "Step 1871, Loss Train: 2.8833, Loss Test: 3.8130, LR: 0.000029\n",
      "Step 1872, Loss Train: 2.9357, Loss Test: 3.7522, LR: 0.000029\n",
      "Step 1873, Loss Train: 2.8963, Loss Test: 3.8375, LR: 0.000029\n",
      "Step 1874, Loss Train: 2.9276, Loss Test: 4.0301, LR: 0.000029\n",
      "Step 1875, Loss Train: 2.9515, Loss Test: 3.6065, LR: 0.000029\n",
      "Step 1876, Loss Train: 2.8680, Loss Test: 3.9181, LR: 0.000029\n",
      "Step 1877, Loss Train: 3.0280, Loss Test: 4.0566, LR: 0.000029\n",
      "Step 1878, Loss Train: 2.9476, Loss Test: 3.9259, LR: 0.000029\n",
      "Step 1879, Loss Train: 2.8716, Loss Test: 3.7105, LR: 0.000029\n",
      "Step 1880, Loss Train: 3.0404, Loss Test: 2.4152, LR: 0.000029\n",
      "Step 1881, Loss Train: 2.8317, Loss Test: 3.1759, LR: 0.000029\n",
      "Step 1882, Loss Train: 2.9224, Loss Test: 3.5700, LR: 0.000029\n",
      "Step 1883, Loss Train: 3.1302, Loss Test: 3.7074, LR: 0.000029\n",
      "Step 1884, Loss Train: 2.9790, Loss Test: 3.7431, LR: 0.000029\n",
      "Step 1885, Loss Train: 2.9522, Loss Test: 3.0481, LR: 0.000029\n",
      "Step 1886, Loss Train: 3.0792, Loss Test: 3.7141, LR: 0.000029\n",
      "Step 1887, Loss Train: 2.9905, Loss Test: 2.0898, LR: 0.000029\n",
      "Step 1888, Loss Train: 3.0543, Loss Test: 3.7996, LR: 0.000029\n",
      "Step 1889, Loss Train: 2.8352, Loss Test: 3.4470, LR: 0.000029\n",
      "Step 1890, Loss Train: 2.8330, Loss Test: 3.2978, LR: 0.000029\n",
      "Step 1891, Loss Train: 2.8776, Loss Test: 4.4958, LR: 0.000029\n",
      "Step 1892, Loss Train: 2.9930, Loss Test: 3.5565, LR: 0.000029\n",
      "Step 1893, Loss Train: 2.8524, Loss Test: 4.1387, LR: 0.000029\n",
      "Step 1894, Loss Train: 3.0268, Loss Test: 3.7377, LR: 0.000029\n",
      "Step 1895, Loss Train: 2.9460, Loss Test: 4.2849, LR: 0.000029\n",
      "Step 1896, Loss Train: 2.9038, Loss Test: 4.1314, LR: 0.000029\n",
      "Step 1897, Loss Train: 2.9531, Loss Test: 3.5753, LR: 0.000029\n",
      "Step 1898, Loss Train: 2.9736, Loss Test: 3.9574, LR: 0.000029\n",
      "Step 1899, Loss Train: 2.8363, Loss Test: 4.0244, LR: 0.000029\n",
      "Step 1900, Loss Train: 3.0758, Loss Test: 4.0752, LR: 0.000029\n",
      "Step 1901, Loss Train: 3.0274, Loss Test: 3.7713, LR: 0.000029\n",
      "Step 1902, Loss Train: 2.8606, Loss Test: 4.0161, LR: 0.000029\n",
      "Step 1903, Loss Train: 2.9628, Loss Test: 3.8394, LR: 0.000029\n",
      "Step 1904, Loss Train: 3.1247, Loss Test: 4.2164, LR: 0.000029\n",
      "Step 1905, Loss Train: 2.9383, Loss Test: 3.6954, LR: 0.000029\n",
      "Step 1906, Loss Train: 2.9930, Loss Test: 3.7450, LR: 0.000029\n",
      "Step 1907, Loss Train: 2.9949, Loss Test: 3.5933, LR: 0.000029\n",
      "Step 1908, Loss Train: 3.0130, Loss Test: 2.9097, LR: 0.000029\n",
      "Step 1909, Loss Train: 3.0490, Loss Test: 3.7639, LR: 0.000029\n",
      "Step 1910, Loss Train: 2.8067, Loss Test: 3.5778, LR: 0.000029\n",
      "Step 1911, Loss Train: 2.8505, Loss Test: 4.3249, LR: 0.000029\n",
      "Step 1912, Loss Train: 3.0681, Loss Test: 3.4623, LR: 0.000029\n",
      "Step 1913, Loss Train: 2.8516, Loss Test: 3.1561, LR: 0.000029\n",
      "Step 1914, Loss Train: 2.8841, Loss Test: 4.2087, LR: 0.000029\n",
      "Step 1915, Loss Train: 2.8236, Loss Test: 3.6894, LR: 0.000029\n",
      "Step 1916, Loss Train: 2.8486, Loss Test: 2.8545, LR: 0.000029\n",
      "Step 1917, Loss Train: 3.0937, Loss Test: 3.9352, LR: 0.000029\n",
      "Step 1918, Loss Train: 2.9115, Loss Test: 3.9290, LR: 0.000029\n",
      "Step 1919, Loss Train: 2.9359, Loss Test: 3.6476, LR: 0.000029\n",
      "Step 1920, Loss Train: 2.9763, Loss Test: 3.5316, LR: 0.000029\n",
      "Step 1921, Loss Train: 2.8840, Loss Test: 3.8080, LR: 0.000029\n",
      "Step 1922, Loss Train: 3.0728, Loss Test: 3.5509, LR: 0.000029\n",
      "Step 1923, Loss Train: 3.1225, Loss Test: 4.2721, LR: 0.000029\n",
      "Step 1924, Loss Train: 2.9333, Loss Test: 2.0884, LR: 0.000029\n",
      "Step 1925, Loss Train: 3.0261, Loss Test: 3.6240, LR: 0.000029\n",
      "Step 1926, Loss Train: 2.9582, Loss Test: 3.3453, LR: 0.000029\n",
      "Step 1927, Loss Train: 2.9881, Loss Test: 3.6985, LR: 0.000029\n",
      "Step 1928, Loss Train: 3.0411, Loss Test: 3.5904, LR: 0.000029\n",
      "Step 1929, Loss Train: 2.8482, Loss Test: 3.5205, LR: 0.000029\n",
      "Step 1930, Loss Train: 2.9385, Loss Test: 3.8013, LR: 0.000029\n",
      "Step 1931, Loss Train: 2.9104, Loss Test: 3.5927, LR: 0.000029\n",
      "Step 1932, Loss Train: 2.9221, Loss Test: 3.2158, LR: 0.000029\n",
      "Step 1933, Loss Train: 2.8825, Loss Test: 3.0211, LR: 0.000029\n",
      "Step 1934, Loss Train: 3.0688, Loss Test: 1.3212, LR: 0.000029\n",
      "Step 1935, Loss Train: 2.9591, Loss Test: 4.1226, LR: 0.000029\n",
      "Step 1936, Loss Train: 2.7598, Loss Test: 4.1607, LR: 0.000029\n",
      "Step 1937, Loss Train: 2.9134, Loss Test: 4.3483, LR: 0.000029\n",
      "Step 1938, Loss Train: 3.0763, Loss Test: 2.7734, LR: 0.000029\n",
      "Step 1939, Loss Train: 2.8454, Loss Test: 2.4905, LR: 0.000029\n",
      "Step 1940, Loss Train: 2.9912, Loss Test: 2.6083, LR: 0.000029\n",
      "Step 1941, Loss Train: 2.7872, Loss Test: 3.7661, LR: 0.000029\n",
      "Step 1942, Loss Train: 2.9190, Loss Test: 3.7083, LR: 0.000029\n",
      "Step 1943, Loss Train: 2.6622, Loss Test: 4.5011, LR: 0.000029\n",
      "Step 1944, Loss Train: 3.0136, Loss Test: 3.9033, LR: 0.000029\n",
      "Step 1945, Loss Train: 2.7724, Loss Test: 3.4766, LR: 0.000029\n",
      "Step 1946, Loss Train: 2.8923, Loss Test: 3.6572, LR: 0.000029\n",
      "Step 1947, Loss Train: 2.9705, Loss Test: 3.7971, LR: 0.000029\n",
      "Step 1948, Loss Train: 3.0579, Loss Test: 3.3829, LR: 0.000029\n",
      "Step 1949, Loss Train: 2.8424, Loss Test: 4.0725, LR: 0.000029\n",
      "Step 1950, Loss Train: 2.8223, Loss Test: 4.1652, LR: 0.000029\n",
      "Step 1951, Loss Train: 2.9558, Loss Test: 3.9667, LR: 0.000029\n",
      "Step 1952, Loss Train: 3.1143, Loss Test: 3.4934, LR: 0.000029\n",
      "Step 1953, Loss Train: 2.9383, Loss Test: 4.0685, LR: 0.000029\n",
      "Step 1954, Loss Train: 2.8249, Loss Test: 3.1575, LR: 0.000029\n",
      "Step 1955, Loss Train: 3.0825, Loss Test: 3.9874, LR: 0.000029\n",
      "Step 1956, Loss Train: 2.9249, Loss Test: 3.8506, LR: 0.000029\n",
      "Step 1957, Loss Train: 2.9403, Loss Test: 3.1191, LR: 0.000029\n",
      "Step 1958, Loss Train: 2.8016, Loss Test: 3.7008, LR: 0.000029\n",
      "Step 1959, Loss Train: 2.8518, Loss Test: 3.9880, LR: 0.000029\n",
      "Step 1960, Loss Train: 2.7058, Loss Test: 4.0630, LR: 0.000029\n",
      "Step 1961, Loss Train: 3.1364, Loss Test: 3.9764, LR: 0.000029\n",
      "Step 1962, Loss Train: 2.9792, Loss Test: 3.5452, LR: 0.000029\n",
      "Step 1963, Loss Train: 3.0166, Loss Test: 3.4489, LR: 0.000029\n",
      "Step 1964, Loss Train: 2.9769, Loss Test: 2.9157, LR: 0.000029\n",
      "Step 1965, Loss Train: 3.0231, Loss Test: 4.3097, LR: 0.000029\n",
      "Step 1966, Loss Train: 2.9970, Loss Test: 4.3556, LR: 0.000029\n",
      "Step 1967, Loss Train: 2.8615, Loss Test: 4.1704, LR: 0.000029\n",
      "Step 1968, Loss Train: 2.9093, Loss Test: 1.0578, LR: 0.000029\n",
      "Step 1969, Loss Train: 2.9794, Loss Test: 4.2560, LR: 0.000029\n",
      "Step 1970, Loss Train: 3.0304, Loss Test: 3.2230, LR: 0.000029\n",
      "Step 1971, Loss Train: 2.8315, Loss Test: 3.6893, LR: 0.000029\n",
      "Step 1972, Loss Train: 2.9208, Loss Test: 4.2588, LR: 0.000029\n",
      "Step 1973, Loss Train: 2.9826, Loss Test: 4.1806, LR: 0.000029\n",
      "Step 1974, Loss Train: 2.9845, Loss Test: 3.0402, LR: 0.000029\n",
      "Step 1975, Loss Train: 3.0480, Loss Test: 4.1153, LR: 0.000029\n",
      "Step 1976, Loss Train: 2.8978, Loss Test: 3.9570, LR: 0.000029\n",
      "Step 1977, Loss Train: 3.0690, Loss Test: 3.8566, LR: 0.000029\n",
      "Step 1978, Loss Train: 2.8095, Loss Test: 3.3559, LR: 0.000029\n",
      "Step 1979, Loss Train: 3.1394, Loss Test: 3.8956, LR: 0.000029\n",
      "Step 1980, Loss Train: 3.0607, Loss Test: 3.9733, LR: 0.000029\n",
      "Step 1981, Loss Train: 3.0660, Loss Test: 3.5408, LR: 0.000029\n",
      "Step 1982, Loss Train: 2.9801, Loss Test: 3.3290, LR: 0.000029\n",
      "Step 1983, Loss Train: 2.8170, Loss Test: 4.2506, LR: 0.000029\n",
      "Step 1984, Loss Train: 2.9157, Loss Test: 2.8489, LR: 0.000029\n",
      "Step 1985, Loss Train: 3.0499, Loss Test: 3.7114, LR: 0.000029\n",
      "Step 1986, Loss Train: 3.1126, Loss Test: 3.7876, LR: 0.000029\n",
      "Step 1987, Loss Train: 2.9398, Loss Test: 4.0586, LR: 0.000029\n",
      "Step 1988, Loss Train: 2.9724, Loss Test: 3.5174, LR: 0.000029\n",
      "Step 1989, Loss Train: 2.9612, Loss Test: 3.7123, LR: 0.000029\n",
      "Step 1990, Loss Train: 3.0671, Loss Test: 3.7714, LR: 0.000029\n",
      "Step 1991, Loss Train: 3.0988, Loss Test: 3.6991, LR: 0.000029\n",
      "Step 1992, Loss Train: 3.1307, Loss Test: 3.5034, LR: 0.000029\n",
      "Step 1993, Loss Train: 2.8585, Loss Test: 4.0399, LR: 0.000029\n",
      "Step 1994, Loss Train: 3.0306, Loss Test: 4.0027, LR: 0.000029\n",
      "Step 1995, Loss Train: 2.8123, Loss Test: 4.1159, LR: 0.000029\n",
      "Step 1996, Loss Train: 3.0526, Loss Test: 3.9438, LR: 0.000029\n",
      "Step 1997, Loss Train: 3.1810, Loss Test: 4.1360, LR: 0.000029\n",
      "Step 1998, Loss Train: 3.0851, Loss Test: 3.5298, LR: 0.000029\n",
      "Step 1999, Loss Train: 3.0355, Loss Test: 4.1693, LR: 0.000029\n",
      "Step 2000, Loss Train: 2.8609, Loss Test: 1.3647, LR: 0.000029\n",
      "Step 2001, Loss Train: 2.9311, Loss Test: 4.5183, LR: 0.000029\n",
      "Step 2002, Loss Train: 3.1678, Loss Test: 4.0989, LR: 0.000029\n",
      "Step 2003, Loss Train: 2.8929, Loss Test: 2.5118, LR: 0.000029\n",
      "Step 2004, Loss Train: 2.9086, Loss Test: 3.4742, LR: 0.000029\n",
      "Step 2005, Loss Train: 3.0550, Loss Test: 3.0940, LR: 0.000029\n",
      "Step 2006, Loss Train: 3.0537, Loss Test: 4.2855, LR: 0.000029\n",
      "Step 2007, Loss Train: 3.0989, Loss Test: 3.8480, LR: 0.000029\n",
      "Step 2008, Loss Train: 2.8294, Loss Test: 3.4710, LR: 0.000029\n",
      "Step 2009, Loss Train: 2.9278, Loss Test: 3.7030, LR: 0.000029\n",
      "Step 2010, Loss Train: 2.8408, Loss Test: 3.1079, LR: 0.000029\n",
      "Step 2011, Loss Train: 2.9723, Loss Test: 4.0532, LR: 0.000029\n",
      "Step 2012, Loss Train: 2.9382, Loss Test: 4.2975, LR: 0.000029\n",
      "Step 2013, Loss Train: 2.9620, Loss Test: 3.7776, LR: 0.000029\n",
      "Step 2014, Loss Train: 2.9164, Loss Test: 3.7172, LR: 0.000029\n",
      "Step 2015, Loss Train: 3.0067, Loss Test: 3.5641, LR: 0.000029\n",
      "Step 2016, Loss Train: 3.0042, Loss Test: 3.2473, LR: 0.000029\n",
      "Step 2017, Loss Train: 3.1660, Loss Test: 3.5222, LR: 0.000029\n",
      "Step 2018, Loss Train: 3.0678, Loss Test: 3.2381, LR: 0.000029\n",
      "Step 2019, Loss Train: 3.0913, Loss Test: 4.2590, LR: 0.000029\n",
      "Step 2020, Loss Train: 2.9870, Loss Test: 3.3981, LR: 0.000029\n",
      "Step 2021, Loss Train: 2.9471, Loss Test: 3.9284, LR: 0.000029\n",
      "Step 2022, Loss Train: 3.0428, Loss Test: 3.9099, LR: 0.000029\n",
      "Step 2023, Loss Train: 3.0122, Loss Test: 3.6540, LR: 0.000029\n",
      "Step 2024, Loss Train: 2.9605, Loss Test: 3.7841, LR: 0.000029\n",
      "Step 2025, Loss Train: 3.0920, Loss Test: 3.7614, LR: 0.000029\n",
      "Step 2026, Loss Train: 2.7989, Loss Test: 3.6644, LR: 0.000029\n",
      "Step 2027, Loss Train: 3.0041, Loss Test: 3.0113, LR: 0.000029\n",
      "Step 2028, Loss Train: 2.9359, Loss Test: 4.0063, LR: 0.000029\n",
      "Step 2029, Loss Train: 2.9036, Loss Test: 4.2457, LR: 0.000029\n",
      "Step 2030, Loss Train: 3.1668, Loss Test: 3.6950, LR: 0.000029\n",
      "Step 2031, Loss Train: 2.8568, Loss Test: 4.1997, LR: 0.000029\n",
      "Step 2032, Loss Train: 2.8772, Loss Test: 3.7083, LR: 0.000029\n",
      "Step 2033, Loss Train: 2.9546, Loss Test: 3.7795, LR: 0.000029\n",
      "Step 2034, Loss Train: 2.9794, Loss Test: 3.8491, LR: 0.000029\n",
      "Step 2035, Loss Train: 2.9827, Loss Test: 3.8347, LR: 0.000029\n",
      "Step 2036, Loss Train: 3.0514, Loss Test: 3.9153, LR: 0.000029\n",
      "Step 2037, Loss Train: 2.9287, Loss Test: 4.2733, LR: 0.000029\n",
      "Step 2038, Loss Train: 3.0157, Loss Test: 3.4905, LR: 0.000029\n",
      "Step 2039, Loss Train: 3.0814, Loss Test: 2.1181, LR: 0.000029\n",
      "Step 2040, Loss Train: 3.0809, Loss Test: 3.7291, LR: 0.000029\n",
      "Step 2041, Loss Train: 2.8192, Loss Test: 4.1754, LR: 0.000029\n",
      "Step 2042, Loss Train: 2.9123, Loss Test: 4.1266, LR: 0.000029\n",
      "Step 2043, Loss Train: 2.9679, Loss Test: 4.0230, LR: 0.000029\n",
      "Step 2044, Loss Train: 2.9355, Loss Test: 3.4204, LR: 0.000029\n",
      "Step 2045, Loss Train: 3.0230, Loss Test: 4.1865, LR: 0.000029\n",
      "Step 2046, Loss Train: 2.7393, Loss Test: 3.6557, LR: 0.000029\n",
      "Step 2047, Loss Train: 3.0282, Loss Test: 3.8210, LR: 0.000029\n",
      "Step 2048, Loss Train: 2.9945, Loss Test: 4.6893, LR: 0.000029\n",
      "Step 2049, Loss Train: 3.0004, Loss Test: 3.3302, LR: 0.000029\n",
      "Step 2050, Loss Train: 3.0329, Loss Test: 3.9402, LR: 0.000029\n",
      "Step 2051, Loss Train: 2.8812, Loss Test: 3.4117, LR: 0.000029\n",
      "Step 2052, Loss Train: 2.8482, Loss Test: 3.7268, LR: 0.000029\n",
      "Step 2053, Loss Train: 2.9289, Loss Test: 4.1904, LR: 0.000029\n",
      "Step 2054, Loss Train: 3.0055, Loss Test: 3.6003, LR: 0.000029\n",
      "Step 2055, Loss Train: 2.9712, Loss Test: 3.4269, LR: 0.000029\n",
      "Step 2056, Loss Train: 2.9030, Loss Test: 3.8845, LR: 0.000029\n",
      "Step 2057, Loss Train: 2.7055, Loss Test: 2.4862, LR: 0.000029\n",
      "Step 2058, Loss Train: 3.0631, Loss Test: 4.0294, LR: 0.000029\n",
      "Step 2059, Loss Train: 3.1140, Loss Test: 4.1294, LR: 0.000029\n",
      "Step 2060, Loss Train: 3.0776, Loss Test: 3.3045, LR: 0.000029\n",
      "Step 2061, Loss Train: 2.9772, Loss Test: 3.9930, LR: 0.000029\n",
      "Step 2062, Loss Train: 2.9547, Loss Test: 3.7879, LR: 0.000029\n",
      "Step 2063, Loss Train: 3.0861, Loss Test: 3.9145, LR: 0.000029\n",
      "Step 2064, Loss Train: 3.0116, Loss Test: 3.1216, LR: 0.000029\n",
      "Step 2065, Loss Train: 2.7388, Loss Test: 4.2010, LR: 0.000029\n",
      "Step 2066, Loss Train: 2.8851, Loss Test: 3.7420, LR: 0.000029\n",
      "Step 2067, Loss Train: 2.8781, Loss Test: 3.7705, LR: 0.000029\n",
      "Step 2068, Loss Train: 2.9313, Loss Test: 3.1520, LR: 0.000029\n",
      "Step 2069, Loss Train: 2.9735, Loss Test: 4.1023, LR: 0.000029\n",
      "Step 2070, Loss Train: 2.8940, Loss Test: 3.9018, LR: 0.000029\n",
      "Step 2071, Loss Train: 2.9019, Loss Test: 2.3718, LR: 0.000029\n",
      "Step 2072, Loss Train: 3.0888, Loss Test: 2.8894, LR: 0.000029\n",
      "Step 2073, Loss Train: 2.9894, Loss Test: 4.2744, LR: 0.000029\n",
      "Step 2074, Loss Train: 2.8482, Loss Test: 2.2937, LR: 0.000029\n",
      "Step 2075, Loss Train: 3.1673, Loss Test: 4.0717, LR: 0.000029\n",
      "Step 2076, Loss Train: 2.8808, Loss Test: 3.2385, LR: 0.000029\n",
      "Step 2077, Loss Train: 2.9194, Loss Test: 4.0478, LR: 0.000029\n",
      "Step 2078, Loss Train: 2.9910, Loss Test: 3.9633, LR: 0.000029\n",
      "Step 2079, Loss Train: 3.0298, Loss Test: 3.2929, LR: 0.000029\n",
      "Step 2080, Loss Train: 2.9343, Loss Test: 2.3071, LR: 0.000029\n",
      "Step 2081, Loss Train: 2.9809, Loss Test: 3.4982, LR: 0.000029\n",
      "Step 2082, Loss Train: 2.7937, Loss Test: 3.6290, LR: 0.000029\n",
      "Step 2083, Loss Train: 3.1502, Loss Test: 2.6935, LR: 0.000029\n",
      "Step 2084, Loss Train: 3.0666, Loss Test: 3.9325, LR: 0.000029\n",
      "Step 2085, Loss Train: 3.0021, Loss Test: 4.2771, LR: 0.000029\n",
      "Step 2086, Loss Train: 2.9496, Loss Test: 4.0332, LR: 0.000029\n",
      "Step 2087, Loss Train: 2.9075, Loss Test: 4.0318, LR: 0.000029\n",
      "Step 2088, Loss Train: 3.0146, Loss Test: 3.5103, LR: 0.000029\n",
      "Step 2089, Loss Train: 3.1383, Loss Test: 4.1978, LR: 0.000029\n",
      "Step 2090, Loss Train: 2.9150, Loss Test: 3.6348, LR: 0.000029\n",
      "Step 2091, Loss Train: 2.8404, Loss Test: 3.9653, LR: 0.000029\n",
      "Step 2092, Loss Train: 3.0898, Loss Test: 3.4732, LR: 0.000029\n",
      "Step 2093, Loss Train: 2.7588, Loss Test: 3.7709, LR: 0.000029\n",
      "Step 2094, Loss Train: 2.9331, Loss Test: 3.5192, LR: 0.000029\n",
      "Step 2095, Loss Train: 2.9951, Loss Test: 3.2915, LR: 0.000029\n",
      "Step 2096, Loss Train: 2.9459, Loss Test: 4.0010, LR: 0.000029\n",
      "Step 2097, Loss Train: 3.1492, Loss Test: 3.7359, LR: 0.000029\n",
      "Step 2098, Loss Train: 3.0560, Loss Test: 3.9731, LR: 0.000029\n",
      "Step 2099, Loss Train: 2.9717, Loss Test: 3.8410, LR: 0.000029\n",
      "Step 2100, Loss Train: 3.0005, Loss Test: 3.7565, LR: 0.000029\n",
      "Step 2101, Loss Train: 2.9585, Loss Test: 3.9481, LR: 0.000029\n",
      "Step 2102, Loss Train: 2.8418, Loss Test: 2.7041, LR: 0.000029\n",
      "Step 2103, Loss Train: 2.9363, Loss Test: 3.9345, LR: 0.000029\n",
      "Step 2104, Loss Train: 2.9012, Loss Test: 3.9766, LR: 0.000029\n",
      "Step 2105, Loss Train: 2.9055, Loss Test: 3.7931, LR: 0.000029\n",
      "Step 2106, Loss Train: 2.9569, Loss Test: 4.1886, LR: 0.000029\n",
      "Step 2107, Loss Train: 3.0396, Loss Test: 4.0723, LR: 0.000029\n",
      "Step 2108, Loss Train: 2.8404, Loss Test: 2.8991, LR: 0.000029\n",
      "Step 2109, Loss Train: 2.8345, Loss Test: 2.4238, LR: 0.000029\n",
      "Step 2110, Loss Train: 2.9583, Loss Test: 4.0486, LR: 0.000029\n",
      "Step 2111, Loss Train: 3.0763, Loss Test: 3.2418, LR: 0.000029\n",
      "Step 2112, Loss Train: 2.8321, Loss Test: 2.5598, LR: 0.000029\n",
      "Step 2113, Loss Train: 2.8831, Loss Test: 3.4234, LR: 0.000029\n",
      "Step 2114, Loss Train: 3.1506, Loss Test: 4.1157, LR: 0.000029\n",
      "Step 2115, Loss Train: 3.0024, Loss Test: 4.0327, LR: 0.000029\n",
      "Step 2116, Loss Train: 2.8623, Loss Test: 4.2770, LR: 0.000029\n",
      "Step 2117, Loss Train: 3.0110, Loss Test: 3.7941, LR: 0.000029\n",
      "Step 2118, Loss Train: 2.9448, Loss Test: 3.8326, LR: 0.000029\n",
      "Step 2119, Loss Train: 2.7945, Loss Test: 4.1770, LR: 0.000029\n",
      "Step 2120, Loss Train: 3.1147, Loss Test: 3.9001, LR: 0.000029\n",
      "Step 2121, Loss Train: 2.9242, Loss Test: 3.7048, LR: 0.000029\n",
      "Step 2122, Loss Train: 2.9926, Loss Test: 4.6628, LR: 0.000029\n",
      "Step 2123, Loss Train: 2.8659, Loss Test: 1.6698, LR: 0.000029\n",
      "Step 2124, Loss Train: 2.9840, Loss Test: 3.7484, LR: 0.000029\n",
      "Step 2125, Loss Train: 3.0383, Loss Test: 3.7735, LR: 0.000029\n",
      "Step 2126, Loss Train: 2.7843, Loss Test: 3.5054, LR: 0.000029\n",
      "Step 2127, Loss Train: 2.9895, Loss Test: 3.8838, LR: 0.000029\n",
      "Step 2128, Loss Train: 2.8102, Loss Test: 2.8400, LR: 0.000029\n",
      "Step 2129, Loss Train: 2.9868, Loss Test: 4.3035, LR: 0.000029\n",
      "Step 2130, Loss Train: 3.1512, Loss Test: 3.9900, LR: 0.000029\n",
      "Step 2131, Loss Train: 2.8665, Loss Test: 3.3759, LR: 0.000029\n",
      "Step 2132, Loss Train: 2.9684, Loss Test: 4.9195, LR: 0.000029\n",
      "Step 2133, Loss Train: 2.9736, Loss Test: 3.7530, LR: 0.000029\n",
      "Step 2134, Loss Train: 3.0478, Loss Test: 3.7229, LR: 0.000029\n",
      "Step 2135, Loss Train: 2.7740, Loss Test: 4.0905, LR: 0.000029\n",
      "Step 2136, Loss Train: 2.9587, Loss Test: 3.8707, LR: 0.000029\n",
      "Step 2137, Loss Train: 3.0711, Loss Test: 3.5106, LR: 0.000029\n",
      "Step 2138, Loss Train: 2.8664, Loss Test: 4.1680, LR: 0.000029\n",
      "Step 2139, Loss Train: 3.0890, Loss Test: 4.0945, LR: 0.000029\n",
      "Step 2140, Loss Train: 2.9735, Loss Test: 3.8716, LR: 0.000029\n",
      "Step 2141, Loss Train: 2.9732, Loss Test: 3.1602, LR: 0.000029\n",
      "Step 2142, Loss Train: 3.0681, Loss Test: 3.7297, LR: 0.000029\n",
      "Step 2143, Loss Train: 2.9238, Loss Test: 3.9329, LR: 0.000029\n",
      "Step 2144, Loss Train: 2.9803, Loss Test: 4.1541, LR: 0.000029\n",
      "Step 2145, Loss Train: 2.9425, Loss Test: 3.9014, LR: 0.000029\n",
      "Step 2146, Loss Train: 3.2154, Loss Test: 3.6710, LR: 0.000029\n",
      "Step 2147, Loss Train: 2.8991, Loss Test: 4.4082, LR: 0.000029\n",
      "Step 2148, Loss Train: 2.9738, Loss Test: 4.1294, LR: 0.000029\n",
      "Step 2149, Loss Train: 2.8020, Loss Test: 3.4744, LR: 0.000029\n",
      "Step 2150, Loss Train: 2.9304, Loss Test: 4.1639, LR: 0.000029\n",
      "Step 2151, Loss Train: 2.9200, Loss Test: 3.9131, LR: 0.000029\n",
      "Step 2152, Loss Train: 2.9507, Loss Test: 3.9884, LR: 0.000029\n",
      "Step 2153, Loss Train: 2.9843, Loss Test: 3.9926, LR: 0.000029\n",
      "Step 2154, Loss Train: 3.1101, Loss Test: 3.9821, LR: 0.000029\n",
      "Step 2155, Loss Train: 2.9226, Loss Test: 3.9500, LR: 0.000029\n",
      "Step 2156, Loss Train: 2.9610, Loss Test: 4.1321, LR: 0.000029\n",
      "Step 2157, Loss Train: 3.1260, Loss Test: 3.4085, LR: 0.000029\n",
      "Step 2158, Loss Train: 3.0257, Loss Test: 3.6195, LR: 0.000029\n",
      "Step 2159, Loss Train: 2.9064, Loss Test: 2.7747, LR: 0.000029\n",
      "Step 2160, Loss Train: 2.9865, Loss Test: 2.9841, LR: 0.000029\n",
      "Step 2161, Loss Train: 2.9802, Loss Test: 2.8673, LR: 0.000029\n",
      "Step 2162, Loss Train: 2.9628, Loss Test: 4.4148, LR: 0.000029\n",
      "Step 2163, Loss Train: 2.8830, Loss Test: 2.7216, LR: 0.000029\n",
      "Step 2164, Loss Train: 2.8538, Loss Test: 4.2844, LR: 0.000029\n",
      "Step 2165, Loss Train: 2.8196, Loss Test: 4.0907, LR: 0.000029\n",
      "Step 2166, Loss Train: 2.8292, Loss Test: 3.6183, LR: 0.000029\n",
      "Step 2167, Loss Train: 2.9755, Loss Test: 3.9302, LR: 0.000029\n",
      "Step 2168, Loss Train: 2.9753, Loss Test: 3.6055, LR: 0.000029\n",
      "Step 2169, Loss Train: 2.8460, Loss Test: 4.1858, LR: 0.000029\n",
      "Step 2170, Loss Train: 3.0079, Loss Test: 3.9749, LR: 0.000029\n",
      "Step 2171, Loss Train: 2.8653, Loss Test: 3.7280, LR: 0.000029\n",
      "Step 2172, Loss Train: 2.9814, Loss Test: 3.3826, LR: 0.000029\n",
      "Step 2173, Loss Train: 2.7875, Loss Test: 3.6220, LR: 0.000029\n",
      "Step 2174, Loss Train: 2.9179, Loss Test: 3.1431, LR: 0.000029\n",
      "Step 2175, Loss Train: 3.0512, Loss Test: 4.2851, LR: 0.000029\n",
      "Step 2176, Loss Train: 3.1644, Loss Test: 3.5752, LR: 0.000029\n",
      "Step 2177, Loss Train: 3.0371, Loss Test: 2.6086, LR: 0.000029\n",
      "Step 2178, Loss Train: 2.9310, Loss Test: 2.7152, LR: 0.000029\n",
      "Step 2179, Loss Train: 2.9528, Loss Test: 2.2479, LR: 0.000029\n",
      "Step 2180, Loss Train: 3.0465, Loss Test: 3.2554, LR: 0.000029\n",
      "Step 2181, Loss Train: 3.0058, Loss Test: 3.1167, LR: 0.000029\n",
      "Step 2182, Loss Train: 3.0163, Loss Test: 4.0302, LR: 0.000029\n",
      "Step 2183, Loss Train: 2.7367, Loss Test: 3.9083, LR: 0.000029\n",
      "Step 2184, Loss Train: 2.8071, Loss Test: 3.2146, LR: 0.000029\n",
      "Step 2185, Loss Train: 3.0376, Loss Test: 4.2765, LR: 0.000029\n",
      "Step 2186, Loss Train: 2.8972, Loss Test: 2.9965, LR: 0.000029\n",
      "Step 2187, Loss Train: 2.9719, Loss Test: 3.8697, LR: 0.000029\n",
      "Step 2188, Loss Train: 3.0229, Loss Test: 4.5188, LR: 0.000029\n",
      "Step 2189, Loss Train: 3.1383, Loss Test: 3.7324, LR: 0.000029\n",
      "Step 2190, Loss Train: 2.7023, Loss Test: 3.7168, LR: 0.000029\n",
      "Step 2191, Loss Train: 2.7345, Loss Test: 4.4439, LR: 0.000029\n",
      "Step 2192, Loss Train: 2.8109, Loss Test: 3.6831, LR: 0.000029\n",
      "Step 2193, Loss Train: 3.0174, Loss Test: 4.7237, LR: 0.000029\n",
      "Step 2194, Loss Train: 2.7972, Loss Test: 3.1134, LR: 0.000029\n",
      "Step 2195, Loss Train: 3.0076, Loss Test: 3.1321, LR: 0.000029\n",
      "Step 2196, Loss Train: 3.0169, Loss Test: 3.9184, LR: 0.000029\n",
      "Step 2197, Loss Train: 2.9646, Loss Test: 3.4318, LR: 0.000029\n",
      "Step 2198, Loss Train: 2.9683, Loss Test: 3.8548, LR: 0.000029\n",
      "Step 2199, Loss Train: 3.0677, Loss Test: 4.3676, LR: 0.000029\n",
      "Step 2200, Loss Train: 2.8784, Loss Test: 3.9337, LR: 0.000029\n",
      "Step 2201, Loss Train: 2.9764, Loss Test: 3.7371, LR: 0.000029\n",
      "Step 2202, Loss Train: 2.9630, Loss Test: 2.9380, LR: 0.000029\n",
      "Step 2203, Loss Train: 2.9693, Loss Test: 4.1142, LR: 0.000029\n",
      "Step 2204, Loss Train: 2.9068, Loss Test: 2.5229, LR: 0.000029\n",
      "Step 2205, Loss Train: 2.9519, Loss Test: 3.9323, LR: 0.000029\n",
      "Step 2206, Loss Train: 3.0464, Loss Test: 4.1977, LR: 0.000029\n",
      "Step 2207, Loss Train: 2.8968, Loss Test: 3.8674, LR: 0.000029\n",
      "Step 2208, Loss Train: 2.9321, Loss Test: 3.9909, LR: 0.000029\n",
      "Step 2209, Loss Train: 3.1114, Loss Test: 3.9292, LR: 0.000029\n",
      "Step 2210, Loss Train: 2.9100, Loss Test: 3.1383, LR: 0.000029\n",
      "Step 2211, Loss Train: 2.9284, Loss Test: 3.7144, LR: 0.000029\n",
      "Step 2212, Loss Train: 3.0556, Loss Test: 4.0108, LR: 0.000029\n",
      "Step 2213, Loss Train: 3.0635, Loss Test: 4.2599, LR: 0.000029\n",
      "Step 2214, Loss Train: 2.9935, Loss Test: 4.0896, LR: 0.000029\n",
      "Step 2215, Loss Train: 2.7585, Loss Test: 3.9820, LR: 0.000029\n",
      "Step 2216, Loss Train: 2.8491, Loss Test: 4.2664, LR: 0.000029\n",
      "Step 2217, Loss Train: 3.0295, Loss Test: 4.4483, LR: 0.000029\n",
      "Step 2218, Loss Train: 2.9012, Loss Test: 4.2161, LR: 0.000029\n",
      "Step 2219, Loss Train: 3.1250, Loss Test: 4.0989, LR: 0.000029\n",
      "Step 2220, Loss Train: 2.8331, Loss Test: 4.3630, LR: 0.000029\n",
      "Step 2221, Loss Train: 3.1486, Loss Test: 4.1468, LR: 0.000029\n",
      "Step 2222, Loss Train: 3.0866, Loss Test: 3.5142, LR: 0.000029\n",
      "Step 2223, Loss Train: 3.0378, Loss Test: 2.0501, LR: 0.000029\n",
      "Step 2224, Loss Train: 2.8824, Loss Test: 3.8328, LR: 0.000029\n",
      "Step 2225, Loss Train: 3.0966, Loss Test: 4.3898, LR: 0.000029\n",
      "Step 2226, Loss Train: 2.9528, Loss Test: 3.3900, LR: 0.000029\n",
      "Step 2227, Loss Train: 3.0935, Loss Test: 3.4626, LR: 0.000029\n",
      "Step 2228, Loss Train: 3.0705, Loss Test: 3.9760, LR: 0.000029\n",
      "Step 2229, Loss Train: 3.1250, Loss Test: 3.9421, LR: 0.000029\n",
      "Step 2230, Loss Train: 2.9869, Loss Test: 4.2542, LR: 0.000029\n",
      "Step 2231, Loss Train: 2.9912, Loss Test: 3.5121, LR: 0.000029\n",
      "Step 2232, Loss Train: 2.9095, Loss Test: 4.2082, LR: 0.000029\n",
      "Step 2233, Loss Train: 2.7545, Loss Test: 3.5926, LR: 0.000029\n",
      "Step 2234, Loss Train: 3.0359, Loss Test: 3.7744, LR: 0.000029\n",
      "Step 2235, Loss Train: 3.0123, Loss Test: 3.6715, LR: 0.000029\n",
      "Step 2236, Loss Train: 2.9767, Loss Test: 3.8128, LR: 0.000029\n",
      "Step 2237, Loss Train: 2.9201, Loss Test: 4.4460, LR: 0.000029\n",
      "Step 2238, Loss Train: 2.9564, Loss Test: 4.3176, LR: 0.000029\n",
      "Step 2239, Loss Train: 2.8967, Loss Test: 3.6930, LR: 0.000029\n",
      "Step 2240, Loss Train: 2.9963, Loss Test: 4.0117, LR: 0.000029\n",
      "Step 2241, Loss Train: 3.0490, Loss Test: 3.2063, LR: 0.000029\n",
      "Step 2242, Loss Train: 2.9038, Loss Test: 3.7708, LR: 0.000029\n",
      "Step 2243, Loss Train: 3.1809, Loss Test: 3.5673, LR: 0.000029\n",
      "Step 2244, Loss Train: 2.8864, Loss Test: 3.3790, LR: 0.000029\n",
      "Step 2245, Loss Train: 2.8099, Loss Test: 3.7354, LR: 0.000029\n",
      "Step 2246, Loss Train: 3.1541, Loss Test: 4.0456, LR: 0.000029\n",
      "Step 2247, Loss Train: 2.9876, Loss Test: 1.6922, LR: 0.000029\n",
      "Step 2248, Loss Train: 2.9444, Loss Test: 3.9810, LR: 0.000029\n",
      "Step 2249, Loss Train: 2.8971, Loss Test: 3.7273, LR: 0.000029\n",
      "Step 2250, Loss Train: 3.0758, Loss Test: 4.3782, LR: 0.000029\n",
      "Step 2251, Loss Train: 2.7410, Loss Test: 3.5451, LR: 0.000029\n",
      "Step 2252, Loss Train: 3.1970, Loss Test: 3.1333, LR: 0.000029\n",
      "Step 2253, Loss Train: 2.9882, Loss Test: 4.5561, LR: 0.000029\n",
      "Step 2254, Loss Train: 2.9093, Loss Test: 3.9104, LR: 0.000029\n",
      "Step 2255, Loss Train: 3.0188, Loss Test: 3.5658, LR: 0.000029\n",
      "Step 2256, Loss Train: 2.9602, Loss Test: 3.3719, LR: 0.000029\n",
      "Step 2257, Loss Train: 2.8689, Loss Test: 3.7348, LR: 0.000029\n",
      "Step 2258, Loss Train: 2.9142, Loss Test: 3.7396, LR: 0.000029\n",
      "Step 2259, Loss Train: 2.8230, Loss Test: 3.7153, LR: 0.000029\n",
      "Step 2260, Loss Train: 3.2413, Loss Test: 4.1878, LR: 0.000029\n",
      "Step 2261, Loss Train: 2.7828, Loss Test: 3.9992, LR: 0.000029\n",
      "Step 2262, Loss Train: 2.8247, Loss Test: 4.0058, LR: 0.000029\n",
      "Step 2263, Loss Train: 3.0210, Loss Test: 4.4262, LR: 0.000029\n",
      "Step 2264, Loss Train: 3.0498, Loss Test: 3.8191, LR: 0.000029\n",
      "Step 2265, Loss Train: 2.9717, Loss Test: 3.2910, LR: 0.000029\n",
      "Step 2266, Loss Train: 3.0107, Loss Test: 1.2607, LR: 0.000029\n",
      "Step 2267, Loss Train: 2.8395, Loss Test: 3.6196, LR: 0.000029\n",
      "Step 2268, Loss Train: 2.7776, Loss Test: 2.6116, LR: 0.000029\n",
      "Step 2269, Loss Train: 2.9378, Loss Test: 4.2430, LR: 0.000029\n",
      "Step 2270, Loss Train: 2.9821, Loss Test: 3.3487, LR: 0.000029\n",
      "Step 2271, Loss Train: 3.0502, Loss Test: 3.7339, LR: 0.000029\n",
      "Step 2272, Loss Train: 3.1126, Loss Test: 3.5486, LR: 0.000029\n",
      "Step 2273, Loss Train: 2.8965, Loss Test: 3.3175, LR: 0.000029\n",
      "Step 2274, Loss Train: 2.8431, Loss Test: 2.5684, LR: 0.000029\n",
      "Step 2275, Loss Train: 2.9641, Loss Test: 3.4511, LR: 0.000029\n",
      "Step 2276, Loss Train: 2.9120, Loss Test: 3.8865, LR: 0.000029\n",
      "Step 2277, Loss Train: 2.8826, Loss Test: 3.6546, LR: 0.000029\n",
      "Step 2278, Loss Train: 2.8300, Loss Test: 3.7456, LR: 0.000029\n",
      "Step 2279, Loss Train: 2.9030, Loss Test: 3.5181, LR: 0.000029\n",
      "Step 2280, Loss Train: 2.8730, Loss Test: 4.0955, LR: 0.000029\n",
      "Step 2281, Loss Train: 2.9207, Loss Test: 4.0535, LR: 0.000029\n",
      "Step 2282, Loss Train: 3.0330, Loss Test: 3.9616, LR: 0.000029\n",
      "Step 2283, Loss Train: 3.0867, Loss Test: 3.7830, LR: 0.000029\n",
      "Step 2284, Loss Train: 3.0071, Loss Test: 4.1632, LR: 0.000029\n",
      "Step 2285, Loss Train: 2.9888, Loss Test: 3.8327, LR: 0.000029\n",
      "Step 2286, Loss Train: 3.0766, Loss Test: 4.1364, LR: 0.000029\n",
      "Step 2287, Loss Train: 2.9428, Loss Test: 3.2946, LR: 0.000029\n",
      "Step 2288, Loss Train: 2.9858, Loss Test: 3.8430, LR: 0.000029\n",
      "Step 2289, Loss Train: 3.0463, Loss Test: 4.3809, LR: 0.000029\n",
      "Step 2290, Loss Train: 3.0351, Loss Test: 3.0791, LR: 0.000029\n",
      "Step 2291, Loss Train: 2.9097, Loss Test: 3.8086, LR: 0.000029\n",
      "Step 2292, Loss Train: 2.7240, Loss Test: 3.5755, LR: 0.000029\n",
      "Step 2293, Loss Train: 3.0531, Loss Test: 3.7229, LR: 0.000029\n",
      "Step 2294, Loss Train: 2.9683, Loss Test: 3.5241, LR: 0.000029\n",
      "Step 2295, Loss Train: 2.9214, Loss Test: 3.0292, LR: 0.000029\n",
      "Step 2296, Loss Train: 3.0139, Loss Test: 3.3928, LR: 0.000029\n",
      "Step 2297, Loss Train: 2.8305, Loss Test: 3.2220, LR: 0.000029\n",
      "Step 2298, Loss Train: 3.0560, Loss Test: 4.4241, LR: 0.000029\n",
      "Step 2299, Loss Train: 3.1046, Loss Test: 3.7998, LR: 0.000029\n",
      "Step 2300, Loss Train: 3.0496, Loss Test: 3.7113, LR: 0.000029\n",
      "Step 2301, Loss Train: 2.7894, Loss Test: 3.7808, LR: 0.000029\n",
      "Step 2302, Loss Train: 2.9035, Loss Test: 4.1300, LR: 0.000029\n",
      "Step 2303, Loss Train: 3.0524, Loss Test: 4.1615, LR: 0.000029\n",
      "Step 2304, Loss Train: 3.0297, Loss Test: 3.9707, LR: 0.000029\n",
      "Step 2305, Loss Train: 2.7333, Loss Test: 4.0557, LR: 0.000029\n",
      "Step 2306, Loss Train: 2.8202, Loss Test: 3.9488, LR: 0.000029\n",
      "Step 2307, Loss Train: 2.6844, Loss Test: 3.7736, LR: 0.000029\n",
      "Step 2308, Loss Train: 2.7567, Loss Test: 4.3517, LR: 0.000029\n",
      "Step 2309, Loss Train: 2.8562, Loss Test: 3.4325, LR: 0.000029\n",
      "Step 2310, Loss Train: 3.0484, Loss Test: 3.5844, LR: 0.000029\n",
      "Step 2311, Loss Train: 2.9536, Loss Test: 4.3802, LR: 0.000029\n",
      "Step 2312, Loss Train: 3.0017, Loss Test: 3.4273, LR: 0.000029\n",
      "Step 2313, Loss Train: 2.9428, Loss Test: 3.6607, LR: 0.000029\n",
      "Step 2314, Loss Train: 3.0206, Loss Test: 3.7651, LR: 0.000029\n",
      "Step 2315, Loss Train: 2.8037, Loss Test: 4.2849, LR: 0.000029\n",
      "Step 2316, Loss Train: 2.8175, Loss Test: 4.1405, LR: 0.000029\n",
      "Step 2317, Loss Train: 2.7858, Loss Test: 3.4509, LR: 0.000029\n",
      "Step 2318, Loss Train: 2.8395, Loss Test: 4.2137, LR: 0.000029\n",
      "Step 2319, Loss Train: 2.9486, Loss Test: 4.0836, LR: 0.000029\n",
      "Step 2320, Loss Train: 3.0264, Loss Test: 4.0988, LR: 0.000029\n",
      "Step 2321, Loss Train: 2.7337, Loss Test: 3.6243, LR: 0.000029\n",
      "Step 2322, Loss Train: 2.8839, Loss Test: 3.9721, LR: 0.000029\n",
      "Step 2323, Loss Train: 3.0770, Loss Test: 2.6937, LR: 0.000029\n",
      "Step 2324, Loss Train: 2.8047, Loss Test: 2.5349, LR: 0.000029\n",
      "Step 2325, Loss Train: 2.8676, Loss Test: 3.1207, LR: 0.000029\n",
      "Step 2326, Loss Train: 2.9022, Loss Test: 3.3234, LR: 0.000029\n",
      "Step 2327, Loss Train: 2.8720, Loss Test: 4.0294, LR: 0.000029\n",
      "Step 2328, Loss Train: 2.8812, Loss Test: 3.8736, LR: 0.000029\n",
      "Step 2329, Loss Train: 2.8666, Loss Test: 3.3358, LR: 0.000029\n",
      "Step 2330, Loss Train: 2.7035, Loss Test: 4.3302, LR: 0.000029\n",
      "Step 2331, Loss Train: 2.9649, Loss Test: 3.7970, LR: 0.000029\n",
      "Step 2332, Loss Train: 3.0133, Loss Test: 3.7622, LR: 0.000029\n",
      "Step 2333, Loss Train: 3.0576, Loss Test: 3.4313, LR: 0.000029\n",
      "Step 2334, Loss Train: 3.0579, Loss Test: 3.7177, LR: 0.000029\n",
      "Step 2335, Loss Train: 2.9278, Loss Test: 3.9693, LR: 0.000029\n",
      "Step 2336, Loss Train: 2.7040, Loss Test: 3.7412, LR: 0.000029\n",
      "Step 2337, Loss Train: 3.0064, Loss Test: 3.7992, LR: 0.000029\n",
      "Step 2338, Loss Train: 3.1517, Loss Test: 3.8944, LR: 0.000029\n",
      "Step 2339, Loss Train: 3.0557, Loss Test: 3.8368, LR: 0.000029\n",
      "Step 2340, Loss Train: 2.9896, Loss Test: 3.6372, LR: 0.000029\n",
      "Step 2341, Loss Train: 2.7773, Loss Test: 4.3244, LR: 0.000029\n",
      "Step 2342, Loss Train: 2.8601, Loss Test: 2.5972, LR: 0.000029\n",
      "Step 2343, Loss Train: 2.8841, Loss Test: 4.4604, LR: 0.000029\n",
      "Step 2344, Loss Train: 2.9130, Loss Test: 2.5624, LR: 0.000029\n",
      "Step 2345, Loss Train: 2.9407, Loss Test: 3.9614, LR: 0.000029\n",
      "Step 2346, Loss Train: 3.0190, Loss Test: 3.9606, LR: 0.000029\n",
      "Step 2347, Loss Train: 2.9251, Loss Test: 3.4841, LR: 0.000029\n",
      "Step 2348, Loss Train: 3.1152, Loss Test: 3.8066, LR: 0.000029\n",
      "Step 2349, Loss Train: 2.9140, Loss Test: 3.4844, LR: 0.000029\n",
      "Step 2350, Loss Train: 2.9689, Loss Test: 3.7707, LR: 0.000029\n",
      "Step 2351, Loss Train: 2.9371, Loss Test: 2.8015, LR: 0.000029\n",
      "Step 2352, Loss Train: 2.9494, Loss Test: 4.2132, LR: 0.000029\n",
      "Step 2353, Loss Train: 3.0557, Loss Test: 4.1700, LR: 0.000029\n",
      "Step 2354, Loss Train: 2.9367, Loss Test: 3.8487, LR: 0.000029\n",
      "Step 2355, Loss Train: 2.9376, Loss Test: 4.1749, LR: 0.000029\n",
      "Step 2356, Loss Train: 2.9859, Loss Test: 4.0440, LR: 0.000029\n",
      "Step 2357, Loss Train: 2.9188, Loss Test: 4.3080, LR: 0.000029\n",
      "Step 2358, Loss Train: 2.9591, Loss Test: 3.7645, LR: 0.000029\n",
      "Step 2359, Loss Train: 3.2198, Loss Test: 3.8603, LR: 0.000029\n",
      "Step 2360, Loss Train: 2.8860, Loss Test: 4.3517, LR: 0.000029\n",
      "Step 2361, Loss Train: 2.8723, Loss Test: 4.0247, LR: 0.000029\n",
      "Step 2362, Loss Train: 3.0433, Loss Test: 3.4081, LR: 0.000029\n",
      "Step 2363, Loss Train: 3.0273, Loss Test: 3.9672, LR: 0.000029\n",
      "Step 2364, Loss Train: 2.8326, Loss Test: 4.0722, LR: 0.000029\n",
      "Step 2365, Loss Train: 2.8104, Loss Test: 3.4714, LR: 0.000029\n",
      "Step 2366, Loss Train: 3.0485, Loss Test: 4.1088, LR: 0.000029\n",
      "Step 2367, Loss Train: 2.9190, Loss Test: 4.2536, LR: 0.000029\n",
      "Step 2368, Loss Train: 2.8900, Loss Test: 4.0017, LR: 0.000029\n",
      "Step 2369, Loss Train: 2.8643, Loss Test: 3.9814, LR: 0.000029\n",
      "Step 2370, Loss Train: 2.9356, Loss Test: 3.5530, LR: 0.000029\n",
      "Step 2371, Loss Train: 2.9327, Loss Test: 3.9350, LR: 0.000029\n",
      "Step 2372, Loss Train: 2.6555, Loss Test: 3.4810, LR: 0.000029\n",
      "Step 2373, Loss Train: 2.8114, Loss Test: 2.9324, LR: 0.000029\n",
      "Step 2374, Loss Train: 2.9741, Loss Test: 2.0490, LR: 0.000029\n",
      "Step 2375, Loss Train: 2.9306, Loss Test: 3.9466, LR: 0.000029\n",
      "Step 2376, Loss Train: 2.9628, Loss Test: 3.9765, LR: 0.000029\n",
      "Step 2377, Loss Train: 2.8392, Loss Test: 3.6557, LR: 0.000029\n",
      "Step 2378, Loss Train: 3.0265, Loss Test: 4.1539, LR: 0.000029\n",
      "Step 2379, Loss Train: 2.8096, Loss Test: 3.4424, LR: 0.000029\n",
      "Step 2380, Loss Train: 3.0206, Loss Test: 3.9751, LR: 0.000029\n",
      "Step 2381, Loss Train: 2.9228, Loss Test: 4.2330, LR: 0.000029\n",
      "Step 2382, Loss Train: 3.0924, Loss Test: 3.2511, LR: 0.000029\n",
      "Step 2383, Loss Train: 2.8717, Loss Test: 3.6982, LR: 0.000029\n",
      "Step 2384, Loss Train: 2.7780, Loss Test: 3.5237, LR: 0.000029\n",
      "Step 2385, Loss Train: 2.8221, Loss Test: 1.9967, LR: 0.000029\n",
      "Step 2386, Loss Train: 2.7627, Loss Test: 2.4506, LR: 0.000029\n",
      "Step 2387, Loss Train: 2.9615, Loss Test: 4.3895, LR: 0.000029\n",
      "Step 2388, Loss Train: 2.9779, Loss Test: 3.8034, LR: 0.000029\n",
      "Step 2389, Loss Train: 3.0685, Loss Test: 3.7280, LR: 0.000029\n",
      "Step 2390, Loss Train: 2.9852, Loss Test: 4.3157, LR: 0.000029\n",
      "Step 2391, Loss Train: 2.9839, Loss Test: 3.3613, LR: 0.000029\n",
      "Step 2392, Loss Train: 2.8333, Loss Test: 3.8297, LR: 0.000029\n",
      "Step 2393, Loss Train: 2.9864, Loss Test: 4.4931, LR: 0.000029\n",
      "Step 2394, Loss Train: 2.9628, Loss Test: 4.3143, LR: 0.000029\n",
      "Step 2395, Loss Train: 2.7997, Loss Test: 4.2245, LR: 0.000029\n",
      "Step 2396, Loss Train: 2.9270, Loss Test: 3.9916, LR: 0.000029\n",
      "Step 2397, Loss Train: 2.9595, Loss Test: 4.2438, LR: 0.000029\n",
      "Step 2398, Loss Train: 2.9648, Loss Test: 3.9349, LR: 0.000029\n",
      "Step 2399, Loss Train: 2.9052, Loss Test: 4.4012, LR: 0.000029\n",
      "Step 2400, Loss Train: 3.0736, Loss Test: 3.6952, LR: 0.000029\n",
      "Step 2401, Loss Train: 2.8126, Loss Test: 2.2104, LR: 0.000029\n",
      "Step 2402, Loss Train: 2.9153, Loss Test: 3.7782, LR: 0.000029\n",
      "Step 2403, Loss Train: 2.9813, Loss Test: 4.1960, LR: 0.000029\n",
      "Step 2404, Loss Train: 2.7730, Loss Test: 3.7385, LR: 0.000029\n",
      "Step 2405, Loss Train: 2.9623, Loss Test: 3.4354, LR: 0.000029\n",
      "Step 2406, Loss Train: 3.0682, Loss Test: 2.4695, LR: 0.000029\n",
      "Step 2407, Loss Train: 3.0512, Loss Test: 4.3874, LR: 0.000029\n",
      "Step 2408, Loss Train: 3.0301, Loss Test: 4.0717, LR: 0.000029\n",
      "Step 2409, Loss Train: 2.8253, Loss Test: 3.6008, LR: 0.000029\n",
      "Step 2410, Loss Train: 2.9458, Loss Test: 4.0211, LR: 0.000029\n",
      "Step 2411, Loss Train: 2.9937, Loss Test: 3.6312, LR: 0.000029\n",
      "Step 2412, Loss Train: 2.9924, Loss Test: 3.8273, LR: 0.000029\n",
      "Step 2413, Loss Train: 3.0477, Loss Test: 3.7367, LR: 0.000029\n",
      "Step 2414, Loss Train: 2.9478, Loss Test: 4.2287, LR: 0.000029\n",
      "Step 2415, Loss Train: 3.0481, Loss Test: 3.6054, LR: 0.000029\n",
      "Step 2416, Loss Train: 3.0546, Loss Test: 3.9772, LR: 0.000029\n",
      "Step 2417, Loss Train: 2.9732, Loss Test: 4.4491, LR: 0.000029\n",
      "Step 2418, Loss Train: 2.8784, Loss Test: 3.9403, LR: 0.000029\n",
      "Step 2419, Loss Train: 2.9621, Loss Test: 3.7707, LR: 0.000029\n",
      "Step 2420, Loss Train: 3.0461, Loss Test: 3.9112, LR: 0.000029\n",
      "Step 2421, Loss Train: 2.9960, Loss Test: 3.8599, LR: 0.000029\n",
      "Step 2422, Loss Train: 2.7265, Loss Test: 3.7191, LR: 0.000029\n",
      "Step 2423, Loss Train: 3.1966, Loss Test: 3.7015, LR: 0.000029\n",
      "Step 2424, Loss Train: 2.9030, Loss Test: 2.8744, LR: 0.000029\n",
      "Step 2425, Loss Train: 3.1173, Loss Test: 3.0970, LR: 0.000029\n",
      "Step 2426, Loss Train: 3.1446, Loss Test: 3.7871, LR: 0.000029\n",
      "Step 2427, Loss Train: 2.9781, Loss Test: 3.6466, LR: 0.000029\n",
      "Step 2428, Loss Train: 2.9612, Loss Test: 3.0159, LR: 0.000029\n",
      "Step 2429, Loss Train: 2.6946, Loss Test: 3.4059, LR: 0.000029\n",
      "Step 2430, Loss Train: 2.9186, Loss Test: 4.4452, LR: 0.000029\n",
      "Step 2431, Loss Train: 3.1332, Loss Test: 3.0962, LR: 0.000029\n",
      "Step 2432, Loss Train: 2.9691, Loss Test: 3.9324, LR: 0.000029\n",
      "Step 2433, Loss Train: 3.0634, Loss Test: 4.1199, LR: 0.000029\n",
      "Step 2434, Loss Train: 2.8635, Loss Test: 4.0736, LR: 0.000029\n",
      "Step 2435, Loss Train: 2.9121, Loss Test: 4.2224, LR: 0.000029\n",
      "Step 2436, Loss Train: 2.8494, Loss Test: 4.3997, LR: 0.000029\n",
      "Step 2437, Loss Train: 2.9119, Loss Test: 3.0478, LR: 0.000029\n",
      "Step 2438, Loss Train: 2.8899, Loss Test: 2.0465, LR: 0.000029\n",
      "Step 2439, Loss Train: 2.7433, Loss Test: 1.6814, LR: 0.000029\n",
      "Step 2440, Loss Train: 2.8932, Loss Test: 3.6734, LR: 0.000029\n",
      "Step 2441, Loss Train: 3.0218, Loss Test: 3.8705, LR: 0.000029\n",
      "Step 2442, Loss Train: 3.0248, Loss Test: 4.4057, LR: 0.000029\n",
      "Step 2443, Loss Train: 2.7026, Loss Test: 3.9995, LR: 0.000029\n",
      "Step 2444, Loss Train: 2.9153, Loss Test: 3.7653, LR: 0.000029\n",
      "Step 2445, Loss Train: 2.8322, Loss Test: 4.2550, LR: 0.000029\n",
      "Step 2446, Loss Train: 2.8669, Loss Test: 4.1925, LR: 0.000029\n",
      "Step 2447, Loss Train: 2.7273, Loss Test: 2.5661, LR: 0.000029\n",
      "Step 2448, Loss Train: 3.0722, Loss Test: 4.0091, LR: 0.000029\n",
      "Step 2449, Loss Train: 3.0347, Loss Test: 4.0558, LR: 0.000029\n",
      "Step 2450, Loss Train: 3.0137, Loss Test: 3.8646, LR: 0.000029\n",
      "Step 2451, Loss Train: 2.9717, Loss Test: 2.3486, LR: 0.000029\n",
      "Step 2452, Loss Train: 3.2040, Loss Test: 4.3743, LR: 0.000029\n",
      "Step 2453, Loss Train: 2.9499, Loss Test: 4.4916, LR: 0.000029\n",
      "Step 2454, Loss Train: 2.8969, Loss Test: 3.8044, LR: 0.000029\n",
      "Step 2455, Loss Train: 3.0457, Loss Test: 3.9542, LR: 0.000029\n",
      "Step 2456, Loss Train: 2.8747, Loss Test: 3.4962, LR: 0.000029\n",
      "Step 2457, Loss Train: 2.9789, Loss Test: 2.1979, LR: 0.000029\n",
      "Step 2458, Loss Train: 2.9691, Loss Test: 2.2270, LR: 0.000029\n",
      "Step 2459, Loss Train: 2.7649, Loss Test: 4.2743, LR: 0.000029\n",
      "Step 2460, Loss Train: 2.8335, Loss Test: 3.7641, LR: 0.000029\n",
      "Step 2461, Loss Train: 3.0638, Loss Test: 3.8725, LR: 0.000029\n",
      "Step 2462, Loss Train: 3.0155, Loss Test: 3.5935, LR: 0.000029\n",
      "Step 2463, Loss Train: 2.9307, Loss Test: 3.6112, LR: 0.000029\n",
      "Step 2464, Loss Train: 2.8006, Loss Test: 3.4735, LR: 0.000029\n",
      "Step 2465, Loss Train: 2.9069, Loss Test: 3.6372, LR: 0.000029\n",
      "Step 2466, Loss Train: 2.8862, Loss Test: 3.8376, LR: 0.000029\n",
      "Step 2467, Loss Train: 2.8696, Loss Test: 2.8296, LR: 0.000029\n",
      "Step 2468, Loss Train: 2.8365, Loss Test: 3.5960, LR: 0.000029\n",
      "Step 2469, Loss Train: 2.8515, Loss Test: 3.7566, LR: 0.000029\n",
      "Step 2470, Loss Train: 2.9305, Loss Test: 3.1506, LR: 0.000029\n",
      "Step 2471, Loss Train: 2.9911, Loss Test: 3.7375, LR: 0.000029\n",
      "Step 2472, Loss Train: 2.9336, Loss Test: 3.1882, LR: 0.000029\n",
      "Step 2473, Loss Train: 2.9631, Loss Test: 3.6135, LR: 0.000029\n",
      "Step 2474, Loss Train: 2.7561, Loss Test: 3.9603, LR: 0.000029\n",
      "Step 2475, Loss Train: 2.8469, Loss Test: 4.0911, LR: 0.000029\n",
      "Step 2476, Loss Train: 2.7200, Loss Test: 3.7400, LR: 0.000029\n",
      "Step 2477, Loss Train: 3.0225, Loss Test: 3.3148, LR: 0.000029\n",
      "Step 2478, Loss Train: 3.0463, Loss Test: 3.3915, LR: 0.000029\n",
      "Step 2479, Loss Train: 2.8212, Loss Test: 3.1591, LR: 0.000029\n",
      "Step 2480, Loss Train: 3.0847, Loss Test: 3.9060, LR: 0.000029\n",
      "Step 2481, Loss Train: 3.0613, Loss Test: 3.0458, LR: 0.000029\n",
      "Step 2482, Loss Train: 2.9395, Loss Test: 4.1502, LR: 0.000029\n",
      "Step 2483, Loss Train: 3.0181, Loss Test: 3.4242, LR: 0.000029\n",
      "Step 2484, Loss Train: 2.9397, Loss Test: 3.9317, LR: 0.000029\n",
      "Step 2485, Loss Train: 2.9500, Loss Test: 3.5809, LR: 0.000029\n",
      "Step 2486, Loss Train: 2.8338, Loss Test: 3.7205, LR: 0.000029\n",
      "Step 2487, Loss Train: 3.0801, Loss Test: 3.6938, LR: 0.000029\n",
      "Step 2488, Loss Train: 2.7978, Loss Test: 3.5465, LR: 0.000029\n",
      "Step 2489, Loss Train: 2.9716, Loss Test: 3.0579, LR: 0.000029\n",
      "Step 2490, Loss Train: 2.9109, Loss Test: 3.3026, LR: 0.000029\n",
      "Step 2491, Loss Train: 2.9590, Loss Test: 3.7732, LR: 0.000029\n",
      "Step 2492, Loss Train: 2.7274, Loss Test: 3.5383, LR: 0.000029\n",
      "Step 2493, Loss Train: 2.9139, Loss Test: 4.2137, LR: 0.000029\n",
      "Step 2494, Loss Train: 3.0781, Loss Test: 3.6720, LR: 0.000029\n",
      "Step 2495, Loss Train: 3.0412, Loss Test: 4.0821, LR: 0.000029\n",
      "Step 2496, Loss Train: 2.8771, Loss Test: 4.1905, LR: 0.000029\n",
      "Step 2497, Loss Train: 2.8111, Loss Test: 4.3587, LR: 0.000029\n",
      "Step 2498, Loss Train: 3.1226, Loss Test: 3.6971, LR: 0.000029\n",
      "Step 2499, Loss Train: 2.8617, Loss Test: 3.1748, LR: 0.000029\n",
      "Step 2500, Loss Train: 2.9019, Loss Test: 4.2416, LR: 0.000029\n",
      "Step 2501, Loss Train: 3.1105, Loss Test: 3.1645, LR: 0.000029\n",
      "Step 2502, Loss Train: 2.7711, Loss Test: 4.1171, LR: 0.000029\n",
      "Step 2503, Loss Train: 2.9405, Loss Test: 3.7303, LR: 0.000029\n",
      "Step 2504, Loss Train: 3.0660, Loss Test: 2.8330, LR: 0.000029\n",
      "Step 2505, Loss Train: 2.8918, Loss Test: 4.0576, LR: 0.000029\n",
      "Step 2506, Loss Train: 3.0800, Loss Test: 3.9738, LR: 0.000029\n",
      "Step 2507, Loss Train: 2.9416, Loss Test: 4.0943, LR: 0.000029\n",
      "Step 2508, Loss Train: 2.9472, Loss Test: 3.7570, LR: 0.000029\n",
      "Step 2509, Loss Train: 2.9440, Loss Test: 4.3148, LR: 0.000029\n",
      "Step 2510, Loss Train: 2.8520, Loss Test: 4.4308, LR: 0.000029\n",
      "Step 2511, Loss Train: 2.9420, Loss Test: 2.4600, LR: 0.000029\n",
      "Step 2512, Loss Train: 3.0003, Loss Test: 2.4395, LR: 0.000029\n",
      "Step 2513, Loss Train: 2.8849, Loss Test: 3.5230, LR: 0.000029\n",
      "Step 2514, Loss Train: 3.1428, Loss Test: 2.5362, LR: 0.000029\n",
      "Step 2515, Loss Train: 2.9855, Loss Test: 3.3025, LR: 0.000029\n",
      "Step 2516, Loss Train: 2.7187, Loss Test: 4.1864, LR: 0.000029\n",
      "Step 2517, Loss Train: 2.9489, Loss Test: 2.9926, LR: 0.000029\n",
      "Step 2518, Loss Train: 2.9178, Loss Test: 3.5871, LR: 0.000029\n",
      "Step 2519, Loss Train: 2.9077, Loss Test: 3.6171, LR: 0.000029\n",
      "Step 2520, Loss Train: 3.0604, Loss Test: 3.8834, LR: 0.000029\n",
      "Step 2521, Loss Train: 3.0338, Loss Test: 2.3577, LR: 0.000029\n",
      "Step 2522, Loss Train: 3.0453, Loss Test: 3.7053, LR: 0.000029\n",
      "Step 2523, Loss Train: 2.9433, Loss Test: 3.0217, LR: 0.000029\n",
      "Step 2524, Loss Train: 2.8849, Loss Test: 4.2596, LR: 0.000029\n",
      "Step 2525, Loss Train: 3.0291, Loss Test: 3.3445, LR: 0.000029\n",
      "Step 2526, Loss Train: 2.9802, Loss Test: 4.2017, LR: 0.000029\n",
      "Step 2527, Loss Train: 2.7383, Loss Test: 4.0441, LR: 0.000029\n",
      "Step 2528, Loss Train: 3.0152, Loss Test: 3.9983, LR: 0.000029\n",
      "Step 2529, Loss Train: 2.8934, Loss Test: 3.2111, LR: 0.000029\n",
      "Step 2530, Loss Train: 3.0219, Loss Test: 4.6514, LR: 0.000029\n",
      "Step 2531, Loss Train: 2.8426, Loss Test: 4.1580, LR: 0.000029\n",
      "Step 2532, Loss Train: 2.9932, Loss Test: 4.1497, LR: 0.000029\n",
      "Step 2533, Loss Train: 2.9118, Loss Test: 4.1167, LR: 0.000029\n",
      "Step 2534, Loss Train: 3.0809, Loss Test: 4.1872, LR: 0.000029\n",
      "Step 2535, Loss Train: 2.9348, Loss Test: 4.0029, LR: 0.000029\n",
      "Step 2536, Loss Train: 2.8877, Loss Test: 3.1532, LR: 0.000029\n",
      "Step 2537, Loss Train: 2.6957, Loss Test: 4.3375, LR: 0.000029\n",
      "Step 2538, Loss Train: 2.8816, Loss Test: 4.2887, LR: 0.000029\n",
      "Step 2539, Loss Train: 2.9245, Loss Test: 3.2653, LR: 0.000029\n",
      "Step 2540, Loss Train: 2.9772, Loss Test: 3.8758, LR: 0.000029\n",
      "Step 2541, Loss Train: 3.0327, Loss Test: 4.0583, LR: 0.000029\n",
      "Step 2542, Loss Train: 2.8464, Loss Test: 3.9640, LR: 0.000029\n",
      "Step 2543, Loss Train: 2.7766, Loss Test: 4.2085, LR: 0.000029\n",
      "Step 2544, Loss Train: 2.8345, Loss Test: 4.0656, LR: 0.000029\n",
      "Step 2545, Loss Train: 2.8988, Loss Test: 3.8666, LR: 0.000029\n",
      "Step 2546, Loss Train: 2.9840, Loss Test: 4.0673, LR: 0.000029\n",
      "Step 2547, Loss Train: 2.9427, Loss Test: 3.9514, LR: 0.000029\n",
      "Step 2548, Loss Train: 2.9978, Loss Test: 3.8204, LR: 0.000029\n",
      "Step 2549, Loss Train: 3.1225, Loss Test: 3.7978, LR: 0.000029\n",
      "Step 2550, Loss Train: 3.0095, Loss Test: 4.3385, LR: 0.000029\n",
      "Step 2551, Loss Train: 3.0470, Loss Test: 4.1219, LR: 0.000029\n",
      "Step 2552, Loss Train: 2.8178, Loss Test: 3.2236, LR: 0.000029\n",
      "Step 2553, Loss Train: 2.9907, Loss Test: 3.8541, LR: 0.000029\n",
      "Step 2554, Loss Train: 2.8643, Loss Test: 3.8315, LR: 0.000029\n",
      "Step 2555, Loss Train: 2.9979, Loss Test: 4.2356, LR: 0.000029\n",
      "Step 2556, Loss Train: 3.0699, Loss Test: 4.3338, LR: 0.000029\n",
      "Step 2557, Loss Train: 2.8497, Loss Test: 4.1533, LR: 0.000029\n",
      "Step 2558, Loss Train: 3.0314, Loss Test: 3.9467, LR: 0.000029\n",
      "Step 2559, Loss Train: 2.8572, Loss Test: 3.1287, LR: 0.000029\n",
      "Step 2560, Loss Train: 2.9564, Loss Test: 2.8896, LR: 0.000029\n",
      "Step 2561, Loss Train: 3.0530, Loss Test: 2.1244, LR: 0.000029\n",
      "Step 2562, Loss Train: 2.8423, Loss Test: 3.2662, LR: 0.000029\n",
      "Step 2563, Loss Train: 2.8022, Loss Test: 4.0130, LR: 0.000029\n",
      "Step 2564, Loss Train: 2.9017, Loss Test: 4.4364, LR: 0.000029\n",
      "Step 2565, Loss Train: 3.0252, Loss Test: 2.5336, LR: 0.000029\n",
      "Step 2566, Loss Train: 2.8285, Loss Test: 4.0713, LR: 0.000029\n",
      "Step 2567, Loss Train: 2.9430, Loss Test: 3.4974, LR: 0.000029\n",
      "Step 2568, Loss Train: 2.9456, Loss Test: 3.0966, LR: 0.000029\n",
      "Step 2569, Loss Train: 2.9176, Loss Test: 3.7409, LR: 0.000029\n",
      "Step 2570, Loss Train: 2.7781, Loss Test: 3.4766, LR: 0.000029\n",
      "Step 2571, Loss Train: 2.9292, Loss Test: 3.6875, LR: 0.000029\n",
      "Step 2572, Loss Train: 2.9753, Loss Test: 4.0700, LR: 0.000029\n",
      "Step 2573, Loss Train: 2.9588, Loss Test: 3.7660, LR: 0.000029\n",
      "Step 2574, Loss Train: 2.8212, Loss Test: 3.0652, LR: 0.000029\n",
      "Step 2575, Loss Train: 2.9015, Loss Test: 3.1789, LR: 0.000029\n",
      "Step 2576, Loss Train: 2.9324, Loss Test: 3.5932, LR: 0.000029\n",
      "Step 2577, Loss Train: 2.7684, Loss Test: 4.2499, LR: 0.000029\n",
      "Step 2578, Loss Train: 2.8321, Loss Test: 3.7509, LR: 0.000029\n",
      "Step 2579, Loss Train: 2.9362, Loss Test: 4.2082, LR: 0.000029\n",
      "Step 2580, Loss Train: 3.0742, Loss Test: 3.7744, LR: 0.000029\n",
      "Step 2581, Loss Train: 2.8979, Loss Test: 3.3963, LR: 0.000029\n",
      "Step 2582, Loss Train: 2.8024, Loss Test: 4.0138, LR: 0.000029\n",
      "Step 2583, Loss Train: 2.9252, Loss Test: 3.1596, LR: 0.000029\n",
      "Step 2584, Loss Train: 2.8389, Loss Test: 2.3081, LR: 0.000029\n",
      "Step 2585, Loss Train: 2.8647, Loss Test: 3.6039, LR: 0.000029\n",
      "Step 2586, Loss Train: 2.8019, Loss Test: 3.0777, LR: 0.000029\n",
      "Step 2587, Loss Train: 3.1708, Loss Test: 2.3601, LR: 0.000029\n",
      "Step 2588, Loss Train: 2.9527, Loss Test: 3.0027, LR: 0.000029\n",
      "Step 2589, Loss Train: 2.9041, Loss Test: 4.2331, LR: 0.000029\n",
      "Step 2590, Loss Train: 3.0769, Loss Test: 4.5872, LR: 0.000029\n",
      "Step 2591, Loss Train: 3.0548, Loss Test: 3.4942, LR: 0.000029\n",
      "Step 2592, Loss Train: 3.1407, Loss Test: 2.4616, LR: 0.000029\n",
      "Step 2593, Loss Train: 3.0563, Loss Test: 4.2255, LR: 0.000029\n",
      "Step 2594, Loss Train: 2.9304, Loss Test: 4.4682, LR: 0.000029\n",
      "Step 2595, Loss Train: 2.9923, Loss Test: 4.3402, LR: 0.000029\n",
      "Step 2596, Loss Train: 2.9350, Loss Test: 3.3198, LR: 0.000029\n",
      "Step 2597, Loss Train: 2.9696, Loss Test: 3.4809, LR: 0.000029\n",
      "Step 2598, Loss Train: 2.9523, Loss Test: 4.6642, LR: 0.000029\n",
      "Step 2599, Loss Train: 2.9933, Loss Test: 3.3490, LR: 0.000029\n",
      "Step 2600, Loss Train: 2.8141, Loss Test: 3.8625, LR: 0.000029\n",
      "Step 2601, Loss Train: 2.9680, Loss Test: 3.7286, LR: 0.000029\n",
      "Step 2602, Loss Train: 2.9056, Loss Test: 3.8101, LR: 0.000029\n",
      "Step 2603, Loss Train: 3.0425, Loss Test: 4.4886, LR: 0.000029\n",
      "Step 2604, Loss Train: 2.9318, Loss Test: 4.3848, LR: 0.000029\n",
      "Step 2605, Loss Train: 3.0113, Loss Test: 4.4878, LR: 0.000029\n",
      "Step 2606, Loss Train: 2.8850, Loss Test: 4.3059, LR: 0.000029\n",
      "Step 2607, Loss Train: 2.9022, Loss Test: 2.1370, LR: 0.000029\n",
      "Step 2608, Loss Train: 3.1912, Loss Test: 3.1771, LR: 0.000029\n",
      "Step 2609, Loss Train: 2.8161, Loss Test: 3.0107, LR: 0.000029\n",
      "Step 2610, Loss Train: 2.9482, Loss Test: 2.8901, LR: 0.000029\n",
      "Step 2611, Loss Train: 3.0060, Loss Test: 3.6740, LR: 0.000029\n",
      "Step 2612, Loss Train: 2.9720, Loss Test: 4.0312, LR: 0.000029\n",
      "Step 2613, Loss Train: 2.8514, Loss Test: 3.8415, LR: 0.000029\n",
      "Step 2614, Loss Train: 2.9128, Loss Test: 2.3244, LR: 0.000029\n",
      "Step 2615, Loss Train: 2.9554, Loss Test: 3.6689, LR: 0.000029\n",
      "Step 2616, Loss Train: 2.8759, Loss Test: 3.8837, LR: 0.000029\n",
      "Step 2617, Loss Train: 2.8326, Loss Test: 3.9876, LR: 0.000029\n",
      "Step 2618, Loss Train: 2.8676, Loss Test: 3.9311, LR: 0.000029\n",
      "Step 2619, Loss Train: 2.8880, Loss Test: 4.1572, LR: 0.000029\n",
      "Step 2620, Loss Train: 2.9889, Loss Test: 3.4623, LR: 0.000029\n",
      "Step 2621, Loss Train: 3.0157, Loss Test: 3.9194, LR: 0.000029\n",
      "Step 2622, Loss Train: 3.0287, Loss Test: 3.5771, LR: 0.000029\n",
      "Step 2623, Loss Train: 2.8952, Loss Test: 4.0963, LR: 0.000029\n",
      "Step 2624, Loss Train: 2.8305, Loss Test: 2.5530, LR: 0.000029\n",
      "Step 2625, Loss Train: 2.9498, Loss Test: 4.0268, LR: 0.000029\n",
      "Step 2626, Loss Train: 2.9835, Loss Test: 3.9859, LR: 0.000029\n",
      "Step 2627, Loss Train: 2.9914, Loss Test: 3.7057, LR: 0.000029\n",
      "Step 2628, Loss Train: 2.8690, Loss Test: 3.5336, LR: 0.000029\n",
      "Step 2629, Loss Train: 3.0851, Loss Test: 3.5295, LR: 0.000029\n",
      "Step 2630, Loss Train: 2.9490, Loss Test: 3.1559, LR: 0.000029\n",
      "Step 2631, Loss Train: 2.9487, Loss Test: 3.9596, LR: 0.000029\n",
      "Step 2632, Loss Train: 2.9737, Loss Test: 2.7804, LR: 0.000029\n",
      "Step 2633, Loss Train: 3.0422, Loss Test: 4.2333, LR: 0.000029\n",
      "Step 2634, Loss Train: 2.8732, Loss Test: 3.1712, LR: 0.000029\n",
      "Step 2635, Loss Train: 2.9363, Loss Test: 3.4560, LR: 0.000029\n",
      "Step 2636, Loss Train: 3.0506, Loss Test: 3.7879, LR: 0.000029\n",
      "Step 2637, Loss Train: 2.9140, Loss Test: 1.6930, LR: 0.000029\n",
      "Step 2638, Loss Train: 2.9482, Loss Test: 3.5013, LR: 0.000029\n",
      "Step 2639, Loss Train: 2.8234, Loss Test: 4.2806, LR: 0.000029\n",
      "Step 2640, Loss Train: 2.8834, Loss Test: 4.3918, LR: 0.000029\n",
      "Step 2641, Loss Train: 3.0738, Loss Test: 3.9034, LR: 0.000029\n",
      "Step 2642, Loss Train: 3.0039, Loss Test: 3.7508, LR: 0.000029\n",
      "Step 2643, Loss Train: 2.9184, Loss Test: 4.3709, LR: 0.000029\n",
      "Step 2644, Loss Train: 2.5962, Loss Test: 4.2145, LR: 0.000029\n",
      "Step 2645, Loss Train: 2.9126, Loss Test: 4.4160, LR: 0.000029\n",
      "Step 2646, Loss Train: 2.7357, Loss Test: 4.5074, LR: 0.000029\n",
      "Step 2647, Loss Train: 2.9605, Loss Test: 1.0191, LR: 0.000029\n",
      "Step 2648, Loss Train: 2.9537, Loss Test: 3.3169, LR: 0.000029\n",
      "Step 2649, Loss Train: 2.9133, Loss Test: 4.3750, LR: 0.000029\n",
      "Step 2650, Loss Train: 2.9677, Loss Test: 3.9553, LR: 0.000029\n",
      "Step 2651, Loss Train: 3.0276, Loss Test: 3.8965, LR: 0.000029\n",
      "Step 2652, Loss Train: 2.9037, Loss Test: 3.8597, LR: 0.000029\n",
      "Step 2653, Loss Train: 3.1011, Loss Test: 3.9461, LR: 0.000029\n",
      "Step 2654, Loss Train: 3.0149, Loss Test: 4.2147, LR: 0.000029\n",
      "Step 2655, Loss Train: 3.0062, Loss Test: 4.1933, LR: 0.000029\n",
      "Step 2656, Loss Train: 2.8626, Loss Test: 3.6147, LR: 0.000029\n",
      "Step 2657, Loss Train: 2.7684, Loss Test: 3.7306, LR: 0.000029\n",
      "Step 2658, Loss Train: 3.0304, Loss Test: 4.1044, LR: 0.000029\n",
      "Step 2659, Loss Train: 2.8039, Loss Test: 3.1330, LR: 0.000029\n",
      "Step 2660, Loss Train: 3.0235, Loss Test: 4.1580, LR: 0.000029\n",
      "Step 2661, Loss Train: 2.9224, Loss Test: 2.6321, LR: 0.000029\n",
      "Step 2662, Loss Train: 2.9835, Loss Test: 3.4817, LR: 0.000029\n",
      "Step 2663, Loss Train: 2.9650, Loss Test: 3.8890, LR: 0.000029\n",
      "Step 2664, Loss Train: 2.9643, Loss Test: 3.7853, LR: 0.000029\n",
      "Step 2665, Loss Train: 2.9428, Loss Test: 3.7238, LR: 0.000029\n",
      "Step 2666, Loss Train: 2.9563, Loss Test: 4.0812, LR: 0.000029\n",
      "Step 2667, Loss Train: 2.8123, Loss Test: 3.4565, LR: 0.000029\n",
      "Step 2668, Loss Train: 2.9371, Loss Test: 2.9616, LR: 0.000029\n",
      "Step 2669, Loss Train: 3.0544, Loss Test: 3.6233, LR: 0.000029\n",
      "Step 2670, Loss Train: 2.8584, Loss Test: 2.9992, LR: 0.000029\n",
      "Step 2671, Loss Train: 2.7666, Loss Test: 3.5280, LR: 0.000029\n",
      "Step 2672, Loss Train: 3.0180, Loss Test: 3.9655, LR: 0.000029\n",
      "Step 2673, Loss Train: 3.0814, Loss Test: 3.5117, LR: 0.000029\n",
      "Step 2674, Loss Train: 2.7657, Loss Test: 3.7797, LR: 0.000029\n",
      "Step 2675, Loss Train: 3.1054, Loss Test: 3.1803, LR: 0.000029\n",
      "Step 2676, Loss Train: 2.8805, Loss Test: 2.9784, LR: 0.000029\n",
      "Step 2677, Loss Train: 2.9929, Loss Test: 4.4207, LR: 0.000029\n",
      "Step 2678, Loss Train: 2.9924, Loss Test: 3.4501, LR: 0.000029\n",
      "Step 2679, Loss Train: 2.8145, Loss Test: 3.7200, LR: 0.000029\n",
      "Step 2680, Loss Train: 2.8987, Loss Test: 4.1119, LR: 0.000029\n",
      "Step 2681, Loss Train: 2.9696, Loss Test: 3.1360, LR: 0.000029\n",
      "Step 2682, Loss Train: 2.7802, Loss Test: 3.3442, LR: 0.000029\n",
      "Step 2683, Loss Train: 3.0290, Loss Test: 4.0474, LR: 0.000029\n",
      "Step 2684, Loss Train: 2.9339, Loss Test: 3.6325, LR: 0.000029\n",
      "Step 2685, Loss Train: 2.9202, Loss Test: 4.2140, LR: 0.000029\n",
      "Step 2686, Loss Train: 2.9749, Loss Test: 4.1796, LR: 0.000029\n",
      "Step 2687, Loss Train: 3.0533, Loss Test: 4.1454, LR: 0.000029\n",
      "Step 2688, Loss Train: 3.0527, Loss Test: 3.3591, LR: 0.000029\n",
      "Step 2689, Loss Train: 3.0198, Loss Test: 3.1498, LR: 0.000029\n",
      "Step 2690, Loss Train: 2.8433, Loss Test: 3.9085, LR: 0.000029\n",
      "Step 2691, Loss Train: 3.1373, Loss Test: 2.7098, LR: 0.000029\n",
      "Step 2692, Loss Train: 2.9144, Loss Test: 2.9228, LR: 0.000029\n",
      "Step 2693, Loss Train: 2.9018, Loss Test: 2.4476, LR: 0.000029\n",
      "Step 2694, Loss Train: 2.7111, Loss Test: 2.9417, LR: 0.000029\n",
      "Step 2695, Loss Train: 3.0837, Loss Test: 4.3137, LR: 0.000029\n",
      "Step 2696, Loss Train: 2.9822, Loss Test: 3.1762, LR: 0.000029\n",
      "Step 2697, Loss Train: 2.9918, Loss Test: 3.6515, LR: 0.000029\n",
      "Step 2698, Loss Train: 3.0305, Loss Test: 3.7521, LR: 0.000029\n",
      "Step 2699, Loss Train: 2.6918, Loss Test: 4.1796, LR: 0.000029\n",
      "Step 2700, Loss Train: 3.0306, Loss Test: 3.3861, LR: 0.000029\n",
      "Step 2701, Loss Train: 2.6585, Loss Test: 3.1186, LR: 0.000029\n",
      "Step 2702, Loss Train: 3.0161, Loss Test: 1.7420, LR: 0.000029\n",
      "Step 2703, Loss Train: 2.8574, Loss Test: 3.6779, LR: 0.000029\n",
      "Step 2704, Loss Train: 2.9250, Loss Test: 4.0686, LR: 0.000029\n",
      "Step 2705, Loss Train: 2.9166, Loss Test: 3.3403, LR: 0.000029\n",
      "Step 2706, Loss Train: 2.7277, Loss Test: 4.0016, LR: 0.000029\n",
      "Step 2707, Loss Train: 2.9158, Loss Test: 3.9656, LR: 0.000029\n",
      "Step 2708, Loss Train: 2.9708, Loss Test: 2.8430, LR: 0.000029\n",
      "Step 2709, Loss Train: 3.1529, Loss Test: 3.5022, LR: 0.000029\n",
      "Step 2710, Loss Train: 2.9538, Loss Test: 4.4254, LR: 0.000029\n",
      "Step 2711, Loss Train: 2.9877, Loss Test: 3.4914, LR: 0.000029\n",
      "Step 2712, Loss Train: 2.8319, Loss Test: 3.9129, LR: 0.000029\n",
      "Step 2713, Loss Train: 2.9736, Loss Test: 4.0857, LR: 0.000029\n",
      "Step 2714, Loss Train: 3.0287, Loss Test: 3.7080, LR: 0.000029\n",
      "Step 2715, Loss Train: 2.9724, Loss Test: 1.7890, LR: 0.000029\n",
      "Step 2716, Loss Train: 3.1619, Loss Test: 4.1134, LR: 0.000029\n",
      "Step 2717, Loss Train: 2.8905, Loss Test: 3.2505, LR: 0.000029\n",
      "Step 2718, Loss Train: 2.9483, Loss Test: 3.2125, LR: 0.000029\n",
      "Step 2719, Loss Train: 3.1185, Loss Test: 3.8290, LR: 0.000029\n",
      "Step 2720, Loss Train: 3.0521, Loss Test: 3.3134, LR: 0.000029\n",
      "Step 2721, Loss Train: 2.9472, Loss Test: 3.7510, LR: 0.000029\n",
      "Step 2722, Loss Train: 3.0105, Loss Test: 3.0708, LR: 0.000029\n",
      "Step 2723, Loss Train: 2.9494, Loss Test: 3.7745, LR: 0.000029\n",
      "Step 2724, Loss Train: 2.6233, Loss Test: 4.0664, LR: 0.000029\n",
      "Step 2725, Loss Train: 3.0814, Loss Test: 3.7979, LR: 0.000029\n",
      "Step 2726, Loss Train: 3.1068, Loss Test: 3.8691, LR: 0.000029\n",
      "Step 2727, Loss Train: 2.8842, Loss Test: 2.6807, LR: 0.000029\n",
      "Step 2728, Loss Train: 2.7766, Loss Test: 4.7641, LR: 0.000029\n",
      "Step 2729, Loss Train: 2.7476, Loss Test: 4.0843, LR: 0.000029\n",
      "Step 2730, Loss Train: 2.8710, Loss Test: 3.7263, LR: 0.000029\n",
      "Step 2731, Loss Train: 2.9846, Loss Test: 3.6071, LR: 0.000029\n",
      "Step 2732, Loss Train: 2.9404, Loss Test: 3.8039, LR: 0.000029\n",
      "Step 2733, Loss Train: 2.7685, Loss Test: 3.3837, LR: 0.000029\n",
      "Step 2734, Loss Train: 3.0737, Loss Test: 3.6764, LR: 0.000029\n",
      "Step 2735, Loss Train: 2.7510, Loss Test: 3.4150, LR: 0.000029\n",
      "Step 2736, Loss Train: 2.9655, Loss Test: 4.1188, LR: 0.000029\n",
      "Step 2737, Loss Train: 2.9563, Loss Test: 3.4233, LR: 0.000029\n",
      "Step 2738, Loss Train: 2.7857, Loss Test: 3.9619, LR: 0.000029\n",
      "Step 2739, Loss Train: 3.1331, Loss Test: 3.3783, LR: 0.000029\n",
      "Step 2740, Loss Train: 3.0194, Loss Test: 4.1466, LR: 0.000029\n",
      "Step 2741, Loss Train: 2.9478, Loss Test: 3.8656, LR: 0.000029\n",
      "Step 2742, Loss Train: 3.0024, Loss Test: 3.9102, LR: 0.000029\n",
      "Step 2743, Loss Train: 2.9886, Loss Test: 4.1352, LR: 0.000029\n",
      "Step 2744, Loss Train: 2.9144, Loss Test: 4.0428, LR: 0.000029\n",
      "Step 2745, Loss Train: 2.9482, Loss Test: 3.5482, LR: 0.000029\n",
      "Step 2746, Loss Train: 3.0276, Loss Test: 3.5644, LR: 0.000029\n",
      "Step 2747, Loss Train: 3.0476, Loss Test: 3.5798, LR: 0.000029\n",
      "Step 2748, Loss Train: 2.8587, Loss Test: 3.8035, LR: 0.000029\n",
      "Step 2749, Loss Train: 2.9749, Loss Test: 4.4866, LR: 0.000029\n",
      "Step 2750, Loss Train: 3.0961, Loss Test: 3.5885, LR: 0.000029\n",
      "Step 2751, Loss Train: 2.9544, Loss Test: 3.5573, LR: 0.000029\n",
      "Step 2752, Loss Train: 2.8138, Loss Test: 3.5676, LR: 0.000029\n",
      "Step 2753, Loss Train: 3.0001, Loss Test: 4.1899, LR: 0.000029\n",
      "Step 2754, Loss Train: 2.5560, Loss Test: 3.8732, LR: 0.000029\n",
      "Step 2755, Loss Train: 3.0214, Loss Test: 3.8051, LR: 0.000029\n",
      "Step 2756, Loss Train: 2.8922, Loss Test: 3.7655, LR: 0.000029\n",
      "Step 2757, Loss Train: 3.0002, Loss Test: 4.6179, LR: 0.000029\n",
      "Step 2758, Loss Train: 3.0013, Loss Test: 4.0801, LR: 0.000029\n",
      "Step 2759, Loss Train: 2.9247, Loss Test: 4.1777, LR: 0.000029\n",
      "Step 2760, Loss Train: 3.0256, Loss Test: 3.1482, LR: 0.000029\n",
      "Step 2761, Loss Train: 2.8224, Loss Test: 4.0288, LR: 0.000029\n",
      "Step 2762, Loss Train: 2.8781, Loss Test: 4.0795, LR: 0.000029\n",
      "Step 2763, Loss Train: 3.1092, Loss Test: 3.7890, LR: 0.000029\n",
      "Step 2764, Loss Train: 3.0789, Loss Test: 2.6511, LR: 0.000029\n",
      "Step 2765, Loss Train: 2.8456, Loss Test: 2.1992, LR: 0.000029\n",
      "Step 2766, Loss Train: 2.8541, Loss Test: 4.5896, LR: 0.000029\n",
      "Step 2767, Loss Train: 2.8479, Loss Test: 3.6618, LR: 0.000029\n",
      "Step 2768, Loss Train: 2.9180, Loss Test: 4.6869, LR: 0.000029\n",
      "Step 2769, Loss Train: 2.9275, Loss Test: 4.1626, LR: 0.000029\n",
      "Step 2770, Loss Train: 2.8994, Loss Test: 3.8356, LR: 0.000029\n",
      "Step 2771, Loss Train: 2.8164, Loss Test: 4.0651, LR: 0.000029\n",
      "Step 2772, Loss Train: 2.9837, Loss Test: 3.1113, LR: 0.000029\n",
      "Step 2773, Loss Train: 2.7075, Loss Test: 4.1330, LR: 0.000029\n",
      "Step 2774, Loss Train: 3.0931, Loss Test: 2.8581, LR: 0.000029\n",
      "Step 2775, Loss Train: 2.9779, Loss Test: 3.1114, LR: 0.000029\n",
      "Step 2776, Loss Train: 2.8441, Loss Test: 3.5621, LR: 0.000029\n",
      "Step 2777, Loss Train: 2.9917, Loss Test: 4.1964, LR: 0.000029\n",
      "Step 2778, Loss Train: 2.9608, Loss Test: 3.9074, LR: 0.000029\n",
      "Step 2779, Loss Train: 2.8431, Loss Test: 3.9101, LR: 0.000029\n",
      "Step 2780, Loss Train: 2.8824, Loss Test: 3.4703, LR: 0.000029\n",
      "Step 2781, Loss Train: 2.9864, Loss Test: 3.6050, LR: 0.000029\n",
      "Step 2782, Loss Train: 2.9693, Loss Test: 3.5275, LR: 0.000029\n",
      "Step 2783, Loss Train: 3.0150, Loss Test: 3.6160, LR: 0.000029\n",
      "Step 2784, Loss Train: 2.7736, Loss Test: 4.3389, LR: 0.000029\n",
      "Step 2785, Loss Train: 2.9449, Loss Test: 2.9758, LR: 0.000029\n",
      "Step 2786, Loss Train: 2.8893, Loss Test: 3.8298, LR: 0.000029\n",
      "Step 2787, Loss Train: 3.1105, Loss Test: 3.6959, LR: 0.000029\n",
      "Step 2788, Loss Train: 2.9614, Loss Test: 4.3249, LR: 0.000029\n",
      "Step 2789, Loss Train: 2.9701, Loss Test: 3.9977, LR: 0.000029\n",
      "Step 2790, Loss Train: 3.0844, Loss Test: 3.2054, LR: 0.000029\n",
      "Step 2791, Loss Train: 3.0017, Loss Test: 4.4916, LR: 0.000029\n",
      "Step 2792, Loss Train: 3.0105, Loss Test: 3.0590, LR: 0.000029\n",
      "Step 2793, Loss Train: 2.9852, Loss Test: 4.3161, LR: 0.000029\n",
      "Step 2794, Loss Train: 2.8779, Loss Test: 3.0997, LR: 0.000029\n",
      "Step 2795, Loss Train: 2.8320, Loss Test: 3.6858, LR: 0.000029\n",
      "Step 2796, Loss Train: 2.9458, Loss Test: 4.6285, LR: 0.000029\n",
      "Step 2797, Loss Train: 2.6453, Loss Test: 4.7372, LR: 0.000029\n",
      "Step 2798, Loss Train: 2.9373, Loss Test: 3.5416, LR: 0.000029\n",
      "Step 2799, Loss Train: 2.9541, Loss Test: 3.3613, LR: 0.000029\n",
      "Step 2800, Loss Train: 2.7116, Loss Test: 4.5202, LR: 0.000029\n",
      "Step 2801, Loss Train: 2.6969, Loss Test: 4.2495, LR: 0.000029\n",
      "Step 2802, Loss Train: 2.9882, Loss Test: 4.1858, LR: 0.000028\n",
      "Step 2803, Loss Train: 2.7162, Loss Test: 4.1274, LR: 0.000028\n",
      "Step 2804, Loss Train: 2.9137, Loss Test: 3.7232, LR: 0.000028\n",
      "Step 2805, Loss Train: 3.0552, Loss Test: 3.8925, LR: 0.000028\n",
      "Step 2806, Loss Train: 2.9934, Loss Test: 2.9805, LR: 0.000028\n",
      "Step 2807, Loss Train: 3.0784, Loss Test: 4.4231, LR: 0.000028\n",
      "Step 2808, Loss Train: 2.9267, Loss Test: 3.7681, LR: 0.000028\n",
      "Step 2809, Loss Train: 2.9099, Loss Test: 4.2738, LR: 0.000028\n",
      "Step 2810, Loss Train: 2.9688, Loss Test: 2.8755, LR: 0.000028\n",
      "Step 2811, Loss Train: 3.0514, Loss Test: 3.4725, LR: 0.000028\n",
      "Step 2812, Loss Train: 2.9800, Loss Test: 4.0543, LR: 0.000028\n",
      "Step 2813, Loss Train: 2.9224, Loss Test: 4.3350, LR: 0.000028\n",
      "Step 2814, Loss Train: 2.8268, Loss Test: 3.7256, LR: 0.000028\n",
      "Step 2815, Loss Train: 2.9086, Loss Test: 3.7971, LR: 0.000028\n",
      "Step 2816, Loss Train: 3.1041, Loss Test: 4.1456, LR: 0.000028\n",
      "Step 2817, Loss Train: 2.8507, Loss Test: 3.6296, LR: 0.000028\n",
      "Step 2818, Loss Train: 3.0622, Loss Test: 3.5429, LR: 0.000028\n",
      "Step 2819, Loss Train: 3.0455, Loss Test: 4.1559, LR: 0.000028\n",
      "Step 2820, Loss Train: 3.0053, Loss Test: 3.9299, LR: 0.000028\n",
      "Step 2821, Loss Train: 2.6899, Loss Test: 4.2524, LR: 0.000028\n",
      "Step 2822, Loss Train: 3.0178, Loss Test: 2.7201, LR: 0.000028\n",
      "Step 2823, Loss Train: 2.9587, Loss Test: 3.4917, LR: 0.000028\n",
      "Step 2824, Loss Train: 2.8301, Loss Test: 3.5849, LR: 0.000028\n",
      "Step 2825, Loss Train: 2.9399, Loss Test: 3.9706, LR: 0.000028\n",
      "Step 2826, Loss Train: 3.0923, Loss Test: 3.3365, LR: 0.000028\n",
      "Step 2827, Loss Train: 2.7656, Loss Test: 3.9892, LR: 0.000028\n",
      "Step 2828, Loss Train: 3.0979, Loss Test: 3.9223, LR: 0.000028\n",
      "Step 2829, Loss Train: 2.9180, Loss Test: 3.0177, LR: 0.000028\n",
      "Step 2830, Loss Train: 2.9839, Loss Test: 3.5813, LR: 0.000028\n",
      "Step 2831, Loss Train: 2.9836, Loss Test: 2.8783, LR: 0.000028\n",
      "Step 2832, Loss Train: 2.9784, Loss Test: 4.4171, LR: 0.000028\n",
      "Step 2833, Loss Train: 3.0589, Loss Test: 4.0702, LR: 0.000028\n",
      "Step 2834, Loss Train: 2.9046, Loss Test: 3.3246, LR: 0.000028\n",
      "Step 2835, Loss Train: 2.9506, Loss Test: 3.5699, LR: 0.000028\n",
      "Step 2836, Loss Train: 2.8957, Loss Test: 2.7790, LR: 0.000028\n",
      "Step 2837, Loss Train: 2.9758, Loss Test: 2.5059, LR: 0.000028\n",
      "Step 2838, Loss Train: 3.0184, Loss Test: 3.6790, LR: 0.000028\n",
      "Step 2839, Loss Train: 2.9619, Loss Test: 4.2877, LR: 0.000028\n",
      "Step 2840, Loss Train: 2.9339, Loss Test: 3.8345, LR: 0.000028\n",
      "Step 2841, Loss Train: 2.8490, Loss Test: 4.2782, LR: 0.000028\n",
      "Step 2842, Loss Train: 2.9337, Loss Test: 3.7954, LR: 0.000028\n",
      "Step 2843, Loss Train: 2.9274, Loss Test: 3.9741, LR: 0.000028\n",
      "Step 2844, Loss Train: 2.8555, Loss Test: 3.7892, LR: 0.000028\n",
      "Step 2845, Loss Train: 2.9846, Loss Test: 4.2166, LR: 0.000028\n",
      "Step 2846, Loss Train: 3.0042, Loss Test: 2.4268, LR: 0.000028\n",
      "Step 2847, Loss Train: 3.0137, Loss Test: 3.5854, LR: 0.000028\n",
      "Step 2848, Loss Train: 2.9488, Loss Test: 3.6609, LR: 0.000028\n",
      "Step 2849, Loss Train: 2.7422, Loss Test: 3.5172, LR: 0.000028\n",
      "Step 2850, Loss Train: 2.8589, Loss Test: 3.0561, LR: 0.000028\n",
      "Step 2851, Loss Train: 2.8636, Loss Test: 4.1972, LR: 0.000028\n",
      "Step 2852, Loss Train: 2.9734, Loss Test: 4.5268, LR: 0.000028\n",
      "Step 2853, Loss Train: 2.9589, Loss Test: 4.0688, LR: 0.000028\n",
      "Step 2854, Loss Train: 2.9942, Loss Test: 3.6993, LR: 0.000028\n",
      "Step 2855, Loss Train: 2.6434, Loss Test: 3.9900, LR: 0.000028\n",
      "Step 2856, Loss Train: 3.1190, Loss Test: 3.5989, LR: 0.000028\n",
      "Step 2857, Loss Train: 2.7717, Loss Test: 3.6668, LR: 0.000028\n",
      "Step 2858, Loss Train: 2.8977, Loss Test: 3.9080, LR: 0.000028\n",
      "Step 2859, Loss Train: 3.1069, Loss Test: 3.7092, LR: 0.000028\n",
      "Step 2860, Loss Train: 3.1046, Loss Test: 3.4059, LR: 0.000028\n",
      "Step 2861, Loss Train: 2.9534, Loss Test: 3.6313, LR: 0.000028\n",
      "Step 2862, Loss Train: 2.9498, Loss Test: 3.1645, LR: 0.000028\n",
      "Step 2863, Loss Train: 2.9666, Loss Test: 3.8481, LR: 0.000028\n",
      "Step 2864, Loss Train: 2.8501, Loss Test: 3.8510, LR: 0.000028\n",
      "Step 2865, Loss Train: 3.1017, Loss Test: 3.0809, LR: 0.000028\n",
      "Step 2866, Loss Train: 2.8534, Loss Test: 2.0714, LR: 0.000028\n",
      "Step 2867, Loss Train: 2.9472, Loss Test: 2.2806, LR: 0.000028\n",
      "Step 2868, Loss Train: 2.8833, Loss Test: 4.2584, LR: 0.000028\n",
      "Step 2869, Loss Train: 2.8441, Loss Test: 2.9806, LR: 0.000028\n",
      "Step 2870, Loss Train: 3.0300, Loss Test: 4.1263, LR: 0.000028\n",
      "Step 2871, Loss Train: 2.9947, Loss Test: 3.9304, LR: 0.000028\n",
      "Step 2872, Loss Train: 2.8265, Loss Test: 3.6451, LR: 0.000028\n",
      "Step 2873, Loss Train: 3.0681, Loss Test: 3.3786, LR: 0.000028\n",
      "Step 2874, Loss Train: 2.8594, Loss Test: 4.9071, LR: 0.000028\n",
      "Step 2875, Loss Train: 3.0495, Loss Test: 3.6932, LR: 0.000028\n",
      "Step 2876, Loss Train: 3.0787, Loss Test: 3.7262, LR: 0.000028\n",
      "Step 2877, Loss Train: 3.0218, Loss Test: 4.0607, LR: 0.000028\n",
      "Step 2878, Loss Train: 2.9526, Loss Test: 3.6747, LR: 0.000028\n",
      "Step 2879, Loss Train: 2.9647, Loss Test: 3.3636, LR: 0.000028\n",
      "Step 2880, Loss Train: 2.8720, Loss Test: 3.8514, LR: 0.000028\n",
      "Step 2881, Loss Train: 2.9395, Loss Test: 3.1887, LR: 0.000028\n",
      "Step 2882, Loss Train: 2.9150, Loss Test: 4.0749, LR: 0.000028\n",
      "Step 2883, Loss Train: 2.8108, Loss Test: 2.9179, LR: 0.000028\n",
      "Step 2884, Loss Train: 2.9369, Loss Test: 3.3837, LR: 0.000028\n",
      "Step 2885, Loss Train: 2.8996, Loss Test: 3.9713, LR: 0.000028\n",
      "Step 2886, Loss Train: 2.9240, Loss Test: 3.8702, LR: 0.000028\n",
      "Step 2887, Loss Train: 2.7049, Loss Test: 3.8417, LR: 0.000028\n",
      "Step 2888, Loss Train: 2.7728, Loss Test: 3.3820, LR: 0.000028\n",
      "Step 2889, Loss Train: 2.8403, Loss Test: 4.0273, LR: 0.000028\n",
      "Step 2890, Loss Train: 2.9215, Loss Test: 4.0536, LR: 0.000028\n",
      "Step 2891, Loss Train: 2.9653, Loss Test: 3.9238, LR: 0.000028\n",
      "Step 2892, Loss Train: 3.0364, Loss Test: 4.0416, LR: 0.000028\n",
      "Step 2893, Loss Train: 2.8833, Loss Test: 4.3829, LR: 0.000028\n",
      "Step 2894, Loss Train: 2.9580, Loss Test: 3.4691, LR: 0.000028\n",
      "Step 2895, Loss Train: 2.8483, Loss Test: 4.2840, LR: 0.000028\n",
      "Step 2896, Loss Train: 2.8369, Loss Test: 4.1572, LR: 0.000028\n",
      "Step 2897, Loss Train: 2.7376, Loss Test: 2.9807, LR: 0.000028\n",
      "Step 2898, Loss Train: 3.1749, Loss Test: 4.0379, LR: 0.000028\n",
      "Step 2899, Loss Train: 2.8314, Loss Test: 2.3181, LR: 0.000028\n",
      "Step 2900, Loss Train: 2.9370, Loss Test: 4.4495, LR: 0.000028\n",
      "Step 2901, Loss Train: 3.0241, Loss Test: 3.9600, LR: 0.000028\n",
      "Step 2902, Loss Train: 2.7615, Loss Test: 4.0290, LR: 0.000028\n",
      "Step 2903, Loss Train: 3.0587, Loss Test: 4.1310, LR: 0.000028\n",
      "Step 2904, Loss Train: 2.9912, Loss Test: 3.4242, LR: 0.000028\n",
      "Step 2905, Loss Train: 3.0275, Loss Test: 3.6901, LR: 0.000028\n",
      "Step 2906, Loss Train: 2.9216, Loss Test: 3.1212, LR: 0.000028\n",
      "Step 2907, Loss Train: 2.9016, Loss Test: 4.4091, LR: 0.000028\n",
      "Step 2908, Loss Train: 2.9271, Loss Test: 2.6681, LR: 0.000028\n",
      "Step 2909, Loss Train: 3.0297, Loss Test: 3.8141, LR: 0.000028\n",
      "Step 2910, Loss Train: 2.8614, Loss Test: 3.9931, LR: 0.000028\n",
      "Step 2911, Loss Train: 2.7371, Loss Test: 4.4621, LR: 0.000028\n",
      "Step 2912, Loss Train: 2.8000, Loss Test: 4.4711, LR: 0.000028\n",
      "Step 2913, Loss Train: 3.0182, Loss Test: 3.7189, LR: 0.000028\n",
      "Step 2914, Loss Train: 3.0957, Loss Test: 4.2325, LR: 0.000028\n",
      "Step 2915, Loss Train: 2.9105, Loss Test: 3.9304, LR: 0.000028\n",
      "Step 2916, Loss Train: 2.9480, Loss Test: 3.5784, LR: 0.000028\n",
      "Step 2917, Loss Train: 2.7937, Loss Test: 3.8638, LR: 0.000028\n",
      "Step 2918, Loss Train: 3.0547, Loss Test: 3.6057, LR: 0.000028\n",
      "Step 2919, Loss Train: 2.9257, Loss Test: 3.4926, LR: 0.000028\n",
      "Step 2920, Loss Train: 2.9006, Loss Test: 3.1801, LR: 0.000028\n",
      "Step 2921, Loss Train: 3.0655, Loss Test: 3.9119, LR: 0.000028\n",
      "Step 2922, Loss Train: 2.7798, Loss Test: 2.7026, LR: 0.000028\n",
      "Step 2923, Loss Train: 2.9580, Loss Test: 4.3850, LR: 0.000028\n",
      "Step 2924, Loss Train: 2.9473, Loss Test: 3.4677, LR: 0.000028\n",
      "Step 2925, Loss Train: 3.0740, Loss Test: 3.0246, LR: 0.000028\n",
      "Step 2926, Loss Train: 2.7962, Loss Test: 2.1700, LR: 0.000028\n",
      "Step 2927, Loss Train: 2.8620, Loss Test: 3.8121, LR: 0.000028\n",
      "Step 2928, Loss Train: 2.9505, Loss Test: 4.4851, LR: 0.000028\n",
      "Step 2929, Loss Train: 2.8430, Loss Test: 4.0209, LR: 0.000028\n",
      "Step 2930, Loss Train: 3.1230, Loss Test: 3.4725, LR: 0.000028\n",
      "Step 2931, Loss Train: 2.7829, Loss Test: 2.9423, LR: 0.000028\n",
      "Step 2932, Loss Train: 2.8906, Loss Test: 4.3633, LR: 0.000028\n",
      "Step 2933, Loss Train: 2.9756, Loss Test: 2.0339, LR: 0.000028\n",
      "Step 2934, Loss Train: 3.0252, Loss Test: 3.4079, LR: 0.000028\n",
      "Step 2935, Loss Train: 2.9551, Loss Test: 4.1150, LR: 0.000028\n",
      "Step 2936, Loss Train: 2.9887, Loss Test: 3.5273, LR: 0.000028\n",
      "Step 2937, Loss Train: 3.0448, Loss Test: 5.0672, LR: 0.000028\n",
      "Step 2938, Loss Train: 2.9941, Loss Test: 4.0078, LR: 0.000028\n",
      "Step 2939, Loss Train: 2.9927, Loss Test: 2.8010, LR: 0.000028\n",
      "Step 2940, Loss Train: 2.7948, Loss Test: 3.7918, LR: 0.000028\n",
      "Step 2941, Loss Train: 3.1115, Loss Test: 3.8023, LR: 0.000028\n",
      "Step 2942, Loss Train: 2.7867, Loss Test: 4.4736, LR: 0.000028\n",
      "Step 2943, Loss Train: 2.7910, Loss Test: 3.8767, LR: 0.000028\n",
      "Step 2944, Loss Train: 2.8766, Loss Test: 4.0040, LR: 0.000028\n",
      "Step 2945, Loss Train: 2.9678, Loss Test: 4.3605, LR: 0.000028\n",
      "Step 2946, Loss Train: 2.8770, Loss Test: 3.9098, LR: 0.000028\n",
      "Step 2947, Loss Train: 2.9757, Loss Test: 3.7623, LR: 0.000028\n",
      "Step 2948, Loss Train: 2.9751, Loss Test: 3.4021, LR: 0.000028\n",
      "Step 2949, Loss Train: 3.0887, Loss Test: 3.6708, LR: 0.000028\n",
      "Step 2950, Loss Train: 2.9286, Loss Test: 2.9877, LR: 0.000028\n",
      "Step 2951, Loss Train: 2.9734, Loss Test: 3.7756, LR: 0.000028\n",
      "Step 2952, Loss Train: 2.9734, Loss Test: 3.7186, LR: 0.000028\n",
      "Step 2953, Loss Train: 3.0067, Loss Test: 3.3099, LR: 0.000028\n",
      "Step 2954, Loss Train: 3.0570, Loss Test: 4.2097, LR: 0.000028\n",
      "Step 2955, Loss Train: 3.0346, Loss Test: 2.5294, LR: 0.000028\n",
      "Step 2956, Loss Train: 2.8439, Loss Test: 3.6824, LR: 0.000028\n",
      "Step 2957, Loss Train: 2.9287, Loss Test: 2.2460, LR: 0.000028\n",
      "Step 2958, Loss Train: 2.9552, Loss Test: 3.3558, LR: 0.000028\n",
      "Step 2959, Loss Train: 3.1649, Loss Test: 4.1286, LR: 0.000028\n",
      "Step 2960, Loss Train: 2.8496, Loss Test: 3.6676, LR: 0.000028\n",
      "Step 2961, Loss Train: 2.9854, Loss Test: 4.3910, LR: 0.000028\n",
      "Step 2962, Loss Train: 2.9148, Loss Test: 3.8896, LR: 0.000028\n",
      "Step 2963, Loss Train: 3.0185, Loss Test: 4.0996, LR: 0.000028\n",
      "Step 2964, Loss Train: 2.8042, Loss Test: 2.7154, LR: 0.000028\n",
      "Step 2965, Loss Train: 3.0060, Loss Test: 4.3685, LR: 0.000028\n",
      "Step 2966, Loss Train: 2.9379, Loss Test: 4.0716, LR: 0.000028\n",
      "Step 2967, Loss Train: 2.8863, Loss Test: 2.1444, LR: 0.000028\n",
      "Step 2968, Loss Train: 2.9707, Loss Test: 3.0357, LR: 0.000028\n",
      "Step 2969, Loss Train: 2.9691, Loss Test: 4.4704, LR: 0.000028\n",
      "Step 2970, Loss Train: 2.9919, Loss Test: 3.0045, LR: 0.000028\n",
      "Step 2971, Loss Train: 3.0229, Loss Test: 4.2827, LR: 0.000028\n",
      "Step 2972, Loss Train: 2.7941, Loss Test: 4.3072, LR: 0.000028\n",
      "Step 2973, Loss Train: 2.8950, Loss Test: 3.2408, LR: 0.000028\n",
      "Step 2974, Loss Train: 2.9018, Loss Test: 2.9456, LR: 0.000028\n",
      "Step 2975, Loss Train: 3.0531, Loss Test: 4.0833, LR: 0.000028\n",
      "Step 2976, Loss Train: 2.8735, Loss Test: 4.1867, LR: 0.000028\n",
      "Step 2977, Loss Train: 3.0701, Loss Test: 2.1960, LR: 0.000028\n",
      "Step 2978, Loss Train: 3.0616, Loss Test: 4.6388, LR: 0.000028\n",
      "Step 2979, Loss Train: 2.9496, Loss Test: 3.7576, LR: 0.000028\n",
      "Step 2980, Loss Train: 3.0678, Loss Test: 3.6131, LR: 0.000028\n",
      "Step 2981, Loss Train: 3.0558, Loss Test: 4.0766, LR: 0.000028\n",
      "Step 2982, Loss Train: 2.9365, Loss Test: 3.9313, LR: 0.000028\n",
      "Step 2983, Loss Train: 2.8990, Loss Test: 3.4075, LR: 0.000028\n",
      "Step 2984, Loss Train: 3.0976, Loss Test: 2.3082, LR: 0.000028\n",
      "Step 2985, Loss Train: 2.9706, Loss Test: 3.9455, LR: 0.000028\n",
      "Step 2986, Loss Train: 2.9462, Loss Test: 2.8393, LR: 0.000028\n",
      "Step 2987, Loss Train: 2.9029, Loss Test: 4.1070, LR: 0.000028\n",
      "Step 2988, Loss Train: 2.8884, Loss Test: 4.0641, LR: 0.000028\n",
      "Step 2989, Loss Train: 2.9036, Loss Test: 4.1677, LR: 0.000028\n",
      "Step 2990, Loss Train: 2.9689, Loss Test: 3.2355, LR: 0.000028\n",
      "Step 2991, Loss Train: 2.9034, Loss Test: 3.5038, LR: 0.000028\n",
      "Step 2992, Loss Train: 2.9274, Loss Test: 3.2105, LR: 0.000028\n",
      "Step 2993, Loss Train: 2.9744, Loss Test: 3.9277, LR: 0.000028\n",
      "Step 2994, Loss Train: 2.9341, Loss Test: 2.3033, LR: 0.000028\n",
      "Step 2995, Loss Train: 3.0590, Loss Test: 3.8239, LR: 0.000028\n",
      "Step 2996, Loss Train: 2.8689, Loss Test: 3.4233, LR: 0.000028\n",
      "Step 2997, Loss Train: 2.8119, Loss Test: 4.1373, LR: 0.000028\n",
      "Step 2998, Loss Train: 2.9413, Loss Test: 2.4157, LR: 0.000028\n",
      "Step 2999, Loss Train: 3.0734, Loss Test: 4.2836, LR: 0.000028\n",
      "Step 3000, Loss Train: 2.9441, Loss Test: 4.1946, LR: 0.000028\n",
      "Step 3001, Loss Train: 3.0705, Loss Test: 4.2158, LR: 0.000028\n",
      "Step 3002, Loss Train: 2.9156, Loss Test: 2.6986, LR: 0.000028\n",
      "Step 3003, Loss Train: 3.0416, Loss Test: 3.2704, LR: 0.000028\n",
      "Step 3004, Loss Train: 2.8395, Loss Test: 3.3273, LR: 0.000028\n",
      "Step 3005, Loss Train: 3.0187, Loss Test: 3.0272, LR: 0.000028\n",
      "Step 3006, Loss Train: 3.0584, Loss Test: 3.7455, LR: 0.000028\n",
      "Step 3007, Loss Train: 2.9356, Loss Test: 3.1915, LR: 0.000028\n",
      "Step 3008, Loss Train: 2.9078, Loss Test: 3.0974, LR: 0.000028\n",
      "Step 3009, Loss Train: 3.0101, Loss Test: 3.5788, LR: 0.000028\n",
      "Step 3010, Loss Train: 2.7614, Loss Test: 2.9561, LR: 0.000028\n",
      "Step 3011, Loss Train: 3.0093, Loss Test: 4.1013, LR: 0.000028\n",
      "Step 3012, Loss Train: 2.8271, Loss Test: 3.0811, LR: 0.000028\n",
      "Step 3013, Loss Train: 2.8848, Loss Test: 3.9199, LR: 0.000028\n",
      "Step 3014, Loss Train: 2.7570, Loss Test: 3.9881, LR: 0.000028\n",
      "Step 3015, Loss Train: 2.9019, Loss Test: 3.9023, LR: 0.000028\n",
      "Step 3016, Loss Train: 3.0577, Loss Test: 3.7868, LR: 0.000028\n",
      "Step 3017, Loss Train: 2.8618, Loss Test: 3.8970, LR: 0.000028\n",
      "Step 3018, Loss Train: 2.9700, Loss Test: 3.8084, LR: 0.000028\n",
      "Step 3019, Loss Train: 2.8288, Loss Test: 4.0339, LR: 0.000028\n",
      "Step 3020, Loss Train: 2.8838, Loss Test: 3.5636, LR: 0.000028\n",
      "Step 3021, Loss Train: 2.9846, Loss Test: 4.5196, LR: 0.000028\n",
      "Step 3022, Loss Train: 2.6907, Loss Test: 3.4002, LR: 0.000028\n",
      "Step 3023, Loss Train: 3.1569, Loss Test: 4.0714, LR: 0.000028\n",
      "Step 3024, Loss Train: 2.9306, Loss Test: 3.8519, LR: 0.000028\n",
      "Step 3025, Loss Train: 2.9875, Loss Test: 3.6728, LR: 0.000028\n",
      "Step 3026, Loss Train: 2.9364, Loss Test: 3.2883, LR: 0.000028\n",
      "Step 3027, Loss Train: 2.9264, Loss Test: 3.9703, LR: 0.000028\n",
      "Step 3028, Loss Train: 3.0213, Loss Test: 3.2869, LR: 0.000028\n",
      "Step 3029, Loss Train: 2.8319, Loss Test: 3.6966, LR: 0.000028\n",
      "Step 3030, Loss Train: 2.7569, Loss Test: 3.5489, LR: 0.000028\n",
      "Step 3031, Loss Train: 2.9640, Loss Test: 3.9386, LR: 0.000028\n",
      "Step 3032, Loss Train: 2.8766, Loss Test: 3.4999, LR: 0.000028\n",
      "Step 3033, Loss Train: 2.8581, Loss Test: 4.3448, LR: 0.000028\n",
      "Step 3034, Loss Train: 2.9877, Loss Test: 2.7231, LR: 0.000028\n",
      "Step 3035, Loss Train: 2.8541, Loss Test: 4.3200, LR: 0.000028\n",
      "Step 3036, Loss Train: 3.0597, Loss Test: 4.5771, LR: 0.000028\n",
      "Step 3037, Loss Train: 2.9428, Loss Test: 3.4721, LR: 0.000028\n",
      "Step 3038, Loss Train: 3.0091, Loss Test: 3.3049, LR: 0.000028\n",
      "Step 3039, Loss Train: 2.9906, Loss Test: 4.1407, LR: 0.000028\n",
      "Step 3040, Loss Train: 3.0750, Loss Test: 1.9604, LR: 0.000028\n",
      "Step 3041, Loss Train: 2.8150, Loss Test: 3.2586, LR: 0.000028\n",
      "Step 3042, Loss Train: 2.9406, Loss Test: 3.9597, LR: 0.000028\n",
      "Step 3043, Loss Train: 2.8920, Loss Test: 4.1769, LR: 0.000028\n",
      "Step 3044, Loss Train: 2.9001, Loss Test: 3.8055, LR: 0.000028\n",
      "Step 3045, Loss Train: 3.0087, Loss Test: 4.0717, LR: 0.000028\n",
      "Step 3046, Loss Train: 2.9335, Loss Test: 3.9729, LR: 0.000028\n",
      "Step 3047, Loss Train: 2.9475, Loss Test: 3.4352, LR: 0.000028\n",
      "Step 3048, Loss Train: 3.1282, Loss Test: 3.8946, LR: 0.000028\n",
      "Step 3049, Loss Train: 2.9730, Loss Test: 3.6762, LR: 0.000028\n",
      "Step 3050, Loss Train: 3.1381, Loss Test: 4.1091, LR: 0.000028\n",
      "Step 3051, Loss Train: 2.8890, Loss Test: 3.5092, LR: 0.000028\n",
      "Step 3052, Loss Train: 3.1070, Loss Test: 2.8469, LR: 0.000028\n",
      "Step 3053, Loss Train: 3.0623, Loss Test: 3.4949, LR: 0.000028\n",
      "Step 3054, Loss Train: 2.8717, Loss Test: 3.9247, LR: 0.000028\n",
      "Step 3055, Loss Train: 2.8930, Loss Test: 3.6343, LR: 0.000028\n",
      "Step 3056, Loss Train: 2.8886, Loss Test: 2.3619, LR: 0.000028\n",
      "Step 3057, Loss Train: 2.9612, Loss Test: 3.9338, LR: 0.000028\n",
      "Step 3058, Loss Train: 2.8709, Loss Test: 4.2080, LR: 0.000028\n",
      "Step 3059, Loss Train: 2.8221, Loss Test: 4.0221, LR: 0.000028\n",
      "Step 3060, Loss Train: 3.0255, Loss Test: 2.9759, LR: 0.000028\n",
      "Step 3061, Loss Train: 2.7664, Loss Test: 4.0459, LR: 0.000028\n",
      "Step 3062, Loss Train: 3.0987, Loss Test: 3.1748, LR: 0.000028\n",
      "Step 3063, Loss Train: 3.0296, Loss Test: 3.9053, LR: 0.000028\n",
      "Step 3064, Loss Train: 2.8747, Loss Test: 3.1354, LR: 0.000028\n",
      "Step 3065, Loss Train: 2.9009, Loss Test: 3.9452, LR: 0.000028\n",
      "Step 3066, Loss Train: 2.8760, Loss Test: 3.4930, LR: 0.000028\n",
      "Step 3067, Loss Train: 2.9886, Loss Test: 3.6459, LR: 0.000028\n",
      "Step 3068, Loss Train: 2.9770, Loss Test: 3.8082, LR: 0.000028\n",
      "Step 3069, Loss Train: 2.8024, Loss Test: 4.1088, LR: 0.000028\n",
      "Step 3070, Loss Train: 2.9473, Loss Test: 3.9721, LR: 0.000028\n",
      "Step 3071, Loss Train: 2.9480, Loss Test: 4.0290, LR: 0.000028\n",
      "Step 3072, Loss Train: 2.9598, Loss Test: 3.8013, LR: 0.000028\n",
      "Step 3073, Loss Train: 3.1353, Loss Test: 4.2770, LR: 0.000028\n",
      "Step 3074, Loss Train: 2.9778, Loss Test: 3.1583, LR: 0.000028\n",
      "Step 3075, Loss Train: 2.9118, Loss Test: 3.2132, LR: 0.000028\n",
      "Step 3076, Loss Train: 3.0011, Loss Test: 3.3466, LR: 0.000028\n",
      "Step 3077, Loss Train: 2.8403, Loss Test: 3.7818, LR: 0.000028\n",
      "Step 3078, Loss Train: 3.0488, Loss Test: 3.4351, LR: 0.000028\n",
      "Step 3079, Loss Train: 2.8858, Loss Test: 4.1440, LR: 0.000028\n",
      "Step 3080, Loss Train: 2.8409, Loss Test: 4.2279, LR: 0.000028\n",
      "Step 3081, Loss Train: 2.9777, Loss Test: 3.6938, LR: 0.000028\n",
      "Step 3082, Loss Train: 2.9643, Loss Test: 3.9276, LR: 0.000028\n",
      "Step 3083, Loss Train: 2.9503, Loss Test: 3.8805, LR: 0.000028\n",
      "Step 3084, Loss Train: 2.9685, Loss Test: 3.9565, LR: 0.000028\n",
      "Step 3085, Loss Train: 2.9895, Loss Test: 4.0893, LR: 0.000028\n",
      "Step 3086, Loss Train: 2.8285, Loss Test: 3.0965, LR: 0.000028\n",
      "Step 3087, Loss Train: 2.9710, Loss Test: 3.5041, LR: 0.000028\n",
      "Step 3088, Loss Train: 3.0013, Loss Test: 3.6770, LR: 0.000028\n",
      "Step 3089, Loss Train: 2.9308, Loss Test: 3.0921, LR: 0.000028\n",
      "Step 3090, Loss Train: 2.9941, Loss Test: 3.9923, LR: 0.000028\n",
      "Step 3091, Loss Train: 2.9772, Loss Test: 4.0312, LR: 0.000028\n",
      "Step 3092, Loss Train: 2.8906, Loss Test: 3.3100, LR: 0.000028\n",
      "Step 3093, Loss Train: 2.9379, Loss Test: 3.6407, LR: 0.000028\n",
      "Step 3094, Loss Train: 2.8480, Loss Test: 4.2017, LR: 0.000028\n",
      "Step 3095, Loss Train: 2.8327, Loss Test: 2.9907, LR: 0.000028\n",
      "Step 3096, Loss Train: 2.9844, Loss Test: 4.3021, LR: 0.000028\n",
      "Step 3097, Loss Train: 2.9270, Loss Test: 2.5897, LR: 0.000028\n",
      "Step 3098, Loss Train: 2.7609, Loss Test: 3.8073, LR: 0.000028\n",
      "Step 3099, Loss Train: 2.9385, Loss Test: 3.6255, LR: 0.000028\n",
      "Step 3100, Loss Train: 3.0648, Loss Test: 4.5478, LR: 0.000028\n",
      "Step 3101, Loss Train: 3.0384, Loss Test: 4.5560, LR: 0.000028\n",
      "Step 3102, Loss Train: 2.9588, Loss Test: 2.0382, LR: 0.000028\n",
      "Step 3103, Loss Train: 2.9311, Loss Test: 4.2280, LR: 0.000028\n",
      "Step 3104, Loss Train: 3.0373, Loss Test: 3.6023, LR: 0.000028\n",
      "Step 3105, Loss Train: 2.9403, Loss Test: 4.3856, LR: 0.000028\n",
      "Step 3106, Loss Train: 3.0274, Loss Test: 3.3192, LR: 0.000028\n",
      "Step 3107, Loss Train: 3.0670, Loss Test: 3.2172, LR: 0.000028\n",
      "Step 3108, Loss Train: 3.0520, Loss Test: 2.0927, LR: 0.000028\n",
      "Step 3109, Loss Train: 2.9244, Loss Test: 3.3578, LR: 0.000028\n",
      "Step 3110, Loss Train: 2.8257, Loss Test: 2.8570, LR: 0.000028\n",
      "Step 3111, Loss Train: 3.0139, Loss Test: 3.2988, LR: 0.000028\n",
      "Step 3112, Loss Train: 2.8193, Loss Test: 2.7737, LR: 0.000028\n",
      "Step 3113, Loss Train: 2.9526, Loss Test: 2.9233, LR: 0.000028\n",
      "Step 3114, Loss Train: 2.8111, Loss Test: 3.9161, LR: 0.000028\n",
      "Step 3115, Loss Train: 2.9860, Loss Test: 4.0743, LR: 0.000028\n",
      "Step 3116, Loss Train: 3.0257, Loss Test: 3.8507, LR: 0.000028\n",
      "Step 3117, Loss Train: 2.9026, Loss Test: 3.6984, LR: 0.000028\n",
      "Step 3118, Loss Train: 3.0105, Loss Test: 3.6584, LR: 0.000028\n",
      "Step 3119, Loss Train: 2.8705, Loss Test: 3.9400, LR: 0.000028\n",
      "Step 3120, Loss Train: 2.9106, Loss Test: 4.1162, LR: 0.000028\n",
      "Step 3121, Loss Train: 2.8755, Loss Test: 3.6408, LR: 0.000028\n",
      "Step 3122, Loss Train: 2.9414, Loss Test: 3.3878, LR: 0.000028\n",
      "Step 3123, Loss Train: 3.0818, Loss Test: 3.9317, LR: 0.000028\n",
      "Step 3124, Loss Train: 3.0398, Loss Test: 3.6309, LR: 0.000028\n",
      "Step 3125, Loss Train: 2.9454, Loss Test: 4.2946, LR: 0.000028\n",
      "Step 3126, Loss Train: 2.8697, Loss Test: 4.4018, LR: 0.000028\n",
      "Step 3127, Loss Train: 2.7925, Loss Test: 3.5198, LR: 0.000028\n",
      "Step 3128, Loss Train: 2.9650, Loss Test: 3.0307, LR: 0.000028\n",
      "Step 3129, Loss Train: 2.9912, Loss Test: 4.2404, LR: 0.000028\n",
      "Step 3130, Loss Train: 2.9936, Loss Test: 3.1654, LR: 0.000028\n",
      "Step 3131, Loss Train: 2.9711, Loss Test: 3.5884, LR: 0.000028\n",
      "Step 3132, Loss Train: 2.8859, Loss Test: 4.0260, LR: 0.000028\n",
      "Step 3133, Loss Train: 2.9324, Loss Test: 3.6351, LR: 0.000028\n",
      "Step 3134, Loss Train: 2.9325, Loss Test: 4.1497, LR: 0.000028\n",
      "Step 3135, Loss Train: 2.9002, Loss Test: 3.1667, LR: 0.000028\n",
      "Step 3136, Loss Train: 3.0499, Loss Test: 4.1749, LR: 0.000028\n",
      "Step 3137, Loss Train: 2.9774, Loss Test: 3.8307, LR: 0.000028\n",
      "Step 3138, Loss Train: 2.9261, Loss Test: 4.0038, LR: 0.000028\n",
      "Step 3139, Loss Train: 2.9635, Loss Test: 3.0494, LR: 0.000028\n",
      "Step 3140, Loss Train: 2.8749, Loss Test: 3.8326, LR: 0.000028\n",
      "Step 3141, Loss Train: 2.8838, Loss Test: 3.4913, LR: 0.000028\n",
      "Step 3142, Loss Train: 2.8706, Loss Test: 3.0385, LR: 0.000028\n",
      "Step 3143, Loss Train: 2.9354, Loss Test: 2.8947, LR: 0.000028\n",
      "Step 3144, Loss Train: 3.0337, Loss Test: 4.2187, LR: 0.000028\n",
      "Step 3145, Loss Train: 2.9562, Loss Test: 4.2405, LR: 0.000028\n",
      "Step 3146, Loss Train: 2.9736, Loss Test: 4.4141, LR: 0.000028\n",
      "Step 3147, Loss Train: 2.8409, Loss Test: 3.7548, LR: 0.000028\n",
      "Step 3148, Loss Train: 2.9442, Loss Test: 4.1526, LR: 0.000028\n",
      "Step 3149, Loss Train: 2.8358, Loss Test: 3.7401, LR: 0.000028\n",
      "Step 3150, Loss Train: 2.8364, Loss Test: 4.1678, LR: 0.000028\n",
      "Step 3151, Loss Train: 2.8986, Loss Test: 3.9450, LR: 0.000028\n",
      "Step 3152, Loss Train: 2.8442, Loss Test: 3.9706, LR: 0.000028\n",
      "Step 3153, Loss Train: 2.9368, Loss Test: 2.7241, LR: 0.000028\n",
      "Step 3154, Loss Train: 2.9274, Loss Test: 2.2217, LR: 0.000028\n",
      "Step 3155, Loss Train: 2.9316, Loss Test: 3.3369, LR: 0.000028\n",
      "Step 3156, Loss Train: 2.8961, Loss Test: 3.1629, LR: 0.000028\n",
      "Step 3157, Loss Train: 2.9197, Loss Test: 3.3950, LR: 0.000028\n",
      "Step 3158, Loss Train: 2.9301, Loss Test: 3.6979, LR: 0.000028\n",
      "Step 3159, Loss Train: 2.8475, Loss Test: 2.7115, LR: 0.000028\n",
      "Step 3160, Loss Train: 3.0304, Loss Test: 2.7065, LR: 0.000028\n",
      "Step 3161, Loss Train: 2.9152, Loss Test: 4.0408, LR: 0.000028\n",
      "Step 3162, Loss Train: 2.8601, Loss Test: 3.9122, LR: 0.000028\n",
      "Step 3163, Loss Train: 3.0547, Loss Test: 3.9179, LR: 0.000028\n",
      "Step 3164, Loss Train: 2.9514, Loss Test: 4.7558, LR: 0.000028\n",
      "Step 3165, Loss Train: 2.9620, Loss Test: 2.2370, LR: 0.000028\n",
      "Step 3166, Loss Train: 2.7033, Loss Test: 3.7969, LR: 0.000028\n",
      "Step 3167, Loss Train: 3.1112, Loss Test: 3.5806, LR: 0.000028\n",
      "Step 3168, Loss Train: 2.8753, Loss Test: 3.8671, LR: 0.000028\n",
      "Step 3169, Loss Train: 3.0693, Loss Test: 3.3825, LR: 0.000028\n",
      "Step 3170, Loss Train: 2.7222, Loss Test: 3.2372, LR: 0.000028\n",
      "Step 3171, Loss Train: 2.7057, Loss Test: 4.0645, LR: 0.000028\n",
      "Step 3172, Loss Train: 2.9242, Loss Test: 3.6986, LR: 0.000028\n",
      "Step 3173, Loss Train: 2.9275, Loss Test: 3.4769, LR: 0.000028\n",
      "Step 3174, Loss Train: 2.9580, Loss Test: 4.0669, LR: 0.000028\n",
      "Step 3175, Loss Train: 2.7241, Loss Test: 3.0684, LR: 0.000028\n",
      "Step 3176, Loss Train: 3.1144, Loss Test: 4.2816, LR: 0.000028\n",
      "Step 3177, Loss Train: 2.8328, Loss Test: 4.1005, LR: 0.000028\n",
      "Step 3178, Loss Train: 2.8398, Loss Test: 4.2538, LR: 0.000028\n",
      "Step 3179, Loss Train: 2.9768, Loss Test: 3.7990, LR: 0.000028\n",
      "Step 3180, Loss Train: 3.1760, Loss Test: 4.2085, LR: 0.000028\n",
      "Step 3181, Loss Train: 3.0033, Loss Test: 4.1599, LR: 0.000028\n",
      "Step 3182, Loss Train: 3.0052, Loss Test: 3.5206, LR: 0.000028\n",
      "Step 3183, Loss Train: 3.0362, Loss Test: 3.7549, LR: 0.000028\n",
      "Step 3184, Loss Train: 2.8697, Loss Test: 3.1708, LR: 0.000028\n",
      "Step 3185, Loss Train: 3.0302, Loss Test: 4.2175, LR: 0.000028\n",
      "Step 3186, Loss Train: 2.9602, Loss Test: 4.3682, LR: 0.000028\n",
      "Step 3187, Loss Train: 3.0129, Loss Test: 3.3665, LR: 0.000028\n",
      "Step 3188, Loss Train: 2.9425, Loss Test: 3.5574, LR: 0.000028\n",
      "Step 3189, Loss Train: 2.8339, Loss Test: 4.2014, LR: 0.000028\n",
      "Step 3190, Loss Train: 2.9449, Loss Test: 3.7015, LR: 0.000028\n",
      "Step 3191, Loss Train: 2.8779, Loss Test: 3.7458, LR: 0.000028\n",
      "Step 3192, Loss Train: 2.9810, Loss Test: 4.3253, LR: 0.000028\n",
      "Step 3193, Loss Train: 2.9088, Loss Test: 4.1999, LR: 0.000028\n",
      "Step 3194, Loss Train: 2.8602, Loss Test: 3.6037, LR: 0.000028\n",
      "Step 3195, Loss Train: 2.8672, Loss Test: 3.8209, LR: 0.000028\n",
      "Step 3196, Loss Train: 2.7605, Loss Test: 3.7855, LR: 0.000028\n",
      "Step 3197, Loss Train: 3.0876, Loss Test: 3.7388, LR: 0.000028\n",
      "Step 3198, Loss Train: 3.0291, Loss Test: 4.2899, LR: 0.000028\n",
      "Step 3199, Loss Train: 3.0316, Loss Test: 2.4480, LR: 0.000028\n",
      "Step 3200, Loss Train: 2.8643, Loss Test: 3.9149, LR: 0.000028\n",
      "Step 3201, Loss Train: 2.5749, Loss Test: 4.7527, LR: 0.000028\n",
      "Step 3202, Loss Train: 3.0539, Loss Test: 4.5253, LR: 0.000028\n",
      "Step 3203, Loss Train: 3.0569, Loss Test: 4.0379, LR: 0.000028\n",
      "Step 3204, Loss Train: 2.9400, Loss Test: 3.0549, LR: 0.000028\n",
      "Step 3205, Loss Train: 3.0073, Loss Test: 3.7831, LR: 0.000028\n",
      "Step 3206, Loss Train: 2.8946, Loss Test: 4.0181, LR: 0.000028\n",
      "Step 3207, Loss Train: 2.7929, Loss Test: 3.6160, LR: 0.000028\n",
      "Step 3208, Loss Train: 3.1354, Loss Test: 3.4058, LR: 0.000028\n",
      "Step 3209, Loss Train: 2.9398, Loss Test: 3.6528, LR: 0.000028\n",
      "Step 3210, Loss Train: 3.0297, Loss Test: 3.6041, LR: 0.000028\n",
      "Step 3211, Loss Train: 2.9632, Loss Test: 3.3741, LR: 0.000028\n",
      "Step 3212, Loss Train: 2.9445, Loss Test: 4.1967, LR: 0.000028\n",
      "Step 3213, Loss Train: 3.0071, Loss Test: 3.8174, LR: 0.000028\n",
      "Step 3214, Loss Train: 2.8845, Loss Test: 3.5301, LR: 0.000028\n",
      "Step 3215, Loss Train: 3.0150, Loss Test: 3.6429, LR: 0.000028\n",
      "Step 3216, Loss Train: 3.0089, Loss Test: 4.2918, LR: 0.000028\n",
      "Step 3217, Loss Train: 2.8650, Loss Test: 3.6176, LR: 0.000028\n",
      "Step 3218, Loss Train: 2.7973, Loss Test: 3.5118, LR: 0.000028\n",
      "Step 3219, Loss Train: 2.9930, Loss Test: 4.2207, LR: 0.000028\n",
      "Step 3220, Loss Train: 2.8877, Loss Test: 4.0824, LR: 0.000028\n",
      "Step 3221, Loss Train: 2.8504, Loss Test: 2.8157, LR: 0.000028\n",
      "Step 3222, Loss Train: 2.8720, Loss Test: 3.3017, LR: 0.000028\n",
      "Step 3223, Loss Train: 2.9249, Loss Test: 4.1707, LR: 0.000028\n",
      "Step 3224, Loss Train: 2.8176, Loss Test: 2.7320, LR: 0.000028\n",
      "Step 3225, Loss Train: 2.8801, Loss Test: 3.3688, LR: 0.000028\n",
      "Step 3226, Loss Train: 2.8179, Loss Test: 4.2459, LR: 0.000028\n",
      "Step 3227, Loss Train: 2.8815, Loss Test: 3.8752, LR: 0.000028\n",
      "Step 3228, Loss Train: 2.8838, Loss Test: 3.5170, LR: 0.000028\n",
      "Step 3229, Loss Train: 2.8674, Loss Test: 4.0340, LR: 0.000028\n",
      "Step 3230, Loss Train: 3.0165, Loss Test: 3.4361, LR: 0.000028\n",
      "Step 3231, Loss Train: 2.8068, Loss Test: 4.1723, LR: 0.000028\n",
      "Step 3232, Loss Train: 2.8336, Loss Test: 4.2920, LR: 0.000028\n",
      "Step 3233, Loss Train: 2.9464, Loss Test: 3.3885, LR: 0.000028\n",
      "Step 3234, Loss Train: 2.8538, Loss Test: 3.9383, LR: 0.000028\n",
      "Step 3235, Loss Train: 3.0635, Loss Test: 3.9940, LR: 0.000028\n",
      "Step 3236, Loss Train: 2.7878, Loss Test: 4.4788, LR: 0.000028\n",
      "Step 3237, Loss Train: 3.0637, Loss Test: 3.5661, LR: 0.000028\n",
      "Step 3238, Loss Train: 2.9719, Loss Test: 4.4761, LR: 0.000028\n",
      "Step 3239, Loss Train: 2.8905, Loss Test: 2.9936, LR: 0.000028\n",
      "Step 3240, Loss Train: 2.9731, Loss Test: 4.1557, LR: 0.000028\n",
      "Step 3241, Loss Train: 2.9634, Loss Test: 4.3244, LR: 0.000028\n",
      "Step 3242, Loss Train: 3.0236, Loss Test: 3.7835, LR: 0.000028\n",
      "Step 3243, Loss Train: 2.8279, Loss Test: 3.6107, LR: 0.000028\n",
      "Step 3244, Loss Train: 2.9601, Loss Test: 3.3986, LR: 0.000028\n",
      "Step 3245, Loss Train: 3.0665, Loss Test: 3.9794, LR: 0.000028\n",
      "Step 3246, Loss Train: 2.8120, Loss Test: 3.4110, LR: 0.000028\n",
      "Step 3247, Loss Train: 2.9644, Loss Test: 3.7406, LR: 0.000028\n",
      "Step 3248, Loss Train: 2.9286, Loss Test: 3.7753, LR: 0.000028\n",
      "Step 3249, Loss Train: 2.9512, Loss Test: 3.5725, LR: 0.000028\n",
      "Step 3250, Loss Train: 2.9408, Loss Test: 3.7318, LR: 0.000028\n",
      "Step 3251, Loss Train: 2.9017, Loss Test: 3.3676, LR: 0.000028\n",
      "Step 3252, Loss Train: 3.0422, Loss Test: 4.2752, LR: 0.000028\n",
      "Step 3253, Loss Train: 2.7415, Loss Test: 3.8129, LR: 0.000028\n",
      "Step 3254, Loss Train: 2.8570, Loss Test: 4.4310, LR: 0.000028\n",
      "Step 3255, Loss Train: 3.0344, Loss Test: 3.0749, LR: 0.000028\n",
      "Step 3256, Loss Train: 2.9039, Loss Test: 3.5563, LR: 0.000028\n",
      "Step 3257, Loss Train: 3.1679, Loss Test: 4.1293, LR: 0.000028\n",
      "Step 3258, Loss Train: 2.9045, Loss Test: 4.3362, LR: 0.000028\n",
      "Step 3259, Loss Train: 3.0418, Loss Test: 4.4332, LR: 0.000028\n",
      "Step 3260, Loss Train: 2.9320, Loss Test: 3.8236, LR: 0.000028\n",
      "Step 3261, Loss Train: 3.0430, Loss Test: 3.0275, LR: 0.000028\n",
      "Step 3262, Loss Train: 2.9406, Loss Test: 4.3570, LR: 0.000028\n",
      "Step 3263, Loss Train: 2.9676, Loss Test: 3.3000, LR: 0.000028\n",
      "Step 3264, Loss Train: 2.8071, Loss Test: 2.8089, LR: 0.000028\n",
      "Step 3265, Loss Train: 2.9758, Loss Test: 3.9815, LR: 0.000028\n",
      "Step 3266, Loss Train: 2.9253, Loss Test: 3.3728, LR: 0.000028\n",
      "Step 3267, Loss Train: 3.0169, Loss Test: 3.7223, LR: 0.000028\n",
      "Step 3268, Loss Train: 2.8692, Loss Test: 3.8170, LR: 0.000028\n",
      "Step 3269, Loss Train: 2.9294, Loss Test: 3.4046, LR: 0.000028\n",
      "Step 3270, Loss Train: 2.9470, Loss Test: 4.2163, LR: 0.000028\n",
      "Step 3271, Loss Train: 2.8112, Loss Test: 3.3839, LR: 0.000028\n",
      "Step 3272, Loss Train: 2.8326, Loss Test: 3.2023, LR: 0.000028\n",
      "Step 3273, Loss Train: 2.8294, Loss Test: 3.1606, LR: 0.000028\n",
      "Step 3274, Loss Train: 2.9017, Loss Test: 3.4649, LR: 0.000028\n",
      "Step 3275, Loss Train: 2.9835, Loss Test: 3.9503, LR: 0.000028\n",
      "Step 3276, Loss Train: 3.0066, Loss Test: 3.8491, LR: 0.000028\n",
      "Step 3277, Loss Train: 2.8860, Loss Test: 3.3607, LR: 0.000028\n",
      "Step 3278, Loss Train: 2.9083, Loss Test: 4.1944, LR: 0.000028\n",
      "Step 3279, Loss Train: 3.0516, Loss Test: 3.8046, LR: 0.000028\n",
      "Step 3280, Loss Train: 2.6910, Loss Test: 4.2447, LR: 0.000028\n",
      "Step 3281, Loss Train: 2.9459, Loss Test: 3.1962, LR: 0.000028\n",
      "Step 3282, Loss Train: 2.9700, Loss Test: 4.2480, LR: 0.000028\n",
      "Step 3283, Loss Train: 2.7804, Loss Test: 3.3459, LR: 0.000028\n",
      "Step 3284, Loss Train: 3.0783, Loss Test: 4.2412, LR: 0.000028\n",
      "Step 3285, Loss Train: 3.0373, Loss Test: 3.8678, LR: 0.000028\n",
      "Step 3286, Loss Train: 2.8578, Loss Test: 3.4065, LR: 0.000028\n",
      "Step 3287, Loss Train: 2.9987, Loss Test: 3.7219, LR: 0.000028\n",
      "Step 3288, Loss Train: 2.8983, Loss Test: 3.6339, LR: 0.000028\n",
      "Step 3289, Loss Train: 2.9367, Loss Test: 3.6277, LR: 0.000028\n",
      "Step 3290, Loss Train: 2.8634, Loss Test: 2.7677, LR: 0.000028\n",
      "Step 3291, Loss Train: 2.9311, Loss Test: 3.7845, LR: 0.000028\n",
      "Step 3292, Loss Train: 2.8980, Loss Test: 4.0020, LR: 0.000028\n",
      "Step 3293, Loss Train: 2.7550, Loss Test: 3.0005, LR: 0.000028\n",
      "Step 3294, Loss Train: 2.7314, Loss Test: 4.3783, LR: 0.000028\n",
      "Step 3295, Loss Train: 2.9525, Loss Test: 3.5361, LR: 0.000028\n",
      "Step 3296, Loss Train: 2.8140, Loss Test: 3.9916, LR: 0.000028\n",
      "Step 3297, Loss Train: 2.6795, Loss Test: 2.4048, LR: 0.000028\n",
      "Step 3298, Loss Train: 2.8756, Loss Test: 3.6574, LR: 0.000028\n",
      "Step 3299, Loss Train: 2.8876, Loss Test: 3.7562, LR: 0.000028\n",
      "Step 3300, Loss Train: 2.8312, Loss Test: 4.1783, LR: 0.000028\n",
      "Step 3301, Loss Train: 2.8217, Loss Test: 3.9258, LR: 0.000028\n",
      "Step 3302, Loss Train: 2.8241, Loss Test: 3.5914, LR: 0.000028\n",
      "Step 3303, Loss Train: 2.8498, Loss Test: 3.3500, LR: 0.000028\n",
      "Step 3304, Loss Train: 2.8850, Loss Test: 4.1312, LR: 0.000028\n",
      "Step 3305, Loss Train: 3.0680, Loss Test: 3.9744, LR: 0.000028\n",
      "Step 3306, Loss Train: 3.0187, Loss Test: 2.3715, LR: 0.000028\n",
      "Step 3307, Loss Train: 2.8933, Loss Test: 3.8379, LR: 0.000028\n",
      "Step 3308, Loss Train: 2.9723, Loss Test: 2.3158, LR: 0.000028\n",
      "Step 3309, Loss Train: 3.0250, Loss Test: 3.0491, LR: 0.000028\n",
      "Step 3310, Loss Train: 3.0143, Loss Test: 4.0762, LR: 0.000028\n",
      "Step 3311, Loss Train: 3.0397, Loss Test: 2.8247, LR: 0.000028\n",
      "Step 3312, Loss Train: 2.9900, Loss Test: 4.0813, LR: 0.000028\n",
      "Step 3313, Loss Train: 2.7925, Loss Test: 3.9585, LR: 0.000028\n",
      "Step 3314, Loss Train: 2.7736, Loss Test: 4.0472, LR: 0.000028\n",
      "Step 3315, Loss Train: 2.8377, Loss Test: 2.5689, LR: 0.000028\n",
      "Step 3316, Loss Train: 2.8237, Loss Test: 3.7087, LR: 0.000028\n",
      "Step 3317, Loss Train: 2.9149, Loss Test: 3.4500, LR: 0.000028\n",
      "Step 3318, Loss Train: 2.9346, Loss Test: 3.8909, LR: 0.000028\n",
      "Step 3319, Loss Train: 2.9343, Loss Test: 3.8930, LR: 0.000028\n",
      "Step 3320, Loss Train: 2.8668, Loss Test: 3.4244, LR: 0.000028\n",
      "Step 3321, Loss Train: 3.0118, Loss Test: 3.8090, LR: 0.000028\n",
      "Step 3322, Loss Train: 2.7626, Loss Test: 4.1853, LR: 0.000028\n",
      "Step 3323, Loss Train: 2.9350, Loss Test: 3.5575, LR: 0.000028\n",
      "Step 3324, Loss Train: 2.9628, Loss Test: 3.4015, LR: 0.000028\n",
      "Step 3325, Loss Train: 2.8832, Loss Test: 4.0035, LR: 0.000028\n",
      "Step 3326, Loss Train: 3.0430, Loss Test: 3.9050, LR: 0.000028\n",
      "Step 3327, Loss Train: 2.9579, Loss Test: 3.4814, LR: 0.000028\n",
      "Step 3328, Loss Train: 2.7473, Loss Test: 3.9010, LR: 0.000028\n",
      "Step 3329, Loss Train: 2.7709, Loss Test: 3.7821, LR: 0.000028\n",
      "Step 3330, Loss Train: 2.9648, Loss Test: 3.8349, LR: 0.000028\n",
      "Step 3331, Loss Train: 3.0009, Loss Test: 4.9131, LR: 0.000028\n",
      "Step 3332, Loss Train: 2.9817, Loss Test: 3.4368, LR: 0.000028\n",
      "Step 3333, Loss Train: 2.8285, Loss Test: 3.0695, LR: 0.000028\n",
      "Step 3334, Loss Train: 2.9162, Loss Test: 4.0766, LR: 0.000028\n",
      "Step 3335, Loss Train: 3.0186, Loss Test: 3.7594, LR: 0.000028\n",
      "Step 3336, Loss Train: 2.8436, Loss Test: 4.1005, LR: 0.000028\n",
      "Step 3337, Loss Train: 2.7404, Loss Test: 3.7806, LR: 0.000028\n",
      "Step 3338, Loss Train: 2.9285, Loss Test: 3.8766, LR: 0.000028\n",
      "Step 3339, Loss Train: 2.7744, Loss Test: 3.4753, LR: 0.000028\n",
      "Step 3340, Loss Train: 3.0382, Loss Test: 3.9135, LR: 0.000028\n",
      "Step 3341, Loss Train: 2.9033, Loss Test: 4.4945, LR: 0.000028\n",
      "Step 3342, Loss Train: 2.8315, Loss Test: 4.3727, LR: 0.000028\n",
      "Step 3343, Loss Train: 2.8618, Loss Test: 3.5896, LR: 0.000028\n",
      "Step 3344, Loss Train: 2.9401, Loss Test: 2.8012, LR: 0.000028\n",
      "Step 3345, Loss Train: 2.8874, Loss Test: 2.8495, LR: 0.000028\n",
      "Step 3346, Loss Train: 3.1088, Loss Test: 4.3613, LR: 0.000028\n",
      "Step 3347, Loss Train: 2.7672, Loss Test: 3.8658, LR: 0.000028\n",
      "Step 3348, Loss Train: 2.6792, Loss Test: 4.0457, LR: 0.000028\n",
      "Step 3349, Loss Train: 2.8102, Loss Test: 4.0569, LR: 0.000028\n",
      "Step 3350, Loss Train: 2.9327, Loss Test: 4.2437, LR: 0.000028\n",
      "Step 3351, Loss Train: 2.9168, Loss Test: 3.9571, LR: 0.000028\n",
      "Step 3352, Loss Train: 3.1051, Loss Test: 4.4328, LR: 0.000028\n",
      "Step 3353, Loss Train: 2.7136, Loss Test: 3.4211, LR: 0.000028\n",
      "Step 3354, Loss Train: 2.8977, Loss Test: 3.8098, LR: 0.000028\n",
      "Step 3355, Loss Train: 2.8562, Loss Test: 4.1646, LR: 0.000028\n",
      "Step 3356, Loss Train: 2.9077, Loss Test: 3.8829, LR: 0.000028\n",
      "Step 3357, Loss Train: 2.9513, Loss Test: 3.9659, LR: 0.000028\n",
      "Step 3358, Loss Train: 3.1033, Loss Test: 3.9169, LR: 0.000028\n",
      "Step 3359, Loss Train: 2.8348, Loss Test: 2.7295, LR: 0.000028\n",
      "Step 3360, Loss Train: 2.8610, Loss Test: 3.9325, LR: 0.000028\n",
      "Step 3361, Loss Train: 2.7359, Loss Test: 4.3355, LR: 0.000028\n",
      "Step 3362, Loss Train: 2.8339, Loss Test: 4.0705, LR: 0.000028\n",
      "Step 3363, Loss Train: 2.9595, Loss Test: 3.9468, LR: 0.000028\n",
      "Step 3364, Loss Train: 2.9818, Loss Test: 4.1158, LR: 0.000028\n",
      "Step 3365, Loss Train: 2.6878, Loss Test: 2.3590, LR: 0.000028\n",
      "Step 3366, Loss Train: 2.7886, Loss Test: 4.3019, LR: 0.000028\n",
      "Step 3367, Loss Train: 2.9245, Loss Test: 4.3569, LR: 0.000028\n",
      "Step 3368, Loss Train: 2.9775, Loss Test: 4.4554, LR: 0.000028\n",
      "Step 3369, Loss Train: 2.9533, Loss Test: 3.6593, LR: 0.000028\n",
      "Step 3370, Loss Train: 3.0477, Loss Test: 4.2705, LR: 0.000028\n",
      "Step 3371, Loss Train: 2.8553, Loss Test: 3.5101, LR: 0.000028\n",
      "Step 3372, Loss Train: 2.9206, Loss Test: 4.1199, LR: 0.000028\n",
      "Step 3373, Loss Train: 2.9311, Loss Test: 4.1318, LR: 0.000028\n",
      "Step 3374, Loss Train: 2.7961, Loss Test: 3.6436, LR: 0.000028\n",
      "Step 3375, Loss Train: 2.8710, Loss Test: 4.6204, LR: 0.000028\n",
      "Step 3376, Loss Train: 2.9918, Loss Test: 3.5387, LR: 0.000028\n",
      "Step 3377, Loss Train: 2.9652, Loss Test: 4.7385, LR: 0.000028\n",
      "Step 3378, Loss Train: 2.8910, Loss Test: 3.9669, LR: 0.000028\n",
      "Step 3379, Loss Train: 2.8086, Loss Test: 3.1105, LR: 0.000028\n",
      "Step 3380, Loss Train: 2.8552, Loss Test: 2.9867, LR: 0.000028\n",
      "Step 3381, Loss Train: 2.8104, Loss Test: 3.6730, LR: 0.000028\n",
      "Step 3382, Loss Train: 3.0083, Loss Test: 3.9630, LR: 0.000028\n",
      "Step 3383, Loss Train: 2.9944, Loss Test: 3.9875, LR: 0.000028\n",
      "Step 3384, Loss Train: 2.9938, Loss Test: 3.5557, LR: 0.000028\n",
      "Step 3385, Loss Train: 3.0266, Loss Test: 3.5459, LR: 0.000028\n",
      "Step 3386, Loss Train: 2.9043, Loss Test: 3.2707, LR: 0.000028\n",
      "Step 3387, Loss Train: 3.0571, Loss Test: 3.4421, LR: 0.000028\n",
      "Step 3388, Loss Train: 3.0660, Loss Test: 2.7280, LR: 0.000028\n",
      "Step 3389, Loss Train: 2.8474, Loss Test: 2.7602, LR: 0.000028\n",
      "Step 3390, Loss Train: 3.0529, Loss Test: 4.5004, LR: 0.000028\n",
      "Step 3391, Loss Train: 2.7913, Loss Test: 4.0634, LR: 0.000028\n",
      "Step 3392, Loss Train: 2.8540, Loss Test: 3.2077, LR: 0.000028\n",
      "Step 3393, Loss Train: 2.9407, Loss Test: 3.2507, LR: 0.000028\n",
      "Step 3394, Loss Train: 2.9537, Loss Test: 3.4631, LR: 0.000028\n",
      "Step 3395, Loss Train: 2.8813, Loss Test: 2.7897, LR: 0.000028\n",
      "Step 3396, Loss Train: 2.8644, Loss Test: 3.1229, LR: 0.000028\n",
      "Step 3397, Loss Train: 2.8521, Loss Test: 3.1656, LR: 0.000028\n",
      "Step 3398, Loss Train: 3.0080, Loss Test: 3.4016, LR: 0.000028\n",
      "Step 3399, Loss Train: 2.8227, Loss Test: 3.9794, LR: 0.000028\n",
      "Step 3400, Loss Train: 3.0567, Loss Test: 3.8423, LR: 0.000028\n",
      "Step 3401, Loss Train: 3.0497, Loss Test: 4.3923, LR: 0.000028\n",
      "Step 3402, Loss Train: 2.9920, Loss Test: 2.8983, LR: 0.000028\n",
      "Step 3403, Loss Train: 2.9211, Loss Test: 3.6612, LR: 0.000028\n",
      "Step 3404, Loss Train: 3.0421, Loss Test: 3.8191, LR: 0.000028\n",
      "Step 3405, Loss Train: 2.9702, Loss Test: 3.9792, LR: 0.000028\n",
      "Step 3406, Loss Train: 2.8743, Loss Test: 3.9497, LR: 0.000028\n",
      "Step 3407, Loss Train: 2.9262, Loss Test: 3.7940, LR: 0.000028\n",
      "Step 3408, Loss Train: 3.0162, Loss Test: 3.9307, LR: 0.000028\n",
      "Step 3409, Loss Train: 2.9461, Loss Test: 4.2497, LR: 0.000028\n",
      "Step 3410, Loss Train: 2.8494, Loss Test: 3.7098, LR: 0.000028\n",
      "Step 3411, Loss Train: 2.9702, Loss Test: 4.2134, LR: 0.000028\n",
      "Step 3412, Loss Train: 2.8856, Loss Test: 3.7192, LR: 0.000028\n",
      "Step 3413, Loss Train: 3.0038, Loss Test: 3.9635, LR: 0.000028\n",
      "Step 3414, Loss Train: 2.9316, Loss Test: 4.1438, LR: 0.000028\n",
      "Step 3415, Loss Train: 2.8454, Loss Test: 3.8393, LR: 0.000028\n",
      "Step 3416, Loss Train: 2.9324, Loss Test: 4.0996, LR: 0.000028\n",
      "Step 3417, Loss Train: 2.9787, Loss Test: 3.6714, LR: 0.000028\n",
      "Step 3418, Loss Train: 2.7642, Loss Test: 3.9913, LR: 0.000028\n",
      "Step 3419, Loss Train: 2.7745, Loss Test: 3.5156, LR: 0.000028\n",
      "Step 3420, Loss Train: 2.8586, Loss Test: 3.3901, LR: 0.000028\n",
      "Step 3421, Loss Train: 2.8638, Loss Test: 3.3354, LR: 0.000028\n",
      "Step 3422, Loss Train: 2.9170, Loss Test: 3.7713, LR: 0.000028\n",
      "Step 3423, Loss Train: 2.9048, Loss Test: 3.8797, LR: 0.000028\n",
      "Step 3424, Loss Train: 2.9861, Loss Test: 4.4172, LR: 0.000028\n",
      "Step 3425, Loss Train: 3.0798, Loss Test: 3.3665, LR: 0.000028\n",
      "Step 3426, Loss Train: 2.9400, Loss Test: 3.5662, LR: 0.000028\n",
      "Step 3427, Loss Train: 2.8801, Loss Test: 3.9284, LR: 0.000028\n",
      "Step 3428, Loss Train: 3.0601, Loss Test: 3.9581, LR: 0.000028\n",
      "Step 3429, Loss Train: 2.9038, Loss Test: 3.9962, LR: 0.000028\n",
      "Step 3430, Loss Train: 2.9644, Loss Test: 2.9981, LR: 0.000028\n",
      "Step 3431, Loss Train: 2.8963, Loss Test: 4.2042, LR: 0.000028\n",
      "Step 3432, Loss Train: 2.8941, Loss Test: 4.1879, LR: 0.000028\n",
      "Step 3433, Loss Train: 3.1569, Loss Test: 3.9680, LR: 0.000028\n",
      "Step 3434, Loss Train: 2.8666, Loss Test: 3.3511, LR: 0.000028\n",
      "Step 3435, Loss Train: 3.1002, Loss Test: 3.5976, LR: 0.000028\n",
      "Step 3436, Loss Train: 3.0811, Loss Test: 3.8562, LR: 0.000028\n",
      "Step 3437, Loss Train: 2.8521, Loss Test: 4.0120, LR: 0.000028\n",
      "Step 3438, Loss Train: 2.9427, Loss Test: 3.7879, LR: 0.000028\n",
      "Step 3439, Loss Train: 2.9103, Loss Test: 3.3617, LR: 0.000028\n",
      "Step 3440, Loss Train: 2.9173, Loss Test: 4.3598, LR: 0.000028\n",
      "Step 3441, Loss Train: 3.0208, Loss Test: 4.0765, LR: 0.000028\n",
      "Step 3442, Loss Train: 2.7869, Loss Test: 4.4896, LR: 0.000028\n",
      "Step 3443, Loss Train: 2.7688, Loss Test: 4.1842, LR: 0.000028\n",
      "Step 3444, Loss Train: 2.7383, Loss Test: 2.8068, LR: 0.000028\n",
      "Step 3445, Loss Train: 2.8672, Loss Test: 3.7582, LR: 0.000028\n",
      "Step 3446, Loss Train: 2.7020, Loss Test: 2.2077, LR: 0.000028\n",
      "Step 3447, Loss Train: 3.0017, Loss Test: 3.8376, LR: 0.000028\n",
      "Step 3448, Loss Train: 2.9511, Loss Test: 3.8405, LR: 0.000028\n",
      "Step 3449, Loss Train: 3.0303, Loss Test: 3.8817, LR: 0.000028\n",
      "Step 3450, Loss Train: 2.9000, Loss Test: 3.8166, LR: 0.000028\n",
      "Step 3451, Loss Train: 2.5677, Loss Test: 3.6144, LR: 0.000028\n",
      "Step 3452, Loss Train: 3.0963, Loss Test: 3.9406, LR: 0.000028\n",
      "Step 3453, Loss Train: 3.0491, Loss Test: 3.7043, LR: 0.000028\n",
      "Step 3454, Loss Train: 2.7999, Loss Test: 3.8227, LR: 0.000028\n",
      "Step 3455, Loss Train: 2.6591, Loss Test: 3.6298, LR: 0.000028\n",
      "Step 3456, Loss Train: 3.0177, Loss Test: 2.2867, LR: 0.000028\n",
      "Step 3457, Loss Train: 2.9658, Loss Test: 4.1323, LR: 0.000028\n",
      "Step 3458, Loss Train: 3.1302, Loss Test: 4.0564, LR: 0.000028\n",
      "Step 3459, Loss Train: 2.8092, Loss Test: 4.1849, LR: 0.000028\n",
      "Step 3460, Loss Train: 2.9214, Loss Test: 3.7650, LR: 0.000028\n",
      "Step 3461, Loss Train: 2.9691, Loss Test: 3.9923, LR: 0.000028\n",
      "Step 3462, Loss Train: 3.0096, Loss Test: 3.2425, LR: 0.000028\n",
      "Step 3463, Loss Train: 2.9685, Loss Test: 3.1908, LR: 0.000028\n",
      "Step 3464, Loss Train: 2.8957, Loss Test: 3.2327, LR: 0.000028\n",
      "Step 3465, Loss Train: 2.9339, Loss Test: 3.4683, LR: 0.000028\n",
      "Step 3466, Loss Train: 2.9137, Loss Test: 2.8794, LR: 0.000028\n",
      "Step 3467, Loss Train: 2.9996, Loss Test: 3.4639, LR: 0.000028\n",
      "Step 3468, Loss Train: 2.9259, Loss Test: 4.2478, LR: 0.000028\n",
      "Step 3469, Loss Train: 2.8574, Loss Test: 3.9822, LR: 0.000028\n",
      "Step 3470, Loss Train: 2.9691, Loss Test: 3.9476, LR: 0.000028\n",
      "Step 3471, Loss Train: 3.0114, Loss Test: 4.1926, LR: 0.000028\n",
      "Step 3472, Loss Train: 2.9565, Loss Test: 3.9458, LR: 0.000028\n",
      "Step 3473, Loss Train: 2.9187, Loss Test: 3.8890, LR: 0.000028\n",
      "Step 3474, Loss Train: 2.8512, Loss Test: 3.1688, LR: 0.000028\n",
      "Step 3475, Loss Train: 2.9800, Loss Test: 4.0182, LR: 0.000028\n",
      "Step 3476, Loss Train: 3.1319, Loss Test: 2.1454, LR: 0.000028\n",
      "Step 3477, Loss Train: 3.0387, Loss Test: 3.6934, LR: 0.000028\n",
      "Step 3478, Loss Train: 2.8874, Loss Test: 3.5806, LR: 0.000028\n",
      "Step 3479, Loss Train: 2.9907, Loss Test: 3.7272, LR: 0.000028\n",
      "Step 3480, Loss Train: 2.8304, Loss Test: 4.0341, LR: 0.000028\n",
      "Step 3481, Loss Train: 2.8529, Loss Test: 2.8819, LR: 0.000028\n",
      "Step 3482, Loss Train: 2.9289, Loss Test: 4.1782, LR: 0.000028\n",
      "Step 3483, Loss Train: 2.8339, Loss Test: 3.2786, LR: 0.000028\n",
      "Step 3484, Loss Train: 3.0272, Loss Test: 3.9910, LR: 0.000028\n",
      "Step 3485, Loss Train: 2.9023, Loss Test: 3.6662, LR: 0.000028\n",
      "Step 3486, Loss Train: 2.7993, Loss Test: 3.5486, LR: 0.000028\n",
      "Step 3487, Loss Train: 3.0715, Loss Test: 3.6273, LR: 0.000028\n",
      "Step 3488, Loss Train: 3.0511, Loss Test: 3.4686, LR: 0.000028\n",
      "Step 3489, Loss Train: 3.0172, Loss Test: 4.5154, LR: 0.000028\n",
      "Step 3490, Loss Train: 2.8659, Loss Test: 3.6455, LR: 0.000028\n",
      "Step 3491, Loss Train: 3.0312, Loss Test: 3.0372, LR: 0.000028\n",
      "Step 3492, Loss Train: 2.9810, Loss Test: 3.7648, LR: 0.000028\n",
      "Step 3493, Loss Train: 2.8674, Loss Test: 3.1480, LR: 0.000028\n",
      "Step 3494, Loss Train: 3.0146, Loss Test: 2.9678, LR: 0.000028\n",
      "Step 3495, Loss Train: 3.0607, Loss Test: 2.6815, LR: 0.000028\n",
      "Step 3496, Loss Train: 3.0289, Loss Test: 3.1691, LR: 0.000028\n",
      "Step 3497, Loss Train: 2.9616, Loss Test: 4.2805, LR: 0.000028\n",
      "Step 3498, Loss Train: 3.0464, Loss Test: 3.7523, LR: 0.000028\n",
      "Step 3499, Loss Train: 3.0202, Loss Test: 3.8401, LR: 0.000028\n",
      "Step 3500, Loss Train: 2.9815, Loss Test: 4.0463, LR: 0.000028\n",
      "Step 3501, Loss Train: 2.8772, Loss Test: 3.7223, LR: 0.000028\n",
      "Step 3502, Loss Train: 2.9782, Loss Test: 3.3875, LR: 0.000028\n",
      "Step 3503, Loss Train: 2.9916, Loss Test: 2.5476, LR: 0.000028\n",
      "Step 3504, Loss Train: 3.0156, Loss Test: 4.0087, LR: 0.000028\n",
      "Step 3505, Loss Train: 3.0005, Loss Test: 4.3608, LR: 0.000028\n",
      "Step 3506, Loss Train: 2.9947, Loss Test: 4.2002, LR: 0.000028\n",
      "Step 3507, Loss Train: 2.8795, Loss Test: 4.5410, LR: 0.000028\n",
      "Step 3508, Loss Train: 2.8664, Loss Test: 3.2804, LR: 0.000028\n",
      "Step 3509, Loss Train: 3.0221, Loss Test: 3.6213, LR: 0.000028\n",
      "Step 3510, Loss Train: 2.8915, Loss Test: 3.9416, LR: 0.000028\n",
      "Step 3511, Loss Train: 3.0953, Loss Test: 3.2208, LR: 0.000028\n",
      "Step 3512, Loss Train: 2.9977, Loss Test: 4.0073, LR: 0.000028\n",
      "Step 3513, Loss Train: 2.9184, Loss Test: 4.0658, LR: 0.000028\n",
      "Step 3514, Loss Train: 3.0606, Loss Test: 3.7320, LR: 0.000028\n",
      "Step 3515, Loss Train: 2.9278, Loss Test: 3.7643, LR: 0.000028\n",
      "Step 3516, Loss Train: 2.9900, Loss Test: 3.4029, LR: 0.000028\n",
      "Step 3517, Loss Train: 2.9919, Loss Test: 4.2973, LR: 0.000028\n",
      "Step 3518, Loss Train: 3.1864, Loss Test: 3.8991, LR: 0.000028\n",
      "Step 3519, Loss Train: 2.8617, Loss Test: 4.0764, LR: 0.000028\n",
      "Step 3520, Loss Train: 2.9228, Loss Test: 3.9694, LR: 0.000028\n",
      "Step 3521, Loss Train: 3.0757, Loss Test: 3.5166, LR: 0.000028\n",
      "Step 3522, Loss Train: 2.9604, Loss Test: 3.8719, LR: 0.000028\n",
      "Step 3523, Loss Train: 2.9332, Loss Test: 3.6080, LR: 0.000028\n",
      "Step 3524, Loss Train: 2.8505, Loss Test: 3.0082, LR: 0.000028\n",
      "Step 3525, Loss Train: 3.0083, Loss Test: 3.4634, LR: 0.000028\n",
      "Step 3526, Loss Train: 2.9135, Loss Test: 3.2708, LR: 0.000028\n",
      "Step 3527, Loss Train: 2.8074, Loss Test: 4.5742, LR: 0.000028\n",
      "Step 3528, Loss Train: 2.9299, Loss Test: 4.1609, LR: 0.000028\n",
      "Step 3529, Loss Train: 2.8125, Loss Test: 3.8750, LR: 0.000028\n",
      "Step 3530, Loss Train: 2.9289, Loss Test: 3.3459, LR: 0.000028\n",
      "Step 3531, Loss Train: 2.7899, Loss Test: 4.1532, LR: 0.000028\n",
      "Step 3532, Loss Train: 2.8951, Loss Test: 4.3325, LR: 0.000028\n",
      "Step 3533, Loss Train: 2.8514, Loss Test: 3.0430, LR: 0.000028\n",
      "Step 3534, Loss Train: 2.9350, Loss Test: 3.7569, LR: 0.000028\n",
      "Step 3535, Loss Train: 3.0582, Loss Test: 4.2252, LR: 0.000028\n",
      "Step 3536, Loss Train: 2.8416, Loss Test: 4.2205, LR: 0.000028\n",
      "Step 3537, Loss Train: 2.8921, Loss Test: 3.9619, LR: 0.000028\n",
      "Step 3538, Loss Train: 2.9740, Loss Test: 4.0539, LR: 0.000028\n",
      "Step 3539, Loss Train: 2.6405, Loss Test: 4.0519, LR: 0.000028\n",
      "Step 3540, Loss Train: 2.9576, Loss Test: 4.3513, LR: 0.000028\n",
      "Step 3541, Loss Train: 2.9308, Loss Test: 4.1292, LR: 0.000028\n",
      "Step 3542, Loss Train: 2.9385, Loss Test: 3.3595, LR: 0.000028\n",
      "Step 3543, Loss Train: 2.9486, Loss Test: 3.6699, LR: 0.000028\n",
      "Step 3544, Loss Train: 3.0584, Loss Test: 3.0862, LR: 0.000028\n",
      "Step 3545, Loss Train: 3.1864, Loss Test: 3.7887, LR: 0.000028\n",
      "Step 3546, Loss Train: 2.9097, Loss Test: 4.2685, LR: 0.000028\n",
      "Step 3547, Loss Train: 3.0325, Loss Test: 4.3112, LR: 0.000028\n",
      "Step 3548, Loss Train: 2.9069, Loss Test: 3.6698, LR: 0.000028\n",
      "Step 3549, Loss Train: 2.8951, Loss Test: 3.8635, LR: 0.000028\n",
      "Step 3550, Loss Train: 2.9377, Loss Test: 4.0871, LR: 0.000028\n",
      "Step 3551, Loss Train: 2.8783, Loss Test: 3.5631, LR: 0.000028\n",
      "Step 3552, Loss Train: 3.1042, Loss Test: 3.6476, LR: 0.000028\n",
      "Step 3553, Loss Train: 2.9513, Loss Test: 2.1685, LR: 0.000028\n",
      "Step 3554, Loss Train: 2.9011, Loss Test: 3.8305, LR: 0.000028\n",
      "Step 3555, Loss Train: 2.9571, Loss Test: 4.1530, LR: 0.000028\n",
      "Step 3556, Loss Train: 2.8780, Loss Test: 2.7265, LR: 0.000028\n",
      "Step 3557, Loss Train: 2.9722, Loss Test: 4.1154, LR: 0.000028\n",
      "Step 3558, Loss Train: 3.0486, Loss Test: 3.3409, LR: 0.000028\n",
      "Step 3559, Loss Train: 2.7956, Loss Test: 3.7046, LR: 0.000028\n",
      "Step 3560, Loss Train: 2.9118, Loss Test: 4.0156, LR: 0.000028\n",
      "Step 3561, Loss Train: 2.9724, Loss Test: 3.6101, LR: 0.000028\n",
      "Step 3562, Loss Train: 3.0050, Loss Test: 4.3461, LR: 0.000028\n",
      "Step 3563, Loss Train: 2.9257, Loss Test: 2.2518, LR: 0.000028\n",
      "Step 3564, Loss Train: 2.7973, Loss Test: 4.3033, LR: 0.000028\n",
      "Step 3565, Loss Train: 2.9432, Loss Test: 1.8059, LR: 0.000028\n",
      "Step 3566, Loss Train: 2.9783, Loss Test: 3.4594, LR: 0.000028\n",
      "Step 3567, Loss Train: 3.0478, Loss Test: 4.2302, LR: 0.000028\n",
      "Step 3568, Loss Train: 2.9303, Loss Test: 4.1019, LR: 0.000028\n",
      "Step 3569, Loss Train: 2.9176, Loss Test: 4.0675, LR: 0.000028\n",
      "Step 3570, Loss Train: 2.9294, Loss Test: 3.8362, LR: 0.000028\n",
      "Step 3571, Loss Train: 2.8348, Loss Test: 2.6666, LR: 0.000028\n",
      "Step 3572, Loss Train: 3.0529, Loss Test: 3.1190, LR: 0.000028\n",
      "Step 3573, Loss Train: 3.1059, Loss Test: 3.9345, LR: 0.000028\n",
      "Step 3574, Loss Train: 2.6802, Loss Test: 3.4334, LR: 0.000028\n",
      "Step 3575, Loss Train: 2.9804, Loss Test: 3.1609, LR: 0.000028\n",
      "Step 3576, Loss Train: 3.0127, Loss Test: 4.3053, LR: 0.000028\n",
      "Step 3577, Loss Train: 2.9394, Loss Test: 4.3040, LR: 0.000028\n",
      "Step 3578, Loss Train: 2.9388, Loss Test: 4.1027, LR: 0.000028\n",
      "Step 3579, Loss Train: 3.0746, Loss Test: 4.0172, LR: 0.000028\n",
      "Step 3580, Loss Train: 3.0561, Loss Test: 3.6322, LR: 0.000028\n",
      "Step 3581, Loss Train: 2.8956, Loss Test: 3.7212, LR: 0.000028\n",
      "Step 3582, Loss Train: 2.8028, Loss Test: 4.0603, LR: 0.000028\n",
      "Step 3583, Loss Train: 2.9904, Loss Test: 3.9261, LR: 0.000028\n",
      "Step 3584, Loss Train: 2.8479, Loss Test: 3.6864, LR: 0.000028\n",
      "Step 3585, Loss Train: 2.9932, Loss Test: 4.0866, LR: 0.000028\n",
      "Step 3586, Loss Train: 3.0058, Loss Test: 3.4742, LR: 0.000028\n",
      "Step 3587, Loss Train: 3.0180, Loss Test: 3.6398, LR: 0.000028\n",
      "Step 3588, Loss Train: 2.9692, Loss Test: 4.1147, LR: 0.000028\n",
      "Step 3589, Loss Train: 2.7917, Loss Test: 3.2515, LR: 0.000028\n",
      "Step 3590, Loss Train: 3.0249, Loss Test: 3.6470, LR: 0.000028\n",
      "Step 3591, Loss Train: 3.0246, Loss Test: 4.1300, LR: 0.000028\n",
      "Step 3592, Loss Train: 2.7994, Loss Test: 4.2767, LR: 0.000028\n",
      "Step 3593, Loss Train: 2.9869, Loss Test: 3.9709, LR: 0.000028\n",
      "Step 3594, Loss Train: 2.8831, Loss Test: 3.2405, LR: 0.000028\n",
      "Step 3595, Loss Train: 2.9241, Loss Test: 3.4782, LR: 0.000028\n",
      "Step 3596, Loss Train: 2.9560, Loss Test: 3.5853, LR: 0.000028\n",
      "Step 3597, Loss Train: 2.9964, Loss Test: 3.6268, LR: 0.000028\n",
      "Step 3598, Loss Train: 2.9536, Loss Test: 4.0761, LR: 0.000028\n",
      "Step 3599, Loss Train: 3.0384, Loss Test: 4.4640, LR: 0.000028\n",
      "Step 3600, Loss Train: 2.7791, Loss Test: 4.4506, LR: 0.000028\n",
      "Step 3601, Loss Train: 3.0587, Loss Test: 3.7362, LR: 0.000028\n",
      "Step 3602, Loss Train: 3.0406, Loss Test: 3.9422, LR: 0.000028\n",
      "Step 3603, Loss Train: 2.9132, Loss Test: 4.1292, LR: 0.000028\n",
      "Step 3604, Loss Train: 2.9306, Loss Test: 4.0986, LR: 0.000028\n",
      "Step 3605, Loss Train: 2.8072, Loss Test: 4.0196, LR: 0.000028\n",
      "Step 3606, Loss Train: 2.9999, Loss Test: 4.4552, LR: 0.000028\n",
      "Step 3607, Loss Train: 2.9238, Loss Test: 3.9546, LR: 0.000028\n",
      "Step 3608, Loss Train: 2.7924, Loss Test: 3.8623, LR: 0.000028\n",
      "Step 3609, Loss Train: 2.8284, Loss Test: 4.0976, LR: 0.000028\n",
      "Step 3610, Loss Train: 2.8907, Loss Test: 4.0249, LR: 0.000028\n",
      "Step 3611, Loss Train: 2.9497, Loss Test: 4.4420, LR: 0.000028\n",
      "Step 3612, Loss Train: 3.0802, Loss Test: 3.8795, LR: 0.000028\n",
      "Step 3613, Loss Train: 2.8630, Loss Test: 4.1172, LR: 0.000028\n",
      "Step 3614, Loss Train: 2.8439, Loss Test: 2.5955, LR: 0.000028\n",
      "Step 3615, Loss Train: 2.9706, Loss Test: 3.9531, LR: 0.000028\n",
      "Step 3616, Loss Train: 2.9080, Loss Test: 3.3443, LR: 0.000028\n",
      "Step 3617, Loss Train: 2.8006, Loss Test: 4.1783, LR: 0.000028\n",
      "Step 3618, Loss Train: 2.8358, Loss Test: 3.9054, LR: 0.000028\n",
      "Step 3619, Loss Train: 2.9346, Loss Test: 3.9206, LR: 0.000028\n",
      "Step 3620, Loss Train: 2.9287, Loss Test: 4.1115, LR: 0.000028\n",
      "Step 3621, Loss Train: 2.9248, Loss Test: 3.7020, LR: 0.000028\n",
      "Step 3622, Loss Train: 2.8922, Loss Test: 3.6883, LR: 0.000028\n",
      "Step 3623, Loss Train: 2.9047, Loss Test: 4.4256, LR: 0.000028\n",
      "Step 3624, Loss Train: 3.0503, Loss Test: 2.9971, LR: 0.000028\n",
      "Step 3625, Loss Train: 2.9512, Loss Test: 3.0927, LR: 0.000028\n",
      "Step 3626, Loss Train: 2.8809, Loss Test: 4.0835, LR: 0.000028\n",
      "Step 3627, Loss Train: 2.8001, Loss Test: 3.1628, LR: 0.000028\n",
      "Step 3628, Loss Train: 2.9761, Loss Test: 3.8269, LR: 0.000028\n",
      "Step 3629, Loss Train: 2.8609, Loss Test: 4.1957, LR: 0.000028\n",
      "Step 3630, Loss Train: 2.9492, Loss Test: 3.0051, LR: 0.000028\n",
      "Step 3631, Loss Train: 3.0005, Loss Test: 4.2141, LR: 0.000028\n",
      "Step 3632, Loss Train: 2.9599, Loss Test: 4.0791, LR: 0.000028\n",
      "Step 3633, Loss Train: 2.9254, Loss Test: 3.6038, LR: 0.000028\n",
      "Step 3634, Loss Train: 2.9875, Loss Test: 4.2659, LR: 0.000028\n",
      "Step 3635, Loss Train: 3.0583, Loss Test: 4.1751, LR: 0.000028\n",
      "Step 3636, Loss Train: 2.8821, Loss Test: 4.0338, LR: 0.000028\n",
      "Step 3637, Loss Train: 2.9608, Loss Test: 4.4947, LR: 0.000028\n",
      "Step 3638, Loss Train: 3.0445, Loss Test: 3.8403, LR: 0.000028\n",
      "Step 3639, Loss Train: 2.9526, Loss Test: 3.3213, LR: 0.000028\n",
      "Step 3640, Loss Train: 3.0946, Loss Test: 3.8536, LR: 0.000028\n",
      "Step 3641, Loss Train: 2.9650, Loss Test: 3.9087, LR: 0.000028\n",
      "Step 3642, Loss Train: 2.9592, Loss Test: 4.1816, LR: 0.000028\n",
      "Step 3643, Loss Train: 3.0095, Loss Test: 3.1439, LR: 0.000028\n",
      "Step 3644, Loss Train: 2.9435, Loss Test: 3.9269, LR: 0.000028\n",
      "Step 3645, Loss Train: 2.8618, Loss Test: 3.7477, LR: 0.000028\n",
      "Step 3646, Loss Train: 2.9334, Loss Test: 1.4057, LR: 0.000028\n",
      "Step 3647, Loss Train: 3.0369, Loss Test: 3.8865, LR: 0.000028\n",
      "Step 3648, Loss Train: 3.0549, Loss Test: 4.0350, LR: 0.000028\n",
      "Step 3649, Loss Train: 3.0204, Loss Test: 3.4066, LR: 0.000028\n",
      "Step 3650, Loss Train: 2.9559, Loss Test: 4.2119, LR: 0.000028\n",
      "Step 3651, Loss Train: 2.8225, Loss Test: 4.4989, LR: 0.000028\n",
      "Step 3652, Loss Train: 2.9344, Loss Test: 2.8063, LR: 0.000028\n",
      "Step 3653, Loss Train: 2.8821, Loss Test: 3.9329, LR: 0.000028\n",
      "Step 3654, Loss Train: 2.8795, Loss Test: 3.8619, LR: 0.000028\n",
      "Step 3655, Loss Train: 2.7945, Loss Test: 3.8421, LR: 0.000028\n",
      "Step 3656, Loss Train: 2.9097, Loss Test: 2.5558, LR: 0.000028\n",
      "Step 3657, Loss Train: 2.8696, Loss Test: 4.1208, LR: 0.000028\n",
      "Step 3658, Loss Train: 2.9710, Loss Test: 3.5021, LR: 0.000028\n",
      "Step 3659, Loss Train: 3.0998, Loss Test: 4.0245, LR: 0.000028\n",
      "Step 3660, Loss Train: 2.9487, Loss Test: 4.1167, LR: 0.000028\n",
      "Step 3661, Loss Train: 2.8097, Loss Test: 2.8187, LR: 0.000028\n",
      "Step 3662, Loss Train: 3.0533, Loss Test: 3.2580, LR: 0.000028\n",
      "Step 3663, Loss Train: 2.9198, Loss Test: 3.8911, LR: 0.000028\n",
      "Step 3664, Loss Train: 2.9134, Loss Test: 1.9124, LR: 0.000028\n",
      "Step 3665, Loss Train: 2.8748, Loss Test: 4.2005, LR: 0.000028\n",
      "Step 3666, Loss Train: 2.8040, Loss Test: 4.3609, LR: 0.000028\n",
      "Step 3667, Loss Train: 3.0118, Loss Test: 4.0891, LR: 0.000028\n",
      "Step 3668, Loss Train: 2.9588, Loss Test: 2.9062, LR: 0.000028\n",
      "Step 3669, Loss Train: 2.9206, Loss Test: 3.5787, LR: 0.000028\n",
      "Step 3670, Loss Train: 3.0190, Loss Test: 3.7451, LR: 0.000028\n",
      "Step 3671, Loss Train: 2.9666, Loss Test: 4.2770, LR: 0.000028\n",
      "Step 3672, Loss Train: 2.8950, Loss Test: 3.5983, LR: 0.000028\n",
      "Step 3673, Loss Train: 2.9205, Loss Test: 3.1256, LR: 0.000028\n",
      "Step 3674, Loss Train: 2.9489, Loss Test: 3.4294, LR: 0.000028\n",
      "Step 3675, Loss Train: 3.0708, Loss Test: 4.4094, LR: 0.000028\n",
      "Step 3676, Loss Train: 3.0582, Loss Test: 3.0850, LR: 0.000028\n",
      "Step 3677, Loss Train: 2.7370, Loss Test: 3.7589, LR: 0.000028\n",
      "Step 3678, Loss Train: 3.0188, Loss Test: 3.7782, LR: 0.000028\n",
      "Step 3679, Loss Train: 2.8292, Loss Test: 3.5693, LR: 0.000028\n",
      "Step 3680, Loss Train: 2.8151, Loss Test: 3.8285, LR: 0.000028\n",
      "Step 3681, Loss Train: 2.8638, Loss Test: 4.3286, LR: 0.000028\n",
      "Step 3682, Loss Train: 2.6892, Loss Test: 4.6768, LR: 0.000028\n",
      "Step 3683, Loss Train: 2.8487, Loss Test: 3.7889, LR: 0.000028\n",
      "Step 3684, Loss Train: 3.1132, Loss Test: 3.1171, LR: 0.000028\n",
      "Step 3685, Loss Train: 3.0264, Loss Test: 4.3986, LR: 0.000028\n",
      "Step 3686, Loss Train: 2.9290, Loss Test: 3.2753, LR: 0.000028\n",
      "Step 3687, Loss Train: 2.9828, Loss Test: 3.2747, LR: 0.000028\n",
      "Step 3688, Loss Train: 2.9894, Loss Test: 4.0279, LR: 0.000028\n",
      "Step 3689, Loss Train: 3.0431, Loss Test: 3.5443, LR: 0.000028\n",
      "Step 3690, Loss Train: 2.8048, Loss Test: 3.8445, LR: 0.000028\n",
      "Step 3691, Loss Train: 2.9766, Loss Test: 4.0049, LR: 0.000028\n",
      "Step 3692, Loss Train: 2.9530, Loss Test: 2.1834, LR: 0.000028\n",
      "Step 3693, Loss Train: 2.7859, Loss Test: 2.0217, LR: 0.000028\n",
      "Step 3694, Loss Train: 2.9171, Loss Test: 4.2210, LR: 0.000028\n",
      "Step 3695, Loss Train: 2.7524, Loss Test: 4.2789, LR: 0.000028\n",
      "Step 3696, Loss Train: 2.9682, Loss Test: 4.2812, LR: 0.000028\n",
      "Step 3697, Loss Train: 2.7544, Loss Test: 4.1073, LR: 0.000028\n",
      "Step 3698, Loss Train: 2.9998, Loss Test: 3.7835, LR: 0.000028\n",
      "Step 3699, Loss Train: 3.1104, Loss Test: 4.4147, LR: 0.000028\n",
      "Step 3700, Loss Train: 2.7883, Loss Test: 3.9098, LR: 0.000028\n",
      "Step 3701, Loss Train: 3.1025, Loss Test: 3.6478, LR: 0.000028\n",
      "Step 3702, Loss Train: 3.0377, Loss Test: 4.0774, LR: 0.000028\n",
      "Step 3703, Loss Train: 2.9480, Loss Test: 3.9056, LR: 0.000028\n",
      "Step 3704, Loss Train: 2.8922, Loss Test: 3.9463, LR: 0.000028\n",
      "Step 3705, Loss Train: 2.8963, Loss Test: 4.3368, LR: 0.000028\n",
      "Step 3706, Loss Train: 2.7149, Loss Test: 4.3180, LR: 0.000028\n",
      "Step 3707, Loss Train: 3.1021, Loss Test: 2.5774, LR: 0.000028\n",
      "Step 3708, Loss Train: 2.8746, Loss Test: 3.0868, LR: 0.000028\n",
      "Step 3709, Loss Train: 2.8895, Loss Test: 3.2283, LR: 0.000028\n",
      "Step 3710, Loss Train: 2.8377, Loss Test: 3.7251, LR: 0.000028\n",
      "Step 3711, Loss Train: 3.0267, Loss Test: 3.7664, LR: 0.000028\n",
      "Step 3712, Loss Train: 3.0564, Loss Test: 3.7544, LR: 0.000028\n",
      "Step 3713, Loss Train: 2.9688, Loss Test: 1.8101, LR: 0.000028\n",
      "Step 3714, Loss Train: 2.8699, Loss Test: 3.2372, LR: 0.000028\n",
      "Step 3715, Loss Train: 3.0582, Loss Test: 3.5561, LR: 0.000028\n",
      "Step 3716, Loss Train: 2.8888, Loss Test: 4.4936, LR: 0.000028\n",
      "Step 3717, Loss Train: 2.9386, Loss Test: 2.3672, LR: 0.000028\n",
      "Step 3718, Loss Train: 2.9815, Loss Test: 3.5212, LR: 0.000028\n",
      "Step 3719, Loss Train: 2.8761, Loss Test: 3.8498, LR: 0.000028\n",
      "Step 3720, Loss Train: 2.9680, Loss Test: 4.3206, LR: 0.000028\n",
      "Step 3721, Loss Train: 3.1094, Loss Test: 4.2905, LR: 0.000028\n",
      "Step 3722, Loss Train: 3.0352, Loss Test: 3.0933, LR: 0.000028\n",
      "Step 3723, Loss Train: 2.6563, Loss Test: 3.3607, LR: 0.000028\n",
      "Step 3724, Loss Train: 3.1039, Loss Test: 2.8194, LR: 0.000028\n",
      "Step 3725, Loss Train: 3.1040, Loss Test: 4.0627, LR: 0.000028\n",
      "Step 3726, Loss Train: 3.1627, Loss Test: 3.7045, LR: 0.000028\n",
      "Step 3727, Loss Train: 2.8552, Loss Test: 3.7127, LR: 0.000028\n",
      "Step 3728, Loss Train: 3.0302, Loss Test: 3.8093, LR: 0.000028\n",
      "Step 3729, Loss Train: 3.1766, Loss Test: 3.2323, LR: 0.000028\n",
      "Step 3730, Loss Train: 2.9494, Loss Test: 3.8043, LR: 0.000028\n",
      "Step 3731, Loss Train: 2.9459, Loss Test: 2.5186, LR: 0.000028\n",
      "Step 3732, Loss Train: 2.7577, Loss Test: 4.0655, LR: 0.000028\n",
      "Step 3733, Loss Train: 2.9355, Loss Test: 2.5271, LR: 0.000028\n",
      "Step 3734, Loss Train: 3.0724, Loss Test: 4.1803, LR: 0.000028\n",
      "Step 3735, Loss Train: 2.9724, Loss Test: 3.4150, LR: 0.000028\n",
      "Step 3736, Loss Train: 2.9137, Loss Test: 3.1544, LR: 0.000028\n",
      "Step 3737, Loss Train: 2.9589, Loss Test: 2.7075, LR: 0.000028\n",
      "Step 3738, Loss Train: 2.9416, Loss Test: 3.9221, LR: 0.000028\n",
      "Step 3739, Loss Train: 2.6957, Loss Test: 4.0715, LR: 0.000028\n",
      "Step 3740, Loss Train: 2.9119, Loss Test: 3.2306, LR: 0.000028\n",
      "Step 3741, Loss Train: 2.9779, Loss Test: 4.0053, LR: 0.000028\n",
      "Step 3742, Loss Train: 2.9433, Loss Test: 4.0085, LR: 0.000028\n",
      "Step 3743, Loss Train: 3.0233, Loss Test: 3.3893, LR: 0.000028\n",
      "Step 3744, Loss Train: 2.9163, Loss Test: 4.2493, LR: 0.000028\n",
      "Step 3745, Loss Train: 2.9015, Loss Test: 4.4070, LR: 0.000028\n",
      "Step 3746, Loss Train: 2.8316, Loss Test: 3.8595, LR: 0.000028\n",
      "Step 3747, Loss Train: 2.9168, Loss Test: 3.3845, LR: 0.000028\n",
      "Step 3748, Loss Train: 2.9766, Loss Test: 3.1715, LR: 0.000028\n",
      "Step 3749, Loss Train: 3.0087, Loss Test: 4.1639, LR: 0.000028\n",
      "Step 3750, Loss Train: 2.9725, Loss Test: 3.6671, LR: 0.000028\n",
      "Step 3751, Loss Train: 2.8718, Loss Test: 2.1496, LR: 0.000028\n",
      "Step 3752, Loss Train: 2.7691, Loss Test: 3.9063, LR: 0.000028\n",
      "Step 3753, Loss Train: 2.9643, Loss Test: 3.0758, LR: 0.000028\n",
      "Step 3754, Loss Train: 3.0136, Loss Test: 3.9699, LR: 0.000028\n",
      "Step 3755, Loss Train: 2.7407, Loss Test: 3.3790, LR: 0.000028\n",
      "Step 3756, Loss Train: 2.8650, Loss Test: 2.3641, LR: 0.000028\n",
      "Step 3757, Loss Train: 2.9729, Loss Test: 3.6246, LR: 0.000028\n",
      "Step 3758, Loss Train: 2.8008, Loss Test: 3.5906, LR: 0.000028\n",
      "Step 3759, Loss Train: 2.8740, Loss Test: 3.1886, LR: 0.000028\n",
      "Step 3760, Loss Train: 3.0428, Loss Test: 3.4702, LR: 0.000028\n",
      "Step 3761, Loss Train: 3.1923, Loss Test: 3.8230, LR: 0.000028\n",
      "Step 3762, Loss Train: 2.9445, Loss Test: 3.9859, LR: 0.000028\n",
      "Step 3763, Loss Train: 2.9035, Loss Test: 4.5847, LR: 0.000028\n",
      "Step 3764, Loss Train: 2.9024, Loss Test: 3.8926, LR: 0.000028\n",
      "Step 3765, Loss Train: 2.9812, Loss Test: 3.4226, LR: 0.000028\n",
      "Step 3766, Loss Train: 3.0042, Loss Test: 3.8415, LR: 0.000028\n",
      "Step 3767, Loss Train: 2.7580, Loss Test: 4.1198, LR: 0.000028\n",
      "Step 3768, Loss Train: 2.8561, Loss Test: 2.1396, LR: 0.000028\n",
      "Step 3769, Loss Train: 2.9605, Loss Test: 4.2828, LR: 0.000028\n",
      "Step 3770, Loss Train: 2.9445, Loss Test: 3.9668, LR: 0.000028\n",
      "Step 3771, Loss Train: 2.8280, Loss Test: 3.6368, LR: 0.000028\n",
      "Step 3772, Loss Train: 2.9561, Loss Test: 3.9208, LR: 0.000028\n",
      "Step 3773, Loss Train: 2.9246, Loss Test: 3.4546, LR: 0.000028\n",
      "Step 3774, Loss Train: 2.9942, Loss Test: 4.0719, LR: 0.000028\n",
      "Step 3775, Loss Train: 2.9033, Loss Test: 3.1511, LR: 0.000028\n",
      "Step 3776, Loss Train: 3.0143, Loss Test: 3.0153, LR: 0.000028\n",
      "Step 3777, Loss Train: 3.0228, Loss Test: 3.8394, LR: 0.000028\n",
      "Step 3778, Loss Train: 2.9096, Loss Test: 3.4097, LR: 0.000028\n",
      "Step 3779, Loss Train: 3.0679, Loss Test: 2.6974, LR: 0.000028\n",
      "Step 3780, Loss Train: 2.9707, Loss Test: 3.7051, LR: 0.000028\n",
      "Step 3781, Loss Train: 2.9267, Loss Test: 4.0748, LR: 0.000028\n",
      "Step 3782, Loss Train: 2.8885, Loss Test: 4.3647, LR: 0.000028\n",
      "Step 3783, Loss Train: 3.0226, Loss Test: 3.2318, LR: 0.000028\n",
      "Step 3784, Loss Train: 2.7182, Loss Test: 3.4773, LR: 0.000028\n",
      "Step 3785, Loss Train: 3.1241, Loss Test: 2.8624, LR: 0.000028\n",
      "Step 3786, Loss Train: 2.8337, Loss Test: 3.8272, LR: 0.000028\n",
      "Step 3787, Loss Train: 3.0501, Loss Test: 3.8572, LR: 0.000028\n",
      "Step 3788, Loss Train: 3.0233, Loss Test: 3.7830, LR: 0.000028\n",
      "Step 3789, Loss Train: 2.9762, Loss Test: 3.9387, LR: 0.000028\n",
      "Step 3790, Loss Train: 2.7881, Loss Test: 3.6019, LR: 0.000028\n",
      "Step 3791, Loss Train: 2.8822, Loss Test: 3.9527, LR: 0.000028\n",
      "Step 3792, Loss Train: 2.9903, Loss Test: 2.3461, LR: 0.000028\n",
      "Step 3793, Loss Train: 2.8497, Loss Test: 3.2149, LR: 0.000028\n",
      "Step 3794, Loss Train: 2.8470, Loss Test: 4.0214, LR: 0.000028\n",
      "Step 3795, Loss Train: 2.9136, Loss Test: 3.5108, LR: 0.000028\n",
      "Step 3796, Loss Train: 3.1473, Loss Test: 4.0626, LR: 0.000028\n",
      "Step 3797, Loss Train: 2.9034, Loss Test: 4.2538, LR: 0.000028\n",
      "Step 3798, Loss Train: 2.8347, Loss Test: 3.8943, LR: 0.000028\n",
      "Step 3799, Loss Train: 2.8979, Loss Test: 2.5530, LR: 0.000028\n",
      "Step 3800, Loss Train: 2.9168, Loss Test: 4.4522, LR: 0.000028\n",
      "Step 3801, Loss Train: 2.8022, Loss Test: 3.2980, LR: 0.000028\n",
      "Step 3802, Loss Train: 2.8120, Loss Test: 3.7216, LR: 0.000028\n",
      "Step 3803, Loss Train: 3.0221, Loss Test: 3.9023, LR: 0.000028\n",
      "Step 3804, Loss Train: 3.1067, Loss Test: 3.1275, LR: 0.000028\n",
      "Step 3805, Loss Train: 2.8082, Loss Test: 4.1862, LR: 0.000028\n",
      "Step 3806, Loss Train: 2.9141, Loss Test: 3.9372, LR: 0.000028\n",
      "Step 3807, Loss Train: 2.7100, Loss Test: 3.2973, LR: 0.000028\n",
      "Step 3808, Loss Train: 2.8590, Loss Test: 4.1711, LR: 0.000028\n",
      "Step 3809, Loss Train: 2.9406, Loss Test: 2.0878, LR: 0.000028\n",
      "Step 3810, Loss Train: 2.9694, Loss Test: 3.9261, LR: 0.000028\n",
      "Step 3811, Loss Train: 2.9025, Loss Test: 3.2530, LR: 0.000028\n",
      "Step 3812, Loss Train: 3.1125, Loss Test: 3.8642, LR: 0.000028\n",
      "Step 3813, Loss Train: 3.0333, Loss Test: 2.4778, LR: 0.000028\n",
      "Step 3814, Loss Train: 2.8098, Loss Test: 3.6414, LR: 0.000028\n",
      "Step 3815, Loss Train: 2.8805, Loss Test: 4.0177, LR: 0.000028\n",
      "Step 3816, Loss Train: 3.0076, Loss Test: 3.6671, LR: 0.000028\n",
      "Step 3817, Loss Train: 3.0069, Loss Test: 4.1356, LR: 0.000028\n",
      "Step 3818, Loss Train: 2.8730, Loss Test: 4.2128, LR: 0.000028\n",
      "Step 3819, Loss Train: 2.8398, Loss Test: 4.2516, LR: 0.000028\n",
      "Step 3820, Loss Train: 2.8843, Loss Test: 2.8120, LR: 0.000028\n",
      "Step 3821, Loss Train: 2.7153, Loss Test: 4.0693, LR: 0.000028\n",
      "Step 3822, Loss Train: 2.7256, Loss Test: 3.2041, LR: 0.000028\n",
      "Step 3823, Loss Train: 2.9594, Loss Test: 3.6907, LR: 0.000028\n",
      "Step 3824, Loss Train: 2.8117, Loss Test: 4.1639, LR: 0.000028\n",
      "Step 3825, Loss Train: 2.9487, Loss Test: 4.0990, LR: 0.000028\n",
      "Step 3826, Loss Train: 2.9264, Loss Test: 3.7794, LR: 0.000028\n",
      "Step 3827, Loss Train: 2.9154, Loss Test: 4.1588, LR: 0.000028\n",
      "Step 3828, Loss Train: 3.0443, Loss Test: 3.6695, LR: 0.000028\n",
      "Step 3829, Loss Train: 2.7866, Loss Test: 3.8616, LR: 0.000028\n",
      "Step 3830, Loss Train: 2.9952, Loss Test: 3.7286, LR: 0.000028\n",
      "Step 3831, Loss Train: 2.8964, Loss Test: 3.7395, LR: 0.000028\n",
      "Step 3832, Loss Train: 3.0380, Loss Test: 4.7968, LR: 0.000027\n",
      "Step 3833, Loss Train: 2.8837, Loss Test: 3.3996, LR: 0.000027\n",
      "Step 3834, Loss Train: 2.8655, Loss Test: 4.0534, LR: 0.000027\n",
      "Step 3835, Loss Train: 2.9085, Loss Test: 3.7024, LR: 0.000027\n",
      "Step 3836, Loss Train: 3.0190, Loss Test: 2.9288, LR: 0.000027\n",
      "Step 3837, Loss Train: 2.8995, Loss Test: 4.1621, LR: 0.000027\n",
      "Step 3838, Loss Train: 2.6666, Loss Test: 4.1388, LR: 0.000027\n",
      "Step 3839, Loss Train: 2.7621, Loss Test: 3.9591, LR: 0.000027\n",
      "Step 3840, Loss Train: 2.7612, Loss Test: 4.2477, LR: 0.000027\n",
      "Step 3841, Loss Train: 2.9448, Loss Test: 4.1852, LR: 0.000027\n",
      "Step 3842, Loss Train: 3.0174, Loss Test: 4.0564, LR: 0.000027\n",
      "Step 3843, Loss Train: 2.9564, Loss Test: 3.4914, LR: 0.000027\n",
      "Step 3844, Loss Train: 3.0215, Loss Test: 4.2790, LR: 0.000027\n",
      "Step 3845, Loss Train: 2.7003, Loss Test: 3.4780, LR: 0.000027\n",
      "Step 3846, Loss Train: 2.9358, Loss Test: 4.0479, LR: 0.000027\n",
      "Step 3847, Loss Train: 2.9860, Loss Test: 4.1940, LR: 0.000027\n",
      "Step 3848, Loss Train: 3.0244, Loss Test: 3.7260, LR: 0.000027\n",
      "Step 3849, Loss Train: 2.8429, Loss Test: 3.9424, LR: 0.000027\n",
      "Step 3850, Loss Train: 3.0132, Loss Test: 4.4842, LR: 0.000027\n",
      "Step 3851, Loss Train: 3.0873, Loss Test: 2.4703, LR: 0.000027\n",
      "Step 3852, Loss Train: 2.9549, Loss Test: 4.2884, LR: 0.000027\n",
      "Step 3853, Loss Train: 2.8657, Loss Test: 3.6799, LR: 0.000027\n",
      "Step 3854, Loss Train: 3.0049, Loss Test: 3.0075, LR: 0.000027\n",
      "Step 3855, Loss Train: 3.0586, Loss Test: 3.6142, LR: 0.000027\n",
      "Step 3856, Loss Train: 2.8125, Loss Test: 3.7988, LR: 0.000027\n",
      "Step 3857, Loss Train: 2.8805, Loss Test: 4.4677, LR: 0.000027\n",
      "Step 3858, Loss Train: 2.8467, Loss Test: 4.3043, LR: 0.000027\n",
      "Step 3859, Loss Train: 2.8548, Loss Test: 3.5715, LR: 0.000027\n",
      "Step 3860, Loss Train: 3.0332, Loss Test: 2.7761, LR: 0.000027\n",
      "Step 3861, Loss Train: 2.8618, Loss Test: 3.9676, LR: 0.000027\n",
      "Step 3862, Loss Train: 3.0045, Loss Test: 3.7806, LR: 0.000027\n",
      "Step 3863, Loss Train: 2.9441, Loss Test: 4.1769, LR: 0.000027\n",
      "Step 3864, Loss Train: 2.7985, Loss Test: 3.9904, LR: 0.000027\n",
      "Step 3865, Loss Train: 3.0177, Loss Test: 3.5334, LR: 0.000027\n",
      "Step 3866, Loss Train: 2.9268, Loss Test: 4.0986, LR: 0.000027\n",
      "Step 3867, Loss Train: 2.9321, Loss Test: 2.8055, LR: 0.000027\n",
      "Step 3868, Loss Train: 2.7352, Loss Test: 3.1134, LR: 0.000027\n",
      "Step 3869, Loss Train: 2.9132, Loss Test: 4.0731, LR: 0.000027\n",
      "Step 3870, Loss Train: 2.9678, Loss Test: 2.7523, LR: 0.000027\n",
      "Step 3871, Loss Train: 2.9878, Loss Test: 3.2246, LR: 0.000027\n",
      "Step 3872, Loss Train: 2.9766, Loss Test: 3.7048, LR: 0.000027\n",
      "Step 3873, Loss Train: 2.9166, Loss Test: 3.4599, LR: 0.000027\n",
      "Step 3874, Loss Train: 2.8429, Loss Test: 4.3326, LR: 0.000027\n",
      "Step 3875, Loss Train: 2.8694, Loss Test: 3.7254, LR: 0.000027\n",
      "Step 3876, Loss Train: 2.8726, Loss Test: 4.3575, LR: 0.000027\n",
      "Step 3877, Loss Train: 2.8633, Loss Test: 3.8542, LR: 0.000027\n",
      "Step 3878, Loss Train: 2.9409, Loss Test: 4.2289, LR: 0.000027\n",
      "Step 3879, Loss Train: 2.9613, Loss Test: 4.4747, LR: 0.000027\n",
      "Step 3880, Loss Train: 2.9008, Loss Test: 4.2695, LR: 0.000027\n",
      "Step 3881, Loss Train: 2.6948, Loss Test: 4.2293, LR: 0.000027\n",
      "Step 3882, Loss Train: 2.8402, Loss Test: 3.2544, LR: 0.000027\n",
      "Step 3883, Loss Train: 2.9236, Loss Test: 3.6022, LR: 0.000027\n",
      "Step 3884, Loss Train: 2.8685, Loss Test: 2.6035, LR: 0.000027\n",
      "Step 3885, Loss Train: 3.0963, Loss Test: 4.0951, LR: 0.000027\n",
      "Step 3886, Loss Train: 2.8747, Loss Test: 3.7930, LR: 0.000027\n",
      "Step 3887, Loss Train: 2.9442, Loss Test: 4.0168, LR: 0.000027\n",
      "Step 3888, Loss Train: 2.9611, Loss Test: 4.1785, LR: 0.000027\n",
      "Step 3889, Loss Train: 2.7745, Loss Test: 3.7368, LR: 0.000027\n",
      "Step 3890, Loss Train: 2.8783, Loss Test: 4.0142, LR: 0.000027\n",
      "Step 3891, Loss Train: 2.8683, Loss Test: 3.9530, LR: 0.000027\n",
      "Step 3892, Loss Train: 2.7302, Loss Test: 3.7440, LR: 0.000027\n",
      "Step 3893, Loss Train: 2.8958, Loss Test: 4.0667, LR: 0.000027\n",
      "Step 3894, Loss Train: 3.0572, Loss Test: 4.2367, LR: 0.000027\n",
      "Step 3895, Loss Train: 2.7290, Loss Test: 3.9468, LR: 0.000027\n",
      "Step 3896, Loss Train: 2.7443, Loss Test: 3.5428, LR: 0.000027\n",
      "Step 3897, Loss Train: 3.0201, Loss Test: 3.0297, LR: 0.000027\n",
      "Step 3898, Loss Train: 2.9804, Loss Test: 3.6045, LR: 0.000027\n",
      "Step 3899, Loss Train: 2.8119, Loss Test: 4.0618, LR: 0.000027\n",
      "Step 3900, Loss Train: 2.8612, Loss Test: 3.9394, LR: 0.000027\n",
      "Step 3901, Loss Train: 2.9805, Loss Test: 3.4976, LR: 0.000027\n",
      "Step 3902, Loss Train: 2.8292, Loss Test: 4.0410, LR: 0.000027\n",
      "Step 3903, Loss Train: 2.7909, Loss Test: 2.6688, LR: 0.000027\n",
      "Step 3904, Loss Train: 3.1798, Loss Test: 3.6628, LR: 0.000027\n",
      "Step 3905, Loss Train: 3.0066, Loss Test: 3.4550, LR: 0.000027\n",
      "Step 3906, Loss Train: 2.8826, Loss Test: 4.1455, LR: 0.000027\n",
      "Step 3907, Loss Train: 3.0335, Loss Test: 3.3535, LR: 0.000027\n",
      "Step 3908, Loss Train: 2.9237, Loss Test: 3.1110, LR: 0.000027\n",
      "Step 3909, Loss Train: 2.9538, Loss Test: 3.9343, LR: 0.000027\n",
      "Step 3910, Loss Train: 2.9786, Loss Test: 4.0832, LR: 0.000027\n",
      "Step 3911, Loss Train: 2.9499, Loss Test: 3.9059, LR: 0.000027\n",
      "Step 3912, Loss Train: 3.0478, Loss Test: 4.0376, LR: 0.000027\n",
      "Step 3913, Loss Train: 2.8381, Loss Test: 3.6504, LR: 0.000027\n",
      "Step 3914, Loss Train: 2.9600, Loss Test: 4.2191, LR: 0.000027\n",
      "Step 3915, Loss Train: 2.8800, Loss Test: 3.2619, LR: 0.000027\n",
      "Step 3916, Loss Train: 2.8394, Loss Test: 4.1244, LR: 0.000027\n",
      "Step 3917, Loss Train: 2.8917, Loss Test: 3.6651, LR: 0.000027\n",
      "Step 3918, Loss Train: 2.8666, Loss Test: 4.1434, LR: 0.000027\n",
      "Step 3919, Loss Train: 2.8947, Loss Test: 3.2864, LR: 0.000027\n",
      "Step 3920, Loss Train: 3.0065, Loss Test: 3.6224, LR: 0.000027\n",
      "Step 3921, Loss Train: 3.0706, Loss Test: 3.6849, LR: 0.000027\n",
      "Step 3922, Loss Train: 2.9944, Loss Test: 3.0641, LR: 0.000027\n",
      "Step 3923, Loss Train: 3.0148, Loss Test: 3.9784, LR: 0.000027\n",
      "Step 3924, Loss Train: 2.7937, Loss Test: 3.4315, LR: 0.000027\n",
      "Step 3925, Loss Train: 2.9488, Loss Test: 3.5226, LR: 0.000027\n",
      "Step 3926, Loss Train: 3.0818, Loss Test: 4.0003, LR: 0.000027\n",
      "Step 3927, Loss Train: 2.9316, Loss Test: 3.3516, LR: 0.000027\n",
      "Step 3928, Loss Train: 2.9652, Loss Test: 4.0689, LR: 0.000027\n",
      "Step 3929, Loss Train: 2.8246, Loss Test: 4.0741, LR: 0.000027\n",
      "Step 3930, Loss Train: 2.7035, Loss Test: 3.7383, LR: 0.000027\n",
      "Step 3931, Loss Train: 2.8981, Loss Test: 4.1745, LR: 0.000027\n",
      "Step 3932, Loss Train: 2.9509, Loss Test: 3.5411, LR: 0.000027\n",
      "Step 3933, Loss Train: 2.9553, Loss Test: 2.9825, LR: 0.000027\n",
      "Step 3934, Loss Train: 2.7203, Loss Test: 3.1666, LR: 0.000027\n",
      "Step 3935, Loss Train: 2.9441, Loss Test: 2.9943, LR: 0.000027\n",
      "Step 3936, Loss Train: 2.8907, Loss Test: 2.2798, LR: 0.000027\n",
      "Step 3937, Loss Train: 3.0694, Loss Test: 3.1733, LR: 0.000027\n",
      "Step 3938, Loss Train: 2.9493, Loss Test: 4.4270, LR: 0.000027\n",
      "Step 3939, Loss Train: 2.8397, Loss Test: 4.1841, LR: 0.000027\n",
      "Step 3940, Loss Train: 3.0423, Loss Test: 3.7127, LR: 0.000027\n",
      "Step 3941, Loss Train: 3.0566, Loss Test: 3.3095, LR: 0.000027\n",
      "Step 3942, Loss Train: 2.6798, Loss Test: 3.9975, LR: 0.000027\n",
      "Step 3943, Loss Train: 3.0095, Loss Test: 4.8473, LR: 0.000027\n",
      "Step 3944, Loss Train: 2.9266, Loss Test: 3.2064, LR: 0.000027\n",
      "Step 3945, Loss Train: 2.8677, Loss Test: 4.0350, LR: 0.000027\n",
      "Step 3946, Loss Train: 2.8133, Loss Test: 4.3524, LR: 0.000027\n",
      "Step 3947, Loss Train: 3.0384, Loss Test: 3.1248, LR: 0.000027\n",
      "Step 3948, Loss Train: 2.9379, Loss Test: 2.2882, LR: 0.000027\n",
      "Step 3949, Loss Train: 2.8046, Loss Test: 4.3444, LR: 0.000027\n",
      "Step 3950, Loss Train: 2.8885, Loss Test: 3.7039, LR: 0.000027\n",
      "Step 3951, Loss Train: 3.0431, Loss Test: 3.9099, LR: 0.000027\n",
      "Step 3952, Loss Train: 2.9280, Loss Test: 3.8894, LR: 0.000027\n",
      "Step 3953, Loss Train: 2.7733, Loss Test: 3.5377, LR: 0.000027\n",
      "Step 3954, Loss Train: 2.9972, Loss Test: 3.2765, LR: 0.000027\n",
      "Step 3955, Loss Train: 2.8813, Loss Test: 4.2659, LR: 0.000027\n",
      "Step 3956, Loss Train: 2.9263, Loss Test: 3.3965, LR: 0.000027\n",
      "Step 3957, Loss Train: 2.8865, Loss Test: 1.6991, LR: 0.000027\n",
      "Step 3958, Loss Train: 2.9833, Loss Test: 3.4389, LR: 0.000027\n",
      "Step 3959, Loss Train: 2.8873, Loss Test: 3.3502, LR: 0.000027\n",
      "Step 3960, Loss Train: 2.8649, Loss Test: 4.5006, LR: 0.000027\n",
      "Step 3961, Loss Train: 3.0628, Loss Test: 3.5094, LR: 0.000027\n",
      "Step 3962, Loss Train: 2.8775, Loss Test: 4.2018, LR: 0.000027\n",
      "Step 3963, Loss Train: 2.8437, Loss Test: 3.9815, LR: 0.000027\n",
      "Step 3964, Loss Train: 2.9946, Loss Test: 3.4065, LR: 0.000027\n",
      "Step 3965, Loss Train: 2.7909, Loss Test: 3.6258, LR: 0.000027\n",
      "Step 3966, Loss Train: 2.7962, Loss Test: 3.2375, LR: 0.000027\n",
      "Step 3967, Loss Train: 2.9537, Loss Test: 3.5181, LR: 0.000027\n",
      "Step 3968, Loss Train: 2.8890, Loss Test: 3.7776, LR: 0.000027\n",
      "Step 3969, Loss Train: 3.0207, Loss Test: 2.6824, LR: 0.000027\n",
      "Step 3970, Loss Train: 2.9854, Loss Test: 2.8186, LR: 0.000027\n",
      "Step 3971, Loss Train: 3.1008, Loss Test: 3.8588, LR: 0.000027\n",
      "Step 3972, Loss Train: 2.8929, Loss Test: 4.3993, LR: 0.000027\n",
      "Step 3973, Loss Train: 2.9261, Loss Test: 3.2351, LR: 0.000027\n",
      "Step 3974, Loss Train: 3.0218, Loss Test: 4.1828, LR: 0.000027\n",
      "Step 3975, Loss Train: 2.8179, Loss Test: 3.8171, LR: 0.000027\n",
      "Step 3976, Loss Train: 3.0654, Loss Test: 4.0703, LR: 0.000027\n",
      "Step 3977, Loss Train: 2.7478, Loss Test: 2.2276, LR: 0.000027\n",
      "Step 3978, Loss Train: 2.9489, Loss Test: 3.8715, LR: 0.000027\n",
      "Step 3979, Loss Train: 2.9666, Loss Test: 4.6253, LR: 0.000027\n",
      "Step 3980, Loss Train: 3.0060, Loss Test: 3.0337, LR: 0.000027\n",
      "Step 3981, Loss Train: 2.7968, Loss Test: 3.6430, LR: 0.000027\n",
      "Step 3982, Loss Train: 3.0411, Loss Test: 3.7271, LR: 0.000027\n",
      "Step 3983, Loss Train: 2.8569, Loss Test: 4.2883, LR: 0.000027\n",
      "Step 3984, Loss Train: 2.8040, Loss Test: 2.9995, LR: 0.000027\n",
      "Step 3985, Loss Train: 2.8375, Loss Test: 4.3390, LR: 0.000027\n",
      "Step 3986, Loss Train: 2.9972, Loss Test: 3.1714, LR: 0.000027\n",
      "Step 3987, Loss Train: 2.8140, Loss Test: 2.5378, LR: 0.000027\n",
      "Step 3988, Loss Train: 2.8946, Loss Test: 3.7391, LR: 0.000027\n",
      "Step 3989, Loss Train: 2.9179, Loss Test: 4.0408, LR: 0.000027\n",
      "Step 3990, Loss Train: 2.7402, Loss Test: 3.9856, LR: 0.000027\n",
      "Step 3991, Loss Train: 2.9093, Loss Test: 4.0567, LR: 0.000027\n",
      "Step 3992, Loss Train: 2.9871, Loss Test: 3.8838, LR: 0.000027\n",
      "Step 3993, Loss Train: 2.9408, Loss Test: 3.9996, LR: 0.000027\n",
      "Step 3994, Loss Train: 2.7093, Loss Test: 3.8372, LR: 0.000027\n",
      "Step 3995, Loss Train: 2.9317, Loss Test: 2.7396, LR: 0.000027\n",
      "Step 3996, Loss Train: 2.8573, Loss Test: 3.5464, LR: 0.000027\n",
      "Step 3997, Loss Train: 3.0449, Loss Test: 4.0433, LR: 0.000027\n",
      "Step 3998, Loss Train: 2.9037, Loss Test: 3.7447, LR: 0.000027\n",
      "Step 3999, Loss Train: 3.0365, Loss Test: 1.9426, LR: 0.000027\n",
      "Step 4000, Loss Train: 2.9825, Loss Test: 3.9489, LR: 0.000027\n",
      "Step 4001, Loss Train: 2.9615, Loss Test: 4.0115, LR: 0.000027\n",
      "Step 4002, Loss Train: 3.0337, Loss Test: 4.2358, LR: 0.000027\n",
      "Step 4003, Loss Train: 3.0258, Loss Test: 4.1610, LR: 0.000027\n",
      "Step 4004, Loss Train: 2.8422, Loss Test: 4.2098, LR: 0.000027\n",
      "Step 4005, Loss Train: 3.0492, Loss Test: 3.9342, LR: 0.000027\n",
      "Step 4006, Loss Train: 2.6773, Loss Test: 3.6337, LR: 0.000027\n",
      "Step 4007, Loss Train: 2.9072, Loss Test: 3.9961, LR: 0.000027\n",
      "Step 4008, Loss Train: 2.9055, Loss Test: 3.9718, LR: 0.000027\n",
      "Step 4009, Loss Train: 3.1090, Loss Test: 4.3991, LR: 0.000027\n",
      "Step 4010, Loss Train: 2.8746, Loss Test: 3.9194, LR: 0.000027\n",
      "Step 4011, Loss Train: 3.1012, Loss Test: 2.5348, LR: 0.000027\n",
      "Step 4012, Loss Train: 2.8924, Loss Test: 3.7926, LR: 0.000027\n",
      "Step 4013, Loss Train: 2.8879, Loss Test: 4.0887, LR: 0.000027\n",
      "Step 4014, Loss Train: 2.7418, Loss Test: 3.8474, LR: 0.000027\n",
      "Step 4015, Loss Train: 2.9030, Loss Test: 3.8983, LR: 0.000027\n",
      "Step 4016, Loss Train: 2.9230, Loss Test: 3.3491, LR: 0.000027\n",
      "Step 4017, Loss Train: 2.9947, Loss Test: 4.5319, LR: 0.000027\n",
      "Step 4018, Loss Train: 3.0033, Loss Test: 3.5755, LR: 0.000027\n",
      "Step 4019, Loss Train: 2.9992, Loss Test: 4.4166, LR: 0.000027\n",
      "Step 4020, Loss Train: 3.0033, Loss Test: 3.5652, LR: 0.000027\n",
      "Step 4021, Loss Train: 2.9235, Loss Test: 3.6537, LR: 0.000027\n",
      "Step 4022, Loss Train: 3.0423, Loss Test: 3.7912, LR: 0.000027\n",
      "Step 4023, Loss Train: 2.7522, Loss Test: 3.2080, LR: 0.000027\n",
      "Step 4024, Loss Train: 3.0558, Loss Test: 4.2588, LR: 0.000027\n",
      "Step 4025, Loss Train: 2.8003, Loss Test: 3.1378, LR: 0.000027\n",
      "Step 4026, Loss Train: 2.9886, Loss Test: 2.5469, LR: 0.000027\n",
      "Step 4027, Loss Train: 3.0218, Loss Test: 4.2864, LR: 0.000027\n",
      "Step 4028, Loss Train: 2.7993, Loss Test: 2.6427, LR: 0.000027\n",
      "Step 4029, Loss Train: 2.8475, Loss Test: 3.8890, LR: 0.000027\n",
      "Step 4030, Loss Train: 3.0346, Loss Test: 3.3395, LR: 0.000027\n",
      "Step 4031, Loss Train: 2.7881, Loss Test: 3.8462, LR: 0.000027\n",
      "Step 4032, Loss Train: 2.9355, Loss Test: 3.3994, LR: 0.000027\n",
      "Step 4033, Loss Train: 2.8443, Loss Test: 3.8983, LR: 0.000027\n",
      "Step 4034, Loss Train: 2.8830, Loss Test: 4.2401, LR: 0.000027\n",
      "Step 4035, Loss Train: 2.8337, Loss Test: 3.5461, LR: 0.000027\n",
      "Step 4036, Loss Train: 3.0008, Loss Test: 3.1693, LR: 0.000027\n",
      "Step 4037, Loss Train: 3.0008, Loss Test: 3.8694, LR: 0.000027\n",
      "Step 4038, Loss Train: 2.8786, Loss Test: 4.2399, LR: 0.000027\n",
      "Step 4039, Loss Train: 2.9214, Loss Test: 3.7871, LR: 0.000027\n",
      "Step 4040, Loss Train: 2.7985, Loss Test: 3.8554, LR: 0.000027\n",
      "Step 4041, Loss Train: 2.9342, Loss Test: 2.0149, LR: 0.000027\n",
      "Step 4042, Loss Train: 2.9362, Loss Test: 4.2145, LR: 0.000027\n",
      "Step 4043, Loss Train: 2.7954, Loss Test: 3.4994, LR: 0.000027\n",
      "Step 4044, Loss Train: 2.8785, Loss Test: 4.2959, LR: 0.000027\n",
      "Step 4045, Loss Train: 2.9433, Loss Test: 3.9472, LR: 0.000027\n",
      "Step 4046, Loss Train: 2.7064, Loss Test: 4.0376, LR: 0.000027\n",
      "Step 4047, Loss Train: 2.9541, Loss Test: 3.7729, LR: 0.000027\n",
      "Step 4048, Loss Train: 3.0451, Loss Test: 3.1093, LR: 0.000027\n",
      "Step 4049, Loss Train: 3.0346, Loss Test: 3.4541, LR: 0.000027\n",
      "Step 4050, Loss Train: 2.8828, Loss Test: 3.2206, LR: 0.000027\n",
      "Step 4051, Loss Train: 3.0587, Loss Test: 3.0650, LR: 0.000027\n",
      "Step 4052, Loss Train: 2.8621, Loss Test: 4.4536, LR: 0.000027\n",
      "Step 4053, Loss Train: 2.9581, Loss Test: 4.1681, LR: 0.000027\n",
      "Step 4054, Loss Train: 2.8932, Loss Test: 3.7451, LR: 0.000027\n",
      "Step 4055, Loss Train: 2.6039, Loss Test: 4.1101, LR: 0.000027\n",
      "Step 4056, Loss Train: 2.9234, Loss Test: 2.3567, LR: 0.000027\n",
      "Step 4057, Loss Train: 2.9381, Loss Test: 3.9871, LR: 0.000027\n",
      "Step 4058, Loss Train: 2.8523, Loss Test: 3.4444, LR: 0.000027\n",
      "Step 4059, Loss Train: 2.9326, Loss Test: 2.0570, LR: 0.000027\n",
      "Step 4060, Loss Train: 2.9204, Loss Test: 3.8833, LR: 0.000027\n",
      "Step 4061, Loss Train: 2.9381, Loss Test: 3.7137, LR: 0.000027\n",
      "Step 4062, Loss Train: 3.0315, Loss Test: 3.7130, LR: 0.000027\n",
      "Step 4063, Loss Train: 2.8119, Loss Test: 4.1073, LR: 0.000027\n",
      "Step 4064, Loss Train: 2.9016, Loss Test: 3.4385, LR: 0.000027\n",
      "Step 4065, Loss Train: 2.4561, Loss Test: 3.0944, LR: 0.000027\n",
      "Step 4066, Loss Train: 2.8819, Loss Test: 3.0361, LR: 0.000027\n",
      "Step 4067, Loss Train: 2.7713, Loss Test: 3.6909, LR: 0.000027\n",
      "Step 4068, Loss Train: 2.6508, Loss Test: 3.8696, LR: 0.000027\n",
      "Step 4069, Loss Train: 2.8958, Loss Test: 3.6216, LR: 0.000027\n",
      "Step 4070, Loss Train: 2.8717, Loss Test: 3.6597, LR: 0.000027\n",
      "Step 4071, Loss Train: 2.9220, Loss Test: 2.9720, LR: 0.000027\n",
      "Step 4072, Loss Train: 2.8900, Loss Test: 3.4906, LR: 0.000027\n",
      "Step 4073, Loss Train: 3.0523, Loss Test: 4.1825, LR: 0.000027\n",
      "Step 4074, Loss Train: 2.9679, Loss Test: 4.2099, LR: 0.000027\n",
      "Step 4075, Loss Train: 2.6614, Loss Test: 4.0248, LR: 0.000027\n",
      "Step 4076, Loss Train: 2.9848, Loss Test: 2.9646, LR: 0.000027\n",
      "Step 4077, Loss Train: 2.7466, Loss Test: 4.5658, LR: 0.000027\n",
      "Step 4078, Loss Train: 3.0215, Loss Test: 3.5987, LR: 0.000027\n",
      "Step 4079, Loss Train: 3.0505, Loss Test: 3.7069, LR: 0.000027\n",
      "Step 4080, Loss Train: 2.8272, Loss Test: 3.7519, LR: 0.000027\n",
      "Step 4081, Loss Train: 2.8565, Loss Test: 3.7674, LR: 0.000027\n",
      "Step 4082, Loss Train: 2.9202, Loss Test: 4.3779, LR: 0.000027\n",
      "Step 4083, Loss Train: 2.9541, Loss Test: 3.1386, LR: 0.000027\n",
      "Step 4084, Loss Train: 2.8481, Loss Test: 3.0980, LR: 0.000027\n",
      "Step 4085, Loss Train: 3.0294, Loss Test: 4.0640, LR: 0.000027\n",
      "Step 4086, Loss Train: 3.0821, Loss Test: 3.8431, LR: 0.000027\n",
      "Step 4087, Loss Train: 2.9160, Loss Test: 3.8043, LR: 0.000027\n",
      "Step 4088, Loss Train: 2.7676, Loss Test: 2.3110, LR: 0.000027\n",
      "Step 4089, Loss Train: 2.8676, Loss Test: 3.9284, LR: 0.000027\n",
      "Step 4090, Loss Train: 2.9768, Loss Test: 3.6728, LR: 0.000027\n",
      "Step 4091, Loss Train: 2.9778, Loss Test: 4.0396, LR: 0.000027\n",
      "Step 4092, Loss Train: 3.0770, Loss Test: 3.2749, LR: 0.000027\n",
      "Step 4093, Loss Train: 2.9356, Loss Test: 4.0229, LR: 0.000027\n",
      "Step 4094, Loss Train: 2.8893, Loss Test: 3.7464, LR: 0.000027\n",
      "Step 4095, Loss Train: 3.0524, Loss Test: 3.7295, LR: 0.000027\n",
      "Step 4096, Loss Train: 3.0859, Loss Test: 3.9924, LR: 0.000027\n",
      "Step 4097, Loss Train: 2.8760, Loss Test: 4.2110, LR: 0.000027\n",
      "Step 4098, Loss Train: 2.9009, Loss Test: 4.2660, LR: 0.000027\n",
      "Step 4099, Loss Train: 2.9335, Loss Test: 4.2323, LR: 0.000027\n",
      "Step 4100, Loss Train: 2.8778, Loss Test: 4.0906, LR: 0.000027\n",
      "Step 4101, Loss Train: 2.8576, Loss Test: 4.0359, LR: 0.000027\n",
      "Step 4102, Loss Train: 2.7950, Loss Test: 4.5873, LR: 0.000027\n",
      "Step 4103, Loss Train: 2.8821, Loss Test: 2.6881, LR: 0.000027\n",
      "Step 4104, Loss Train: 2.9358, Loss Test: 4.2196, LR: 0.000027\n",
      "Step 4105, Loss Train: 2.7701, Loss Test: 3.7665, LR: 0.000027\n",
      "Step 4106, Loss Train: 2.9557, Loss Test: 3.0995, LR: 0.000027\n",
      "Step 4107, Loss Train: 3.0970, Loss Test: 3.2417, LR: 0.000027\n",
      "Step 4108, Loss Train: 2.8569, Loss Test: 3.9054, LR: 0.000027\n",
      "Step 4109, Loss Train: 2.8060, Loss Test: 4.0576, LR: 0.000027\n",
      "Step 4110, Loss Train: 2.9592, Loss Test: 3.8334, LR: 0.000027\n",
      "Step 4111, Loss Train: 2.8401, Loss Test: 3.5569, LR: 0.000027\n",
      "Step 4112, Loss Train: 3.0145, Loss Test: 4.4071, LR: 0.000027\n",
      "Step 4113, Loss Train: 2.7295, Loss Test: 3.8923, LR: 0.000027\n",
      "Step 4114, Loss Train: 2.9404, Loss Test: 4.3124, LR: 0.000027\n",
      "Step 4115, Loss Train: 3.0612, Loss Test: 3.2369, LR: 0.000027\n",
      "Step 4116, Loss Train: 2.8715, Loss Test: 3.8892, LR: 0.000027\n",
      "Step 4117, Loss Train: 3.0125, Loss Test: 4.0134, LR: 0.000027\n",
      "Step 4118, Loss Train: 2.9493, Loss Test: 3.7582, LR: 0.000027\n",
      "Step 4119, Loss Train: 2.9046, Loss Test: 3.1706, LR: 0.000027\n",
      "Step 4120, Loss Train: 2.9827, Loss Test: 4.6456, LR: 0.000027\n",
      "Step 4121, Loss Train: 3.0942, Loss Test: 3.5867, LR: 0.000027\n",
      "Step 4122, Loss Train: 2.9439, Loss Test: 3.5988, LR: 0.000027\n",
      "Step 4123, Loss Train: 3.0050, Loss Test: 4.0722, LR: 0.000027\n",
      "Step 4124, Loss Train: 2.9320, Loss Test: 3.9133, LR: 0.000027\n",
      "Step 4125, Loss Train: 2.8089, Loss Test: 4.2020, LR: 0.000027\n",
      "Step 4126, Loss Train: 2.9440, Loss Test: 3.3888, LR: 0.000027\n",
      "Step 4127, Loss Train: 2.9013, Loss Test: 2.7831, LR: 0.000027\n",
      "Step 4128, Loss Train: 2.9679, Loss Test: 3.8905, LR: 0.000027\n",
      "Step 4129, Loss Train: 2.9146, Loss Test: 3.8159, LR: 0.000027\n",
      "Step 4130, Loss Train: 2.9773, Loss Test: 4.0385, LR: 0.000027\n",
      "Step 4131, Loss Train: 2.9574, Loss Test: 3.5774, LR: 0.000027\n",
      "Step 4132, Loss Train: 2.8784, Loss Test: 3.9871, LR: 0.000027\n",
      "Step 4133, Loss Train: 2.9683, Loss Test: 2.2328, LR: 0.000027\n",
      "Step 4134, Loss Train: 3.0020, Loss Test: 4.1034, LR: 0.000027\n",
      "Step 4135, Loss Train: 3.0398, Loss Test: 3.6853, LR: 0.000027\n",
      "Step 4136, Loss Train: 2.7527, Loss Test: 4.0842, LR: 0.000027\n",
      "Step 4137, Loss Train: 2.9443, Loss Test: 4.0480, LR: 0.000027\n",
      "Step 4138, Loss Train: 2.8030, Loss Test: 3.0404, LR: 0.000027\n",
      "Step 4139, Loss Train: 2.7977, Loss Test: 2.4935, LR: 0.000027\n",
      "Step 4140, Loss Train: 2.9008, Loss Test: 3.7444, LR: 0.000027\n",
      "Step 4141, Loss Train: 3.1203, Loss Test: 2.9042, LR: 0.000027\n",
      "Step 4142, Loss Train: 2.8470, Loss Test: 3.6686, LR: 0.000027\n",
      "Step 4143, Loss Train: 2.8338, Loss Test: 4.2290, LR: 0.000027\n",
      "Step 4144, Loss Train: 2.9579, Loss Test: 3.2490, LR: 0.000027\n",
      "Step 4145, Loss Train: 3.0631, Loss Test: 4.0635, LR: 0.000027\n",
      "Step 4146, Loss Train: 2.9546, Loss Test: 3.5211, LR: 0.000027\n",
      "Step 4147, Loss Train: 3.0213, Loss Test: 1.7351, LR: 0.000027\n",
      "Step 4148, Loss Train: 2.9038, Loss Test: 3.8007, LR: 0.000027\n",
      "Step 4149, Loss Train: 2.9520, Loss Test: 4.7791, LR: 0.000027\n",
      "Step 4150, Loss Train: 2.8394, Loss Test: 3.7895, LR: 0.000027\n",
      "Step 4151, Loss Train: 3.0376, Loss Test: 2.5558, LR: 0.000027\n",
      "Step 4152, Loss Train: 2.8687, Loss Test: 3.7553, LR: 0.000027\n",
      "Step 4153, Loss Train: 2.7785, Loss Test: 3.5811, LR: 0.000027\n",
      "Step 4154, Loss Train: 2.8698, Loss Test: 4.1913, LR: 0.000027\n",
      "Step 4155, Loss Train: 2.9169, Loss Test: 3.8157, LR: 0.000027\n",
      "Step 4156, Loss Train: 2.9145, Loss Test: 3.7133, LR: 0.000027\n",
      "Step 4157, Loss Train: 2.9040, Loss Test: 3.4463, LR: 0.000027\n",
      "Step 4158, Loss Train: 2.9291, Loss Test: 3.8647, LR: 0.000027\n",
      "Step 4159, Loss Train: 2.8355, Loss Test: 4.0693, LR: 0.000027\n",
      "Step 4160, Loss Train: 2.7741, Loss Test: 3.9228, LR: 0.000027\n",
      "Step 4161, Loss Train: 2.7303, Loss Test: 3.9208, LR: 0.000027\n",
      "Step 4162, Loss Train: 3.0125, Loss Test: 3.0615, LR: 0.000027\n",
      "Step 4163, Loss Train: 2.8941, Loss Test: 4.0359, LR: 0.000027\n",
      "Step 4164, Loss Train: 2.8672, Loss Test: 3.8961, LR: 0.000027\n",
      "Step 4165, Loss Train: 2.8707, Loss Test: 2.9539, LR: 0.000027\n",
      "Step 4166, Loss Train: 2.9159, Loss Test: 3.8071, LR: 0.000027\n",
      "Step 4167, Loss Train: 3.0000, Loss Test: 3.9122, LR: 0.000027\n",
      "Step 4168, Loss Train: 2.6922, Loss Test: 4.2692, LR: 0.000027\n",
      "Step 4169, Loss Train: 2.8782, Loss Test: 4.1217, LR: 0.000027\n",
      "Step 4170, Loss Train: 2.8015, Loss Test: 4.3121, LR: 0.000027\n",
      "Step 4171, Loss Train: 2.9718, Loss Test: 4.1464, LR: 0.000027\n",
      "Step 4172, Loss Train: 3.0246, Loss Test: 3.9194, LR: 0.000027\n",
      "Step 4173, Loss Train: 2.9312, Loss Test: 3.3847, LR: 0.000027\n",
      "Step 4174, Loss Train: 3.0478, Loss Test: 4.3426, LR: 0.000027\n",
      "Step 4175, Loss Train: 2.7741, Loss Test: 4.1278, LR: 0.000027\n",
      "Step 4176, Loss Train: 2.8191, Loss Test: 3.9610, LR: 0.000027\n",
      "Step 4177, Loss Train: 2.9046, Loss Test: 4.1230, LR: 0.000027\n",
      "Step 4178, Loss Train: 2.8558, Loss Test: 4.0328, LR: 0.000027\n",
      "Step 4179, Loss Train: 2.8246, Loss Test: 4.1348, LR: 0.000027\n",
      "Step 4180, Loss Train: 3.0687, Loss Test: 3.7402, LR: 0.000027\n",
      "Step 4181, Loss Train: 2.8480, Loss Test: 3.5302, LR: 0.000027\n",
      "Step 4182, Loss Train: 3.0142, Loss Test: 4.3469, LR: 0.000027\n",
      "Step 4183, Loss Train: 3.0412, Loss Test: 3.5998, LR: 0.000027\n",
      "Step 4184, Loss Train: 2.8970, Loss Test: 4.0400, LR: 0.000027\n",
      "Step 4185, Loss Train: 2.9162, Loss Test: 3.5972, LR: 0.000027\n",
      "Step 4186, Loss Train: 2.9283, Loss Test: 3.5867, LR: 0.000027\n",
      "Step 4187, Loss Train: 3.0409, Loss Test: 4.0234, LR: 0.000027\n",
      "Step 4188, Loss Train: 2.8589, Loss Test: 3.6217, LR: 0.000027\n",
      "Step 4189, Loss Train: 2.9618, Loss Test: 4.2180, LR: 0.000027\n",
      "Step 4190, Loss Train: 2.9113, Loss Test: 4.2390, LR: 0.000027\n",
      "Step 4191, Loss Train: 2.8886, Loss Test: 4.0146, LR: 0.000027\n",
      "Step 4192, Loss Train: 3.0054, Loss Test: 3.9639, LR: 0.000027\n",
      "Step 4193, Loss Train: 2.8744, Loss Test: 3.4986, LR: 0.000027\n",
      "Step 4194, Loss Train: 3.0106, Loss Test: 3.6721, LR: 0.000027\n",
      "Step 4195, Loss Train: 2.8743, Loss Test: 3.6285, LR: 0.000027\n",
      "Step 4196, Loss Train: 3.0702, Loss Test: 3.4248, LR: 0.000027\n",
      "Step 4197, Loss Train: 2.9231, Loss Test: 2.2113, LR: 0.000027\n",
      "Step 4198, Loss Train: 2.8934, Loss Test: 3.6311, LR: 0.000027\n",
      "Step 4199, Loss Train: 2.9022, Loss Test: 4.4308, LR: 0.000027\n",
      "Step 4200, Loss Train: 2.7192, Loss Test: 4.0009, LR: 0.000027\n",
      "Step 4201, Loss Train: 2.7385, Loss Test: 4.3497, LR: 0.000027\n",
      "Step 4202, Loss Train: 3.0076, Loss Test: 3.3719, LR: 0.000027\n",
      "Step 4203, Loss Train: 3.0002, Loss Test: 3.1615, LR: 0.000027\n",
      "Step 4204, Loss Train: 3.1292, Loss Test: 3.6259, LR: 0.000027\n",
      "Step 4205, Loss Train: 3.1185, Loss Test: 3.7340, LR: 0.000027\n",
      "Step 4206, Loss Train: 2.8315, Loss Test: 4.3061, LR: 0.000027\n",
      "Step 4207, Loss Train: 2.9043, Loss Test: 2.6275, LR: 0.000027\n",
      "Step 4208, Loss Train: 2.9161, Loss Test: 4.1894, LR: 0.000027\n",
      "Step 4209, Loss Train: 2.8761, Loss Test: 3.2172, LR: 0.000027\n",
      "Step 4210, Loss Train: 2.8661, Loss Test: 4.5528, LR: 0.000027\n",
      "Step 4211, Loss Train: 3.0820, Loss Test: 4.0046, LR: 0.000027\n",
      "Step 4212, Loss Train: 2.8654, Loss Test: 4.2072, LR: 0.000027\n",
      "Step 4213, Loss Train: 2.9909, Loss Test: 3.6163, LR: 0.000027\n",
      "Step 4214, Loss Train: 2.8960, Loss Test: 3.3566, LR: 0.000027\n",
      "Step 4215, Loss Train: 3.0654, Loss Test: 3.0519, LR: 0.000027\n",
      "Step 4216, Loss Train: 2.8878, Loss Test: 3.1586, LR: 0.000027\n",
      "Step 4217, Loss Train: 2.8749, Loss Test: 3.7506, LR: 0.000027\n",
      "Step 4218, Loss Train: 2.9758, Loss Test: 3.9744, LR: 0.000027\n",
      "Step 4219, Loss Train: 2.8011, Loss Test: 3.2087, LR: 0.000027\n",
      "Step 4220, Loss Train: 2.8092, Loss Test: 3.5410, LR: 0.000027\n",
      "Step 4221, Loss Train: 2.8667, Loss Test: 4.0386, LR: 0.000027\n",
      "Step 4222, Loss Train: 2.8502, Loss Test: 3.1741, LR: 0.000027\n",
      "Step 4223, Loss Train: 2.9094, Loss Test: 4.3179, LR: 0.000027\n",
      "Step 4224, Loss Train: 2.9065, Loss Test: 4.0750, LR: 0.000027\n",
      "Step 4225, Loss Train: 2.9938, Loss Test: 4.1285, LR: 0.000027\n",
      "Step 4226, Loss Train: 2.7934, Loss Test: 4.1462, LR: 0.000027\n",
      "Step 4227, Loss Train: 2.8428, Loss Test: 3.2171, LR: 0.000027\n",
      "Step 4228, Loss Train: 2.8872, Loss Test: 2.7734, LR: 0.000027\n",
      "Step 4229, Loss Train: 2.8927, Loss Test: 4.0200, LR: 0.000027\n",
      "Step 4230, Loss Train: 3.0020, Loss Test: 3.8078, LR: 0.000027\n",
      "Step 4231, Loss Train: 2.9297, Loss Test: 4.1145, LR: 0.000027\n",
      "Step 4232, Loss Train: 3.0527, Loss Test: 4.0163, LR: 0.000027\n",
      "Step 4233, Loss Train: 2.9212, Loss Test: 3.9657, LR: 0.000027\n",
      "Step 4234, Loss Train: 2.9452, Loss Test: 3.1333, LR: 0.000027\n",
      "Step 4235, Loss Train: 2.8097, Loss Test: 3.9206, LR: 0.000027\n",
      "Step 4236, Loss Train: 2.9862, Loss Test: 3.8853, LR: 0.000027\n",
      "Step 4237, Loss Train: 2.9841, Loss Test: 3.9714, LR: 0.000027\n",
      "Step 4238, Loss Train: 2.9421, Loss Test: 2.2110, LR: 0.000027\n",
      "Step 4239, Loss Train: 2.9616, Loss Test: 4.2753, LR: 0.000027\n",
      "Step 4240, Loss Train: 2.8574, Loss Test: 3.1847, LR: 0.000027\n",
      "Step 4241, Loss Train: 2.9086, Loss Test: 3.7319, LR: 0.000027\n",
      "Step 4242, Loss Train: 2.8342, Loss Test: 3.9055, LR: 0.000027\n",
      "Step 4243, Loss Train: 3.1767, Loss Test: 4.3484, LR: 0.000027\n",
      "Step 4244, Loss Train: 3.0566, Loss Test: 3.1471, LR: 0.000027\n",
      "Step 4245, Loss Train: 2.9570, Loss Test: 3.8302, LR: 0.000027\n",
      "Step 4246, Loss Train: 3.0216, Loss Test: 3.8793, LR: 0.000027\n",
      "Step 4247, Loss Train: 2.9826, Loss Test: 4.4559, LR: 0.000027\n",
      "Step 4248, Loss Train: 2.8308, Loss Test: 3.8504, LR: 0.000027\n",
      "Step 4249, Loss Train: 2.9664, Loss Test: 3.8101, LR: 0.000027\n",
      "Step 4250, Loss Train: 2.8892, Loss Test: 2.6414, LR: 0.000027\n",
      "Step 4251, Loss Train: 3.1508, Loss Test: 4.0593, LR: 0.000027\n",
      "Step 4252, Loss Train: 2.7642, Loss Test: 4.0977, LR: 0.000027\n",
      "Step 4253, Loss Train: 2.9266, Loss Test: 4.3257, LR: 0.000027\n",
      "Step 4254, Loss Train: 2.6869, Loss Test: 3.8401, LR: 0.000027\n",
      "Step 4255, Loss Train: 2.9661, Loss Test: 2.2201, LR: 0.000027\n",
      "Step 4256, Loss Train: 2.6944, Loss Test: 3.9393, LR: 0.000027\n",
      "Step 4257, Loss Train: 2.8726, Loss Test: 4.0974, LR: 0.000027\n",
      "Step 4258, Loss Train: 2.9476, Loss Test: 3.8846, LR: 0.000027\n",
      "Step 4259, Loss Train: 3.0561, Loss Test: 3.9205, LR: 0.000027\n",
      "Step 4260, Loss Train: 3.1077, Loss Test: 3.4983, LR: 0.000027\n",
      "Step 4261, Loss Train: 2.9606, Loss Test: 4.4510, LR: 0.000027\n",
      "Step 4262, Loss Train: 2.8342, Loss Test: 3.4398, LR: 0.000027\n",
      "Step 4263, Loss Train: 2.9445, Loss Test: 3.7363, LR: 0.000027\n",
      "Step 4264, Loss Train: 2.7716, Loss Test: 3.4877, LR: 0.000027\n",
      "Step 4265, Loss Train: 2.8846, Loss Test: 3.5856, LR: 0.000027\n",
      "Step 4266, Loss Train: 3.0214, Loss Test: 3.6929, LR: 0.000027\n",
      "Step 4267, Loss Train: 2.8834, Loss Test: 3.9956, LR: 0.000027\n",
      "Step 4268, Loss Train: 2.8502, Loss Test: 4.0170, LR: 0.000027\n",
      "Step 4269, Loss Train: 2.9515, Loss Test: 3.3956, LR: 0.000027\n",
      "Step 4270, Loss Train: 2.8052, Loss Test: 3.6811, LR: 0.000027\n",
      "Step 4271, Loss Train: 2.7035, Loss Test: 4.0940, LR: 0.000027\n",
      "Step 4272, Loss Train: 2.8745, Loss Test: 3.4914, LR: 0.000027\n",
      "Step 4273, Loss Train: 2.8101, Loss Test: 3.6288, LR: 0.000027\n",
      "Step 4274, Loss Train: 2.7537, Loss Test: 3.6969, LR: 0.000027\n",
      "Step 4275, Loss Train: 2.9894, Loss Test: 3.7325, LR: 0.000027\n",
      "Step 4276, Loss Train: 2.7706, Loss Test: 3.7559, LR: 0.000027\n",
      "Step 4277, Loss Train: 2.9403, Loss Test: 3.7436, LR: 0.000027\n",
      "Step 4278, Loss Train: 3.0167, Loss Test: 4.2255, LR: 0.000027\n",
      "Step 4279, Loss Train: 2.9243, Loss Test: 3.9903, LR: 0.000027\n",
      "Step 4280, Loss Train: 2.9244, Loss Test: 3.6581, LR: 0.000027\n",
      "Step 4281, Loss Train: 2.8122, Loss Test: 3.7982, LR: 0.000027\n",
      "Step 4282, Loss Train: 2.7935, Loss Test: 3.6138, LR: 0.000027\n",
      "Step 4283, Loss Train: 3.0483, Loss Test: 3.8588, LR: 0.000027\n",
      "Step 4284, Loss Train: 2.6932, Loss Test: 3.5867, LR: 0.000027\n",
      "Step 4285, Loss Train: 3.0943, Loss Test: 4.3531, LR: 0.000027\n",
      "Step 4286, Loss Train: 2.8660, Loss Test: 4.2723, LR: 0.000027\n",
      "Step 4287, Loss Train: 2.9185, Loss Test: 2.8152, LR: 0.000027\n",
      "Step 4288, Loss Train: 2.8880, Loss Test: 4.3313, LR: 0.000027\n",
      "Step 4289, Loss Train: 2.8971, Loss Test: 4.1623, LR: 0.000027\n",
      "Step 4290, Loss Train: 2.8785, Loss Test: 3.7554, LR: 0.000027\n",
      "Step 4291, Loss Train: 2.9626, Loss Test: 4.2669, LR: 0.000027\n",
      "Step 4292, Loss Train: 3.0289, Loss Test: 2.3317, LR: 0.000027\n",
      "Step 4293, Loss Train: 2.9675, Loss Test: 2.1960, LR: 0.000027\n",
      "Step 4294, Loss Train: 2.9785, Loss Test: 3.5366, LR: 0.000027\n",
      "Step 4295, Loss Train: 2.6552, Loss Test: 2.8403, LR: 0.000027\n",
      "Step 4296, Loss Train: 2.9733, Loss Test: 3.2734, LR: 0.000027\n",
      "Step 4297, Loss Train: 3.0108, Loss Test: 3.8849, LR: 0.000027\n",
      "Step 4298, Loss Train: 3.0473, Loss Test: 3.5384, LR: 0.000027\n",
      "Step 4299, Loss Train: 2.7356, Loss Test: 4.9597, LR: 0.000027\n",
      "Step 4300, Loss Train: 2.8672, Loss Test: 3.3761, LR: 0.000027\n",
      "Step 4301, Loss Train: 2.8225, Loss Test: 2.5495, LR: 0.000027\n",
      "Step 4302, Loss Train: 2.9640, Loss Test: 3.7120, LR: 0.000027\n",
      "Step 4303, Loss Train: 2.7691, Loss Test: 4.0091, LR: 0.000027\n",
      "Step 4304, Loss Train: 2.8311, Loss Test: 3.4354, LR: 0.000027\n",
      "Step 4305, Loss Train: 2.7616, Loss Test: 4.4436, LR: 0.000027\n",
      "Step 4306, Loss Train: 2.9649, Loss Test: 2.8122, LR: 0.000027\n",
      "Step 4307, Loss Train: 2.8800, Loss Test: 3.4339, LR: 0.000027\n",
      "Step 4308, Loss Train: 2.7673, Loss Test: 3.8677, LR: 0.000027\n",
      "Step 4309, Loss Train: 2.9776, Loss Test: 4.1872, LR: 0.000027\n",
      "Step 4310, Loss Train: 2.9904, Loss Test: 4.0621, LR: 0.000027\n",
      "Step 4311, Loss Train: 2.8152, Loss Test: 4.7346, LR: 0.000027\n",
      "Step 4312, Loss Train: 2.7842, Loss Test: 3.6248, LR: 0.000027\n",
      "Step 4313, Loss Train: 2.9791, Loss Test: 3.6962, LR: 0.000027\n",
      "Step 4314, Loss Train: 2.8247, Loss Test: 4.0957, LR: 0.000027\n",
      "Step 4315, Loss Train: 2.8093, Loss Test: 3.8905, LR: 0.000027\n",
      "Step 4316, Loss Train: 3.0099, Loss Test: 3.5504, LR: 0.000027\n",
      "Step 4317, Loss Train: 2.9098, Loss Test: 4.3486, LR: 0.000027\n",
      "Step 4318, Loss Train: 3.0558, Loss Test: 3.9245, LR: 0.000027\n",
      "Step 4319, Loss Train: 2.8849, Loss Test: 2.9739, LR: 0.000027\n",
      "Step 4320, Loss Train: 2.9497, Loss Test: 4.0889, LR: 0.000027\n",
      "Step 4321, Loss Train: 2.9269, Loss Test: 3.7454, LR: 0.000027\n",
      "Step 4322, Loss Train: 2.9577, Loss Test: 4.1825, LR: 0.000027\n",
      "Step 4323, Loss Train: 3.0698, Loss Test: 3.8078, LR: 0.000027\n",
      "Step 4324, Loss Train: 2.7749, Loss Test: 4.5274, LR: 0.000027\n",
      "Step 4325, Loss Train: 3.0337, Loss Test: 4.2388, LR: 0.000027\n",
      "Step 4326, Loss Train: 2.8283, Loss Test: 2.5620, LR: 0.000027\n",
      "Step 4327, Loss Train: 2.8336, Loss Test: 3.7210, LR: 0.000027\n",
      "Step 4328, Loss Train: 3.1100, Loss Test: 4.4193, LR: 0.000027\n",
      "Step 4329, Loss Train: 3.0749, Loss Test: 4.0692, LR: 0.000027\n",
      "Step 4330, Loss Train: 2.9714, Loss Test: 3.4963, LR: 0.000027\n",
      "Step 4331, Loss Train: 3.0281, Loss Test: 4.5042, LR: 0.000027\n",
      "Step 4332, Loss Train: 2.8763, Loss Test: 3.7227, LR: 0.000027\n",
      "Step 4333, Loss Train: 2.8698, Loss Test: 3.8551, LR: 0.000027\n",
      "Step 4334, Loss Train: 2.8691, Loss Test: 3.9303, LR: 0.000027\n",
      "Step 4335, Loss Train: 2.9013, Loss Test: 3.6949, LR: 0.000027\n",
      "Step 4336, Loss Train: 2.9934, Loss Test: 3.9058, LR: 0.000027\n",
      "Step 4337, Loss Train: 2.9914, Loss Test: 3.0397, LR: 0.000027\n",
      "Step 4338, Loss Train: 2.5994, Loss Test: 3.3556, LR: 0.000027\n",
      "Step 4339, Loss Train: 2.9132, Loss Test: 4.2194, LR: 0.000027\n",
      "Step 4340, Loss Train: 2.9324, Loss Test: 4.0436, LR: 0.000027\n",
      "Step 4341, Loss Train: 3.0254, Loss Test: 3.8874, LR: 0.000027\n",
      "Step 4342, Loss Train: 2.7531, Loss Test: 3.9391, LR: 0.000027\n",
      "Step 4343, Loss Train: 2.8362, Loss Test: 4.1326, LR: 0.000027\n",
      "Step 4344, Loss Train: 3.0051, Loss Test: 3.6680, LR: 0.000027\n",
      "Step 4345, Loss Train: 2.9732, Loss Test: 4.1254, LR: 0.000027\n",
      "Step 4346, Loss Train: 2.9162, Loss Test: 4.0257, LR: 0.000027\n",
      "Step 4347, Loss Train: 2.8842, Loss Test: 3.9613, LR: 0.000027\n",
      "Step 4348, Loss Train: 2.9094, Loss Test: 3.6784, LR: 0.000027\n",
      "Step 4349, Loss Train: 2.8227, Loss Test: 3.9992, LR: 0.000027\n",
      "Step 4350, Loss Train: 2.8545, Loss Test: 3.3487, LR: 0.000027\n",
      "Step 4351, Loss Train: 3.0310, Loss Test: 4.3285, LR: 0.000027\n",
      "Step 4352, Loss Train: 3.0255, Loss Test: 4.1195, LR: 0.000027\n",
      "Step 4353, Loss Train: 2.9005, Loss Test: 4.0957, LR: 0.000027\n",
      "Step 4354, Loss Train: 2.9285, Loss Test: 4.3471, LR: 0.000027\n",
      "Step 4355, Loss Train: 3.0069, Loss Test: 3.1716, LR: 0.000027\n",
      "Step 4356, Loss Train: 2.9737, Loss Test: 4.3626, LR: 0.000027\n",
      "Step 4357, Loss Train: 2.7917, Loss Test: 3.7058, LR: 0.000027\n",
      "Step 4358, Loss Train: 2.8218, Loss Test: 2.9220, LR: 0.000027\n",
      "Step 4359, Loss Train: 3.1916, Loss Test: 3.9936, LR: 0.000027\n",
      "Step 4360, Loss Train: 2.9483, Loss Test: 3.4562, LR: 0.000027\n",
      "Step 4361, Loss Train: 2.9460, Loss Test: 4.0020, LR: 0.000027\n",
      "Step 4362, Loss Train: 2.9158, Loss Test: 2.8505, LR: 0.000027\n",
      "Step 4363, Loss Train: 2.9217, Loss Test: 3.9252, LR: 0.000027\n",
      "Step 4364, Loss Train: 2.7818, Loss Test: 3.7464, LR: 0.000027\n",
      "Step 4365, Loss Train: 2.7954, Loss Test: 3.7082, LR: 0.000027\n",
      "Step 4366, Loss Train: 2.9746, Loss Test: 3.7269, LR: 0.000027\n",
      "Step 4367, Loss Train: 2.7847, Loss Test: 4.1632, LR: 0.000027\n",
      "Step 4368, Loss Train: 2.9716, Loss Test: 3.8969, LR: 0.000027\n",
      "Step 4369, Loss Train: 2.8517, Loss Test: 3.3551, LR: 0.000027\n",
      "Step 4370, Loss Train: 3.0437, Loss Test: 3.4435, LR: 0.000027\n",
      "Step 4371, Loss Train: 2.9632, Loss Test: 4.0493, LR: 0.000027\n",
      "Step 4372, Loss Train: 3.0182, Loss Test: 3.6391, LR: 0.000027\n",
      "Step 4373, Loss Train: 2.7991, Loss Test: 3.8347, LR: 0.000027\n",
      "Step 4374, Loss Train: 2.9669, Loss Test: 4.5831, LR: 0.000027\n",
      "Step 4375, Loss Train: 2.9833, Loss Test: 3.8150, LR: 0.000027\n",
      "Step 4376, Loss Train: 2.9030, Loss Test: 3.2116, LR: 0.000027\n",
      "Step 4377, Loss Train: 2.9773, Loss Test: 4.3653, LR: 0.000027\n",
      "Step 4378, Loss Train: 2.9897, Loss Test: 3.5936, LR: 0.000027\n",
      "Step 4379, Loss Train: 2.7590, Loss Test: 3.7798, LR: 0.000027\n",
      "Step 4380, Loss Train: 2.9151, Loss Test: 3.8903, LR: 0.000027\n",
      "Step 4381, Loss Train: 3.1654, Loss Test: 3.9002, LR: 0.000027\n",
      "Step 4382, Loss Train: 2.9670, Loss Test: 3.8354, LR: 0.000027\n",
      "Step 4383, Loss Train: 2.8125, Loss Test: 3.9349, LR: 0.000027\n",
      "Step 4384, Loss Train: 2.9644, Loss Test: 3.6632, LR: 0.000027\n",
      "Step 4385, Loss Train: 2.5021, Loss Test: 3.1537, LR: 0.000027\n",
      "Step 4386, Loss Train: 2.8597, Loss Test: 3.8026, LR: 0.000027\n",
      "Step 4387, Loss Train: 3.0245, Loss Test: 3.4131, LR: 0.000027\n",
      "Step 4388, Loss Train: 2.7607, Loss Test: 3.8145, LR: 0.000027\n",
      "Step 4389, Loss Train: 2.7595, Loss Test: 4.0150, LR: 0.000027\n",
      "Step 4390, Loss Train: 2.7453, Loss Test: 2.4259, LR: 0.000027\n",
      "Step 4391, Loss Train: 2.6778, Loss Test: 2.3917, LR: 0.000027\n",
      "Step 4392, Loss Train: 2.8816, Loss Test: 3.6194, LR: 0.000027\n",
      "Step 4393, Loss Train: 2.8102, Loss Test: 3.5038, LR: 0.000027\n",
      "Step 4394, Loss Train: 3.0360, Loss Test: 4.0704, LR: 0.000027\n",
      "Step 4395, Loss Train: 2.8615, Loss Test: 2.7716, LR: 0.000027\n",
      "Step 4396, Loss Train: 2.8697, Loss Test: 4.0392, LR: 0.000027\n",
      "Step 4397, Loss Train: 2.9548, Loss Test: 3.9931, LR: 0.000027\n",
      "Step 4398, Loss Train: 2.9985, Loss Test: 2.5009, LR: 0.000027\n",
      "Step 4399, Loss Train: 2.8585, Loss Test: 3.0954, LR: 0.000027\n",
      "Step 4400, Loss Train: 3.0516, Loss Test: 4.1015, LR: 0.000027\n",
      "Step 4401, Loss Train: 2.8396, Loss Test: 3.7576, LR: 0.000027\n",
      "Step 4402, Loss Train: 2.8859, Loss Test: 3.1795, LR: 0.000027\n",
      "Step 4403, Loss Train: 2.9870, Loss Test: 3.5806, LR: 0.000027\n",
      "Step 4404, Loss Train: 3.0853, Loss Test: 2.8208, LR: 0.000027\n",
      "Step 4405, Loss Train: 2.9133, Loss Test: 3.8895, LR: 0.000027\n",
      "Step 4406, Loss Train: 2.8019, Loss Test: 4.0100, LR: 0.000027\n",
      "Step 4407, Loss Train: 2.8802, Loss Test: 4.0335, LR: 0.000027\n",
      "Step 4408, Loss Train: 3.0162, Loss Test: 3.5279, LR: 0.000027\n",
      "Step 4409, Loss Train: 2.9364, Loss Test: 3.3178, LR: 0.000027\n",
      "Step 4410, Loss Train: 2.9576, Loss Test: 2.2640, LR: 0.000027\n",
      "Step 4411, Loss Train: 3.0544, Loss Test: 2.2953, LR: 0.000027\n",
      "Step 4412, Loss Train: 2.9612, Loss Test: 2.3351, LR: 0.000027\n",
      "Step 4413, Loss Train: 2.8430, Loss Test: 4.1248, LR: 0.000027\n",
      "Step 4414, Loss Train: 2.9030, Loss Test: 4.0825, LR: 0.000027\n",
      "Step 4415, Loss Train: 2.7666, Loss Test: 3.5649, LR: 0.000027\n",
      "Step 4416, Loss Train: 2.9595, Loss Test: 1.9005, LR: 0.000027\n",
      "Step 4417, Loss Train: 2.9367, Loss Test: 4.0776, LR: 0.000027\n",
      "Step 4418, Loss Train: 2.9372, Loss Test: 4.0164, LR: 0.000027\n",
      "Step 4419, Loss Train: 3.1470, Loss Test: 3.8115, LR: 0.000027\n",
      "Step 4420, Loss Train: 2.9424, Loss Test: 4.2548, LR: 0.000027\n",
      "Step 4421, Loss Train: 2.8767, Loss Test: 4.1224, LR: 0.000027\n",
      "Step 4422, Loss Train: 2.8652, Loss Test: 2.4600, LR: 0.000027\n",
      "Step 4423, Loss Train: 2.7752, Loss Test: 2.3770, LR: 0.000027\n",
      "Step 4424, Loss Train: 2.9024, Loss Test: 3.7240, LR: 0.000027\n",
      "Step 4425, Loss Train: 2.6930, Loss Test: 2.6236, LR: 0.000027\n",
      "Step 4426, Loss Train: 2.9036, Loss Test: 4.3037, LR: 0.000027\n",
      "Step 4427, Loss Train: 2.8274, Loss Test: 3.8577, LR: 0.000027\n",
      "Step 4428, Loss Train: 2.7525, Loss Test: 3.4313, LR: 0.000027\n",
      "Step 4429, Loss Train: 2.8939, Loss Test: 3.8699, LR: 0.000027\n",
      "Step 4430, Loss Train: 3.1347, Loss Test: 2.3776, LR: 0.000027\n",
      "Step 4431, Loss Train: 3.0556, Loss Test: 4.1261, LR: 0.000027\n",
      "Step 4432, Loss Train: 2.8211, Loss Test: 3.8875, LR: 0.000027\n",
      "Step 4433, Loss Train: 3.0085, Loss Test: 3.6284, LR: 0.000027\n",
      "Step 4434, Loss Train: 2.8929, Loss Test: 4.2958, LR: 0.000027\n",
      "Step 4435, Loss Train: 2.9616, Loss Test: 3.9907, LR: 0.000027\n",
      "Step 4436, Loss Train: 2.8714, Loss Test: 4.1413, LR: 0.000027\n",
      "Step 4437, Loss Train: 2.8338, Loss Test: 3.9184, LR: 0.000027\n",
      "Step 4438, Loss Train: 2.9942, Loss Test: 3.8071, LR: 0.000027\n",
      "Step 4439, Loss Train: 2.8687, Loss Test: 3.4708, LR: 0.000027\n",
      "Step 4440, Loss Train: 2.9237, Loss Test: 4.0147, LR: 0.000027\n",
      "Step 4441, Loss Train: 3.0098, Loss Test: 3.9339, LR: 0.000027\n",
      "Step 4442, Loss Train: 2.9431, Loss Test: 4.0090, LR: 0.000027\n",
      "Step 4443, Loss Train: 2.8228, Loss Test: 4.1697, LR: 0.000027\n",
      "Step 4444, Loss Train: 2.9118, Loss Test: 4.1867, LR: 0.000027\n",
      "Step 4445, Loss Train: 2.7514, Loss Test: 3.6403, LR: 0.000027\n",
      "Step 4446, Loss Train: 2.9391, Loss Test: 4.1322, LR: 0.000027\n",
      "Step 4447, Loss Train: 2.9473, Loss Test: 3.7837, LR: 0.000027\n",
      "Step 4448, Loss Train: 2.9847, Loss Test: 4.3562, LR: 0.000027\n",
      "Step 4449, Loss Train: 2.8000, Loss Test: 3.6667, LR: 0.000027\n",
      "Step 4450, Loss Train: 2.9354, Loss Test: 4.3304, LR: 0.000027\n",
      "Step 4451, Loss Train: 2.8720, Loss Test: 3.2253, LR: 0.000027\n",
      "Step 4452, Loss Train: 2.9895, Loss Test: 2.3098, LR: 0.000027\n",
      "Step 4453, Loss Train: 3.1730, Loss Test: 3.9304, LR: 0.000027\n",
      "Step 4454, Loss Train: 3.0684, Loss Test: 3.5291, LR: 0.000027\n",
      "Step 4455, Loss Train: 2.9871, Loss Test: 3.5249, LR: 0.000027\n",
      "Step 4456, Loss Train: 2.8658, Loss Test: 4.1958, LR: 0.000027\n",
      "Step 4457, Loss Train: 2.9188, Loss Test: 4.0705, LR: 0.000027\n",
      "Step 4458, Loss Train: 2.8643, Loss Test: 3.6837, LR: 0.000027\n",
      "Step 4459, Loss Train: 2.9825, Loss Test: 3.9711, LR: 0.000027\n",
      "Step 4460, Loss Train: 3.0075, Loss Test: 2.9135, LR: 0.000027\n",
      "Step 4461, Loss Train: 2.9148, Loss Test: 3.7715, LR: 0.000027\n",
      "Step 4462, Loss Train: 2.8947, Loss Test: 4.0509, LR: 0.000027\n",
      "Step 4463, Loss Train: 3.0714, Loss Test: 4.0010, LR: 0.000027\n",
      "Step 4464, Loss Train: 2.9074, Loss Test: 4.0985, LR: 0.000027\n",
      "Step 4465, Loss Train: 2.9465, Loss Test: 4.0767, LR: 0.000027\n",
      "Step 4466, Loss Train: 3.0572, Loss Test: 4.4852, LR: 0.000027\n",
      "Step 4467, Loss Train: 2.7968, Loss Test: 4.1048, LR: 0.000027\n",
      "Step 4468, Loss Train: 2.8967, Loss Test: 3.8626, LR: 0.000027\n",
      "Step 4469, Loss Train: 2.8639, Loss Test: 2.4865, LR: 0.000027\n",
      "Step 4470, Loss Train: 2.9994, Loss Test: 4.4597, LR: 0.000027\n",
      "Step 4471, Loss Train: 2.9151, Loss Test: 4.0121, LR: 0.000027\n",
      "Step 4472, Loss Train: 2.8550, Loss Test: 3.4406, LR: 0.000027\n",
      "Step 4473, Loss Train: 2.9530, Loss Test: 4.2063, LR: 0.000027\n",
      "Step 4474, Loss Train: 2.9162, Loss Test: 2.9434, LR: 0.000027\n",
      "Step 4475, Loss Train: 3.0068, Loss Test: 3.2032, LR: 0.000027\n",
      "Step 4476, Loss Train: 2.8679, Loss Test: 1.8314, LR: 0.000027\n",
      "Step 4477, Loss Train: 2.8320, Loss Test: 3.3337, LR: 0.000027\n",
      "Step 4478, Loss Train: 3.0660, Loss Test: 4.5147, LR: 0.000027\n",
      "Step 4479, Loss Train: 2.7026, Loss Test: 2.3506, LR: 0.000027\n",
      "Step 4480, Loss Train: 3.0068, Loss Test: 3.5748, LR: 0.000027\n",
      "Step 4481, Loss Train: 3.0057, Loss Test: 3.9496, LR: 0.000027\n",
      "Step 4482, Loss Train: 3.0488, Loss Test: 3.3509, LR: 0.000027\n",
      "Step 4483, Loss Train: 2.9847, Loss Test: 1.9505, LR: 0.000027\n",
      "Step 4484, Loss Train: 3.0464, Loss Test: 4.1490, LR: 0.000027\n",
      "Step 4485, Loss Train: 2.8625, Loss Test: 3.5583, LR: 0.000027\n",
      "Step 4486, Loss Train: 3.0123, Loss Test: 2.6320, LR: 0.000027\n",
      "Step 4487, Loss Train: 2.7632, Loss Test: 2.7284, LR: 0.000027\n",
      "Step 4488, Loss Train: 2.9673, Loss Test: 3.0491, LR: 0.000027\n",
      "Step 4489, Loss Train: 3.0059, Loss Test: 3.5626, LR: 0.000027\n",
      "Step 4490, Loss Train: 2.8524, Loss Test: 3.6914, LR: 0.000027\n",
      "Step 4491, Loss Train: 2.9077, Loss Test: 4.0176, LR: 0.000027\n",
      "Step 4492, Loss Train: 3.0516, Loss Test: 3.9944, LR: 0.000027\n",
      "Step 4493, Loss Train: 2.8639, Loss Test: 3.5809, LR: 0.000027\n",
      "Step 4494, Loss Train: 2.8910, Loss Test: 2.2055, LR: 0.000027\n",
      "Step 4495, Loss Train: 2.9302, Loss Test: 4.2181, LR: 0.000027\n",
      "Step 4496, Loss Train: 2.9549, Loss Test: 2.3696, LR: 0.000027\n",
      "Step 4497, Loss Train: 2.8622, Loss Test: 1.9777, LR: 0.000027\n",
      "Step 4498, Loss Train: 3.0205, Loss Test: 3.9315, LR: 0.000027\n",
      "Step 4499, Loss Train: 2.9587, Loss Test: 3.3752, LR: 0.000027\n",
      "Step 4500, Loss Train: 3.0151, Loss Test: 4.0490, LR: 0.000027\n",
      "Step 4501, Loss Train: 2.9453, Loss Test: 4.1162, LR: 0.000027\n",
      "Step 4502, Loss Train: 3.0002, Loss Test: 4.0563, LR: 0.000027\n",
      "Step 4503, Loss Train: 2.9642, Loss Test: 2.3704, LR: 0.000027\n",
      "Step 4504, Loss Train: 2.8689, Loss Test: 4.0563, LR: 0.000027\n",
      "Step 4505, Loss Train: 2.9406, Loss Test: 4.1791, LR: 0.000027\n",
      "Step 4506, Loss Train: 2.9557, Loss Test: 3.9772, LR: 0.000027\n",
      "Step 4507, Loss Train: 2.9529, Loss Test: 4.0220, LR: 0.000027\n",
      "Step 4508, Loss Train: 2.7768, Loss Test: 3.4842, LR: 0.000027\n",
      "Step 4509, Loss Train: 2.9900, Loss Test: 3.7470, LR: 0.000027\n",
      "Step 4510, Loss Train: 2.8623, Loss Test: 3.5771, LR: 0.000027\n",
      "Step 4511, Loss Train: 2.8838, Loss Test: 3.7922, LR: 0.000027\n",
      "Step 4512, Loss Train: 2.9111, Loss Test: 3.7523, LR: 0.000027\n",
      "Step 4513, Loss Train: 3.0813, Loss Test: 3.8125, LR: 0.000027\n",
      "Step 4514, Loss Train: 2.9863, Loss Test: 3.6340, LR: 0.000027\n",
      "Step 4515, Loss Train: 2.8870, Loss Test: 4.2519, LR: 0.000027\n",
      "Step 4516, Loss Train: 2.9337, Loss Test: 3.9669, LR: 0.000027\n",
      "Step 4517, Loss Train: 2.8989, Loss Test: 2.6217, LR: 0.000027\n",
      "Step 4518, Loss Train: 2.8204, Loss Test: 3.2837, LR: 0.000027\n",
      "Step 4519, Loss Train: 2.8916, Loss Test: 3.5796, LR: 0.000027\n",
      "Step 4520, Loss Train: 2.8426, Loss Test: 3.7315, LR: 0.000027\n",
      "Step 4521, Loss Train: 2.6923, Loss Test: 4.3456, LR: 0.000027\n",
      "Step 4522, Loss Train: 2.8195, Loss Test: 3.8821, LR: 0.000027\n",
      "Step 4523, Loss Train: 2.9795, Loss Test: 4.1855, LR: 0.000027\n",
      "Step 4524, Loss Train: 2.9933, Loss Test: 4.2395, LR: 0.000027\n",
      "Step 4525, Loss Train: 3.0895, Loss Test: 4.0042, LR: 0.000027\n",
      "Step 4526, Loss Train: 3.0912, Loss Test: 3.0612, LR: 0.000027\n",
      "Step 4527, Loss Train: 2.8746, Loss Test: 4.3552, LR: 0.000027\n",
      "Step 4528, Loss Train: 2.8963, Loss Test: 2.9959, LR: 0.000027\n",
      "Step 4529, Loss Train: 2.7693, Loss Test: 3.5223, LR: 0.000027\n",
      "Step 4530, Loss Train: 2.8733, Loss Test: 4.0757, LR: 0.000027\n",
      "Step 4531, Loss Train: 2.8124, Loss Test: 3.4992, LR: 0.000027\n",
      "Step 4532, Loss Train: 2.9017, Loss Test: 4.4089, LR: 0.000027\n",
      "Step 4533, Loss Train: 2.8344, Loss Test: 1.4350, LR: 0.000027\n",
      "Step 4534, Loss Train: 3.0269, Loss Test: 4.1328, LR: 0.000027\n",
      "Step 4535, Loss Train: 2.8953, Loss Test: 3.8068, LR: 0.000027\n",
      "Step 4536, Loss Train: 2.8799, Loss Test: 3.2101, LR: 0.000027\n",
      "Step 4537, Loss Train: 3.0698, Loss Test: 2.7053, LR: 0.000027\n",
      "Step 4538, Loss Train: 2.8965, Loss Test: 3.9850, LR: 0.000027\n",
      "Step 4539, Loss Train: 3.0893, Loss Test: 4.1267, LR: 0.000027\n",
      "Step 4540, Loss Train: 2.8873, Loss Test: 3.5387, LR: 0.000027\n",
      "Step 4541, Loss Train: 2.8733, Loss Test: 4.2591, LR: 0.000027\n",
      "Step 4542, Loss Train: 2.9956, Loss Test: 3.5435, LR: 0.000027\n",
      "Step 4543, Loss Train: 3.0852, Loss Test: 2.3687, LR: 0.000027\n",
      "Step 4544, Loss Train: 2.9215, Loss Test: 3.5566, LR: 0.000027\n",
      "Step 4545, Loss Train: 2.9996, Loss Test: 3.7478, LR: 0.000027\n",
      "Step 4546, Loss Train: 2.7835, Loss Test: 2.1710, LR: 0.000027\n",
      "Step 4547, Loss Train: 2.9008, Loss Test: 3.5299, LR: 0.000027\n",
      "Step 4548, Loss Train: 2.9955, Loss Test: 4.7235, LR: 0.000027\n",
      "Step 4549, Loss Train: 2.9626, Loss Test: 2.2986, LR: 0.000027\n",
      "Step 4550, Loss Train: 2.9074, Loss Test: 3.6325, LR: 0.000027\n",
      "Step 4551, Loss Train: 2.8122, Loss Test: 4.2492, LR: 0.000027\n",
      "Step 4552, Loss Train: 2.8897, Loss Test: 3.8811, LR: 0.000027\n",
      "Step 4553, Loss Train: 2.9036, Loss Test: 4.0923, LR: 0.000027\n",
      "Step 4554, Loss Train: 2.9368, Loss Test: 4.3935, LR: 0.000027\n",
      "Step 4555, Loss Train: 2.9409, Loss Test: 3.9912, LR: 0.000027\n",
      "Step 4556, Loss Train: 3.1319, Loss Test: 4.1833, LR: 0.000027\n",
      "Step 4557, Loss Train: 2.9264, Loss Test: 4.0493, LR: 0.000027\n",
      "Step 4558, Loss Train: 2.9370, Loss Test: 3.3012, LR: 0.000027\n",
      "Step 4559, Loss Train: 2.9369, Loss Test: 4.3554, LR: 0.000027\n",
      "Step 4560, Loss Train: 2.8500, Loss Test: 3.9418, LR: 0.000027\n",
      "Step 4561, Loss Train: 2.9175, Loss Test: 3.3114, LR: 0.000027\n",
      "Step 4562, Loss Train: 3.0612, Loss Test: 3.6713, LR: 0.000027\n",
      "Step 4563, Loss Train: 2.9809, Loss Test: 2.4039, LR: 0.000027\n",
      "Step 4564, Loss Train: 2.7249, Loss Test: 3.9749, LR: 0.000027\n",
      "Step 4565, Loss Train: 3.0885, Loss Test: 3.8830, LR: 0.000027\n",
      "Step 4566, Loss Train: 2.8298, Loss Test: 4.4827, LR: 0.000027\n",
      "Step 4567, Loss Train: 2.7363, Loss Test: 4.1201, LR: 0.000027\n",
      "Step 4568, Loss Train: 3.0804, Loss Test: 4.5889, LR: 0.000027\n",
      "Step 4569, Loss Train: 2.9403, Loss Test: 4.0234, LR: 0.000027\n",
      "Step 4570, Loss Train: 2.9657, Loss Test: 4.1125, LR: 0.000027\n",
      "Step 4571, Loss Train: 2.9743, Loss Test: 4.6639, LR: 0.000027\n",
      "Step 4572, Loss Train: 2.8752, Loss Test: 3.8328, LR: 0.000027\n",
      "Step 4573, Loss Train: 2.8747, Loss Test: 3.9976, LR: 0.000027\n",
      "Step 4574, Loss Train: 3.0259, Loss Test: 4.0169, LR: 0.000027\n",
      "Step 4575, Loss Train: 2.9515, Loss Test: 3.0403, LR: 0.000027\n",
      "Step 4576, Loss Train: 2.8188, Loss Test: 3.8374, LR: 0.000027\n",
      "Step 4577, Loss Train: 2.8863, Loss Test: 3.2422, LR: 0.000027\n",
      "Step 4578, Loss Train: 2.9001, Loss Test: 2.4899, LR: 0.000027\n",
      "Step 4579, Loss Train: 2.9141, Loss Test: 3.8259, LR: 0.000027\n",
      "Step 4580, Loss Train: 3.0324, Loss Test: 3.3187, LR: 0.000027\n",
      "Step 4581, Loss Train: 3.0179, Loss Test: 3.4423, LR: 0.000027\n",
      "Step 4582, Loss Train: 2.7848, Loss Test: 4.5031, LR: 0.000027\n",
      "Step 4583, Loss Train: 3.0461, Loss Test: 3.8084, LR: 0.000027\n",
      "Step 4584, Loss Train: 2.9399, Loss Test: 4.4087, LR: 0.000027\n",
      "Step 4585, Loss Train: 2.8455, Loss Test: 3.7861, LR: 0.000027\n",
      "Step 4586, Loss Train: 2.9150, Loss Test: 3.4547, LR: 0.000027\n",
      "Step 4587, Loss Train: 3.0523, Loss Test: 3.1799, LR: 0.000027\n",
      "Step 4588, Loss Train: 2.8278, Loss Test: 4.2439, LR: 0.000027\n",
      "Step 4589, Loss Train: 2.8592, Loss Test: 3.9123, LR: 0.000027\n",
      "Step 4590, Loss Train: 2.8814, Loss Test: 2.0901, LR: 0.000027\n",
      "Step 4591, Loss Train: 2.9174, Loss Test: 3.8061, LR: 0.000027\n",
      "Step 4592, Loss Train: 2.9146, Loss Test: 3.3118, LR: 0.000027\n",
      "Step 4593, Loss Train: 2.8322, Loss Test: 3.7187, LR: 0.000027\n",
      "Step 4594, Loss Train: 2.8486, Loss Test: 3.5110, LR: 0.000027\n",
      "Step 4595, Loss Train: 2.7291, Loss Test: 3.3294, LR: 0.000027\n",
      "Step 4596, Loss Train: 2.7639, Loss Test: 3.9385, LR: 0.000027\n",
      "Step 4597, Loss Train: 2.9811, Loss Test: 3.6158, LR: 0.000027\n",
      "Step 4598, Loss Train: 2.9398, Loss Test: 3.7686, LR: 0.000027\n",
      "Step 4599, Loss Train: 3.1338, Loss Test: 3.6115, LR: 0.000027\n",
      "Step 4600, Loss Train: 3.0012, Loss Test: 3.9336, LR: 0.000027\n",
      "Step 4601, Loss Train: 2.8615, Loss Test: 3.2012, LR: 0.000027\n",
      "Step 4602, Loss Train: 2.9619, Loss Test: 2.1954, LR: 0.000027\n",
      "Step 4603, Loss Train: 2.6722, Loss Test: 3.9486, LR: 0.000027\n",
      "Step 4604, Loss Train: 3.1452, Loss Test: 3.9366, LR: 0.000027\n",
      "Step 4605, Loss Train: 2.8527, Loss Test: 3.3260, LR: 0.000027\n",
      "Step 4606, Loss Train: 2.9054, Loss Test: 3.9220, LR: 0.000027\n",
      "Step 4607, Loss Train: 2.8909, Loss Test: 2.3487, LR: 0.000027\n",
      "Step 4608, Loss Train: 2.8102, Loss Test: 4.0089, LR: 0.000027\n",
      "Step 4609, Loss Train: 2.9624, Loss Test: 4.2193, LR: 0.000027\n",
      "Step 4610, Loss Train: 2.9910, Loss Test: 3.9179, LR: 0.000027\n",
      "Step 4611, Loss Train: 3.0693, Loss Test: 3.4726, LR: 0.000027\n",
      "Step 4612, Loss Train: 3.0282, Loss Test: 3.8544, LR: 0.000027\n",
      "Step 4613, Loss Train: 2.9071, Loss Test: 3.4569, LR: 0.000027\n",
      "Step 4614, Loss Train: 2.8236, Loss Test: 3.8321, LR: 0.000027\n",
      "Step 4615, Loss Train: 3.0247, Loss Test: 4.3430, LR: 0.000027\n",
      "Step 4616, Loss Train: 2.8381, Loss Test: 2.9345, LR: 0.000027\n",
      "Step 4617, Loss Train: 2.9763, Loss Test: 4.6349, LR: 0.000027\n",
      "Step 4618, Loss Train: 3.0191, Loss Test: 3.7130, LR: 0.000027\n",
      "Step 4619, Loss Train: 2.8180, Loss Test: 3.8331, LR: 0.000027\n",
      "Step 4620, Loss Train: 2.9962, Loss Test: 3.2500, LR: 0.000027\n",
      "Step 4621, Loss Train: 2.8691, Loss Test: 3.4753, LR: 0.000027\n",
      "Step 4622, Loss Train: 2.8378, Loss Test: 4.3908, LR: 0.000027\n",
      "Step 4623, Loss Train: 2.9313, Loss Test: 4.0197, LR: 0.000027\n",
      "Step 4624, Loss Train: 3.0611, Loss Test: 4.2962, LR: 0.000027\n",
      "Step 4625, Loss Train: 2.9631, Loss Test: 3.6772, LR: 0.000027\n",
      "Step 4626, Loss Train: 2.9956, Loss Test: 2.4936, LR: 0.000027\n",
      "Step 4627, Loss Train: 2.8753, Loss Test: 3.8690, LR: 0.000027\n",
      "Step 4628, Loss Train: 2.8627, Loss Test: 2.7474, LR: 0.000027\n",
      "Step 4629, Loss Train: 2.7412, Loss Test: 3.7535, LR: 0.000027\n",
      "Step 4630, Loss Train: 2.9449, Loss Test: 4.5711, LR: 0.000027\n",
      "Step 4631, Loss Train: 2.7812, Loss Test: 3.9361, LR: 0.000027\n",
      "Step 4632, Loss Train: 2.8505, Loss Test: 3.9390, LR: 0.000027\n",
      "Step 4633, Loss Train: 3.0534, Loss Test: 4.1051, LR: 0.000027\n",
      "Step 4634, Loss Train: 3.1102, Loss Test: 4.0053, LR: 0.000027\n",
      "Step 4635, Loss Train: 2.8332, Loss Test: 4.0170, LR: 0.000027\n",
      "Step 4636, Loss Train: 2.9263, Loss Test: 4.1998, LR: 0.000027\n",
      "Step 4637, Loss Train: 2.8891, Loss Test: 3.6445, LR: 0.000027\n",
      "Step 4638, Loss Train: 2.8334, Loss Test: 3.5595, LR: 0.000027\n",
      "Step 4639, Loss Train: 3.1164, Loss Test: 4.0016, LR: 0.000027\n",
      "Step 4640, Loss Train: 3.0416, Loss Test: 3.0400, LR: 0.000027\n",
      "Step 4641, Loss Train: 3.0028, Loss Test: 3.3615, LR: 0.000027\n",
      "Step 4642, Loss Train: 2.8845, Loss Test: 4.5149, LR: 0.000027\n",
      "Step 4643, Loss Train: 2.7478, Loss Test: 3.1857, LR: 0.000027\n",
      "Step 4644, Loss Train: 3.0006, Loss Test: 3.7655, LR: 0.000027\n",
      "Step 4645, Loss Train: 3.1048, Loss Test: 3.8466, LR: 0.000027\n",
      "Step 4646, Loss Train: 2.8558, Loss Test: 2.4831, LR: 0.000027\n",
      "Step 4647, Loss Train: 2.8707, Loss Test: 3.8987, LR: 0.000027\n",
      "Step 4648, Loss Train: 3.0430, Loss Test: 3.1490, LR: 0.000027\n",
      "Step 4649, Loss Train: 3.0224, Loss Test: 3.5505, LR: 0.000027\n",
      "Step 4650, Loss Train: 2.8394, Loss Test: 3.8521, LR: 0.000027\n",
      "Step 4651, Loss Train: 3.0135, Loss Test: 4.4582, LR: 0.000027\n",
      "Step 4652, Loss Train: 2.9244, Loss Test: 3.6588, LR: 0.000027\n",
      "Step 4653, Loss Train: 2.7979, Loss Test: 4.1662, LR: 0.000027\n",
      "Step 4654, Loss Train: 2.8850, Loss Test: 4.3916, LR: 0.000027\n",
      "Step 4655, Loss Train: 2.9753, Loss Test: 3.4084, LR: 0.000027\n",
      "Step 4656, Loss Train: 2.8509, Loss Test: 3.8724, LR: 0.000027\n",
      "Step 4657, Loss Train: 3.0133, Loss Test: 3.6635, LR: 0.000027\n",
      "Step 4658, Loss Train: 2.8919, Loss Test: 3.7391, LR: 0.000027\n",
      "Step 4659, Loss Train: 2.7425, Loss Test: 3.9916, LR: 0.000027\n",
      "Step 4660, Loss Train: 2.9888, Loss Test: 3.5723, LR: 0.000027\n",
      "Step 4661, Loss Train: 2.8988, Loss Test: 4.2106, LR: 0.000027\n",
      "Step 4662, Loss Train: 2.8380, Loss Test: 3.5916, LR: 0.000027\n",
      "Step 4663, Loss Train: 3.0198, Loss Test: 4.1873, LR: 0.000027\n",
      "Step 4664, Loss Train: 2.9038, Loss Test: 2.7725, LR: 0.000027\n",
      "Step 4665, Loss Train: 2.8301, Loss Test: 3.9051, LR: 0.000027\n",
      "Step 4666, Loss Train: 2.9564, Loss Test: 2.3388, LR: 0.000027\n",
      "Step 4667, Loss Train: 2.7680, Loss Test: 3.8818, LR: 0.000027\n",
      "Step 4668, Loss Train: 2.9180, Loss Test: 3.7877, LR: 0.000027\n",
      "Step 4669, Loss Train: 3.0226, Loss Test: 3.6821, LR: 0.000027\n",
      "Step 4670, Loss Train: 3.0248, Loss Test: 3.0467, LR: 0.000027\n",
      "Step 4671, Loss Train: 2.9901, Loss Test: 3.9684, LR: 0.000027\n",
      "Step 4672, Loss Train: 2.8673, Loss Test: 3.9579, LR: 0.000027\n",
      "Step 4673, Loss Train: 3.0700, Loss Test: 3.8353, LR: 0.000027\n",
      "Step 4674, Loss Train: 2.9211, Loss Test: 4.1147, LR: 0.000027\n",
      "Step 4675, Loss Train: 2.9060, Loss Test: 3.0981, LR: 0.000027\n",
      "Step 4676, Loss Train: 2.8346, Loss Test: 3.1385, LR: 0.000027\n",
      "Step 4677, Loss Train: 2.9346, Loss Test: 3.0177, LR: 0.000027\n",
      "Step 4678, Loss Train: 2.9175, Loss Test: 2.3990, LR: 0.000027\n",
      "Step 4679, Loss Train: 2.9829, Loss Test: 3.7192, LR: 0.000027\n",
      "Step 4680, Loss Train: 3.0233, Loss Test: 3.4472, LR: 0.000027\n",
      "Step 4681, Loss Train: 2.8884, Loss Test: 3.8618, LR: 0.000027\n",
      "Step 4682, Loss Train: 2.8542, Loss Test: 3.5082, LR: 0.000027\n",
      "Step 4683, Loss Train: 2.7932, Loss Test: 3.5032, LR: 0.000027\n",
      "Step 4684, Loss Train: 2.7855, Loss Test: 3.8333, LR: 0.000027\n",
      "Step 4685, Loss Train: 2.8918, Loss Test: 4.1269, LR: 0.000027\n",
      "Step 4686, Loss Train: 3.0255, Loss Test: 2.8041, LR: 0.000027\n",
      "Step 4687, Loss Train: 3.0219, Loss Test: 3.8309, LR: 0.000027\n",
      "Step 4688, Loss Train: 2.8373, Loss Test: 3.9900, LR: 0.000027\n",
      "Step 4689, Loss Train: 2.8443, Loss Test: 3.3798, LR: 0.000027\n",
      "Step 4690, Loss Train: 2.7922, Loss Test: 4.0916, LR: 0.000027\n",
      "Step 4691, Loss Train: 2.9809, Loss Test: 2.5997, LR: 0.000027\n",
      "Step 4692, Loss Train: 2.8761, Loss Test: 3.9361, LR: 0.000027\n",
      "Step 4693, Loss Train: 2.8744, Loss Test: 3.4905, LR: 0.000027\n",
      "Step 4694, Loss Train: 3.0387, Loss Test: 3.6658, LR: 0.000027\n",
      "Step 4695, Loss Train: 2.8563, Loss Test: 3.9764, LR: 0.000027\n",
      "Step 4696, Loss Train: 2.9103, Loss Test: 3.8623, LR: 0.000027\n",
      "Step 4697, Loss Train: 2.9110, Loss Test: 3.3525, LR: 0.000027\n",
      "Step 4698, Loss Train: 2.9718, Loss Test: 3.7802, LR: 0.000027\n",
      "Step 4699, Loss Train: 2.5603, Loss Test: 4.1713, LR: 0.000027\n",
      "Step 4700, Loss Train: 2.9180, Loss Test: 3.0084, LR: 0.000027\n",
      "Step 4701, Loss Train: 2.9672, Loss Test: 2.8024, LR: 0.000027\n",
      "Step 4702, Loss Train: 2.9027, Loss Test: 3.8814, LR: 0.000027\n",
      "Step 4703, Loss Train: 3.0497, Loss Test: 3.3772, LR: 0.000027\n",
      "Step 4704, Loss Train: 2.7922, Loss Test: 4.2843, LR: 0.000027\n",
      "Step 4705, Loss Train: 2.8429, Loss Test: 4.5837, LR: 0.000027\n",
      "Step 4706, Loss Train: 2.9749, Loss Test: 2.9243, LR: 0.000027\n",
      "Step 4707, Loss Train: 2.9226, Loss Test: 3.8221, LR: 0.000027\n",
      "Step 4708, Loss Train: 2.9117, Loss Test: 4.1665, LR: 0.000027\n",
      "Step 4709, Loss Train: 2.9178, Loss Test: 3.8172, LR: 0.000027\n",
      "Step 4710, Loss Train: 3.0491, Loss Test: 4.2165, LR: 0.000027\n",
      "Step 4711, Loss Train: 2.7496, Loss Test: 3.3365, LR: 0.000027\n",
      "Step 4712, Loss Train: 2.7693, Loss Test: 3.8886, LR: 0.000027\n",
      "Step 4713, Loss Train: 2.8666, Loss Test: 4.6176, LR: 0.000027\n",
      "Step 4714, Loss Train: 2.8560, Loss Test: 3.2713, LR: 0.000027\n",
      "Step 4715, Loss Train: 2.7188, Loss Test: 3.9760, LR: 0.000027\n",
      "Step 4716, Loss Train: 3.0871, Loss Test: 3.3447, LR: 0.000027\n",
      "Step 4717, Loss Train: 2.9807, Loss Test: 4.1131, LR: 0.000027\n",
      "Step 4718, Loss Train: 2.8899, Loss Test: 4.1416, LR: 0.000027\n",
      "Step 4719, Loss Train: 2.9669, Loss Test: 3.7621, LR: 0.000027\n",
      "Step 4720, Loss Train: 3.0692, Loss Test: 4.4438, LR: 0.000027\n",
      "Step 4721, Loss Train: 2.8393, Loss Test: 4.1155, LR: 0.000027\n",
      "Step 4722, Loss Train: 2.8954, Loss Test: 3.7372, LR: 0.000027\n",
      "Step 4723, Loss Train: 2.6417, Loss Test: 3.9724, LR: 0.000027\n",
      "Step 4724, Loss Train: 3.0646, Loss Test: 2.0815, LR: 0.000027\n",
      "Step 4725, Loss Train: 2.8283, Loss Test: 3.7836, LR: 0.000027\n",
      "Step 4726, Loss Train: 2.9554, Loss Test: 2.5268, LR: 0.000027\n",
      "Step 4727, Loss Train: 2.7330, Loss Test: 4.3201, LR: 0.000027\n",
      "Step 4728, Loss Train: 2.9304, Loss Test: 3.8846, LR: 0.000027\n",
      "Step 4729, Loss Train: 2.8085, Loss Test: 3.4259, LR: 0.000027\n",
      "Step 4730, Loss Train: 2.9799, Loss Test: 3.7145, LR: 0.000027\n",
      "Step 4731, Loss Train: 2.9419, Loss Test: 3.1052, LR: 0.000027\n",
      "Step 4732, Loss Train: 2.9047, Loss Test: 3.1390, LR: 0.000027\n",
      "Step 4733, Loss Train: 2.8828, Loss Test: 4.3917, LR: 0.000027\n",
      "Step 4734, Loss Train: 2.9633, Loss Test: 3.9096, LR: 0.000027\n",
      "Step 4735, Loss Train: 2.6913, Loss Test: 3.8275, LR: 0.000027\n",
      "Step 4736, Loss Train: 2.9072, Loss Test: 4.4720, LR: 0.000027\n",
      "Step 4737, Loss Train: 2.8948, Loss Test: 2.5286, LR: 0.000027\n",
      "Step 4738, Loss Train: 2.8896, Loss Test: 4.0705, LR: 0.000027\n",
      "Step 4739, Loss Train: 3.0030, Loss Test: 3.1045, LR: 0.000027\n",
      "Step 4740, Loss Train: 2.7878, Loss Test: 3.0700, LR: 0.000027\n",
      "Step 4741, Loss Train: 2.9568, Loss Test: 4.2883, LR: 0.000027\n",
      "Step 4742, Loss Train: 3.0158, Loss Test: 1.8006, LR: 0.000027\n",
      "Step 4743, Loss Train: 2.8041, Loss Test: 3.4232, LR: 0.000027\n",
      "Step 4744, Loss Train: 2.8448, Loss Test: 3.7605, LR: 0.000027\n",
      "Step 4745, Loss Train: 2.8161, Loss Test: 3.4810, LR: 0.000027\n",
      "Step 4746, Loss Train: 2.8373, Loss Test: 4.1214, LR: 0.000027\n",
      "Step 4747, Loss Train: 2.9475, Loss Test: 3.7813, LR: 0.000027\n",
      "Step 4748, Loss Train: 3.0190, Loss Test: 3.5907, LR: 0.000027\n",
      "Step 4749, Loss Train: 2.7601, Loss Test: 2.3615, LR: 0.000027\n",
      "Step 4750, Loss Train: 2.9259, Loss Test: 4.2282, LR: 0.000027\n",
      "Step 4751, Loss Train: 2.8443, Loss Test: 3.1336, LR: 0.000027\n",
      "Step 4752, Loss Train: 2.9506, Loss Test: 3.9944, LR: 0.000027\n",
      "Step 4753, Loss Train: 3.0365, Loss Test: 3.9254, LR: 0.000027\n",
      "Step 4754, Loss Train: 3.0580, Loss Test: 3.6626, LR: 0.000027\n",
      "Step 4755, Loss Train: 3.0332, Loss Test: 3.6711, LR: 0.000027\n",
      "Step 4756, Loss Train: 2.7867, Loss Test: 4.2312, LR: 0.000027\n",
      "Step 4757, Loss Train: 2.9450, Loss Test: 4.1814, LR: 0.000027\n",
      "Step 4758, Loss Train: 2.8958, Loss Test: 4.2290, LR: 0.000027\n",
      "Step 4759, Loss Train: 2.9559, Loss Test: 3.8099, LR: 0.000027\n",
      "Step 4760, Loss Train: 2.8291, Loss Test: 4.2166, LR: 0.000027\n",
      "Step 4761, Loss Train: 3.0107, Loss Test: 4.6658, LR: 0.000027\n",
      "Step 4762, Loss Train: 2.9284, Loss Test: 4.0582, LR: 0.000027\n",
      "Step 4763, Loss Train: 2.9780, Loss Test: 3.0523, LR: 0.000027\n",
      "Step 4764, Loss Train: 2.7843, Loss Test: 4.0898, LR: 0.000027\n",
      "Step 4765, Loss Train: 2.9410, Loss Test: 3.7881, LR: 0.000027\n",
      "Step 4766, Loss Train: 3.1234, Loss Test: 4.1581, LR: 0.000027\n",
      "Step 4767, Loss Train: 2.7468, Loss Test: 3.9054, LR: 0.000027\n",
      "Step 4768, Loss Train: 2.8366, Loss Test: 2.8095, LR: 0.000027\n",
      "Step 4769, Loss Train: 2.8142, Loss Test: 3.7099, LR: 0.000027\n",
      "Step 4770, Loss Train: 2.8811, Loss Test: 3.7165, LR: 0.000027\n",
      "Step 4771, Loss Train: 2.9628, Loss Test: 2.9561, LR: 0.000027\n",
      "Step 4772, Loss Train: 2.8690, Loss Test: 3.9571, LR: 0.000027\n",
      "Step 4773, Loss Train: 3.0022, Loss Test: 3.5606, LR: 0.000027\n",
      "Step 4774, Loss Train: 3.0220, Loss Test: 3.8175, LR: 0.000027\n",
      "Step 4775, Loss Train: 2.9781, Loss Test: 4.1150, LR: 0.000027\n",
      "Step 4776, Loss Train: 2.8851, Loss Test: 3.9373, LR: 0.000027\n",
      "Step 4777, Loss Train: 2.7896, Loss Test: 4.0684, LR: 0.000027\n",
      "Step 4778, Loss Train: 2.9343, Loss Test: 4.0803, LR: 0.000027\n",
      "Step 4779, Loss Train: 3.0545, Loss Test: 3.8487, LR: 0.000027\n",
      "Step 4780, Loss Train: 2.9739, Loss Test: 0.7467, LR: 0.000027\n",
      "Step 4781, Loss Train: 2.7552, Loss Test: 3.8998, LR: 0.000027\n",
      "Step 4782, Loss Train: 2.8841, Loss Test: 2.9689, LR: 0.000027\n",
      "Step 4783, Loss Train: 2.9640, Loss Test: 3.9724, LR: 0.000027\n",
      "Step 4784, Loss Train: 2.9276, Loss Test: 2.9618, LR: 0.000027\n",
      "Step 4785, Loss Train: 2.9785, Loss Test: 3.7753, LR: 0.000027\n",
      "Step 4786, Loss Train: 2.6888, Loss Test: 4.5538, LR: 0.000027\n",
      "Step 4787, Loss Train: 2.8856, Loss Test: 4.3422, LR: 0.000027\n",
      "Step 4788, Loss Train: 3.0221, Loss Test: 4.0082, LR: 0.000027\n",
      "Step 4789, Loss Train: 2.9579, Loss Test: 4.2037, LR: 0.000027\n",
      "Step 4790, Loss Train: 2.7213, Loss Test: 3.5931, LR: 0.000027\n",
      "Step 4791, Loss Train: 3.0493, Loss Test: 3.6060, LR: 0.000027\n",
      "Step 4792, Loss Train: 2.7738, Loss Test: 3.9661, LR: 0.000027\n",
      "Step 4793, Loss Train: 2.7416, Loss Test: 4.0356, LR: 0.000027\n",
      "Step 4794, Loss Train: 2.8508, Loss Test: 4.0496, LR: 0.000027\n",
      "Step 4795, Loss Train: 2.7264, Loss Test: 3.5912, LR: 0.000027\n",
      "Step 4796, Loss Train: 2.6899, Loss Test: 3.6032, LR: 0.000027\n",
      "Step 4797, Loss Train: 2.9078, Loss Test: 3.3205, LR: 0.000027\n",
      "Step 4798, Loss Train: 2.9716, Loss Test: 4.2207, LR: 0.000027\n",
      "Step 4799, Loss Train: 2.7546, Loss Test: 2.9913, LR: 0.000027\n",
      "Step 4800, Loss Train: 2.9331, Loss Test: 3.3426, LR: 0.000027\n",
      "Step 4801, Loss Train: 2.7876, Loss Test: 4.0667, LR: 0.000027\n",
      "Step 4802, Loss Train: 2.8539, Loss Test: 2.9544, LR: 0.000027\n",
      "Step 4803, Loss Train: 2.8268, Loss Test: 4.0674, LR: 0.000027\n",
      "Step 4804, Loss Train: 2.9611, Loss Test: 3.1973, LR: 0.000027\n",
      "Step 4805, Loss Train: 3.0133, Loss Test: 3.9261, LR: 0.000027\n",
      "Step 4806, Loss Train: 2.7864, Loss Test: 2.4562, LR: 0.000027\n",
      "Step 4807, Loss Train: 2.9707, Loss Test: 1.4767, LR: 0.000027\n",
      "Step 4808, Loss Train: 2.7786, Loss Test: 3.1264, LR: 0.000027\n",
      "Step 4809, Loss Train: 2.9113, Loss Test: 4.5347, LR: 0.000027\n",
      "Step 4810, Loss Train: 3.0989, Loss Test: 3.2836, LR: 0.000027\n",
      "Step 4811, Loss Train: 3.0271, Loss Test: 3.9044, LR: 0.000027\n",
      "Step 4812, Loss Train: 2.9521, Loss Test: 3.9097, LR: 0.000027\n",
      "Step 4813, Loss Train: 2.9087, Loss Test: 3.4444, LR: 0.000027\n",
      "Step 4814, Loss Train: 2.8820, Loss Test: 3.8366, LR: 0.000027\n",
      "Step 4815, Loss Train: 2.7445, Loss Test: 3.4389, LR: 0.000027\n",
      "Step 4816, Loss Train: 2.9764, Loss Test: 3.8711, LR: 0.000027\n",
      "Step 4817, Loss Train: 2.8868, Loss Test: 3.3974, LR: 0.000027\n",
      "Step 4818, Loss Train: 2.9306, Loss Test: 4.1114, LR: 0.000027\n",
      "Step 4819, Loss Train: 2.9235, Loss Test: 3.5845, LR: 0.000027\n",
      "Step 4820, Loss Train: 2.8858, Loss Test: 3.7637, LR: 0.000027\n",
      "Step 4821, Loss Train: 3.0245, Loss Test: 3.9822, LR: 0.000027\n",
      "Step 4822, Loss Train: 2.8964, Loss Test: 3.4784, LR: 0.000027\n",
      "Step 4823, Loss Train: 2.8074, Loss Test: 3.8896, LR: 0.000027\n",
      "Step 4824, Loss Train: 3.1490, Loss Test: 3.6624, LR: 0.000027\n",
      "Step 4825, Loss Train: 2.7329, Loss Test: 3.4465, LR: 0.000027\n",
      "Step 4826, Loss Train: 2.8701, Loss Test: 3.7525, LR: 0.000027\n",
      "Step 4827, Loss Train: 2.9131, Loss Test: 3.7092, LR: 0.000027\n",
      "Step 4828, Loss Train: 2.9414, Loss Test: 3.5233, LR: 0.000027\n",
      "Step 4829, Loss Train: 2.8701, Loss Test: 3.5390, LR: 0.000027\n",
      "Step 4830, Loss Train: 2.6686, Loss Test: 4.2227, LR: 0.000027\n",
      "Step 4831, Loss Train: 2.9817, Loss Test: 3.2551, LR: 0.000027\n",
      "Step 4832, Loss Train: 2.9631, Loss Test: 3.7854, LR: 0.000027\n",
      "Step 4833, Loss Train: 2.7025, Loss Test: 2.9631, LR: 0.000027\n",
      "Step 4834, Loss Train: 3.1010, Loss Test: 3.5358, LR: 0.000027\n",
      "Step 4835, Loss Train: 2.8291, Loss Test: 3.6482, LR: 0.000027\n",
      "Step 4836, Loss Train: 2.9984, Loss Test: 4.1301, LR: 0.000027\n",
      "Step 4837, Loss Train: 2.8133, Loss Test: 3.9110, LR: 0.000027\n",
      "Step 4838, Loss Train: 3.1231, Loss Test: 3.7826, LR: 0.000027\n",
      "Step 4839, Loss Train: 2.8602, Loss Test: 3.9757, LR: 0.000027\n",
      "Step 4840, Loss Train: 2.8294, Loss Test: 2.8871, LR: 0.000027\n",
      "Step 4841, Loss Train: 2.8452, Loss Test: 3.9501, LR: 0.000027\n",
      "Step 4842, Loss Train: 2.8561, Loss Test: 2.9095, LR: 0.000027\n",
      "Step 4843, Loss Train: 2.8808, Loss Test: 4.5369, LR: 0.000027\n",
      "Step 4844, Loss Train: 3.0375, Loss Test: 4.3158, LR: 0.000027\n",
      "Step 4845, Loss Train: 2.6433, Loss Test: 3.6631, LR: 0.000027\n",
      "Step 4846, Loss Train: 2.6416, Loss Test: 4.0712, LR: 0.000027\n",
      "Step 4847, Loss Train: 2.8739, Loss Test: 3.2112, LR: 0.000027\n",
      "Step 4848, Loss Train: 2.7725, Loss Test: 3.3717, LR: 0.000027\n",
      "Step 4849, Loss Train: 2.9808, Loss Test: 2.4053, LR: 0.000027\n",
      "Step 4850, Loss Train: 2.9605, Loss Test: 3.7761, LR: 0.000027\n",
      "Step 4851, Loss Train: 2.8396, Loss Test: 3.7907, LR: 0.000027\n",
      "Step 4852, Loss Train: 2.7669, Loss Test: 2.9229, LR: 0.000027\n",
      "Step 4853, Loss Train: 2.9024, Loss Test: 3.4816, LR: 0.000027\n",
      "Step 4854, Loss Train: 2.8730, Loss Test: 3.9271, LR: 0.000027\n",
      "Step 4855, Loss Train: 2.7466, Loss Test: 4.3472, LR: 0.000027\n",
      "Step 4856, Loss Train: 2.7916, Loss Test: 3.1623, LR: 0.000027\n",
      "Step 4857, Loss Train: 2.7637, Loss Test: 4.2604, LR: 0.000027\n",
      "Step 4858, Loss Train: 2.8593, Loss Test: 3.7579, LR: 0.000027\n",
      "Step 4859, Loss Train: 2.9644, Loss Test: 3.7950, LR: 0.000027\n",
      "Step 4860, Loss Train: 2.9383, Loss Test: 3.8112, LR: 0.000027\n",
      "Step 4861, Loss Train: 2.8573, Loss Test: 3.5182, LR: 0.000027\n",
      "Step 4862, Loss Train: 2.8087, Loss Test: 3.8586, LR: 0.000027\n",
      "Step 4863, Loss Train: 2.7113, Loss Test: 3.4880, LR: 0.000027\n",
      "Step 4864, Loss Train: 2.8376, Loss Test: 4.1137, LR: 0.000027\n",
      "Step 4865, Loss Train: 3.0112, Loss Test: 4.0916, LR: 0.000027\n",
      "Step 4866, Loss Train: 2.9705, Loss Test: 3.2655, LR: 0.000027\n",
      "Step 4867, Loss Train: 2.8780, Loss Test: 2.5577, LR: 0.000027\n",
      "Step 4868, Loss Train: 2.8887, Loss Test: 3.8537, LR: 0.000027\n",
      "Step 4869, Loss Train: 2.8257, Loss Test: 3.1792, LR: 0.000027\n",
      "Step 4870, Loss Train: 2.8989, Loss Test: 3.5139, LR: 0.000027\n",
      "Step 4871, Loss Train: 3.0700, Loss Test: 3.9865, LR: 0.000027\n",
      "Step 4872, Loss Train: 3.0274, Loss Test: 3.8945, LR: 0.000027\n",
      "Step 4873, Loss Train: 2.7964, Loss Test: 2.8506, LR: 0.000027\n",
      "Step 4874, Loss Train: 2.8114, Loss Test: 4.1908, LR: 0.000027\n",
      "Step 4875, Loss Train: 2.8935, Loss Test: 4.1302, LR: 0.000027\n",
      "Step 4876, Loss Train: 2.8719, Loss Test: 3.6987, LR: 0.000027\n",
      "Step 4877, Loss Train: 2.6972, Loss Test: 4.1996, LR: 0.000027\n",
      "Step 4878, Loss Train: 2.8971, Loss Test: 4.0977, LR: 0.000027\n",
      "Step 4879, Loss Train: 2.7391, Loss Test: 3.6775, LR: 0.000027\n",
      "Step 4880, Loss Train: 2.8345, Loss Test: 3.9241, LR: 0.000027\n",
      "Step 4881, Loss Train: 2.9468, Loss Test: 3.3092, LR: 0.000027\n",
      "Step 4882, Loss Train: 2.8460, Loss Test: 3.7938, LR: 0.000027\n",
      "Step 4883, Loss Train: 3.0432, Loss Test: 4.5757, LR: 0.000027\n",
      "Step 4884, Loss Train: 2.9645, Loss Test: 3.6126, LR: 0.000027\n",
      "Step 4885, Loss Train: 2.9231, Loss Test: 3.5805, LR: 0.000027\n",
      "Step 4886, Loss Train: 2.8827, Loss Test: 3.9050, LR: 0.000027\n",
      "Step 4887, Loss Train: 2.8926, Loss Test: 3.5126, LR: 0.000027\n",
      "Step 4888, Loss Train: 2.9993, Loss Test: 2.4694, LR: 0.000027\n",
      "Step 4889, Loss Train: 2.7667, Loss Test: 4.4851, LR: 0.000027\n",
      "Step 4890, Loss Train: 2.5988, Loss Test: 3.9778, LR: 0.000027\n",
      "Step 4891, Loss Train: 2.9571, Loss Test: 3.7829, LR: 0.000027\n",
      "Step 4892, Loss Train: 2.9150, Loss Test: 3.9776, LR: 0.000027\n",
      "Step 4893, Loss Train: 2.8543, Loss Test: 3.6492, LR: 0.000027\n",
      "Step 4894, Loss Train: 2.9206, Loss Test: 4.1446, LR: 0.000027\n",
      "Step 4895, Loss Train: 2.9593, Loss Test: 3.7656, LR: 0.000027\n",
      "Step 4896, Loss Train: 2.7801, Loss Test: 2.8110, LR: 0.000027\n",
      "Step 4897, Loss Train: 3.1247, Loss Test: 3.8666, LR: 0.000027\n",
      "Step 4898, Loss Train: 2.9759, Loss Test: 3.7962, LR: 0.000027\n",
      "Step 4899, Loss Train: 2.9891, Loss Test: 3.7357, LR: 0.000027\n",
      "Step 4900, Loss Train: 2.8688, Loss Test: 3.8458, LR: 0.000027\n",
      "Step 4901, Loss Train: 3.0368, Loss Test: 2.8939, LR: 0.000026\n",
      "Step 4902, Loss Train: 2.9733, Loss Test: 3.8552, LR: 0.000026\n",
      "Step 4903, Loss Train: 2.9320, Loss Test: 4.0412, LR: 0.000026\n",
      "Step 4904, Loss Train: 2.8565, Loss Test: 3.8687, LR: 0.000026\n",
      "Step 4905, Loss Train: 2.7503, Loss Test: 2.5230, LR: 0.000026\n",
      "Step 4906, Loss Train: 2.7061, Loss Test: 3.6651, LR: 0.000026\n",
      "Step 4907, Loss Train: 2.7972, Loss Test: 3.7287, LR: 0.000026\n",
      "Step 4908, Loss Train: 2.9869, Loss Test: 2.9639, LR: 0.000026\n",
      "Step 4909, Loss Train: 2.7492, Loss Test: 2.7981, LR: 0.000026\n",
      "Step 4910, Loss Train: 2.9898, Loss Test: 4.1598, LR: 0.000026\n",
      "Step 4911, Loss Train: 3.1578, Loss Test: 3.5577, LR: 0.000026\n",
      "Step 4912, Loss Train: 3.0469, Loss Test: 3.0279, LR: 0.000026\n",
      "Step 4913, Loss Train: 3.0104, Loss Test: 3.5270, LR: 0.000026\n",
      "Step 4914, Loss Train: 2.7918, Loss Test: 3.7793, LR: 0.000026\n",
      "Step 4915, Loss Train: 3.0089, Loss Test: 3.9440, LR: 0.000026\n",
      "Step 4916, Loss Train: 3.0000, Loss Test: 3.6367, LR: 0.000026\n",
      "Step 4917, Loss Train: 2.9611, Loss Test: 2.2443, LR: 0.000026\n",
      "Step 4918, Loss Train: 2.9351, Loss Test: 3.3874, LR: 0.000026\n",
      "Step 4919, Loss Train: 2.9678, Loss Test: 3.9138, LR: 0.000026\n",
      "Step 4920, Loss Train: 2.9957, Loss Test: 3.8373, LR: 0.000026\n",
      "Step 4921, Loss Train: 2.9700, Loss Test: 3.6652, LR: 0.000026\n",
      "Step 4922, Loss Train: 2.8750, Loss Test: 3.7551, LR: 0.000026\n",
      "Step 4923, Loss Train: 2.9401, Loss Test: 4.1594, LR: 0.000026\n",
      "Step 4924, Loss Train: 2.8598, Loss Test: 4.2725, LR: 0.000026\n",
      "Step 4925, Loss Train: 2.8744, Loss Test: 2.5444, LR: 0.000026\n",
      "Step 4926, Loss Train: 2.9509, Loss Test: 3.8397, LR: 0.000026\n",
      "Step 4927, Loss Train: 2.7490, Loss Test: 2.9862, LR: 0.000026\n",
      "Step 4928, Loss Train: 2.6478, Loss Test: 2.5052, LR: 0.000026\n",
      "Step 4929, Loss Train: 2.9266, Loss Test: 3.3467, LR: 0.000026\n",
      "Step 4930, Loss Train: 2.9220, Loss Test: 3.0296, LR: 0.000026\n",
      "Step 4931, Loss Train: 2.9635, Loss Test: 4.0351, LR: 0.000026\n",
      "Step 4932, Loss Train: 2.9441, Loss Test: 3.8300, LR: 0.000026\n",
      "Step 4933, Loss Train: 2.8994, Loss Test: 3.9761, LR: 0.000026\n",
      "Step 4934, Loss Train: 2.9679, Loss Test: 3.2023, LR: 0.000026\n",
      "Step 4935, Loss Train: 2.8541, Loss Test: 4.2230, LR: 0.000026\n",
      "Step 4936, Loss Train: 2.9496, Loss Test: 1.0981, LR: 0.000026\n",
      "Step 4937, Loss Train: 2.7912, Loss Test: 3.8681, LR: 0.000026\n",
      "Step 4938, Loss Train: 2.9596, Loss Test: 4.1538, LR: 0.000026\n",
      "Step 4939, Loss Train: 2.8583, Loss Test: 3.4897, LR: 0.000026\n",
      "Step 4940, Loss Train: 2.8887, Loss Test: 4.2078, LR: 0.000026\n",
      "Step 4941, Loss Train: 2.9068, Loss Test: 3.4784, LR: 0.000026\n",
      "Step 4942, Loss Train: 2.9079, Loss Test: 3.6990, LR: 0.000026\n",
      "Step 4943, Loss Train: 2.8177, Loss Test: 3.7742, LR: 0.000026\n",
      "Step 4944, Loss Train: 2.9753, Loss Test: 3.3424, LR: 0.000026\n",
      "Step 4945, Loss Train: 2.9296, Loss Test: 3.5282, LR: 0.000026\n",
      "Step 4946, Loss Train: 2.8266, Loss Test: 2.8424, LR: 0.000026\n",
      "Step 4947, Loss Train: 2.8409, Loss Test: 4.0856, LR: 0.000026\n",
      "Step 4948, Loss Train: 2.8117, Loss Test: 4.2082, LR: 0.000026\n",
      "Step 4949, Loss Train: 2.9466, Loss Test: 4.1296, LR: 0.000026\n",
      "Step 4950, Loss Train: 3.0468, Loss Test: 4.1684, LR: 0.000026\n",
      "Step 4951, Loss Train: 2.9134, Loss Test: 3.5159, LR: 0.000026\n",
      "Step 4952, Loss Train: 2.6972, Loss Test: 2.9010, LR: 0.000026\n",
      "Step 4953, Loss Train: 2.9395, Loss Test: 2.8064, LR: 0.000026\n",
      "Step 4954, Loss Train: 2.8772, Loss Test: 3.9072, LR: 0.000026\n",
      "Step 4955, Loss Train: 2.7506, Loss Test: 3.4924, LR: 0.000026\n",
      "Step 4956, Loss Train: 3.0990, Loss Test: 2.9584, LR: 0.000026\n",
      "Step 4957, Loss Train: 3.0020, Loss Test: 4.1614, LR: 0.000026\n",
      "Step 4958, Loss Train: 2.8582, Loss Test: 3.9511, LR: 0.000026\n",
      "Step 4959, Loss Train: 2.8171, Loss Test: 3.8276, LR: 0.000026\n",
      "Step 4960, Loss Train: 3.0440, Loss Test: 3.6912, LR: 0.000026\n",
      "Step 4961, Loss Train: 2.8978, Loss Test: 3.7957, LR: 0.000026\n",
      "Step 4962, Loss Train: 3.0501, Loss Test: 4.1029, LR: 0.000026\n",
      "Step 4963, Loss Train: 3.0542, Loss Test: 3.5565, LR: 0.000026\n",
      "Step 4964, Loss Train: 2.7141, Loss Test: 2.9598, LR: 0.000026\n",
      "Step 4965, Loss Train: 2.6639, Loss Test: 3.8741, LR: 0.000026\n",
      "Step 4966, Loss Train: 2.9771, Loss Test: 3.0708, LR: 0.000026\n",
      "Step 4967, Loss Train: 2.8956, Loss Test: 3.1881, LR: 0.000026\n",
      "Step 4968, Loss Train: 3.0291, Loss Test: 3.4173, LR: 0.000026\n",
      "Step 4969, Loss Train: 3.0525, Loss Test: 4.3558, LR: 0.000026\n",
      "Step 4970, Loss Train: 2.9324, Loss Test: 3.1797, LR: 0.000026\n",
      "Step 4971, Loss Train: 2.9400, Loss Test: 4.0343, LR: 0.000026\n",
      "Step 4972, Loss Train: 2.8460, Loss Test: 3.3507, LR: 0.000026\n",
      "Step 4973, Loss Train: 3.0341, Loss Test: 4.1652, LR: 0.000026\n",
      "Step 4974, Loss Train: 3.0733, Loss Test: 2.9717, LR: 0.000026\n",
      "Step 4975, Loss Train: 2.9257, Loss Test: 2.9136, LR: 0.000026\n",
      "Step 4976, Loss Train: 2.9444, Loss Test: 3.6865, LR: 0.000026\n",
      "Step 4977, Loss Train: 2.9910, Loss Test: 4.0563, LR: 0.000026\n",
      "Step 4978, Loss Train: 2.8441, Loss Test: 2.3172, LR: 0.000026\n",
      "Step 4979, Loss Train: 3.1669, Loss Test: 2.8026, LR: 0.000026\n",
      "Step 4980, Loss Train: 2.9611, Loss Test: 4.0920, LR: 0.000026\n",
      "Step 4981, Loss Train: 2.8751, Loss Test: 3.2949, LR: 0.000026\n",
      "Step 4982, Loss Train: 3.0142, Loss Test: 3.8791, LR: 0.000026\n",
      "Step 4983, Loss Train: 2.8750, Loss Test: 3.5663, LR: 0.000026\n",
      "Step 4984, Loss Train: 2.9243, Loss Test: 4.3287, LR: 0.000026\n",
      "Step 4985, Loss Train: 2.9362, Loss Test: 4.1504, LR: 0.000026\n",
      "Step 4986, Loss Train: 2.8210, Loss Test: 4.0764, LR: 0.000026\n",
      "Step 4987, Loss Train: 2.8965, Loss Test: 4.0324, LR: 0.000026\n",
      "Step 4988, Loss Train: 2.7978, Loss Test: 3.7356, LR: 0.000026\n",
      "Step 4989, Loss Train: 2.9910, Loss Test: 2.5958, LR: 0.000026\n",
      "Step 4990, Loss Train: 2.9327, Loss Test: 3.4778, LR: 0.000026\n",
      "Step 4991, Loss Train: 2.9302, Loss Test: 2.2713, LR: 0.000026\n",
      "Step 4992, Loss Train: 2.9361, Loss Test: 3.0000, LR: 0.000026\n",
      "Step 4993, Loss Train: 2.8565, Loss Test: 3.7559, LR: 0.000026\n",
      "Step 4994, Loss Train: 2.8291, Loss Test: 4.3821, LR: 0.000026\n",
      "Step 4995, Loss Train: 3.0019, Loss Test: 4.4277, LR: 0.000026\n",
      "Step 4996, Loss Train: 3.0878, Loss Test: 4.0162, LR: 0.000026\n",
      "Step 4997, Loss Train: 3.1162, Loss Test: 4.3912, LR: 0.000026\n",
      "Step 4998, Loss Train: 3.0719, Loss Test: 3.9302, LR: 0.000026\n",
      "Step 4999, Loss Train: 2.8295, Loss Test: 2.5253, LR: 0.000026\n",
      "Step 5000, Loss Train: 2.9273, Loss Test: 2.6273, LR: 0.000026\n",
      "Step 5001, Loss Train: 2.9193, Loss Test: 4.0801, LR: 0.000026\n",
      "Step 5002, Loss Train: 2.9694, Loss Test: 3.5410, LR: 0.000026\n",
      "Step 5003, Loss Train: 2.6603, Loss Test: 2.5050, LR: 0.000026\n",
      "Step 5004, Loss Train: 2.8853, Loss Test: 3.3418, LR: 0.000026\n",
      "Step 5005, Loss Train: 2.8682, Loss Test: 4.0831, LR: 0.000026\n",
      "Step 5006, Loss Train: 2.8202, Loss Test: 4.2774, LR: 0.000026\n",
      "Step 5007, Loss Train: 2.9494, Loss Test: 2.9682, LR: 0.000026\n",
      "Step 5008, Loss Train: 3.0901, Loss Test: 3.9014, LR: 0.000026\n",
      "Step 5009, Loss Train: 2.8867, Loss Test: 4.1163, LR: 0.000026\n",
      "Step 5010, Loss Train: 2.9161, Loss Test: 3.8005, LR: 0.000026\n",
      "Step 5011, Loss Train: 2.8780, Loss Test: 4.1486, LR: 0.000026\n",
      "Step 5012, Loss Train: 3.0880, Loss Test: 1.7354, LR: 0.000026\n",
      "Step 5013, Loss Train: 3.1561, Loss Test: 3.8866, LR: 0.000026\n",
      "Step 5014, Loss Train: 2.8843, Loss Test: 4.1892, LR: 0.000026\n",
      "Step 5015, Loss Train: 2.8915, Loss Test: 1.9161, LR: 0.000026\n",
      "Step 5016, Loss Train: 3.0527, Loss Test: 4.3569, LR: 0.000026\n",
      "Step 5017, Loss Train: 2.9501, Loss Test: 4.3341, LR: 0.000026\n",
      "Step 5018, Loss Train: 3.0246, Loss Test: 3.6482, LR: 0.000026\n",
      "Step 5019, Loss Train: 2.9309, Loss Test: 4.0313, LR: 0.000026\n",
      "Step 5020, Loss Train: 2.8965, Loss Test: 3.1879, LR: 0.000026\n",
      "Step 5021, Loss Train: 2.9555, Loss Test: 3.4163, LR: 0.000026\n",
      "Step 5022, Loss Train: 2.8005, Loss Test: 4.1401, LR: 0.000026\n",
      "Step 5023, Loss Train: 3.0100, Loss Test: 3.8230, LR: 0.000026\n",
      "Step 5024, Loss Train: 3.0118, Loss Test: 3.7402, LR: 0.000026\n",
      "Step 5025, Loss Train: 2.9310, Loss Test: 3.2516, LR: 0.000026\n",
      "Step 5026, Loss Train: 2.9140, Loss Test: 4.1613, LR: 0.000026\n",
      "Step 5027, Loss Train: 2.8193, Loss Test: 3.8606, LR: 0.000026\n",
      "Step 5028, Loss Train: 2.9344, Loss Test: 4.3545, LR: 0.000026\n",
      "Step 5029, Loss Train: 2.9763, Loss Test: 2.2667, LR: 0.000026\n",
      "Step 5030, Loss Train: 2.9139, Loss Test: 4.5117, LR: 0.000026\n",
      "Step 5031, Loss Train: 2.8361, Loss Test: 3.3224, LR: 0.000026\n",
      "Step 5032, Loss Train: 3.0268, Loss Test: 3.0325, LR: 0.000026\n",
      "Step 5033, Loss Train: 2.8964, Loss Test: 3.5155, LR: 0.000026\n",
      "Step 5034, Loss Train: 2.8494, Loss Test: 3.7515, LR: 0.000026\n",
      "Step 5035, Loss Train: 2.8334, Loss Test: 3.9535, LR: 0.000026\n",
      "Step 5036, Loss Train: 2.8044, Loss Test: 4.0900, LR: 0.000026\n",
      "Step 5037, Loss Train: 2.9998, Loss Test: 3.6966, LR: 0.000026\n",
      "Step 5038, Loss Train: 2.9269, Loss Test: 3.4203, LR: 0.000026\n",
      "Step 5039, Loss Train: 2.9389, Loss Test: 2.5575, LR: 0.000026\n",
      "Step 5040, Loss Train: 2.8077, Loss Test: 2.4143, LR: 0.000026\n",
      "Step 5041, Loss Train: 2.9830, Loss Test: 3.5958, LR: 0.000026\n",
      "Step 5042, Loss Train: 2.9188, Loss Test: 4.0705, LR: 0.000026\n",
      "Step 5043, Loss Train: 2.9568, Loss Test: 2.3480, LR: 0.000026\n",
      "Step 5044, Loss Train: 2.9262, Loss Test: 4.2219, LR: 0.000026\n",
      "Step 5045, Loss Train: 3.0510, Loss Test: 4.2619, LR: 0.000026\n",
      "Step 5046, Loss Train: 2.9106, Loss Test: 4.1591, LR: 0.000026\n",
      "Step 5047, Loss Train: 2.9572, Loss Test: 4.1336, LR: 0.000026\n",
      "Step 5048, Loss Train: 3.1061, Loss Test: 3.7948, LR: 0.000026\n",
      "Step 5049, Loss Train: 2.8445, Loss Test: 4.0599, LR: 0.000026\n",
      "Step 5050, Loss Train: 2.6571, Loss Test: 3.7563, LR: 0.000026\n",
      "Step 5051, Loss Train: 2.9901, Loss Test: 3.7548, LR: 0.000026\n",
      "Step 5052, Loss Train: 2.8138, Loss Test: 3.7596, LR: 0.000026\n",
      "Step 5053, Loss Train: 3.0200, Loss Test: 3.8131, LR: 0.000026\n",
      "Step 5054, Loss Train: 2.6740, Loss Test: 4.1653, LR: 0.000026\n",
      "Step 5055, Loss Train: 2.8937, Loss Test: 4.0491, LR: 0.000026\n",
      "Step 5056, Loss Train: 2.8922, Loss Test: 2.9685, LR: 0.000026\n",
      "Step 5057, Loss Train: 2.9092, Loss Test: 4.1336, LR: 0.000026\n",
      "Step 5058, Loss Train: 2.8780, Loss Test: 3.2539, LR: 0.000026\n",
      "Step 5059, Loss Train: 2.7416, Loss Test: 3.4748, LR: 0.000026\n",
      "Step 5060, Loss Train: 2.9841, Loss Test: 3.9249, LR: 0.000026\n",
      "Step 5061, Loss Train: 2.8681, Loss Test: 4.1275, LR: 0.000026\n",
      "Step 5062, Loss Train: 2.9230, Loss Test: 3.5289, LR: 0.000026\n",
      "Step 5063, Loss Train: 2.9195, Loss Test: 3.2137, LR: 0.000026\n",
      "Step 5064, Loss Train: 3.1507, Loss Test: 3.5427, LR: 0.000026\n",
      "Step 5065, Loss Train: 2.9241, Loss Test: 2.8425, LR: 0.000026\n",
      "Step 5066, Loss Train: 2.9050, Loss Test: 4.2117, LR: 0.000026\n",
      "Step 5067, Loss Train: 2.8966, Loss Test: 3.9453, LR: 0.000026\n",
      "Step 5068, Loss Train: 2.9474, Loss Test: 3.9325, LR: 0.000026\n",
      "Step 5069, Loss Train: 2.9475, Loss Test: 3.5406, LR: 0.000026\n",
      "Step 5070, Loss Train: 2.7175, Loss Test: 3.8020, LR: 0.000026\n",
      "Step 5071, Loss Train: 2.8369, Loss Test: 3.2846, LR: 0.000026\n",
      "Step 5072, Loss Train: 2.8067, Loss Test: 4.0302, LR: 0.000026\n",
      "Step 5073, Loss Train: 2.8957, Loss Test: 4.3915, LR: 0.000026\n",
      "Step 5074, Loss Train: 2.9842, Loss Test: 2.2254, LR: 0.000026\n",
      "Step 5075, Loss Train: 2.9650, Loss Test: 4.0070, LR: 0.000026\n",
      "Step 5076, Loss Train: 2.9554, Loss Test: 3.5254, LR: 0.000026\n",
      "Step 5077, Loss Train: 2.8302, Loss Test: 4.3137, LR: 0.000026\n",
      "Step 5078, Loss Train: 3.0744, Loss Test: 2.5279, LR: 0.000026\n",
      "Step 5079, Loss Train: 2.8451, Loss Test: 4.0827, LR: 0.000026\n",
      "Step 5080, Loss Train: 2.9957, Loss Test: 3.6880, LR: 0.000026\n",
      "Step 5081, Loss Train: 2.9737, Loss Test: 3.1054, LR: 0.000026\n",
      "Step 5082, Loss Train: 3.0403, Loss Test: 3.1805, LR: 0.000026\n",
      "Step 5083, Loss Train: 2.8933, Loss Test: 3.6999, LR: 0.000026\n",
      "Step 5084, Loss Train: 2.9806, Loss Test: 4.0263, LR: 0.000026\n",
      "Step 5085, Loss Train: 2.9682, Loss Test: 3.1214, LR: 0.000026\n",
      "Step 5086, Loss Train: 2.9401, Loss Test: 3.6318, LR: 0.000026\n",
      "Step 5087, Loss Train: 2.9129, Loss Test: 2.6638, LR: 0.000026\n",
      "Step 5088, Loss Train: 2.9065, Loss Test: 4.7212, LR: 0.000026\n",
      "Step 5089, Loss Train: 2.8550, Loss Test: 3.9769, LR: 0.000026\n",
      "Step 5090, Loss Train: 2.9994, Loss Test: 2.4499, LR: 0.000026\n",
      "Step 5091, Loss Train: 2.9399, Loss Test: 3.8342, LR: 0.000026\n",
      "Step 5092, Loss Train: 2.9257, Loss Test: 3.9088, LR: 0.000026\n",
      "Step 5093, Loss Train: 2.9136, Loss Test: 4.2749, LR: 0.000026\n",
      "Step 5094, Loss Train: 3.0497, Loss Test: 3.7239, LR: 0.000026\n",
      "Step 5095, Loss Train: 2.7761, Loss Test: 3.7311, LR: 0.000026\n",
      "Step 5096, Loss Train: 2.9716, Loss Test: 2.6832, LR: 0.000026\n",
      "Step 5097, Loss Train: 3.0096, Loss Test: 2.9754, LR: 0.000026\n",
      "Step 5098, Loss Train: 2.7863, Loss Test: 4.5105, LR: 0.000026\n",
      "Step 5099, Loss Train: 2.9360, Loss Test: 3.8679, LR: 0.000026\n",
      "Step 5100, Loss Train: 2.8397, Loss Test: 3.1973, LR: 0.000026\n",
      "Step 5101, Loss Train: 2.9619, Loss Test: 3.2640, LR: 0.000026\n",
      "Step 5102, Loss Train: 2.9833, Loss Test: 3.1750, LR: 0.000026\n",
      "Step 5103, Loss Train: 2.8452, Loss Test: 3.5454, LR: 0.000026\n",
      "Step 5104, Loss Train: 2.7036, Loss Test: 2.1925, LR: 0.000026\n",
      "Step 5105, Loss Train: 2.9423, Loss Test: 3.3609, LR: 0.000026\n",
      "Step 5106, Loss Train: 2.9591, Loss Test: 3.7951, LR: 0.000026\n",
      "Step 5107, Loss Train: 2.9110, Loss Test: 4.0153, LR: 0.000026\n",
      "Step 5108, Loss Train: 2.9483, Loss Test: 3.4207, LR: 0.000026\n",
      "Step 5109, Loss Train: 2.7824, Loss Test: 3.8817, LR: 0.000026\n",
      "Step 5110, Loss Train: 2.8751, Loss Test: 3.0098, LR: 0.000026\n",
      "Step 5111, Loss Train: 2.9895, Loss Test: 4.3560, LR: 0.000026\n",
      "Step 5112, Loss Train: 2.8390, Loss Test: 4.0766, LR: 0.000026\n",
      "Step 5113, Loss Train: 2.9678, Loss Test: 3.7361, LR: 0.000026\n",
      "Step 5114, Loss Train: 2.9518, Loss Test: 3.2234, LR: 0.000026\n",
      "Step 5115, Loss Train: 2.9141, Loss Test: 3.6435, LR: 0.000026\n",
      "Step 5116, Loss Train: 2.8752, Loss Test: 2.6174, LR: 0.000026\n",
      "Step 5117, Loss Train: 2.8445, Loss Test: 4.1776, LR: 0.000026\n",
      "Step 5118, Loss Train: 2.8772, Loss Test: 3.5459, LR: 0.000026\n",
      "Step 5119, Loss Train: 3.0027, Loss Test: 3.8926, LR: 0.000026\n",
      "Step 5120, Loss Train: 3.0972, Loss Test: 2.1555, LR: 0.000026\n",
      "Step 5121, Loss Train: 2.9155, Loss Test: 3.5007, LR: 0.000026\n",
      "Step 5122, Loss Train: 2.9313, Loss Test: 4.0489, LR: 0.000026\n",
      "Step 5123, Loss Train: 3.0405, Loss Test: 4.1102, LR: 0.000026\n",
      "Step 5124, Loss Train: 2.7134, Loss Test: 3.7224, LR: 0.000026\n",
      "Step 5125, Loss Train: 2.7245, Loss Test: 3.9096, LR: 0.000026\n",
      "Step 5126, Loss Train: 2.8986, Loss Test: 3.6173, LR: 0.000026\n",
      "Step 5127, Loss Train: 2.9391, Loss Test: 2.0595, LR: 0.000026\n",
      "Step 5128, Loss Train: 2.8960, Loss Test: 4.3146, LR: 0.000026\n",
      "Step 5129, Loss Train: 3.2044, Loss Test: 2.4987, LR: 0.000026\n",
      "Step 5130, Loss Train: 2.7812, Loss Test: 4.2275, LR: 0.000026\n",
      "Step 5131, Loss Train: 2.7782, Loss Test: 2.8588, LR: 0.000026\n",
      "Step 5132, Loss Train: 3.1067, Loss Test: 3.9510, LR: 0.000026\n",
      "Step 5133, Loss Train: 3.0017, Loss Test: 3.9812, LR: 0.000026\n",
      "Step 5134, Loss Train: 2.8645, Loss Test: 3.5660, LR: 0.000026\n",
      "Step 5135, Loss Train: 2.7968, Loss Test: 4.3866, LR: 0.000026\n",
      "Step 5136, Loss Train: 2.8285, Loss Test: 3.4277, LR: 0.000026\n",
      "Step 5137, Loss Train: 2.8505, Loss Test: 3.9804, LR: 0.000026\n",
      "Step 5138, Loss Train: 2.8318, Loss Test: 3.7267, LR: 0.000026\n",
      "Step 5139, Loss Train: 2.9976, Loss Test: 4.3413, LR: 0.000026\n",
      "Step 5140, Loss Train: 2.8393, Loss Test: 3.6157, LR: 0.000026\n",
      "Step 5141, Loss Train: 3.0577, Loss Test: 4.0759, LR: 0.000026\n",
      "Step 5142, Loss Train: 2.9910, Loss Test: 2.0895, LR: 0.000026\n",
      "Step 5143, Loss Train: 2.8745, Loss Test: 3.7825, LR: 0.000026\n",
      "Step 5144, Loss Train: 2.8758, Loss Test: 3.6216, LR: 0.000026\n",
      "Step 5145, Loss Train: 2.9583, Loss Test: 3.4431, LR: 0.000026\n",
      "Step 5146, Loss Train: 2.9808, Loss Test: 3.4507, LR: 0.000026\n",
      "Step 5147, Loss Train: 3.0030, Loss Test: 4.2238, LR: 0.000026\n",
      "Step 5148, Loss Train: 3.0838, Loss Test: 3.6548, LR: 0.000026\n",
      "Step 5149, Loss Train: 2.9085, Loss Test: 1.9005, LR: 0.000026\n",
      "Step 5150, Loss Train: 2.9626, Loss Test: 3.6479, LR: 0.000026\n",
      "Step 5151, Loss Train: 2.8996, Loss Test: 4.2206, LR: 0.000026\n",
      "Step 5152, Loss Train: 3.1467, Loss Test: 3.1797, LR: 0.000026\n",
      "Step 5153, Loss Train: 3.0418, Loss Test: 2.5142, LR: 0.000026\n",
      "Step 5154, Loss Train: 2.9921, Loss Test: 4.6346, LR: 0.000026\n",
      "Step 5155, Loss Train: 2.8971, Loss Test: 3.3462, LR: 0.000026\n",
      "Step 5156, Loss Train: 3.0006, Loss Test: 3.9396, LR: 0.000026\n",
      "Step 5157, Loss Train: 3.1279, Loss Test: 3.1653, LR: 0.000026\n",
      "Step 5158, Loss Train: 2.8955, Loss Test: 4.7526, LR: 0.000026\n",
      "Step 5159, Loss Train: 2.9596, Loss Test: 3.5671, LR: 0.000026\n",
      "Step 5160, Loss Train: 2.8496, Loss Test: 4.1900, LR: 0.000026\n",
      "Step 5161, Loss Train: 2.7441, Loss Test: 3.3622, LR: 0.000026\n",
      "Step 5162, Loss Train: 2.8695, Loss Test: 3.6142, LR: 0.000026\n",
      "Step 5163, Loss Train: 2.8534, Loss Test: 4.1376, LR: 0.000026\n",
      "Step 5164, Loss Train: 2.8896, Loss Test: 2.5151, LR: 0.000026\n",
      "Step 5165, Loss Train: 2.8473, Loss Test: 3.5032, LR: 0.000026\n",
      "Step 5166, Loss Train: 2.8967, Loss Test: 2.5744, LR: 0.000026\n",
      "Step 5167, Loss Train: 2.7938, Loss Test: 4.2073, LR: 0.000026\n",
      "Step 5168, Loss Train: 2.9556, Loss Test: 3.4512, LR: 0.000026\n",
      "Step 5169, Loss Train: 2.8403, Loss Test: 3.9658, LR: 0.000026\n",
      "Step 5170, Loss Train: 2.8606, Loss Test: 3.9892, LR: 0.000026\n",
      "Step 5171, Loss Train: 2.8037, Loss Test: 4.1239, LR: 0.000026\n",
      "Step 5172, Loss Train: 2.8770, Loss Test: 1.0474, LR: 0.000026\n",
      "Step 5173, Loss Train: 3.0049, Loss Test: 2.9394, LR: 0.000026\n",
      "Step 5174, Loss Train: 2.8333, Loss Test: 4.3684, LR: 0.000026\n",
      "Step 5175, Loss Train: 2.8070, Loss Test: 2.9960, LR: 0.000026\n",
      "Step 5176, Loss Train: 2.7888, Loss Test: 4.4007, LR: 0.000026\n",
      "Step 5177, Loss Train: 2.9583, Loss Test: 3.6532, LR: 0.000026\n",
      "Step 5178, Loss Train: 2.8774, Loss Test: 3.5274, LR: 0.000026\n",
      "Step 5179, Loss Train: 2.9738, Loss Test: 3.9018, LR: 0.000026\n",
      "Step 5180, Loss Train: 2.9590, Loss Test: 2.5123, LR: 0.000026\n",
      "Step 5181, Loss Train: 2.6603, Loss Test: 2.1114, LR: 0.000026\n",
      "Step 5182, Loss Train: 2.8505, Loss Test: 3.8929, LR: 0.000026\n",
      "Step 5183, Loss Train: 2.9593, Loss Test: 3.3887, LR: 0.000026\n",
      "Step 5184, Loss Train: 2.8995, Loss Test: 3.5985, LR: 0.000026\n",
      "Step 5185, Loss Train: 3.0012, Loss Test: 4.3967, LR: 0.000026\n",
      "Step 5186, Loss Train: 2.9908, Loss Test: 3.6267, LR: 0.000026\n",
      "Step 5187, Loss Train: 2.9637, Loss Test: 3.6406, LR: 0.000026\n",
      "Step 5188, Loss Train: 2.8917, Loss Test: 4.2467, LR: 0.000026\n",
      "Step 5189, Loss Train: 2.9767, Loss Test: 4.0927, LR: 0.000026\n",
      "Step 5190, Loss Train: 2.9607, Loss Test: 3.6414, LR: 0.000026\n",
      "Step 5191, Loss Train: 2.9423, Loss Test: 4.2288, LR: 0.000026\n",
      "Step 5192, Loss Train: 2.8768, Loss Test: 3.1819, LR: 0.000026\n",
      "Step 5193, Loss Train: 2.8340, Loss Test: 2.9612, LR: 0.000026\n",
      "Step 5194, Loss Train: 2.8174, Loss Test: 3.5735, LR: 0.000026\n",
      "Step 5195, Loss Train: 2.8062, Loss Test: 4.0186, LR: 0.000026\n",
      "Step 5196, Loss Train: 2.9893, Loss Test: 2.5411, LR: 0.000026\n",
      "Step 5197, Loss Train: 2.9410, Loss Test: 4.1973, LR: 0.000026\n",
      "Step 5198, Loss Train: 2.9464, Loss Test: 4.4090, LR: 0.000026\n",
      "Step 5199, Loss Train: 2.7685, Loss Test: 2.3077, LR: 0.000026\n",
      "Step 5200, Loss Train: 3.0048, Loss Test: 4.3883, LR: 0.000026\n",
      "Step 5201, Loss Train: 2.9056, Loss Test: 3.6790, LR: 0.000026\n",
      "Step 5202, Loss Train: 2.9274, Loss Test: 3.3979, LR: 0.000026\n",
      "Step 5203, Loss Train: 2.8278, Loss Test: 3.8616, LR: 0.000026\n",
      "Step 5204, Loss Train: 2.9857, Loss Test: 3.4067, LR: 0.000026\n",
      "Step 5205, Loss Train: 3.1219, Loss Test: 3.2480, LR: 0.000026\n",
      "Step 5206, Loss Train: 2.7786, Loss Test: 3.4955, LR: 0.000026\n",
      "Step 5207, Loss Train: 2.9209, Loss Test: 3.6878, LR: 0.000026\n",
      "Step 5208, Loss Train: 2.7690, Loss Test: 2.8779, LR: 0.000026\n",
      "Step 5209, Loss Train: 2.8668, Loss Test: 4.3907, LR: 0.000026\n",
      "Step 5210, Loss Train: 3.0634, Loss Test: 4.1499, LR: 0.000026\n",
      "Step 5211, Loss Train: 2.8719, Loss Test: 3.4928, LR: 0.000026\n",
      "Step 5212, Loss Train: 2.8879, Loss Test: 3.5953, LR: 0.000026\n",
      "Step 5213, Loss Train: 2.9826, Loss Test: 4.2582, LR: 0.000026\n",
      "Step 5214, Loss Train: 2.7659, Loss Test: 3.5957, LR: 0.000026\n",
      "Step 5215, Loss Train: 2.8537, Loss Test: 4.0232, LR: 0.000026\n",
      "Step 5216, Loss Train: 2.9344, Loss Test: 3.1570, LR: 0.000026\n",
      "Step 5217, Loss Train: 3.0194, Loss Test: 4.1268, LR: 0.000026\n",
      "Step 5218, Loss Train: 3.0861, Loss Test: 3.4449, LR: 0.000026\n",
      "Step 5219, Loss Train: 2.9408, Loss Test: 4.0922, LR: 0.000026\n",
      "Step 5220, Loss Train: 3.0091, Loss Test: 4.2521, LR: 0.000026\n",
      "Step 5221, Loss Train: 3.0942, Loss Test: 3.5755, LR: 0.000026\n",
      "Step 5222, Loss Train: 2.8822, Loss Test: 4.2096, LR: 0.000026\n",
      "Step 5223, Loss Train: 2.9400, Loss Test: 2.5133, LR: 0.000026\n",
      "Step 5224, Loss Train: 2.9052, Loss Test: 3.5959, LR: 0.000026\n",
      "Step 5225, Loss Train: 3.2329, Loss Test: 4.0201, LR: 0.000026\n",
      "Step 5226, Loss Train: 2.8856, Loss Test: 4.1048, LR: 0.000026\n",
      "Step 5227, Loss Train: 3.0501, Loss Test: 4.3119, LR: 0.000026\n",
      "Step 5228, Loss Train: 2.9446, Loss Test: 3.3337, LR: 0.000026\n",
      "Step 5229, Loss Train: 2.9876, Loss Test: 3.8791, LR: 0.000026\n",
      "Step 5230, Loss Train: 2.9802, Loss Test: 3.6430, LR: 0.000026\n",
      "Step 5231, Loss Train: 2.9281, Loss Test: 4.1458, LR: 0.000026\n",
      "Step 5232, Loss Train: 2.7995, Loss Test: 4.2528, LR: 0.000026\n",
      "Step 5233, Loss Train: 2.9052, Loss Test: 3.8655, LR: 0.000026\n",
      "Step 5234, Loss Train: 2.7655, Loss Test: 3.0051, LR: 0.000026\n",
      "Step 5235, Loss Train: 2.9132, Loss Test: 2.5160, LR: 0.000026\n",
      "Step 5236, Loss Train: 2.7754, Loss Test: 4.0048, LR: 0.000026\n",
      "Step 5237, Loss Train: 3.0712, Loss Test: 3.6215, LR: 0.000026\n",
      "Step 5238, Loss Train: 2.7674, Loss Test: 3.8687, LR: 0.000026\n",
      "Step 5239, Loss Train: 3.0623, Loss Test: 4.0441, LR: 0.000026\n",
      "Step 5240, Loss Train: 2.9403, Loss Test: 3.5602, LR: 0.000026\n",
      "Step 5241, Loss Train: 2.8999, Loss Test: 3.6546, LR: 0.000026\n",
      "Step 5242, Loss Train: 2.8656, Loss Test: 3.8296, LR: 0.000026\n",
      "Step 5243, Loss Train: 2.8511, Loss Test: 4.0351, LR: 0.000026\n",
      "Step 5244, Loss Train: 2.7615, Loss Test: 3.7310, LR: 0.000026\n",
      "Step 5245, Loss Train: 2.8453, Loss Test: 3.9009, LR: 0.000026\n",
      "Step 5246, Loss Train: 2.7437, Loss Test: 3.7562, LR: 0.000026\n",
      "Step 5247, Loss Train: 2.9047, Loss Test: 3.6892, LR: 0.000026\n",
      "Step 5248, Loss Train: 2.8642, Loss Test: 3.8983, LR: 0.000026\n",
      "Step 5249, Loss Train: 2.8073, Loss Test: 4.2061, LR: 0.000026\n",
      "Step 5250, Loss Train: 3.0036, Loss Test: 2.1775, LR: 0.000026\n",
      "Step 5251, Loss Train: 2.9687, Loss Test: 3.7302, LR: 0.000026\n",
      "Step 5252, Loss Train: 2.6785, Loss Test: 3.6413, LR: 0.000026\n",
      "Step 5253, Loss Train: 3.0109, Loss Test: 3.6963, LR: 0.000026\n",
      "Step 5254, Loss Train: 2.7817, Loss Test: 4.1823, LR: 0.000026\n",
      "Step 5255, Loss Train: 3.0814, Loss Test: 3.6578, LR: 0.000026\n",
      "Step 5256, Loss Train: 3.0761, Loss Test: 4.1210, LR: 0.000026\n",
      "Step 5257, Loss Train: 2.8991, Loss Test: 3.2978, LR: 0.000026\n",
      "Step 5258, Loss Train: 2.8657, Loss Test: 4.4066, LR: 0.000026\n",
      "Step 5259, Loss Train: 2.9961, Loss Test: 4.2300, LR: 0.000026\n",
      "Step 5260, Loss Train: 2.9083, Loss Test: 3.7540, LR: 0.000026\n",
      "Step 5261, Loss Train: 2.9146, Loss Test: 3.6064, LR: 0.000026\n",
      "Step 5262, Loss Train: 2.9307, Loss Test: 3.5173, LR: 0.000026\n",
      "Step 5263, Loss Train: 2.8581, Loss Test: 3.9734, LR: 0.000026\n",
      "Step 5264, Loss Train: 2.7723, Loss Test: 4.3228, LR: 0.000026\n",
      "Step 5265, Loss Train: 2.9968, Loss Test: 4.0841, LR: 0.000026\n",
      "Step 5266, Loss Train: 2.9493, Loss Test: 2.5658, LR: 0.000026\n",
      "Step 5267, Loss Train: 2.9553, Loss Test: 3.6236, LR: 0.000026\n",
      "Step 5268, Loss Train: 3.0336, Loss Test: 2.7541, LR: 0.000026\n",
      "Step 5269, Loss Train: 2.7961, Loss Test: 4.0811, LR: 0.000026\n",
      "Step 5270, Loss Train: 2.8268, Loss Test: 3.4970, LR: 0.000026\n",
      "Step 5271, Loss Train: 3.0569, Loss Test: 3.8497, LR: 0.000026\n",
      "Step 5272, Loss Train: 2.7790, Loss Test: 3.8469, LR: 0.000026\n",
      "Step 5273, Loss Train: 2.9249, Loss Test: 3.7488, LR: 0.000026\n",
      "Step 5274, Loss Train: 2.9996, Loss Test: 3.3074, LR: 0.000026\n",
      "Step 5275, Loss Train: 2.8764, Loss Test: 3.3598, LR: 0.000026\n",
      "Step 5276, Loss Train: 2.9557, Loss Test: 3.6157, LR: 0.000026\n",
      "Step 5277, Loss Train: 2.9250, Loss Test: 3.4745, LR: 0.000026\n",
      "Step 5278, Loss Train: 2.8880, Loss Test: 1.0324, LR: 0.000026\n",
      "Step 5279, Loss Train: 2.9309, Loss Test: 2.4439, LR: 0.000026\n",
      "Step 5280, Loss Train: 2.8843, Loss Test: 4.4109, LR: 0.000026\n",
      "Step 5281, Loss Train: 2.9950, Loss Test: 2.5772, LR: 0.000026\n",
      "Step 5282, Loss Train: 2.9026, Loss Test: 4.9978, LR: 0.000026\n",
      "Step 5283, Loss Train: 2.8743, Loss Test: 3.6648, LR: 0.000026\n",
      "Step 5284, Loss Train: 2.8097, Loss Test: 3.9110, LR: 0.000026\n",
      "Step 5285, Loss Train: 2.8816, Loss Test: 3.9060, LR: 0.000026\n",
      "Step 5286, Loss Train: 3.0430, Loss Test: 3.7501, LR: 0.000026\n",
      "Step 5287, Loss Train: 3.0793, Loss Test: 3.9218, LR: 0.000026\n",
      "Step 5288, Loss Train: 2.9501, Loss Test: 3.6640, LR: 0.000026\n",
      "Step 5289, Loss Train: 2.9918, Loss Test: 3.6932, LR: 0.000026\n",
      "Step 5290, Loss Train: 2.7841, Loss Test: 3.3000, LR: 0.000026\n",
      "Step 5291, Loss Train: 3.0285, Loss Test: 4.0643, LR: 0.000026\n",
      "Step 5292, Loss Train: 2.7607, Loss Test: 2.2830, LR: 0.000026\n",
      "Step 5293, Loss Train: 2.7957, Loss Test: 4.2697, LR: 0.000026\n",
      "Step 5294, Loss Train: 2.8250, Loss Test: 2.4369, LR: 0.000026\n",
      "Step 5295, Loss Train: 2.9599, Loss Test: 3.8957, LR: 0.000026\n",
      "Step 5296, Loss Train: 2.9540, Loss Test: 3.4671, LR: 0.000026\n",
      "Step 5297, Loss Train: 2.7445, Loss Test: 3.9994, LR: 0.000026\n",
      "Step 5298, Loss Train: 2.9538, Loss Test: 3.9495, LR: 0.000026\n",
      "Step 5299, Loss Train: 2.9141, Loss Test: 4.0740, LR: 0.000026\n",
      "Step 5300, Loss Train: 2.9925, Loss Test: 2.2789, LR: 0.000026\n",
      "Step 5301, Loss Train: 2.7714, Loss Test: 2.8900, LR: 0.000026\n",
      "Step 5302, Loss Train: 2.8914, Loss Test: 3.0664, LR: 0.000026\n",
      "Step 5303, Loss Train: 2.8560, Loss Test: 3.0509, LR: 0.000026\n",
      "Step 5304, Loss Train: 2.6110, Loss Test: 4.2135, LR: 0.000026\n",
      "Step 5305, Loss Train: 2.8404, Loss Test: 4.0847, LR: 0.000026\n",
      "Step 5306, Loss Train: 2.9195, Loss Test: 4.3205, LR: 0.000026\n",
      "Step 5307, Loss Train: 2.8263, Loss Test: 3.8441, LR: 0.000026\n",
      "Step 5308, Loss Train: 2.9153, Loss Test: 4.0529, LR: 0.000026\n",
      "Step 5309, Loss Train: 3.0452, Loss Test: 3.7406, LR: 0.000026\n",
      "Step 5310, Loss Train: 3.0363, Loss Test: 3.7145, LR: 0.000026\n",
      "Step 5311, Loss Train: 2.7226, Loss Test: 3.4245, LR: 0.000026\n",
      "Step 5312, Loss Train: 2.9620, Loss Test: 3.8110, LR: 0.000026\n",
      "Step 5313, Loss Train: 2.8181, Loss Test: 3.7767, LR: 0.000026\n",
      "Step 5314, Loss Train: 2.9259, Loss Test: 3.9807, LR: 0.000026\n",
      "Step 5315, Loss Train: 3.0743, Loss Test: 3.7137, LR: 0.000026\n",
      "Step 5316, Loss Train: 2.9101, Loss Test: 3.7184, LR: 0.000026\n",
      "Step 5317, Loss Train: 2.9786, Loss Test: 3.8420, LR: 0.000026\n",
      "Step 5318, Loss Train: 2.7900, Loss Test: 3.6373, LR: 0.000026\n",
      "Step 5319, Loss Train: 3.0019, Loss Test: 3.1511, LR: 0.000026\n",
      "Step 5320, Loss Train: 2.7668, Loss Test: 4.2206, LR: 0.000026\n",
      "Step 5321, Loss Train: 2.9784, Loss Test: 4.1411, LR: 0.000026\n",
      "Step 5322, Loss Train: 3.0232, Loss Test: 2.4702, LR: 0.000026\n",
      "Step 5323, Loss Train: 3.0108, Loss Test: 2.2491, LR: 0.000026\n",
      "Step 5324, Loss Train: 2.9105, Loss Test: 3.1395, LR: 0.000026\n",
      "Step 5325, Loss Train: 2.8135, Loss Test: 3.7104, LR: 0.000026\n",
      "Step 5326, Loss Train: 2.6314, Loss Test: 3.8435, LR: 0.000026\n",
      "Step 5327, Loss Train: 3.0428, Loss Test: 3.3306, LR: 0.000026\n",
      "Step 5328, Loss Train: 2.5891, Loss Test: 2.2995, LR: 0.000026\n",
      "Step 5329, Loss Train: 2.8668, Loss Test: 3.1943, LR: 0.000026\n",
      "Step 5330, Loss Train: 2.9756, Loss Test: 3.4730, LR: 0.000026\n",
      "Step 5331, Loss Train: 2.6465, Loss Test: 2.2732, LR: 0.000026\n",
      "Step 5332, Loss Train: 2.8223, Loss Test: 4.0736, LR: 0.000026\n",
      "Step 5333, Loss Train: 2.6540, Loss Test: 3.6229, LR: 0.000026\n",
      "Step 5334, Loss Train: 2.7868, Loss Test: 4.0911, LR: 0.000026\n",
      "Step 5335, Loss Train: 2.9934, Loss Test: 4.1050, LR: 0.000026\n",
      "Step 5336, Loss Train: 2.5982, Loss Test: 3.7937, LR: 0.000026\n",
      "Step 5337, Loss Train: 2.9784, Loss Test: 2.5429, LR: 0.000026\n",
      "Step 5338, Loss Train: 2.8187, Loss Test: 3.9179, LR: 0.000026\n",
      "Step 5339, Loss Train: 2.7868, Loss Test: 3.7116, LR: 0.000026\n",
      "Step 5340, Loss Train: 2.9159, Loss Test: 4.2307, LR: 0.000026\n",
      "Step 5341, Loss Train: 3.0488, Loss Test: 2.2085, LR: 0.000026\n",
      "Step 5342, Loss Train: 3.0000, Loss Test: 2.3236, LR: 0.000026\n",
      "Step 5343, Loss Train: 2.8818, Loss Test: 3.4745, LR: 0.000026\n",
      "Step 5344, Loss Train: 2.9807, Loss Test: 3.3832, LR: 0.000026\n",
      "Step 5345, Loss Train: 3.0014, Loss Test: 2.7553, LR: 0.000026\n",
      "Step 5346, Loss Train: 2.7185, Loss Test: 4.0504, LR: 0.000026\n",
      "Step 5347, Loss Train: 3.0886, Loss Test: 3.6110, LR: 0.000026\n",
      "Step 5348, Loss Train: 2.8416, Loss Test: 3.9965, LR: 0.000026\n",
      "Step 5349, Loss Train: 2.9261, Loss Test: 2.9890, LR: 0.000026\n",
      "Step 5350, Loss Train: 3.0624, Loss Test: 2.5291, LR: 0.000026\n",
      "Step 5351, Loss Train: 2.9215, Loss Test: 3.4794, LR: 0.000026\n",
      "Step 5352, Loss Train: 2.7357, Loss Test: 3.9634, LR: 0.000026\n",
      "Step 5353, Loss Train: 2.8313, Loss Test: 3.5420, LR: 0.000026\n",
      "Step 5354, Loss Train: 2.9073, Loss Test: 2.9490, LR: 0.000026\n",
      "Step 5355, Loss Train: 2.9867, Loss Test: 3.9899, LR: 0.000026\n",
      "Step 5356, Loss Train: 2.8835, Loss Test: 3.5756, LR: 0.000026\n",
      "Step 5357, Loss Train: 2.9360, Loss Test: 3.5690, LR: 0.000026\n",
      "Step 5358, Loss Train: 2.8695, Loss Test: 4.6469, LR: 0.000026\n",
      "Step 5359, Loss Train: 2.8337, Loss Test: 3.5018, LR: 0.000026\n",
      "Step 5360, Loss Train: 2.9186, Loss Test: 3.6290, LR: 0.000026\n",
      "Step 5361, Loss Train: 2.9098, Loss Test: 3.3585, LR: 0.000026\n",
      "Step 5362, Loss Train: 3.0109, Loss Test: 4.2982, LR: 0.000026\n",
      "Step 5363, Loss Train: 3.0186, Loss Test: 4.0676, LR: 0.000026\n",
      "Step 5364, Loss Train: 3.0474, Loss Test: 3.7378, LR: 0.000026\n",
      "Step 5365, Loss Train: 2.6213, Loss Test: 3.3145, LR: 0.000026\n",
      "Step 5366, Loss Train: 2.7132, Loss Test: 4.2394, LR: 0.000026\n",
      "Step 5367, Loss Train: 2.8666, Loss Test: 4.5757, LR: 0.000026\n",
      "Step 5368, Loss Train: 2.8458, Loss Test: 4.0622, LR: 0.000026\n",
      "Step 5369, Loss Train: 2.9377, Loss Test: 4.0303, LR: 0.000026\n",
      "Step 5370, Loss Train: 2.8521, Loss Test: 3.5766, LR: 0.000026\n",
      "Step 5371, Loss Train: 3.1027, Loss Test: 3.3603, LR: 0.000026\n",
      "Step 5372, Loss Train: 2.7795, Loss Test: 3.6144, LR: 0.000026\n",
      "Step 5373, Loss Train: 2.8826, Loss Test: 2.7752, LR: 0.000026\n",
      "Step 5374, Loss Train: 2.9861, Loss Test: 4.3434, LR: 0.000026\n",
      "Step 5375, Loss Train: 2.8799, Loss Test: 3.2215, LR: 0.000026\n",
      "Step 5376, Loss Train: 2.9714, Loss Test: 4.0231, LR: 0.000026\n",
      "Step 5377, Loss Train: 3.0056, Loss Test: 3.4131, LR: 0.000026\n",
      "Step 5378, Loss Train: 2.9113, Loss Test: 3.9082, LR: 0.000026\n",
      "Step 5379, Loss Train: 2.9392, Loss Test: 3.8486, LR: 0.000026\n",
      "Step 5380, Loss Train: 2.8271, Loss Test: 3.8060, LR: 0.000026\n",
      "Step 5381, Loss Train: 2.7845, Loss Test: 3.4806, LR: 0.000026\n",
      "Step 5382, Loss Train: 2.9093, Loss Test: 4.0361, LR: 0.000026\n",
      "Step 5383, Loss Train: 2.7478, Loss Test: 4.7090, LR: 0.000026\n",
      "Step 5384, Loss Train: 2.6017, Loss Test: 3.7012, LR: 0.000026\n",
      "Step 5385, Loss Train: 2.8618, Loss Test: 3.9548, LR: 0.000026\n",
      "Step 5386, Loss Train: 2.9616, Loss Test: 3.9661, LR: 0.000026\n",
      "Step 5387, Loss Train: 2.9481, Loss Test: 3.9905, LR: 0.000026\n",
      "Step 5388, Loss Train: 2.7475, Loss Test: 4.4427, LR: 0.000026\n",
      "Step 5389, Loss Train: 2.8868, Loss Test: 4.2747, LR: 0.000026\n",
      "Step 5390, Loss Train: 2.9611, Loss Test: 4.0140, LR: 0.000026\n",
      "Step 5391, Loss Train: 2.7747, Loss Test: 3.8654, LR: 0.000026\n",
      "Step 5392, Loss Train: 3.1501, Loss Test: 3.7922, LR: 0.000026\n",
      "Step 5393, Loss Train: 3.0729, Loss Test: 3.7072, LR: 0.000026\n",
      "Step 5394, Loss Train: 2.9253, Loss Test: 4.2465, LR: 0.000026\n",
      "Step 5395, Loss Train: 2.7261, Loss Test: 3.8411, LR: 0.000026\n",
      "Step 5396, Loss Train: 2.7504, Loss Test: 3.2370, LR: 0.000026\n",
      "Step 5397, Loss Train: 2.9417, Loss Test: 3.7674, LR: 0.000026\n",
      "Step 5398, Loss Train: 2.9246, Loss Test: 3.6999, LR: 0.000026\n",
      "Step 5399, Loss Train: 2.8897, Loss Test: 3.8749, LR: 0.000026\n",
      "Step 5400, Loss Train: 3.0734, Loss Test: 4.0347, LR: 0.000026\n",
      "Step 5401, Loss Train: 2.9476, Loss Test: 2.2212, LR: 0.000026\n",
      "Step 5402, Loss Train: 2.9377, Loss Test: 3.7037, LR: 0.000026\n",
      "Step 5403, Loss Train: 2.9014, Loss Test: 3.7912, LR: 0.000026\n",
      "Step 5404, Loss Train: 3.0003, Loss Test: 4.1356, LR: 0.000026\n",
      "Step 5405, Loss Train: 2.8451, Loss Test: 3.6622, LR: 0.000026\n",
      "Step 5406, Loss Train: 2.8926, Loss Test: 3.9519, LR: 0.000026\n",
      "Step 5407, Loss Train: 3.0373, Loss Test: 3.5807, LR: 0.000026\n",
      "Step 5408, Loss Train: 3.0097, Loss Test: 3.3252, LR: 0.000026\n",
      "Step 5409, Loss Train: 2.9619, Loss Test: 3.5473, LR: 0.000026\n",
      "Step 5410, Loss Train: 2.9230, Loss Test: 3.4918, LR: 0.000026\n",
      "Step 5411, Loss Train: 3.0017, Loss Test: 2.9646, LR: 0.000026\n",
      "Step 5412, Loss Train: 3.0092, Loss Test: 3.2519, LR: 0.000026\n",
      "Step 5413, Loss Train: 2.7140, Loss Test: 3.9421, LR: 0.000026\n",
      "Step 5414, Loss Train: 3.0155, Loss Test: 4.3296, LR: 0.000026\n",
      "Step 5415, Loss Train: 2.6973, Loss Test: 3.6942, LR: 0.000026\n",
      "Step 5416, Loss Train: 2.9503, Loss Test: 4.2304, LR: 0.000026\n",
      "Step 5417, Loss Train: 2.9379, Loss Test: 4.0295, LR: 0.000026\n",
      "Step 5418, Loss Train: 2.7558, Loss Test: 3.9346, LR: 0.000026\n",
      "Step 5419, Loss Train: 2.9747, Loss Test: 4.0590, LR: 0.000026\n",
      "Step 5420, Loss Train: 2.9329, Loss Test: 0.9975, LR: 0.000026\n",
      "Step 5421, Loss Train: 2.9168, Loss Test: 4.7566, LR: 0.000026\n",
      "Step 5422, Loss Train: 2.9233, Loss Test: 3.2153, LR: 0.000026\n",
      "Step 5423, Loss Train: 2.7991, Loss Test: 4.0024, LR: 0.000026\n",
      "Step 5424, Loss Train: 2.8855, Loss Test: 3.8054, LR: 0.000026\n",
      "Step 5425, Loss Train: 2.9101, Loss Test: 3.6359, LR: 0.000026\n",
      "Step 5426, Loss Train: 2.7743, Loss Test: 4.3536, LR: 0.000026\n",
      "Step 5427, Loss Train: 2.8697, Loss Test: 3.9781, LR: 0.000026\n",
      "Step 5428, Loss Train: 2.8597, Loss Test: 3.7734, LR: 0.000026\n",
      "Step 5429, Loss Train: 2.8949, Loss Test: 2.7685, LR: 0.000026\n",
      "Step 5430, Loss Train: 3.0466, Loss Test: 4.1253, LR: 0.000026\n",
      "Step 5431, Loss Train: 2.7626, Loss Test: 4.3261, LR: 0.000026\n",
      "Step 5432, Loss Train: 3.0746, Loss Test: 3.8270, LR: 0.000026\n",
      "Step 5433, Loss Train: 2.8097, Loss Test: 3.2179, LR: 0.000026\n",
      "Step 5434, Loss Train: 2.7800, Loss Test: 4.1297, LR: 0.000026\n",
      "Step 5435, Loss Train: 2.9238, Loss Test: 2.0869, LR: 0.000026\n",
      "Step 5436, Loss Train: 2.9612, Loss Test: 3.9275, LR: 0.000026\n",
      "Step 5437, Loss Train: 2.9207, Loss Test: 4.1893, LR: 0.000026\n",
      "Step 5438, Loss Train: 2.9115, Loss Test: 3.4834, LR: 0.000026\n",
      "Step 5439, Loss Train: 2.9578, Loss Test: 3.7106, LR: 0.000026\n",
      "Step 5440, Loss Train: 2.8374, Loss Test: 3.7244, LR: 0.000026\n",
      "Step 5441, Loss Train: 2.9574, Loss Test: 4.1338, LR: 0.000026\n",
      "Step 5442, Loss Train: 2.9039, Loss Test: 4.0555, LR: 0.000026\n",
      "Step 5443, Loss Train: 2.7771, Loss Test: 3.5908, LR: 0.000026\n",
      "Step 5444, Loss Train: 2.9083, Loss Test: 3.8309, LR: 0.000026\n",
      "Step 5445, Loss Train: 2.8207, Loss Test: 3.8784, LR: 0.000026\n",
      "Step 5446, Loss Train: 2.8987, Loss Test: 4.0320, LR: 0.000026\n",
      "Step 5447, Loss Train: 2.9824, Loss Test: 3.6075, LR: 0.000026\n",
      "Step 5448, Loss Train: 2.9722, Loss Test: 4.0442, LR: 0.000026\n",
      "Step 5449, Loss Train: 2.8427, Loss Test: 3.7354, LR: 0.000026\n",
      "Step 5450, Loss Train: 2.8241, Loss Test: 3.8112, LR: 0.000026\n",
      "Step 5451, Loss Train: 2.9278, Loss Test: 3.9419, LR: 0.000026\n",
      "Step 5452, Loss Train: 2.8480, Loss Test: 2.9509, LR: 0.000026\n",
      "Step 5453, Loss Train: 3.0037, Loss Test: 4.0457, LR: 0.000026\n",
      "Step 5454, Loss Train: 2.8456, Loss Test: 3.8678, LR: 0.000026\n",
      "Step 5455, Loss Train: 2.8778, Loss Test: 3.0042, LR: 0.000026\n",
      "Step 5456, Loss Train: 2.7990, Loss Test: 4.3557, LR: 0.000026\n",
      "Step 5457, Loss Train: 2.7930, Loss Test: 4.2566, LR: 0.000026\n",
      "Step 5458, Loss Train: 2.7462, Loss Test: 3.8459, LR: 0.000026\n",
      "Step 5459, Loss Train: 3.0256, Loss Test: 3.1132, LR: 0.000026\n",
      "Step 5460, Loss Train: 2.8067, Loss Test: 3.5880, LR: 0.000026\n",
      "Step 5461, Loss Train: 3.0205, Loss Test: 4.1376, LR: 0.000026\n",
      "Step 5462, Loss Train: 2.8446, Loss Test: 4.0717, LR: 0.000026\n",
      "Step 5463, Loss Train: 2.7644, Loss Test: 4.3297, LR: 0.000026\n",
      "Step 5464, Loss Train: 2.8878, Loss Test: 3.9506, LR: 0.000026\n",
      "Step 5465, Loss Train: 2.9698, Loss Test: 4.2937, LR: 0.000026\n",
      "Step 5466, Loss Train: 2.9366, Loss Test: 3.8824, LR: 0.000026\n",
      "Step 5467, Loss Train: 3.0372, Loss Test: 3.4676, LR: 0.000026\n",
      "Step 5468, Loss Train: 3.0421, Loss Test: 3.8811, LR: 0.000026\n",
      "Step 5469, Loss Train: 2.7500, Loss Test: 3.9530, LR: 0.000026\n",
      "Step 5470, Loss Train: 2.6668, Loss Test: 3.9655, LR: 0.000026\n",
      "Step 5471, Loss Train: 2.9538, Loss Test: 4.2676, LR: 0.000026\n",
      "Step 5472, Loss Train: 2.9067, Loss Test: 3.2307, LR: 0.000026\n",
      "Step 5473, Loss Train: 2.7822, Loss Test: 3.7002, LR: 0.000026\n",
      "Step 5474, Loss Train: 2.7844, Loss Test: 3.5668, LR: 0.000026\n",
      "Step 5475, Loss Train: 3.0478, Loss Test: 2.3175, LR: 0.000026\n",
      "Step 5476, Loss Train: 2.8393, Loss Test: 4.2514, LR: 0.000026\n",
      "Step 5477, Loss Train: 2.8084, Loss Test: 4.0028, LR: 0.000026\n",
      "Step 5478, Loss Train: 2.8963, Loss Test: 3.8050, LR: 0.000026\n",
      "Step 5479, Loss Train: 2.9273, Loss Test: 3.2220, LR: 0.000026\n",
      "Step 5480, Loss Train: 2.8541, Loss Test: 3.7768, LR: 0.000026\n",
      "Step 5481, Loss Train: 2.8402, Loss Test: 3.2058, LR: 0.000026\n",
      "Step 5482, Loss Train: 2.9926, Loss Test: 3.5384, LR: 0.000026\n",
      "Step 5483, Loss Train: 2.9758, Loss Test: 3.9109, LR: 0.000026\n",
      "Step 5484, Loss Train: 2.8119, Loss Test: 4.1581, LR: 0.000026\n",
      "Step 5485, Loss Train: 2.8438, Loss Test: 4.2248, LR: 0.000026\n",
      "Step 5486, Loss Train: 2.7480, Loss Test: 3.5758, LR: 0.000026\n",
      "Step 5487, Loss Train: 2.9710, Loss Test: 4.0945, LR: 0.000026\n",
      "Step 5488, Loss Train: 2.9087, Loss Test: 3.7192, LR: 0.000026\n",
      "Step 5489, Loss Train: 2.9269, Loss Test: 4.1243, LR: 0.000026\n",
      "Step 5490, Loss Train: 3.0291, Loss Test: 4.5375, LR: 0.000026\n",
      "Step 5491, Loss Train: 2.8670, Loss Test: 3.9036, LR: 0.000026\n",
      "Step 5492, Loss Train: 2.8582, Loss Test: 3.9305, LR: 0.000026\n",
      "Step 5493, Loss Train: 2.9198, Loss Test: 1.9634, LR: 0.000026\n",
      "Step 5494, Loss Train: 3.0120, Loss Test: 3.8380, LR: 0.000026\n",
      "Step 5495, Loss Train: 3.0082, Loss Test: 3.3956, LR: 0.000026\n",
      "Step 5496, Loss Train: 2.7818, Loss Test: 4.0823, LR: 0.000026\n",
      "Step 5497, Loss Train: 2.7894, Loss Test: 3.8616, LR: 0.000026\n",
      "Step 5498, Loss Train: 2.9324, Loss Test: 3.6959, LR: 0.000026\n",
      "Step 5499, Loss Train: 2.8969, Loss Test: 4.0815, LR: 0.000026\n",
      "Step 5500, Loss Train: 3.0663, Loss Test: 3.8309, LR: 0.000026\n",
      "Step 5501, Loss Train: 2.6766, Loss Test: 3.6435, LR: 0.000026\n",
      "Step 5502, Loss Train: 3.0172, Loss Test: 4.0718, LR: 0.000026\n",
      "Step 5503, Loss Train: 2.7931, Loss Test: 4.0933, LR: 0.000026\n",
      "Step 5504, Loss Train: 2.9619, Loss Test: 3.6978, LR: 0.000026\n",
      "Step 5505, Loss Train: 2.9092, Loss Test: 3.0133, LR: 0.000026\n",
      "Step 5506, Loss Train: 2.9768, Loss Test: 3.9362, LR: 0.000026\n",
      "Step 5507, Loss Train: 2.6681, Loss Test: 2.9769, LR: 0.000026\n",
      "Step 5508, Loss Train: 2.9213, Loss Test: 4.0034, LR: 0.000026\n",
      "Step 5509, Loss Train: 2.7770, Loss Test: 3.8697, LR: 0.000026\n",
      "Step 5510, Loss Train: 2.9143, Loss Test: 4.6994, LR: 0.000026\n",
      "Step 5511, Loss Train: 2.6400, Loss Test: 3.7762, LR: 0.000026\n",
      "Step 5512, Loss Train: 3.1722, Loss Test: 3.2818, LR: 0.000026\n",
      "Step 5513, Loss Train: 3.1165, Loss Test: 4.4538, LR: 0.000026\n",
      "Step 5514, Loss Train: 2.8567, Loss Test: 2.7840, LR: 0.000026\n",
      "Step 5515, Loss Train: 2.9566, Loss Test: 4.0464, LR: 0.000026\n",
      "Step 5516, Loss Train: 2.8165, Loss Test: 3.6140, LR: 0.000026\n",
      "Step 5517, Loss Train: 2.8460, Loss Test: 2.4985, LR: 0.000026\n",
      "Step 5518, Loss Train: 2.8513, Loss Test: 3.9962, LR: 0.000026\n",
      "Step 5519, Loss Train: 3.0051, Loss Test: 4.2052, LR: 0.000026\n",
      "Step 5520, Loss Train: 2.8575, Loss Test: 4.0653, LR: 0.000026\n",
      "Step 5521, Loss Train: 2.8954, Loss Test: 2.7197, LR: 0.000026\n",
      "Step 5522, Loss Train: 3.0350, Loss Test: 4.1254, LR: 0.000026\n",
      "Step 5523, Loss Train: 2.9636, Loss Test: 2.9840, LR: 0.000026\n",
      "Step 5524, Loss Train: 3.0657, Loss Test: 4.2665, LR: 0.000026\n",
      "Step 5525, Loss Train: 2.8362, Loss Test: 3.7648, LR: 0.000026\n",
      "Step 5526, Loss Train: 2.9811, Loss Test: 4.0743, LR: 0.000026\n",
      "Step 5527, Loss Train: 2.8877, Loss Test: 3.9688, LR: 0.000026\n",
      "Step 5528, Loss Train: 2.7878, Loss Test: 4.6481, LR: 0.000026\n",
      "Step 5529, Loss Train: 2.9486, Loss Test: 2.6767, LR: 0.000026\n",
      "Step 5530, Loss Train: 2.7525, Loss Test: 4.1863, LR: 0.000026\n",
      "Step 5531, Loss Train: 2.8768, Loss Test: 4.0784, LR: 0.000026\n",
      "Step 5532, Loss Train: 3.0164, Loss Test: 3.4952, LR: 0.000026\n",
      "Step 5533, Loss Train: 2.9513, Loss Test: 4.3833, LR: 0.000026\n",
      "Step 5534, Loss Train: 2.8791, Loss Test: 4.0830, LR: 0.000026\n",
      "Step 5535, Loss Train: 2.8175, Loss Test: 4.3433, LR: 0.000026\n",
      "Step 5536, Loss Train: 3.0618, Loss Test: 3.8318, LR: 0.000026\n",
      "Step 5537, Loss Train: 3.0619, Loss Test: 4.4547, LR: 0.000026\n",
      "Step 5538, Loss Train: 2.7771, Loss Test: 4.3681, LR: 0.000026\n",
      "Step 5539, Loss Train: 2.7932, Loss Test: 3.3114, LR: 0.000026\n",
      "Step 5540, Loss Train: 2.7363, Loss Test: 3.3292, LR: 0.000026\n",
      "Step 5541, Loss Train: 2.8330, Loss Test: 4.3158, LR: 0.000026\n",
      "Step 5542, Loss Train: 2.7376, Loss Test: 3.3369, LR: 0.000026\n",
      "Step 5543, Loss Train: 2.9123, Loss Test: 2.6010, LR: 0.000026\n",
      "Step 5544, Loss Train: 2.7856, Loss Test: 3.6299, LR: 0.000026\n",
      "Step 5545, Loss Train: 2.8953, Loss Test: 4.3148, LR: 0.000026\n",
      "Step 5546, Loss Train: 3.0491, Loss Test: 3.1546, LR: 0.000026\n",
      "Step 5547, Loss Train: 2.9302, Loss Test: 4.0960, LR: 0.000026\n",
      "Step 5548, Loss Train: 2.7888, Loss Test: 3.1198, LR: 0.000026\n",
      "Step 5549, Loss Train: 2.8948, Loss Test: 3.8994, LR: 0.000026\n",
      "Step 5550, Loss Train: 2.9695, Loss Test: 4.1210, LR: 0.000026\n",
      "Step 5551, Loss Train: 2.9285, Loss Test: 4.8478, LR: 0.000026\n",
      "Step 5552, Loss Train: 2.8814, Loss Test: 3.7636, LR: 0.000026\n",
      "Step 5553, Loss Train: 2.8031, Loss Test: 3.9036, LR: 0.000026\n",
      "Step 5554, Loss Train: 3.0460, Loss Test: 2.5417, LR: 0.000026\n",
      "Step 5555, Loss Train: 2.8119, Loss Test: 4.1166, LR: 0.000026\n",
      "Step 5556, Loss Train: 2.9007, Loss Test: 3.2465, LR: 0.000026\n",
      "Step 5557, Loss Train: 2.8344, Loss Test: 4.0232, LR: 0.000026\n",
      "Step 5558, Loss Train: 2.8858, Loss Test: 4.1073, LR: 0.000026\n",
      "Step 5559, Loss Train: 2.8734, Loss Test: 4.1527, LR: 0.000026\n",
      "Step 5560, Loss Train: 2.8124, Loss Test: 3.8233, LR: 0.000026\n",
      "Step 5561, Loss Train: 2.9431, Loss Test: 4.0442, LR: 0.000026\n",
      "Step 5562, Loss Train: 2.6965, Loss Test: 3.4731, LR: 0.000026\n",
      "Step 5563, Loss Train: 2.9583, Loss Test: 3.8918, LR: 0.000026\n",
      "Step 5564, Loss Train: 2.6890, Loss Test: 2.8737, LR: 0.000026\n",
      "Step 5565, Loss Train: 2.8650, Loss Test: 3.9607, LR: 0.000026\n",
      "Step 5566, Loss Train: 2.8849, Loss Test: 3.9256, LR: 0.000026\n",
      "Step 5567, Loss Train: 2.7027, Loss Test: 3.8351, LR: 0.000026\n",
      "Step 5568, Loss Train: 2.6809, Loss Test: 3.9741, LR: 0.000026\n",
      "Step 5569, Loss Train: 2.8639, Loss Test: 3.3565, LR: 0.000026\n",
      "Step 5570, Loss Train: 3.0176, Loss Test: 3.4569, LR: 0.000026\n",
      "Step 5571, Loss Train: 2.8375, Loss Test: 3.3949, LR: 0.000026\n",
      "Step 5572, Loss Train: 2.8833, Loss Test: 4.1276, LR: 0.000026\n",
      "Step 5573, Loss Train: 2.8841, Loss Test: 3.1429, LR: 0.000026\n",
      "Step 5574, Loss Train: 2.9603, Loss Test: 3.8876, LR: 0.000026\n",
      "Step 5575, Loss Train: 2.6178, Loss Test: 4.2696, LR: 0.000026\n",
      "Step 5576, Loss Train: 2.9448, Loss Test: 3.6033, LR: 0.000026\n",
      "Step 5577, Loss Train: 2.9191, Loss Test: 4.0616, LR: 0.000026\n",
      "Step 5578, Loss Train: 3.0097, Loss Test: 3.8404, LR: 0.000026\n",
      "Step 5579, Loss Train: 2.8138, Loss Test: 3.3755, LR: 0.000026\n",
      "Step 5580, Loss Train: 2.6781, Loss Test: 3.7443, LR: 0.000026\n",
      "Step 5581, Loss Train: 3.0001, Loss Test: 3.6365, LR: 0.000026\n",
      "Step 5582, Loss Train: 3.0640, Loss Test: 4.5298, LR: 0.000026\n",
      "Step 5583, Loss Train: 2.7566, Loss Test: 3.7005, LR: 0.000026\n",
      "Step 5584, Loss Train: 2.9485, Loss Test: 4.1123, LR: 0.000026\n",
      "Step 5585, Loss Train: 3.0584, Loss Test: 4.1053, LR: 0.000026\n",
      "Step 5586, Loss Train: 3.0600, Loss Test: 3.7479, LR: 0.000026\n",
      "Step 5587, Loss Train: 3.0375, Loss Test: 3.9861, LR: 0.000026\n",
      "Step 5588, Loss Train: 2.9419, Loss Test: 2.4795, LR: 0.000026\n",
      "Step 5589, Loss Train: 2.9520, Loss Test: 4.0860, LR: 0.000026\n",
      "Step 5590, Loss Train: 2.9877, Loss Test: 3.4785, LR: 0.000026\n",
      "Step 5591, Loss Train: 2.7907, Loss Test: 3.4380, LR: 0.000026\n",
      "Step 5592, Loss Train: 2.8657, Loss Test: 4.0362, LR: 0.000026\n",
      "Step 5593, Loss Train: 2.8988, Loss Test: 4.0412, LR: 0.000026\n",
      "Step 5594, Loss Train: 2.9160, Loss Test: 3.5372, LR: 0.000026\n",
      "Step 5595, Loss Train: 3.0076, Loss Test: 3.9939, LR: 0.000026\n",
      "Step 5596, Loss Train: 2.9719, Loss Test: 0.9036, LR: 0.000026\n",
      "Step 5597, Loss Train: 2.9638, Loss Test: 2.8297, LR: 0.000026\n",
      "Step 5598, Loss Train: 2.8234, Loss Test: 3.9977, LR: 0.000026\n",
      "Step 5599, Loss Train: 2.8467, Loss Test: 4.3065, LR: 0.000026\n",
      "Step 5600, Loss Train: 2.8103, Loss Test: 3.3656, LR: 0.000026\n",
      "Step 5601, Loss Train: 2.8306, Loss Test: 4.0652, LR: 0.000026\n",
      "Step 5602, Loss Train: 2.9460, Loss Test: 3.9867, LR: 0.000026\n",
      "Step 5603, Loss Train: 2.9754, Loss Test: 3.9758, LR: 0.000026\n",
      "Step 5604, Loss Train: 2.8489, Loss Test: 4.2728, LR: 0.000026\n",
      "Step 5605, Loss Train: 2.8397, Loss Test: 4.2462, LR: 0.000026\n",
      "Step 5606, Loss Train: 2.9112, Loss Test: 3.2088, LR: 0.000026\n",
      "Step 5607, Loss Train: 2.7350, Loss Test: 2.9704, LR: 0.000026\n",
      "Step 5608, Loss Train: 2.9899, Loss Test: 3.0059, LR: 0.000026\n",
      "Step 5609, Loss Train: 2.8552, Loss Test: 3.5147, LR: 0.000026\n",
      "Step 5610, Loss Train: 2.8699, Loss Test: 4.0096, LR: 0.000026\n",
      "Step 5611, Loss Train: 2.8631, Loss Test: 4.2273, LR: 0.000026\n",
      "Step 5612, Loss Train: 2.9015, Loss Test: 3.3854, LR: 0.000026\n",
      "Step 5613, Loss Train: 2.8113, Loss Test: 3.9000, LR: 0.000026\n",
      "Step 5614, Loss Train: 2.7383, Loss Test: 2.9366, LR: 0.000026\n",
      "Step 5615, Loss Train: 2.7833, Loss Test: 3.5254, LR: 0.000026\n",
      "Step 5616, Loss Train: 2.8060, Loss Test: 4.1295, LR: 0.000026\n",
      "Step 5617, Loss Train: 2.7930, Loss Test: 3.3887, LR: 0.000026\n",
      "Step 5618, Loss Train: 2.8980, Loss Test: 2.7338, LR: 0.000026\n",
      "Step 5619, Loss Train: 3.0096, Loss Test: 4.0514, LR: 0.000026\n",
      "Step 5620, Loss Train: 2.8309, Loss Test: 3.4679, LR: 0.000026\n",
      "Step 5621, Loss Train: 2.8720, Loss Test: 4.0736, LR: 0.000026\n",
      "Step 5622, Loss Train: 3.0390, Loss Test: 3.9224, LR: 0.000026\n",
      "Step 5623, Loss Train: 2.8074, Loss Test: 3.5979, LR: 0.000026\n",
      "Step 5624, Loss Train: 3.0697, Loss Test: 3.0643, LR: 0.000026\n",
      "Step 5625, Loss Train: 3.0966, Loss Test: 4.0416, LR: 0.000026\n",
      "Step 5626, Loss Train: 2.8797, Loss Test: 4.0638, LR: 0.000026\n",
      "Step 5627, Loss Train: 2.7865, Loss Test: 3.5222, LR: 0.000026\n",
      "Step 5628, Loss Train: 2.9805, Loss Test: 3.9844, LR: 0.000026\n",
      "Step 5629, Loss Train: 2.9581, Loss Test: 3.4636, LR: 0.000026\n",
      "Step 5630, Loss Train: 2.8645, Loss Test: 4.0677, LR: 0.000026\n",
      "Step 5631, Loss Train: 3.0665, Loss Test: 3.7032, LR: 0.000026\n",
      "Step 5632, Loss Train: 3.0464, Loss Test: 3.8533, LR: 0.000026\n",
      "Step 5633, Loss Train: 2.9519, Loss Test: 4.6941, LR: 0.000026\n",
      "Step 5634, Loss Train: 2.7441, Loss Test: 4.3894, LR: 0.000026\n",
      "Step 5635, Loss Train: 2.7731, Loss Test: 2.7399, LR: 0.000026\n",
      "Step 5636, Loss Train: 2.9780, Loss Test: 3.9470, LR: 0.000026\n",
      "Step 5637, Loss Train: 2.9367, Loss Test: 4.1920, LR: 0.000026\n",
      "Step 5638, Loss Train: 2.8540, Loss Test: 3.8536, LR: 0.000026\n",
      "Step 5639, Loss Train: 2.7591, Loss Test: 3.4349, LR: 0.000026\n",
      "Step 5640, Loss Train: 2.9272, Loss Test: 4.1924, LR: 0.000026\n",
      "Step 5641, Loss Train: 2.9165, Loss Test: 3.4190, LR: 0.000026\n",
      "Step 5642, Loss Train: 2.8266, Loss Test: 3.9542, LR: 0.000026\n",
      "Step 5643, Loss Train: 2.8986, Loss Test: 3.1905, LR: 0.000026\n",
      "Step 5644, Loss Train: 3.0380, Loss Test: 2.6868, LR: 0.000026\n",
      "Step 5645, Loss Train: 2.6533, Loss Test: 4.1229, LR: 0.000026\n",
      "Step 5646, Loss Train: 3.0223, Loss Test: 3.6001, LR: 0.000026\n",
      "Step 5647, Loss Train: 2.8928, Loss Test: 4.1771, LR: 0.000026\n",
      "Step 5648, Loss Train: 2.8135, Loss Test: 3.4190, LR: 0.000026\n",
      "Step 5649, Loss Train: 2.8467, Loss Test: 3.7640, LR: 0.000026\n",
      "Step 5650, Loss Train: 2.8065, Loss Test: 3.6252, LR: 0.000026\n",
      "Step 5651, Loss Train: 2.9332, Loss Test: 3.6902, LR: 0.000026\n",
      "Step 5652, Loss Train: 2.9833, Loss Test: 2.8046, LR: 0.000026\n",
      "Step 5653, Loss Train: 2.9282, Loss Test: 3.9366, LR: 0.000026\n",
      "Step 5654, Loss Train: 2.7335, Loss Test: 4.0033, LR: 0.000026\n",
      "Step 5655, Loss Train: 2.9587, Loss Test: 4.1562, LR: 0.000026\n",
      "Step 5656, Loss Train: 2.7443, Loss Test: 3.5385, LR: 0.000026\n",
      "Step 5657, Loss Train: 2.8364, Loss Test: 3.2803, LR: 0.000026\n",
      "Step 5658, Loss Train: 2.6137, Loss Test: 4.0234, LR: 0.000026\n",
      "Step 5659, Loss Train: 2.8262, Loss Test: 4.1392, LR: 0.000026\n",
      "Step 5660, Loss Train: 2.9583, Loss Test: 4.3470, LR: 0.000026\n",
      "Step 5661, Loss Train: 2.9454, Loss Test: 3.1353, LR: 0.000026\n",
      "Step 5662, Loss Train: 2.7557, Loss Test: 4.3355, LR: 0.000026\n",
      "Step 5663, Loss Train: 3.0643, Loss Test: 4.3101, LR: 0.000026\n",
      "Step 5664, Loss Train: 2.9779, Loss Test: 3.8284, LR: 0.000026\n",
      "Step 5665, Loss Train: 2.6668, Loss Test: 2.1004, LR: 0.000026\n",
      "Step 5666, Loss Train: 2.9014, Loss Test: 4.3752, LR: 0.000026\n",
      "Step 5667, Loss Train: 2.9322, Loss Test: 1.5862, LR: 0.000026\n",
      "Step 5668, Loss Train: 2.9931, Loss Test: 3.6452, LR: 0.000026\n",
      "Step 5669, Loss Train: 2.9752, Loss Test: 4.1286, LR: 0.000026\n",
      "Step 5670, Loss Train: 3.0717, Loss Test: 3.9037, LR: 0.000026\n",
      "Step 5671, Loss Train: 2.9497, Loss Test: 3.2136, LR: 0.000026\n",
      "Step 5672, Loss Train: 2.9884, Loss Test: 4.6046, LR: 0.000026\n",
      "Step 5673, Loss Train: 2.9293, Loss Test: 3.8868, LR: 0.000026\n",
      "Step 5674, Loss Train: 2.9214, Loss Test: 3.8125, LR: 0.000026\n",
      "Step 5675, Loss Train: 2.8838, Loss Test: 3.8945, LR: 0.000026\n",
      "Step 5676, Loss Train: 2.8014, Loss Test: 3.6483, LR: 0.000026\n",
      "Step 5677, Loss Train: 2.8604, Loss Test: 3.8848, LR: 0.000026\n",
      "Step 5678, Loss Train: 2.8187, Loss Test: 3.9566, LR: 0.000026\n",
      "Step 5679, Loss Train: 2.9809, Loss Test: 2.9548, LR: 0.000026\n",
      "Step 5680, Loss Train: 2.9430, Loss Test: 2.7244, LR: 0.000026\n",
      "Step 5681, Loss Train: 3.0362, Loss Test: 3.7609, LR: 0.000026\n",
      "Step 5682, Loss Train: 2.9350, Loss Test: 3.6039, LR: 0.000026\n",
      "Step 5683, Loss Train: 2.9635, Loss Test: 2.5467, LR: 0.000026\n",
      "Step 5684, Loss Train: 2.8735, Loss Test: 3.9327, LR: 0.000026\n",
      "Step 5685, Loss Train: 2.7478, Loss Test: 2.5339, LR: 0.000026\n",
      "Step 5686, Loss Train: 2.9557, Loss Test: 4.1110, LR: 0.000026\n",
      "Step 5687, Loss Train: 2.8828, Loss Test: 3.9290, LR: 0.000026\n",
      "Step 5688, Loss Train: 2.8729, Loss Test: 3.8250, LR: 0.000026\n",
      "Step 5689, Loss Train: 3.0458, Loss Test: 3.7771, LR: 0.000026\n",
      "Step 5690, Loss Train: 2.7994, Loss Test: 4.3110, LR: 0.000026\n",
      "Step 5691, Loss Train: 2.9376, Loss Test: 4.2722, LR: 0.000026\n",
      "Step 5692, Loss Train: 2.7714, Loss Test: 3.8767, LR: 0.000026\n",
      "Step 5693, Loss Train: 2.9934, Loss Test: 4.1400, LR: 0.000026\n",
      "Step 5694, Loss Train: 2.8962, Loss Test: 4.3072, LR: 0.000026\n",
      "Step 5695, Loss Train: 2.8534, Loss Test: 2.0935, LR: 0.000026\n",
      "Step 5696, Loss Train: 2.9436, Loss Test: 4.7824, LR: 0.000026\n",
      "Step 5697, Loss Train: 2.7303, Loss Test: 3.5697, LR: 0.000026\n",
      "Step 5698, Loss Train: 2.9455, Loss Test: 3.3897, LR: 0.000026\n",
      "Step 5699, Loss Train: 2.9294, Loss Test: 3.8180, LR: 0.000026\n",
      "Step 5700, Loss Train: 2.8400, Loss Test: 4.3064, LR: 0.000026\n",
      "Step 5701, Loss Train: 3.0006, Loss Test: 4.3923, LR: 0.000026\n",
      "Step 5702, Loss Train: 2.7954, Loss Test: 2.7500, LR: 0.000026\n",
      "Step 5703, Loss Train: 3.1587, Loss Test: 3.8551, LR: 0.000026\n",
      "Step 5704, Loss Train: 2.9498, Loss Test: 3.7241, LR: 0.000026\n",
      "Step 5705, Loss Train: 2.9262, Loss Test: 3.9085, LR: 0.000026\n",
      "Step 5706, Loss Train: 2.8453, Loss Test: 3.9976, LR: 0.000026\n",
      "Step 5707, Loss Train: 2.9834, Loss Test: 2.9045, LR: 0.000026\n",
      "Step 5708, Loss Train: 2.8326, Loss Test: 4.4953, LR: 0.000026\n",
      "Step 5709, Loss Train: 2.7413, Loss Test: 3.4821, LR: 0.000026\n",
      "Step 5710, Loss Train: 2.8625, Loss Test: 4.5528, LR: 0.000026\n",
      "Step 5711, Loss Train: 2.8948, Loss Test: 4.0316, LR: 0.000026\n",
      "Step 5712, Loss Train: 2.9707, Loss Test: 3.9400, LR: 0.000026\n",
      "Step 5713, Loss Train: 2.7685, Loss Test: 4.2751, LR: 0.000026\n",
      "Step 5714, Loss Train: 2.8475, Loss Test: 3.2478, LR: 0.000026\n",
      "Step 5715, Loss Train: 2.7228, Loss Test: 3.9203, LR: 0.000026\n",
      "Step 5716, Loss Train: 3.0147, Loss Test: 3.5320, LR: 0.000026\n",
      "Step 5717, Loss Train: 2.9758, Loss Test: 4.3465, LR: 0.000026\n",
      "Step 5718, Loss Train: 2.8187, Loss Test: 3.4964, LR: 0.000026\n",
      "Step 5719, Loss Train: 3.0913, Loss Test: 4.1434, LR: 0.000026\n",
      "Step 5720, Loss Train: 3.0391, Loss Test: 4.0122, LR: 0.000026\n",
      "Step 5721, Loss Train: 2.8974, Loss Test: 4.0104, LR: 0.000026\n",
      "Step 5722, Loss Train: 2.8732, Loss Test: 3.7351, LR: 0.000026\n",
      "Step 5723, Loss Train: 2.8982, Loss Test: 3.3097, LR: 0.000026\n",
      "Step 5724, Loss Train: 2.7347, Loss Test: 4.1087, LR: 0.000026\n",
      "Step 5725, Loss Train: 3.0978, Loss Test: 4.5352, LR: 0.000026\n",
      "Step 5726, Loss Train: 2.7641, Loss Test: 4.2476, LR: 0.000026\n",
      "Step 5727, Loss Train: 2.7878, Loss Test: 4.1969, LR: 0.000026\n",
      "Step 5728, Loss Train: 2.8846, Loss Test: 4.0339, LR: 0.000026\n",
      "Step 5729, Loss Train: 2.9938, Loss Test: 3.4691, LR: 0.000026\n",
      "Step 5730, Loss Train: 2.9741, Loss Test: 3.6506, LR: 0.000026\n",
      "Step 5731, Loss Train: 2.9200, Loss Test: 2.1401, LR: 0.000026\n",
      "Step 5732, Loss Train: 2.9106, Loss Test: 3.7941, LR: 0.000026\n",
      "Step 5733, Loss Train: 2.8442, Loss Test: 4.0641, LR: 0.000026\n",
      "Step 5734, Loss Train: 2.8072, Loss Test: 4.4015, LR: 0.000026\n",
      "Step 5735, Loss Train: 2.9045, Loss Test: 3.9752, LR: 0.000026\n",
      "Step 5736, Loss Train: 2.8180, Loss Test: 3.6987, LR: 0.000026\n",
      "Step 5737, Loss Train: 2.8602, Loss Test: 3.5155, LR: 0.000026\n",
      "Step 5738, Loss Train: 2.9786, Loss Test: 3.9459, LR: 0.000026\n",
      "Step 5739, Loss Train: 2.9291, Loss Test: 3.5703, LR: 0.000026\n",
      "Step 5740, Loss Train: 3.0284, Loss Test: 3.0315, LR: 0.000026\n",
      "Step 5741, Loss Train: 2.9226, Loss Test: 3.9286, LR: 0.000026\n",
      "Step 5742, Loss Train: 2.8795, Loss Test: 4.4219, LR: 0.000026\n",
      "Step 5743, Loss Train: 2.8861, Loss Test: 2.9812, LR: 0.000026\n",
      "Step 5744, Loss Train: 3.0644, Loss Test: 3.9898, LR: 0.000026\n",
      "Step 5745, Loss Train: 2.9409, Loss Test: 4.8648, LR: 0.000026\n",
      "Step 5746, Loss Train: 2.8912, Loss Test: 3.8705, LR: 0.000026\n",
      "Step 5747, Loss Train: 2.9193, Loss Test: 4.2930, LR: 0.000026\n",
      "Step 5748, Loss Train: 2.8102, Loss Test: 3.8109, LR: 0.000026\n",
      "Step 5749, Loss Train: 2.9148, Loss Test: 3.8632, LR: 0.000026\n",
      "Step 5750, Loss Train: 2.7001, Loss Test: 4.1573, LR: 0.000026\n",
      "Step 5751, Loss Train: 2.8972, Loss Test: 3.9788, LR: 0.000026\n",
      "Step 5752, Loss Train: 2.9542, Loss Test: 2.3919, LR: 0.000026\n",
      "Step 5753, Loss Train: 2.7028, Loss Test: 2.3626, LR: 0.000026\n",
      "Step 5754, Loss Train: 2.8797, Loss Test: 4.1081, LR: 0.000026\n",
      "Step 5755, Loss Train: 2.9593, Loss Test: 3.5063, LR: 0.000026\n",
      "Step 5756, Loss Train: 2.7318, Loss Test: 4.5153, LR: 0.000026\n",
      "Step 5757, Loss Train: 3.0159, Loss Test: 4.0468, LR: 0.000026\n",
      "Step 5758, Loss Train: 2.9659, Loss Test: 3.2773, LR: 0.000026\n",
      "Step 5759, Loss Train: 2.7684, Loss Test: 4.2302, LR: 0.000026\n",
      "Step 5760, Loss Train: 2.9325, Loss Test: 4.3878, LR: 0.000026\n",
      "Step 5761, Loss Train: 2.8799, Loss Test: 4.0376, LR: 0.000026\n",
      "Step 5762, Loss Train: 2.7170, Loss Test: 2.8467, LR: 0.000026\n",
      "Step 5763, Loss Train: 3.0352, Loss Test: 3.2301, LR: 0.000026\n",
      "Step 5764, Loss Train: 2.9219, Loss Test: 4.0826, LR: 0.000026\n",
      "Step 5765, Loss Train: 2.8167, Loss Test: 3.7920, LR: 0.000026\n",
      "Step 5766, Loss Train: 2.7331, Loss Test: 3.5994, LR: 0.000026\n",
      "Step 5767, Loss Train: 2.8743, Loss Test: 3.1476, LR: 0.000026\n",
      "Step 5768, Loss Train: 2.9983, Loss Test: 2.4524, LR: 0.000026\n",
      "Step 5769, Loss Train: 2.9177, Loss Test: 3.4987, LR: 0.000026\n",
      "Step 5770, Loss Train: 2.8505, Loss Test: 3.2934, LR: 0.000026\n",
      "Step 5771, Loss Train: 2.7360, Loss Test: 4.2623, LR: 0.000026\n",
      "Step 5772, Loss Train: 3.0584, Loss Test: 3.1796, LR: 0.000026\n",
      "Step 5773, Loss Train: 2.9263, Loss Test: 3.9582, LR: 0.000026\n",
      "Step 5774, Loss Train: 2.7494, Loss Test: 3.3016, LR: 0.000026\n",
      "Step 5775, Loss Train: 2.8696, Loss Test: 3.8844, LR: 0.000026\n",
      "Step 5776, Loss Train: 2.8937, Loss Test: 4.0166, LR: 0.000026\n",
      "Step 5777, Loss Train: 2.7585, Loss Test: 3.7414, LR: 0.000026\n",
      "Step 5778, Loss Train: 2.8858, Loss Test: 3.1209, LR: 0.000026\n",
      "Step 5779, Loss Train: 3.0389, Loss Test: 3.5979, LR: 0.000026\n",
      "Step 5780, Loss Train: 3.1456, Loss Test: 3.9222, LR: 0.000026\n",
      "Step 5781, Loss Train: 2.8342, Loss Test: 3.1138, LR: 0.000026\n",
      "Step 5782, Loss Train: 2.9514, Loss Test: 3.6847, LR: 0.000026\n",
      "Step 5783, Loss Train: 2.9671, Loss Test: 3.7979, LR: 0.000026\n",
      "Step 5784, Loss Train: 2.7520, Loss Test: 4.1105, LR: 0.000026\n",
      "Step 5785, Loss Train: 2.6885, Loss Test: 4.3308, LR: 0.000026\n",
      "Step 5786, Loss Train: 2.7773, Loss Test: 3.9428, LR: 0.000026\n",
      "Step 5787, Loss Train: 2.8995, Loss Test: 1.6236, LR: 0.000026\n",
      "Step 5788, Loss Train: 2.8880, Loss Test: 4.2266, LR: 0.000026\n",
      "Step 5789, Loss Train: 2.8846, Loss Test: 4.2335, LR: 0.000026\n",
      "Step 5790, Loss Train: 2.4886, Loss Test: 3.4971, LR: 0.000026\n",
      "Step 5791, Loss Train: 2.8659, Loss Test: 3.5961, LR: 0.000026\n",
      "Step 5792, Loss Train: 3.0032, Loss Test: 4.1380, LR: 0.000026\n",
      "Step 5793, Loss Train: 2.8138, Loss Test: 2.9746, LR: 0.000026\n",
      "Step 5794, Loss Train: 2.9924, Loss Test: 3.5697, LR: 0.000026\n",
      "Step 5795, Loss Train: 2.9839, Loss Test: 4.0114, LR: 0.000026\n",
      "Step 5796, Loss Train: 3.1378, Loss Test: 2.8652, LR: 0.000026\n",
      "Step 5797, Loss Train: 2.8073, Loss Test: 3.8934, LR: 0.000026\n",
      "Step 5798, Loss Train: 2.9279, Loss Test: 3.3494, LR: 0.000026\n",
      "Step 5799, Loss Train: 2.8366, Loss Test: 3.3788, LR: 0.000026\n",
      "Step 5800, Loss Train: 2.6312, Loss Test: 4.5794, LR: 0.000026\n",
      "Step 5801, Loss Train: 2.8518, Loss Test: 3.7935, LR: 0.000026\n",
      "Step 5802, Loss Train: 3.0371, Loss Test: 3.9593, LR: 0.000026\n",
      "Step 5803, Loss Train: 2.6497, Loss Test: 4.1530, LR: 0.000026\n",
      "Step 5804, Loss Train: 2.7990, Loss Test: 4.1139, LR: 0.000026\n",
      "Step 5805, Loss Train: 2.8082, Loss Test: 4.4462, LR: 0.000026\n",
      "Step 5806, Loss Train: 2.9663, Loss Test: 3.7314, LR: 0.000026\n",
      "Step 5807, Loss Train: 2.8576, Loss Test: 3.7343, LR: 0.000026\n",
      "Step 5808, Loss Train: 2.9490, Loss Test: 4.4657, LR: 0.000026\n",
      "Step 5809, Loss Train: 2.9725, Loss Test: 3.6474, LR: 0.000026\n",
      "Step 5810, Loss Train: 2.9189, Loss Test: 4.2322, LR: 0.000026\n",
      "Step 5811, Loss Train: 2.9441, Loss Test: 3.5553, LR: 0.000026\n",
      "Step 5812, Loss Train: 2.7733, Loss Test: 4.0412, LR: 0.000026\n",
      "Step 5813, Loss Train: 2.9791, Loss Test: 3.3604, LR: 0.000026\n",
      "Step 5814, Loss Train: 3.0233, Loss Test: 3.0738, LR: 0.000026\n",
      "Step 5815, Loss Train: 2.8815, Loss Test: 3.6476, LR: 0.000026\n",
      "Step 5816, Loss Train: 3.0458, Loss Test: 3.2106, LR: 0.000026\n",
      "Step 5817, Loss Train: 2.7989, Loss Test: 4.3127, LR: 0.000026\n",
      "Step 5818, Loss Train: 2.7048, Loss Test: 4.2212, LR: 0.000026\n",
      "Step 5819, Loss Train: 3.0171, Loss Test: 4.1757, LR: 0.000026\n",
      "Step 5820, Loss Train: 2.7851, Loss Test: 3.7820, LR: 0.000026\n",
      "Step 5821, Loss Train: 3.0977, Loss Test: 4.0184, LR: 0.000026\n",
      "Step 5822, Loss Train: 2.9733, Loss Test: 3.9873, LR: 0.000026\n",
      "Step 5823, Loss Train: 2.9887, Loss Test: 3.5250, LR: 0.000026\n",
      "Step 5824, Loss Train: 2.9001, Loss Test: 2.2848, LR: 0.000026\n",
      "Step 5825, Loss Train: 2.6704, Loss Test: 3.2020, LR: 0.000026\n",
      "Step 5826, Loss Train: 2.8819, Loss Test: 4.0039, LR: 0.000026\n",
      "Step 5827, Loss Train: 2.9431, Loss Test: 4.2559, LR: 0.000026\n",
      "Step 5828, Loss Train: 3.0208, Loss Test: 3.9308, LR: 0.000026\n",
      "Step 5829, Loss Train: 2.8726, Loss Test: 4.2070, LR: 0.000026\n",
      "Step 5830, Loss Train: 2.9542, Loss Test: 4.3249, LR: 0.000026\n",
      "Step 5831, Loss Train: 2.9817, Loss Test: 3.9318, LR: 0.000026\n",
      "Step 5832, Loss Train: 2.9661, Loss Test: 3.6618, LR: 0.000026\n",
      "Step 5833, Loss Train: 2.7070, Loss Test: 4.1566, LR: 0.000026\n",
      "Step 5834, Loss Train: 3.0445, Loss Test: 4.0031, LR: 0.000026\n",
      "Step 5835, Loss Train: 2.8150, Loss Test: 3.8657, LR: 0.000026\n",
      "Step 5836, Loss Train: 2.7518, Loss Test: 4.5905, LR: 0.000026\n",
      "Step 5837, Loss Train: 2.8052, Loss Test: 3.5119, LR: 0.000026\n",
      "Step 5838, Loss Train: 2.9378, Loss Test: 4.2953, LR: 0.000026\n",
      "Step 5839, Loss Train: 2.9404, Loss Test: 3.9222, LR: 0.000026\n",
      "Step 5840, Loss Train: 2.7608, Loss Test: 3.7760, LR: 0.000026\n",
      "Step 5841, Loss Train: 2.8691, Loss Test: 2.8067, LR: 0.000026\n",
      "Step 5842, Loss Train: 3.1307, Loss Test: 4.2479, LR: 0.000026\n",
      "Step 5843, Loss Train: 2.7137, Loss Test: 3.9474, LR: 0.000026\n",
      "Step 5844, Loss Train: 2.7145, Loss Test: 4.3831, LR: 0.000026\n",
      "Step 5845, Loss Train: 2.7270, Loss Test: 3.9603, LR: 0.000026\n",
      "Step 5846, Loss Train: 2.9701, Loss Test: 4.4539, LR: 0.000026\n",
      "Step 5847, Loss Train: 2.7673, Loss Test: 4.2434, LR: 0.000026\n",
      "Step 5848, Loss Train: 3.0276, Loss Test: 3.8689, LR: 0.000026\n",
      "Step 5849, Loss Train: 2.7750, Loss Test: 2.8993, LR: 0.000026\n",
      "Step 5850, Loss Train: 2.8380, Loss Test: 3.9034, LR: 0.000026\n",
      "Step 5851, Loss Train: 2.9169, Loss Test: 3.0995, LR: 0.000026\n",
      "Step 5852, Loss Train: 2.9742, Loss Test: 3.7551, LR: 0.000026\n",
      "Step 5853, Loss Train: 2.9598, Loss Test: 3.4530, LR: 0.000026\n",
      "Step 5854, Loss Train: 2.9764, Loss Test: 4.5276, LR: 0.000026\n",
      "Step 5855, Loss Train: 2.8372, Loss Test: 2.3523, LR: 0.000026\n",
      "Step 5856, Loss Train: 2.8375, Loss Test: 3.7647, LR: 0.000026\n",
      "Step 5857, Loss Train: 2.7619, Loss Test: 3.8802, LR: 0.000026\n",
      "Step 5858, Loss Train: 2.8110, Loss Test: 3.0451, LR: 0.000026\n",
      "Step 5859, Loss Train: 2.9827, Loss Test: 3.3033, LR: 0.000026\n",
      "Step 5860, Loss Train: 3.0589, Loss Test: 3.8866, LR: 0.000026\n",
      "Step 5861, Loss Train: 2.8737, Loss Test: 4.2257, LR: 0.000026\n",
      "Step 5862, Loss Train: 2.7155, Loss Test: 3.1607, LR: 0.000026\n",
      "Step 5863, Loss Train: 2.9867, Loss Test: 3.8481, LR: 0.000026\n",
      "Step 5864, Loss Train: 2.9206, Loss Test: 3.8790, LR: 0.000026\n",
      "Step 5865, Loss Train: 2.9316, Loss Test: 3.8797, LR: 0.000026\n",
      "Step 5866, Loss Train: 2.9594, Loss Test: 3.9010, LR: 0.000026\n",
      "Step 5867, Loss Train: 2.8214, Loss Test: 3.7783, LR: 0.000026\n",
      "Step 5868, Loss Train: 2.7645, Loss Test: 3.4017, LR: 0.000026\n",
      "Step 5869, Loss Train: 2.8949, Loss Test: 3.9771, LR: 0.000026\n",
      "Step 5870, Loss Train: 2.7546, Loss Test: 3.5438, LR: 0.000026\n",
      "Step 5871, Loss Train: 3.1535, Loss Test: 3.5767, LR: 0.000026\n",
      "Step 5872, Loss Train: 2.9836, Loss Test: 3.8563, LR: 0.000026\n",
      "Step 5873, Loss Train: 2.9672, Loss Test: 3.7565, LR: 0.000026\n",
      "Step 5874, Loss Train: 2.8452, Loss Test: 3.7806, LR: 0.000026\n",
      "Step 5875, Loss Train: 2.9726, Loss Test: 4.1074, LR: 0.000026\n",
      "Step 5876, Loss Train: 2.8745, Loss Test: 3.3879, LR: 0.000026\n",
      "Step 5877, Loss Train: 2.8825, Loss Test: 2.5109, LR: 0.000026\n",
      "Step 5878, Loss Train: 2.8982, Loss Test: 4.3627, LR: 0.000026\n",
      "Step 5879, Loss Train: 3.0411, Loss Test: 4.3124, LR: 0.000026\n",
      "Step 5880, Loss Train: 2.8412, Loss Test: 4.0668, LR: 0.000026\n",
      "Step 5881, Loss Train: 3.0628, Loss Test: 3.6335, LR: 0.000026\n",
      "Step 5882, Loss Train: 2.7652, Loss Test: 4.2868, LR: 0.000026\n",
      "Step 5883, Loss Train: 2.8671, Loss Test: 2.7190, LR: 0.000026\n",
      "Step 5884, Loss Train: 2.8194, Loss Test: 4.3563, LR: 0.000026\n",
      "Step 5885, Loss Train: 2.9847, Loss Test: 3.7113, LR: 0.000026\n",
      "Step 5886, Loss Train: 2.8428, Loss Test: 3.8563, LR: 0.000026\n",
      "Step 5887, Loss Train: 2.9366, Loss Test: 3.4327, LR: 0.000026\n",
      "Step 5888, Loss Train: 2.7718, Loss Test: 3.7305, LR: 0.000026\n",
      "Step 5889, Loss Train: 2.8850, Loss Test: 3.7669, LR: 0.000026\n",
      "Step 5890, Loss Train: 2.9762, Loss Test: 3.7270, LR: 0.000026\n",
      "Step 5891, Loss Train: 2.9336, Loss Test: 3.7797, LR: 0.000026\n",
      "Step 5892, Loss Train: 2.9672, Loss Test: 2.9742, LR: 0.000026\n",
      "Step 5893, Loss Train: 2.5614, Loss Test: 3.3341, LR: 0.000026\n",
      "Step 5894, Loss Train: 2.8443, Loss Test: 3.8548, LR: 0.000026\n",
      "Step 5895, Loss Train: 2.9365, Loss Test: 4.2443, LR: 0.000026\n",
      "Step 5896, Loss Train: 2.8161, Loss Test: 4.2480, LR: 0.000026\n",
      "Step 5897, Loss Train: 2.9547, Loss Test: 4.2372, LR: 0.000026\n",
      "Step 5898, Loss Train: 2.7131, Loss Test: 3.5149, LR: 0.000026\n",
      "Step 5899, Loss Train: 2.9567, Loss Test: 3.4568, LR: 0.000026\n",
      "Step 5900, Loss Train: 2.7697, Loss Test: 3.8985, LR: 0.000026\n",
      "Step 5901, Loss Train: 3.0248, Loss Test: 2.3508, LR: 0.000026\n",
      "Step 5902, Loss Train: 2.9210, Loss Test: 3.4301, LR: 0.000026\n",
      "Step 5903, Loss Train: 2.7971, Loss Test: 3.8514, LR: 0.000026\n",
      "Step 5904, Loss Train: 2.8893, Loss Test: 3.4857, LR: 0.000026\n",
      "Step 5905, Loss Train: 2.9850, Loss Test: 2.1704, LR: 0.000026\n",
      "Step 5906, Loss Train: 2.7268, Loss Test: 3.6570, LR: 0.000026\n",
      "Step 5907, Loss Train: 2.9127, Loss Test: 4.2099, LR: 0.000026\n",
      "Step 5908, Loss Train: 2.9063, Loss Test: 3.6923, LR: 0.000026\n",
      "Step 5909, Loss Train: 2.8316, Loss Test: 4.1645, LR: 0.000026\n",
      "Step 5910, Loss Train: 3.0254, Loss Test: 3.8973, LR: 0.000026\n",
      "Step 5911, Loss Train: 2.8743, Loss Test: 4.3575, LR: 0.000026\n",
      "Step 5912, Loss Train: 3.0558, Loss Test: 3.7777, LR: 0.000026\n",
      "Step 5913, Loss Train: 2.7436, Loss Test: 2.9954, LR: 0.000026\n",
      "Step 5914, Loss Train: 2.8453, Loss Test: 3.5176, LR: 0.000026\n",
      "Step 5915, Loss Train: 2.8702, Loss Test: 4.3129, LR: 0.000026\n",
      "Step 5916, Loss Train: 2.8407, Loss Test: 4.1402, LR: 0.000026\n",
      "Step 5917, Loss Train: 3.0258, Loss Test: 3.2106, LR: 0.000026\n",
      "Step 5918, Loss Train: 2.7125, Loss Test: 3.8993, LR: 0.000026\n",
      "Step 5919, Loss Train: 2.9846, Loss Test: 3.8384, LR: 0.000026\n",
      "Step 5920, Loss Train: 2.9019, Loss Test: 3.9867, LR: 0.000026\n",
      "Step 5921, Loss Train: 2.8141, Loss Test: 3.3200, LR: 0.000026\n",
      "Step 5922, Loss Train: 2.7788, Loss Test: 2.9246, LR: 0.000026\n",
      "Step 5923, Loss Train: 2.8936, Loss Test: 4.5037, LR: 0.000026\n",
      "Step 5924, Loss Train: 2.9118, Loss Test: 2.6567, LR: 0.000026\n",
      "Step 5925, Loss Train: 2.8621, Loss Test: 2.8456, LR: 0.000026\n",
      "Step 5926, Loss Train: 2.8636, Loss Test: 4.0559, LR: 0.000026\n",
      "Step 5927, Loss Train: 2.8131, Loss Test: 3.2862, LR: 0.000026\n",
      "Step 5928, Loss Train: 2.9673, Loss Test: 3.6761, LR: 0.000026\n",
      "Step 5929, Loss Train: 2.8068, Loss Test: 3.6454, LR: 0.000026\n",
      "Step 5930, Loss Train: 2.9236, Loss Test: 3.3609, LR: 0.000026\n",
      "Step 5931, Loss Train: 2.8684, Loss Test: 3.0332, LR: 0.000026\n",
      "Step 5932, Loss Train: 2.9258, Loss Test: 3.6154, LR: 0.000026\n",
      "Step 5933, Loss Train: 2.9850, Loss Test: 4.4699, LR: 0.000026\n",
      "Step 5934, Loss Train: 2.9554, Loss Test: 3.7978, LR: 0.000026\n",
      "Step 5935, Loss Train: 3.0549, Loss Test: 3.0574, LR: 0.000026\n",
      "Step 5936, Loss Train: 2.6814, Loss Test: 2.3504, LR: 0.000026\n",
      "Step 5937, Loss Train: 2.9066, Loss Test: 3.4973, LR: 0.000026\n",
      "Step 5938, Loss Train: 2.8593, Loss Test: 4.2690, LR: 0.000026\n",
      "Step 5939, Loss Train: 2.7559, Loss Test: 3.9342, LR: 0.000026\n",
      "Step 5940, Loss Train: 2.8072, Loss Test: 2.8256, LR: 0.000026\n",
      "Step 5941, Loss Train: 3.0456, Loss Test: 3.6232, LR: 0.000026\n",
      "Step 5942, Loss Train: 2.6353, Loss Test: 3.4096, LR: 0.000026\n",
      "Step 5943, Loss Train: 2.9321, Loss Test: 3.4075, LR: 0.000026\n",
      "Step 5944, Loss Train: 3.0293, Loss Test: 4.4787, LR: 0.000026\n",
      "Step 5945, Loss Train: 3.0667, Loss Test: 4.3394, LR: 0.000026\n",
      "Step 5946, Loss Train: 2.9419, Loss Test: 3.4851, LR: 0.000026\n",
      "Step 5947, Loss Train: 3.1306, Loss Test: 4.3335, LR: 0.000026\n",
      "Step 5948, Loss Train: 3.1184, Loss Test: 4.0382, LR: 0.000026\n",
      "Step 5949, Loss Train: 2.9738, Loss Test: 3.7840, LR: 0.000026\n",
      "Step 5950, Loss Train: 2.8481, Loss Test: 3.8781, LR: 0.000026\n",
      "Step 5951, Loss Train: 3.1242, Loss Test: 3.7792, LR: 0.000026\n",
      "Step 5952, Loss Train: 2.9255, Loss Test: 3.7865, LR: 0.000026\n",
      "Step 5953, Loss Train: 2.9848, Loss Test: 3.6595, LR: 0.000026\n",
      "Step 5954, Loss Train: 2.8661, Loss Test: 3.7975, LR: 0.000026\n",
      "Step 5955, Loss Train: 2.7458, Loss Test: 3.5894, LR: 0.000026\n",
      "Step 5956, Loss Train: 2.8476, Loss Test: 3.5140, LR: 0.000026\n",
      "Step 5957, Loss Train: 2.9319, Loss Test: 4.3971, LR: 0.000026\n",
      "Step 5958, Loss Train: 2.9616, Loss Test: 4.3774, LR: 0.000026\n",
      "Step 5959, Loss Train: 2.8889, Loss Test: 3.8182, LR: 0.000026\n",
      "Step 5960, Loss Train: 2.9395, Loss Test: 3.7878, LR: 0.000026\n",
      "Step 5961, Loss Train: 2.8268, Loss Test: 3.9963, LR: 0.000026\n",
      "Step 5962, Loss Train: 3.0166, Loss Test: 4.3740, LR: 0.000026\n",
      "Step 5963, Loss Train: 2.8465, Loss Test: 4.2393, LR: 0.000026\n",
      "Step 5964, Loss Train: 2.8636, Loss Test: 3.6354, LR: 0.000026\n",
      "Step 5965, Loss Train: 2.9974, Loss Test: 4.2920, LR: 0.000026\n",
      "Step 5966, Loss Train: 2.8206, Loss Test: 3.7598, LR: 0.000026\n",
      "Step 5967, Loss Train: 2.9590, Loss Test: 3.1418, LR: 0.000026\n",
      "Step 5968, Loss Train: 2.8413, Loss Test: 4.1295, LR: 0.000026\n",
      "Step 5969, Loss Train: 2.7966, Loss Test: 4.1640, LR: 0.000026\n",
      "Step 5970, Loss Train: 2.8718, Loss Test: 3.7089, LR: 0.000026\n",
      "Step 5971, Loss Train: 2.9605, Loss Test: 4.4033, LR: 0.000026\n",
      "Step 5972, Loss Train: 2.9247, Loss Test: 2.9024, LR: 0.000026\n",
      "Step 5973, Loss Train: 2.9530, Loss Test: 3.6555, LR: 0.000026\n",
      "Step 5974, Loss Train: 2.9894, Loss Test: 3.7660, LR: 0.000026\n",
      "Step 5975, Loss Train: 2.9901, Loss Test: 3.7019, LR: 0.000026\n",
      "Step 5976, Loss Train: 3.0140, Loss Test: 2.3915, LR: 0.000026\n",
      "Step 5977, Loss Train: 2.9424, Loss Test: 3.2671, LR: 0.000026\n",
      "Step 5978, Loss Train: 2.8447, Loss Test: 4.0986, LR: 0.000026\n",
      "Step 5979, Loss Train: 2.7964, Loss Test: 4.0084, LR: 0.000026\n",
      "Step 5980, Loss Train: 2.7858, Loss Test: 2.2042, LR: 0.000026\n",
      "Step 5981, Loss Train: 2.9686, Loss Test: 3.7987, LR: 0.000026\n",
      "Step 5982, Loss Train: 2.9210, Loss Test: 4.0691, LR: 0.000026\n",
      "Step 5983, Loss Train: 2.7608, Loss Test: 3.9831, LR: 0.000026\n",
      "Step 5984, Loss Train: 2.9060, Loss Test: 3.2835, LR: 0.000026\n",
      "Step 5985, Loss Train: 2.8074, Loss Test: 4.1384, LR: 0.000026\n",
      "Step 5986, Loss Train: 2.8880, Loss Test: 2.6818, LR: 0.000026\n",
      "Step 5987, Loss Train: 2.8605, Loss Test: 4.0959, LR: 0.000026\n",
      "Step 5988, Loss Train: 2.7952, Loss Test: 4.6807, LR: 0.000026\n",
      "Step 5989, Loss Train: 2.8618, Loss Test: 3.5722, LR: 0.000026\n",
      "Step 5990, Loss Train: 2.8572, Loss Test: 4.1947, LR: 0.000026\n",
      "Step 5991, Loss Train: 2.8998, Loss Test: 2.9648, LR: 0.000026\n",
      "Step 5992, Loss Train: 2.8890, Loss Test: 4.2351, LR: 0.000026\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([14.869240283966064,\n",
       "  14.843252420425415,\n",
       "  14.892844319343567,\n",
       "  14.80180025100708,\n",
       "  14.724263310432434,\n",
       "  14.663204073905945,\n",
       "  14.5652015209198,\n",
       "  14.610636353492737,\n",
       "  14.481077671051025,\n",
       "  14.371236801147461,\n",
       "  14.16866409778595,\n",
       "  14.179565668106079,\n",
       "  13.955062866210938,\n",
       "  13.723238348960876,\n",
       "  13.687171220779419,\n",
       "  13.42956829071045,\n",
       "  13.270652055740356,\n",
       "  13.215447545051575,\n",
       "  12.976373434066772,\n",
       "  12.854222774505615,\n",
       "  12.749372601509094,\n",
       "  12.356987118721008,\n",
       "  12.408194184303284,\n",
       "  12.012135982513428,\n",
       "  11.96212911605835,\n",
       "  11.955255627632141,\n",
       "  11.720670461654663,\n",
       "  11.728261590003967,\n",
       "  11.652526021003723,\n",
       "  11.41602349281311,\n",
       "  11.378599643707275,\n",
       "  11.379032373428345,\n",
       "  11.259947061538696,\n",
       "  11.20085597038269,\n",
       "  11.081208109855652,\n",
       "  11.026306986808777,\n",
       "  10.874039649963379,\n",
       "  10.795849680900574,\n",
       "  10.749486804008484,\n",
       "  10.714420795440674,\n",
       "  10.663718104362488,\n",
       "  10.72929573059082,\n",
       "  10.490420460700989,\n",
       "  10.387823343276978,\n",
       "  10.372750401496887,\n",
       "  10.354374289512634,\n",
       "  10.305824279785156,\n",
       "  10.2736736536026,\n",
       "  10.14458453655243,\n",
       "  9.901753306388855,\n",
       "  9.992146372795105,\n",
       "  9.878798723220825,\n",
       "  9.902457594871521,\n",
       "  9.872841000556946,\n",
       "  9.481918394565582,\n",
       "  9.683887124061584,\n",
       "  9.752283692359924,\n",
       "  9.545053124427795,\n",
       "  9.58554756641388,\n",
       "  9.655122876167297,\n",
       "  9.421174168586731,\n",
       "  9.46059262752533,\n",
       "  9.479745268821716,\n",
       "  9.408859610557556,\n",
       "  9.321407079696655,\n",
       "  9.226871609687805,\n",
       "  9.39151930809021,\n",
       "  9.185379385948181,\n",
       "  9.183030128479004,\n",
       "  9.064829349517822,\n",
       "  9.115532994270325,\n",
       "  9.24446976184845,\n",
       "  9.03639829158783,\n",
       "  9.035901308059692,\n",
       "  8.904642939567566,\n",
       "  8.9437757730484,\n",
       "  8.848620653152466,\n",
       "  8.885308146476746,\n",
       "  8.928211569786072,\n",
       "  8.892765045166016,\n",
       "  8.508425176143646,\n",
       "  8.373901665210724,\n",
       "  8.813341617584229,\n",
       "  8.708722710609436,\n",
       "  8.728722095489502,\n",
       "  8.692708492279053,\n",
       "  8.752121806144714,\n",
       "  8.706982374191284,\n",
       "  8.528019070625305,\n",
       "  8.656201958656311,\n",
       "  8.640343308448792,\n",
       "  8.559788703918457,\n",
       "  8.574115514755249,\n",
       "  8.574528813362122,\n",
       "  8.498138785362244,\n",
       "  8.582430601119995,\n",
       "  8.481647253036499,\n",
       "  8.536707520484924,\n",
       "  8.452418088912964,\n",
       "  8.402024388313293,\n",
       "  8.408856987953186,\n",
       "  8.559467077255249,\n",
       "  8.236798167228699,\n",
       "  8.407177090644836,\n",
       "  8.3248291015625,\n",
       "  8.37954556941986,\n",
       "  8.465127229690552,\n",
       "  8.314025163650513,\n",
       "  8.274499535560608,\n",
       "  8.334903001785278,\n",
       "  8.218450546264648,\n",
       "  8.260448694229126,\n",
       "  8.25174343585968,\n",
       "  8.329123854637146,\n",
       "  8.167908430099487,\n",
       "  8.109358847141266,\n",
       "  8.141645550727844,\n",
       "  8.14083456993103,\n",
       "  8.047592103481293,\n",
       "  7.989515244960785,\n",
       "  8.2424835562706,\n",
       "  8.03106153011322,\n",
       "  8.05864143371582,\n",
       "  8.135566771030426,\n",
       "  7.839189827442169,\n",
       "  8.086612939834595,\n",
       "  8.063035547733307,\n",
       "  8.16999763250351,\n",
       "  8.01139372587204,\n",
       "  7.986390054225922,\n",
       "  7.999702513217926,\n",
       "  8.043037116527557,\n",
       "  8.041152536869049,\n",
       "  7.919551372528076,\n",
       "  7.936166763305664,\n",
       "  7.928799152374268,\n",
       "  7.980775952339172,\n",
       "  7.960516154766083,\n",
       "  7.871735394001007,\n",
       "  8.024839878082275,\n",
       "  7.91179883480072,\n",
       "  7.898492455482483,\n",
       "  7.850313782691956,\n",
       "  7.910573065280914,\n",
       "  7.925641059875488,\n",
       "  7.967422723770142,\n",
       "  7.911327362060547,\n",
       "  7.836143970489502,\n",
       "  7.829103410243988,\n",
       "  7.835844933986664,\n",
       "  7.789305329322815,\n",
       "  7.619710147380829,\n",
       "  7.939393401145935,\n",
       "  7.681045830249786,\n",
       "  7.898219645023346,\n",
       "  7.826290011405945,\n",
       "  7.783074915409088,\n",
       "  7.837504744529724,\n",
       "  7.833715617656708,\n",
       "  7.71849662065506,\n",
       "  7.750098407268524,\n",
       "  7.736029744148254,\n",
       "  7.803517580032349,\n",
       "  7.7908855676651,\n",
       "  7.666176795959473,\n",
       "  7.693248093128204,\n",
       "  7.523518323898315,\n",
       "  7.719449698925018,\n",
       "  7.591737866401672,\n",
       "  7.671370267868042,\n",
       "  7.759645700454712,\n",
       "  7.636951744556427,\n",
       "  7.7264732122421265,\n",
       "  7.721580386161804,\n",
       "  7.677805304527283,\n",
       "  7.64877861738205,\n",
       "  7.630495548248291,\n",
       "  7.605368673801422,\n",
       "  7.642989099025726,\n",
       "  7.553281784057617,\n",
       "  7.6523520946502686,\n",
       "  7.74013888835907,\n",
       "  7.564862430095673,\n",
       "  7.5640785694122314,\n",
       "  7.632627785205841,\n",
       "  7.550257444381714,\n",
       "  7.6301780343055725,\n",
       "  7.518487811088562,\n",
       "  7.418476760387421,\n",
       "  7.755082428455353,\n",
       "  7.6355831027030945,\n",
       "  7.656303286552429,\n",
       "  7.703516125679016,\n",
       "  7.6759257316589355,\n",
       "  7.744034767150879,\n",
       "  7.6921533942222595,\n",
       "  7.681589424610138,\n",
       "  7.641464591026306,\n",
       "  7.493708252906799,\n",
       "  7.422909259796143,\n",
       "  7.684671461582184,\n",
       "  7.7221150398254395,\n",
       "  7.527076840400696,\n",
       "  7.664350986480713,\n",
       "  7.397019743919373,\n",
       "  7.602528989315033,\n",
       "  7.644307196140289,\n",
       "  7.62853479385376,\n",
       "  7.438344955444336,\n",
       "  7.61453503370285,\n",
       "  7.600816130638123,\n",
       "  7.587625861167908,\n",
       "  7.590938925743103,\n",
       "  7.637240350246429,\n",
       "  7.62195760011673,\n",
       "  7.503248870372772,\n",
       "  7.5912415981292725,\n",
       "  7.744106590747833,\n",
       "  7.656745910644531,\n",
       "  7.6375755071640015,\n",
       "  7.598880887031555,\n",
       "  7.737542510032654,\n",
       "  7.555216372013092,\n",
       "  7.585620641708374,\n",
       "  7.6762635707855225,\n",
       "  7.461401522159576,\n",
       "  7.643076419830322,\n",
       "  7.657416105270386,\n",
       "  7.585910081863403,\n",
       "  7.673440754413605,\n",
       "  7.736599087715149,\n",
       "  7.567021727561951,\n",
       "  7.586625456809998,\n",
       "  7.600853800773621,\n",
       "  7.608118951320648,\n",
       "  7.640503644943237,\n",
       "  7.532717406749725,\n",
       "  7.578346073627472,\n",
       "  7.528696835041046,\n",
       "  7.507144272327423,\n",
       "  7.518078804016113,\n",
       "  7.469261884689331,\n",
       "  7.510950803756714,\n",
       "  7.543830335140228,\n",
       "  7.586394906044006,\n",
       "  7.447005987167358,\n",
       "  7.589936852455139,\n",
       "  7.560298979282379,\n",
       "  7.63020247220993,\n",
       "  7.507551431655884,\n",
       "  7.526033401489258,\n",
       "  7.428734302520752,\n",
       "  7.553571462631226,\n",
       "  7.530203580856323,\n",
       "  7.662282407283783,\n",
       "  7.613839864730835,\n",
       "  7.642033636569977,\n",
       "  7.66418319940567,\n",
       "  7.5486990213394165,\n",
       "  7.658584296703339,\n",
       "  7.584397733211517,\n",
       "  7.593499481678009,\n",
       "  7.6792256236076355,\n",
       "  7.452955603599548,\n",
       "  7.646698832511902,\n",
       "  7.32393616437912,\n",
       "  7.4129562973976135,\n",
       "  7.54299008846283,\n",
       "  7.560735762119293,\n",
       "  7.564076900482178,\n",
       "  7.591596364974976,\n",
       "  7.572528004646301,\n",
       "  7.5471749901771545,\n",
       "  7.193912386894226,\n",
       "  7.49607640504837,\n",
       "  7.564323425292969,\n",
       "  7.476269602775574,\n",
       "  7.530788958072662,\n",
       "  7.55449378490448,\n",
       "  7.5630329847335815,\n",
       "  7.584151864051819,\n",
       "  7.584697723388672,\n",
       "  7.62507289648056,\n",
       "  7.513273119926453,\n",
       "  7.597517192363739,\n",
       "  7.515363276004791,\n",
       "  7.510829329490662,\n",
       "  7.5837362408638,\n",
       "  7.628950715065002,\n",
       "  7.623761355876923,\n",
       "  7.684216380119324,\n",
       "  7.58335942029953,\n",
       "  7.560522794723511,\n",
       "  7.549068570137024,\n",
       "  7.480887711048126,\n",
       "  7.560596585273743,\n",
       "  7.455989480018616,\n",
       "  7.605378985404968,\n",
       "  7.543204605579376,\n",
       "  7.635709822177887,\n",
       "  7.516802668571472,\n",
       "  7.512638747692108,\n",
       "  7.600841462612152,\n",
       "  7.519295036792755,\n",
       "  7.533047378063202,\n",
       "  7.368821322917938,\n",
       "  7.520158886909485,\n",
       "  7.660661220550537,\n",
       "  7.542831301689148,\n",
       "  7.5713478326797485,\n",
       "  7.492346286773682,\n",
       "  7.4875354170799255,\n",
       "  7.576364874839783,\n",
       "  7.426819860935211,\n",
       "  7.593828856945038,\n",
       "  7.601981043815613,\n",
       "  7.507324516773224,\n",
       "  7.576490044593811,\n",
       "  7.675716817378998,\n",
       "  7.54567414522171,\n",
       "  7.4870463609695435,\n",
       "  7.5275909304618835,\n",
       "  7.640510678291321,\n",
       "  7.324604868888855,\n",
       "  7.629378259181976,\n",
       "  7.359950006008148,\n",
       "  7.395215451717377,\n",
       "  7.569701015949249,\n",
       "  7.595631778240204,\n",
       "  7.533772230148315,\n",
       "  7.58323872089386,\n",
       "  7.493369102478027,\n",
       "  7.196848809719086,\n",
       "  7.521827518939972,\n",
       "  7.55061799287796,\n",
       "  7.528382420539856,\n",
       "  7.441093683242798,\n",
       "  7.55390864610672,\n",
       "  7.462451636791229,\n",
       "  7.508727669715881,\n",
       "  7.4603570103645325,\n",
       "  7.551034450531006,\n",
       "  7.490131318569183,\n",
       "  7.607862293720245,\n",
       "  7.47279953956604,\n",
       "  7.542597770690918,\n",
       "  7.567237734794617,\n",
       "  7.620518505573273,\n",
       "  7.458514153957367,\n",
       "  7.591890871524811,\n",
       "  7.539885640144348,\n",
       "  7.526788592338562,\n",
       "  7.539446592330933,\n",
       "  7.61294287443161,\n",
       "  7.455827355384827,\n",
       "  7.574449777603149,\n",
       "  7.332553923130035,\n",
       "  7.601869821548462,\n",
       "  7.539597392082214,\n",
       "  7.373608350753784,\n",
       "  7.5171685218811035,\n",
       "  7.588257193565369,\n",
       "  7.414452135562897,\n",
       "  7.572691380977631,\n",
       "  7.519017040729523,\n",
       "  7.523181676864624,\n",
       "  7.53398209810257,\n",
       "  7.543472468852997,\n",
       "  7.469270348548889,\n",
       "  7.6404255628585815,\n",
       "  7.615244925022125,\n",
       "  7.439621090888977,\n",
       "  7.532099425792694,\n",
       "  7.4806137681007385,\n",
       "  7.499990105628967,\n",
       "  7.578740894794464,\n",
       "  7.31979238986969,\n",
       "  7.512048006057739,\n",
       "  7.627574622631073,\n",
       "  7.584625661373138,\n",
       "  7.597374618053436,\n",
       "  7.4788379073143005,\n",
       "  7.5314249992370605,\n",
       "  7.437650442123413,\n",
       "  7.601114392280579,\n",
       "  7.409551918506622,\n",
       "  7.44617760181427,\n",
       "  7.51532518863678,\n",
       "  7.490023076534271,\n",
       "  7.516854226589203,\n",
       "  7.479503095149994,\n",
       "  7.331713497638702,\n",
       "  7.50769430398941,\n",
       "  7.504287838935852,\n",
       "  7.354774653911591,\n",
       "  7.456340670585632,\n",
       "  7.461810290813446,\n",
       "  7.612112939357758,\n",
       "  7.387804567813873,\n",
       "  7.483652710914612,\n",
       "  7.268766462802887,\n",
       "  7.542916417121887,\n",
       "  7.612897336483002,\n",
       "  7.455163896083832,\n",
       "  7.622902870178223,\n",
       "  7.285703778266907,\n",
       "  7.447362422943115,\n",
       "  7.457834541797638,\n",
       "  7.565390467643738,\n",
       "  7.575896799564362,\n",
       "  7.54581892490387,\n",
       "  7.433621525764465,\n",
       "  7.509098410606384,\n",
       "  7.415786564350128,\n",
       "  7.4422847628593445,\n",
       "  7.455801069736481,\n",
       "  7.556813418865204,\n",
       "  7.499854326248169,\n",
       "  7.431453168392181,\n",
       "  7.518150329589844,\n",
       "  7.49408745765686,\n",
       "  7.544054388999939,\n",
       "  7.487651824951172,\n",
       "  7.388974606990814,\n",
       "  7.531670868396759,\n",
       "  7.498423337936401,\n",
       "  7.420516550540924,\n",
       "  7.499535918235779,\n",
       "  7.634806990623474,\n",
       "  7.481140673160553,\n",
       "  7.599817216396332,\n",
       "  7.61939549446106,\n",
       "  7.6015066504478455,\n",
       "  7.237600088119507,\n",
       "  7.574107587337494,\n",
       "  7.423101484775543,\n",
       "  7.487774312496185,\n",
       "  7.488263249397278,\n",
       "  7.443865239620209,\n",
       "  7.512459576129913,\n",
       "  7.541913270950317,\n",
       "  7.489395558834076,\n",
       "  7.459266781806946,\n",
       "  7.484457492828369,\n",
       "  7.44015634059906,\n",
       "  7.484767258167267,\n",
       "  7.5024573802948,\n",
       "  7.307060897350311,\n",
       "  7.516502857208252,\n",
       "  7.3060561418533325,\n",
       "  7.452593982219696,\n",
       "  7.575528681278229,\n",
       "  7.243623852729797,\n",
       "  7.160459280014038,\n",
       "  7.48274827003479,\n",
       "  7.470224618911743,\n",
       "  7.5616079568862915,\n",
       "  7.4522563219070435,\n",
       "  7.5060314536094666,\n",
       "  7.542979538440704,\n",
       "  7.495746612548828,\n",
       "  7.467610120773315,\n",
       "  7.266309440135956,\n",
       "  7.493635058403015,\n",
       "  7.507561445236206,\n",
       "  7.5212308168411255,\n",
       "  7.21861869096756,\n",
       "  7.490710556507111,\n",
       "  7.481177449226379,\n",
       "  7.470568537712097,\n",
       "  7.435915946960449,\n",
       "  7.45877081155777,\n",
       "  7.4859395027160645,\n",
       "  7.416872799396515,\n",
       "  7.43106073141098,\n",
       "  7.584185361862183,\n",
       "  7.405932009220123,\n",
       "  7.392085075378418,\n",
       "  7.493071913719177,\n",
       "  7.395121455192566,\n",
       "  7.417759001255035,\n",
       "  7.421989560127258,\n",
       "  7.471594989299774,\n",
       "  7.37652325630188,\n",
       "  7.474835216999054,\n",
       "  7.46818345785141,\n",
       "  7.457500636577606,\n",
       "  7.611497640609741,\n",
       "  7.57076758146286,\n",
       "  7.497620761394501,\n",
       "  7.505426228046417,\n",
       "  7.5928345918655396,\n",
       "  7.498357892036438,\n",
       "  7.489726006984711,\n",
       "  7.586670994758606,\n",
       "  7.439835965633392,\n",
       "  7.451888859272003,\n",
       "  7.119780659675598,\n",
       "  7.4533578753471375,\n",
       "  7.340964317321777,\n",
       "  7.501219213008881,\n",
       "  7.553515315055847,\n",
       "  7.520775020122528,\n",
       "  7.478796303272247,\n",
       "  7.456608772277832,\n",
       "  7.5080119371414185,\n",
       "  7.495622277259827,\n",
       "  7.349676728248596,\n",
       "  7.313401401042938,\n",
       "  7.504117846488953,\n",
       "  7.488162994384766,\n",
       "  7.464781761169434,\n",
       "  7.487912237644196,\n",
       "  7.476090669631958,\n",
       "  7.546013951301575,\n",
       "  7.2702847719192505,\n",
       "  7.546536147594452,\n",
       "  7.526122093200684,\n",
       "  7.531277716159821,\n",
       "  7.4168994426727295,\n",
       "  7.519913136959076,\n",
       "  7.408553659915924,\n",
       "  7.548611760139465,\n",
       "  7.37140017747879,\n",
       "  7.595087051391602,\n",
       "  7.418415784835815,\n",
       "  7.412657797336578,\n",
       "  7.312899231910706,\n",
       "  7.443956732749939,\n",
       "  7.55036598443985,\n",
       "  7.425079703330994,\n",
       "  7.3510377407073975,\n",
       "  7.554770231246948,\n",
       "  7.413356065750122,\n",
       "  7.452128887176514,\n",
       "  7.451488256454468,\n",
       "  7.422698676586151,\n",
       "  7.425031542778015,\n",
       "  7.466740608215332,\n",
       "  7.505280137062073,\n",
       "  7.498492419719696,\n",
       "  7.545412957668304,\n",
       "  7.434446156024933,\n",
       "  7.571989059448242,\n",
       "  7.413366973400116,\n",
       "  7.479982793331146,\n",
       "  7.216456294059753,\n",
       "  7.461001813411713,\n",
       "  7.308776676654816,\n",
       "  7.492943465709686,\n",
       "  7.453518927097321,\n",
       "  7.40008544921875,\n",
       "  7.4751094579696655,\n",
       "  7.55665796995163,\n",
       "  7.457141399383545,\n",
       "  7.391620337963104,\n",
       "  7.36110246181488,\n",
       "  7.354602158069611,\n",
       "  7.4638219475746155,\n",
       "  7.46822452545166,\n",
       "  7.436152696609497,\n",
       "  7.525635361671448,\n",
       "  7.43050354719162,\n",
       "  7.465556025505066,\n",
       "  7.3659263253211975,\n",
       "  7.065348446369171,\n",
       "  7.086911976337433,\n",
       "  7.3683019280433655,\n",
       "  7.348727285861969,\n",
       "  7.361161291599274,\n",
       "  7.33956116437912,\n",
       "  7.437395632266998,\n",
       "  7.405838668346405,\n",
       "  7.3916818499565125,\n",
       "  7.448700129985809,\n",
       "  7.266410648822784,\n",
       "  7.41091376543045,\n",
       "  7.3938740491867065,\n",
       "  7.5120333433151245,\n",
       "  7.294513285160065,\n",
       "  7.474875807762146,\n",
       "  7.318355441093445,\n",
       "  7.552941203117371,\n",
       "  7.393469750881195,\n",
       "  7.245150923728943,\n",
       "  7.328470885753632,\n",
       "  7.33466374874115,\n",
       "  7.490945279598236,\n",
       "  7.444417238235474,\n",
       "  7.312938928604126,\n",
       "  7.468515694141388,\n",
       "  7.354703783988953,\n",
       "  7.379059731960297,\n",
       "  7.466924846172333,\n",
       "  7.306651949882507,\n",
       "  7.453995108604431,\n",
       "  7.545142590999603,\n",
       "  7.467765808105469,\n",
       "  7.194770038127899,\n",
       "  7.432550311088562,\n",
       "  7.4907572865486145,\n",
       "  7.477331340312958,\n",
       "  7.369114696979523,\n",
       "  7.303313612937927,\n",
       "  7.316752672195435,\n",
       "  7.324047029018402,\n",
       "  7.339993596076965,\n",
       "  7.243503034114838,\n",
       "  7.430861055850983,\n",
       "  7.373635172843933,\n",
       "  7.437894105911255,\n",
       "  7.463258802890778,\n",
       "  7.251105487346649,\n",
       "  7.244373798370361,\n",
       "  7.505034506320953,\n",
       "  7.527311205863953,\n",
       "  7.407698273658752,\n",
       "  7.318994879722595,\n",
       "  7.323623895645142,\n",
       "  7.405591666698456,\n",
       "  7.241048872470856,\n",
       "  7.303811192512512,\n",
       "  7.473810076713562,\n",
       "  7.455776393413544,\n",
       "  7.374999046325684,\n",
       "  7.286768853664398,\n",
       "  7.309460341930389,\n",
       "  7.374163627624512,\n",
       "  7.383151054382324,\n",
       "  7.262686729431152,\n",
       "  7.347776114940643,\n",
       "  7.336794376373291,\n",
       "  7.173269212245941,\n",
       "  7.469344854354858,\n",
       "  7.316513776779175,\n",
       "  7.418562710285187,\n",
       "  7.422672510147095,\n",
       "  7.236156105995178,\n",
       "  7.371456325054169,\n",
       "  7.5081451535224915,\n",
       "  7.394477367401123,\n",
       "  7.361778974533081,\n",
       "  7.308760225772858,\n",
       "  7.184239745140076,\n",
       "  7.421034097671509,\n",
       "  7.414493381977081,\n",
       "  7.32173079252243,\n",
       "  7.293353378772736,\n",
       "  7.368147790431976,\n",
       "  7.250933766365051,\n",
       "  7.378726303577423,\n",
       "  7.154354512691498,\n",
       "  7.43111914396286,\n",
       "  7.387667894363403,\n",
       "  7.157315611839294,\n",
       "  7.327393472194672,\n",
       "  7.526444613933563,\n",
       "  7.4805039167404175,\n",
       "  7.389732658863068,\n",
       "  7.247573554515839,\n",
       "  7.389015376567841,\n",
       "  7.324845910072327,\n",
       "  7.367791771888733,\n",
       "  7.068138003349304,\n",
       "  7.152960002422333,\n",
       "  7.177678406238556,\n",
       "  7.478135585784912,\n",
       "  7.471023499965668,\n",
       "  7.290968060493469,\n",
       "  7.21706622838974,\n",
       "  7.373224198818207,\n",
       "  7.309712171554565,\n",
       "  7.264845967292786,\n",
       "  7.227208197116852,\n",
       "  7.180724024772644,\n",
       "  7.397474944591522,\n",
       "  7.235383868217468,\n",
       "  7.390273869037628,\n",
       "  7.2804582715034485,\n",
       "  7.332818865776062,\n",
       "  7.300521016120911,\n",
       "  7.401448130607605,\n",
       "  7.371882379055023,\n",
       "  7.193590879440308,\n",
       "  7.095641374588013,\n",
       "  7.330815255641937,\n",
       "  7.364736378192902,\n",
       "  7.2915937304496765,\n",
       "  7.372007429599762,\n",
       "  7.4258922934532166,\n",
       "  7.405335485935211,\n",
       "  7.2217206954956055,\n",
       "  7.374365448951721,\n",
       "  7.345472991466522,\n",
       "  7.2946308851242065,\n",
       "  7.412993609905243,\n",
       "  7.283403694629669,\n",
       "  7.387602269649506,\n",
       "  7.196067690849304,\n",
       "  7.112342953681946,\n",
       "  7.3805171251297,\n",
       "  7.285957038402557,\n",
       "  7.3703466057777405,\n",
       "  7.417230010032654,\n",
       "  7.256482899188995,\n",
       "  7.354742765426636,\n",
       "  7.361187398433685,\n",
       "  7.3192519545555115,\n",
       "  7.391825914382935,\n",
       "  7.311054289340973,\n",
       "  7.344001591205597,\n",
       "  7.1986260414123535,\n",
       "  7.34633082151413,\n",
       "  7.313784599304199,\n",
       "  7.4082435965538025,\n",
       "  7.054947018623352,\n",
       "  7.449123203754425,\n",
       "  7.314560353755951,\n",
       "  7.321963429450989,\n",
       "  7.133196771144867,\n",
       "  7.331187903881073,\n",
       "  7.246909499168396,\n",
       "  7.283083975315094,\n",
       "  7.25148069858551,\n",
       "  7.249430954456329,\n",
       "  7.398545026779175,\n",
       "  7.080403387546539,\n",
       "  7.162073910236359,\n",
       "  7.292413651943207,\n",
       "  7.109175980091095,\n",
       "  7.312936425209045,\n",
       "  7.183657884597778,\n",
       "  7.379415690898895,\n",
       "  7.2893165946006775,\n",
       "  7.273460865020752,\n",
       "  7.308262288570404,\n",
       "  7.1190192103385925,\n",
       "  6.915151536464691,\n",
       "  7.0245614647865295,\n",
       "  7.261068403720856,\n",
       "  7.110117435455322,\n",
       "  7.409357368946075,\n",
       "  7.2106605768203735,\n",
       "  7.330581367015839,\n",
       "  7.281890511512756,\n",
       "  7.392450511455536,\n",
       "  7.220168054103851,\n",
       "  7.281635284423828,\n",
       "  6.924275577068329,\n",
       "  6.991413056850433,\n",
       "  7.3084452748298645,\n",
       "  7.05864155292511,\n",
       "  7.329024851322174,\n",
       "  7.041665494441986,\n",
       "  7.367112040519714,\n",
       "  7.247897922992706,\n",
       "  7.284064948558807,\n",
       "  7.290105402469635,\n",
       "  7.246618866920471,\n",
       "  7.014981329441071,\n",
       "  7.3558191657066345,\n",
       "  7.27624636888504,\n",
       "  7.250815391540527,\n",
       "  7.260364294052124,\n",
       "  7.346173822879791,\n",
       "  7.166840136051178,\n",
       "  7.347751617431641,\n",
       "  7.401671230792999,\n",
       "  7.336860656738281,\n",
       "  7.151098012924194,\n",
       "  7.077507972717285,\n",
       "  7.232699036598206,\n",
       "  7.250534296035767,\n",
       "  7.280484914779663,\n",
       "  7.28064638376236,\n",
       "  7.232250988483429,\n",
       "  7.061797320842743,\n",
       "  7.386008441448212,\n",
       "  7.266767084598541,\n",
       "  7.262722730636597,\n",
       "  7.230774104595184,\n",
       "  7.163677871227264,\n",
       "  7.167877495288849,\n",
       "  7.013707995414734,\n",
       "  7.229435980319977,\n",
       "  7.21374124288559,\n",
       "  7.2701287269592285,\n",
       "  7.220938086509705,\n",
       "  7.294423162937164,\n",
       "  7.1554816365242,\n",
       "  7.163128733634949,\n",
       "  7.204086899757385,\n",
       "  7.201479732990265,\n",
       "  7.3197021484375,\n",
       "  7.241713464260101,\n",
       "  7.277654230594635,\n",
       "  7.143414914608002,\n",
       "  7.240368962287903,\n",
       "  7.176111578941345,\n",
       "  7.124986231327057,\n",
       "  7.279443681240082,\n",
       "  7.276878774166107,\n",
       "  7.2100014090538025,\n",
       "  7.189390540122986,\n",
       "  7.28338086605072,\n",
       "  7.1275821924209595,\n",
       "  7.239258289337158,\n",
       "  6.940593242645264,\n",
       "  7.271691262722015,\n",
       "  6.90337073802948,\n",
       "  7.2545634508132935,\n",
       "  7.017652750015259,\n",
       "  6.9582714438438416,\n",
       "  7.32455986738205,\n",
       "  7.14067405462265,\n",
       "  7.161720275878906,\n",
       "  7.211028218269348,\n",
       "  7.2512102127075195,\n",
       "  7.106730580329895,\n",
       "  7.317931056022644,\n",
       "  7.209534525871277,\n",
       "  7.28461080789566,\n",
       "  7.036928713321686,\n",
       "  7.254627227783203,\n",
       "  7.10593456029892,\n",
       "  7.253064453601837,\n",
       "  7.156310856342316,\n",
       "  7.252690553665161,\n",
       "  7.1748656034469604,\n",
       "  7.1909326910972595,\n",
       "  7.19874507188797,\n",
       "  7.311255991458893,\n",
       "  6.827122330665588,\n",
       "  7.004867494106293,\n",
       "  7.272222876548767,\n",
       "  7.203447997570038,\n",
       "  6.974881827831268,\n",
       "  7.05021071434021,\n",
       "  6.952145218849182,\n",
       "  7.061061859130859,\n",
       "  7.187855541706085,\n",
       "  7.053778290748596,\n",
       "  7.094117879867554,\n",
       "  7.18408477306366,\n",
       "  7.144989788532257,\n",
       "  7.080302178859711,\n",
       "  7.201892912387848,\n",
       "  7.03204071521759,\n",
       "  6.917507290840149,\n",
       "  7.172376871109009,\n",
       "  7.213503777980804,\n",
       "  7.029607892036438,\n",
       "  7.0761648416519165,\n",
       "  7.095637023448944,\n",
       "  7.123954355716705,\n",
       "  7.178668260574341,\n",
       "  7.103934288024902,\n",
       "  6.996391832828522,\n",
       "  7.077453970909119,\n",
       "  6.999195098876953,\n",
       "  7.0864251255989075,\n",
       "  7.081515729427338,\n",
       "  7.245936155319214,\n",
       "  7.2190552949905396,\n",
       "  7.216972470283508,\n",
       "  7.134055495262146,\n",
       "  6.963401734828949,\n",
       "  7.206940114498138,\n",
       "  7.10591846704483,\n",
       "  7.0164653062820435,\n",
       "  7.126327574253082,\n",
       "  7.05233508348465,\n",
       "  7.070411205291748,\n",
       "  7.221548914909363,\n",
       "  7.239735662937164,\n",
       "  7.105134010314941,\n",
       "  6.943773806095123,\n",
       "  7.13959264755249,\n",
       "  7.049724996089935,\n",
       "  6.898726046085358,\n",
       "  7.139243006706238,\n",
       "  7.125143766403198,\n",
       "  7.088381052017212,\n",
       "  7.0552714467048645,\n",
       "  6.879934251308441,\n",
       "  7.148588001728058,\n",
       "  6.851407706737518,\n",
       "  7.232322633266449,\n",
       "  7.177699267864227,\n",
       "  6.9138840436935425,\n",
       "  6.968302845954895,\n",
       "  7.004782557487488,\n",
       "  7.057115852832794,\n",
       "  6.899014532566071,\n",
       "  7.160815715789795,\n",
       "  7.049301505088806,\n",
       "  6.932998299598694,\n",
       "  6.9754087924957275,\n",
       "  6.995044112205505,\n",
       "  6.946041584014893,\n",
       "  7.091707110404968,\n",
       "  6.859890818595886,\n",
       "  6.993202984333038,\n",
       "  7.154152452945709,\n",
       "  6.9260135889053345,\n",
       "  7.0786057114601135,\n",
       "  7.064137518405914,\n",
       "  6.985072314739227,\n",
       "  6.986640632152557,\n",
       "  6.933243691921234,\n",
       "  6.865104973316193,\n",
       "  6.964225649833679,\n",
       "  7.0769383907318115,\n",
       "  7.104723513126373,\n",
       "  6.811712205410004,\n",
       "  7.032889306545258,\n",
       "  6.905389964580536,\n",
       "  6.956554293632507,\n",
       "  6.87561959028244,\n",
       "  6.848185360431671,\n",
       "  6.9472768902778625,\n",
       "  6.862741768360138,\n",
       "  6.9692941308021545,\n",
       "  6.725641250610352,\n",
       "  6.888579845428467,\n",
       "  6.872258186340332,\n",
       "  6.871446192264557,\n",
       "  6.845066726207733,\n",
       "  6.869559288024902,\n",
       "  6.991949737071991,\n",
       "  6.951054930686951,\n",
       "  6.781775116920471,\n",
       "  6.98539400100708,\n",
       "  6.838210940361023,\n",
       "  7.04842871427536,\n",
       "  6.95425671339035,\n",
       "  7.061362266540527,\n",
       "  6.770966410636902,\n",
       "  6.8957313895225525,\n",
       "  6.849399089813232,\n",
       "  6.645462334156036,\n",
       "  6.857720792293549,\n",
       "  6.860517859458923,\n",
       "  6.766431987285614,\n",
       "  6.912898540496826,\n",
       "  6.989452004432678,\n",
       "  7.009303987026215,\n",
       "  6.73664265871048,\n",
       "  6.985707521438599,\n",
       "  6.679460883140564,\n",
       "  6.954185485839844,\n",
       "  6.8199480175971985,\n",
       "  6.734821856021881,\n",
       "  6.763715028762817,\n",
       "  6.877370893955231,\n",
       "  6.7467262744903564,\n",
       "  6.884157955646515,\n",
       "  6.656542718410492,\n",
       "  6.856229364871979,\n",
       "  6.732444941997528,\n",
       "  6.85095489025116,\n",
       "  6.883129060268402,\n",
       "  6.82056599855423,\n",
       "  6.86603444814682,\n",
       "  6.772893726825714,\n",
       "  6.751495063304901,\n",
       "  6.667680740356445,\n",
       "  6.842353165149689,\n",
       "  6.770838975906372,\n",
       "  6.693493962287903,\n",
       "  6.760435044765472,\n",
       "  6.959105730056763,\n",
       "  6.440370798110962,\n",
       "  6.776954412460327,\n",
       "  6.856451749801636,\n",
       "  6.806946814060211,\n",
       "  6.854919910430908,\n",
       "  6.827282071113586,\n",
       "  6.711566209793091,\n",
       "  6.900376081466675,\n",
       "  6.61502730846405,\n",
       "  6.608675062656403,\n",
       "  6.739116370677948,\n",
       "  6.738421082496643,\n",
       "  6.761173725128174,\n",
       "  6.595138788223267,\n",
       "  6.8856834173202515,\n",
       "  6.88932478427887,\n",
       "  6.887676656246185,\n",
       "  6.6648876667022705,\n",
       "  6.782756149768829,\n",
       "  6.757203698158264,\n",
       "  6.696372151374817,\n",
       "  6.805437326431274,\n",
       "  6.673820853233337,\n",
       "  6.816027581691742,\n",
       "  6.851371884346008,\n",
       "  6.831833004951477,\n",
       "  6.677055060863495,\n",
       "  6.695175468921661,\n",
       "  ...],\n",
       " [15.005925297737122,\n",
       "  14.782678484916687,\n",
       "  14.902092099189758,\n",
       "  14.832943201065063,\n",
       "  14.686774849891663,\n",
       "  14.757515549659729,\n",
       "  14.629490494728088,\n",
       "  14.370787739753723,\n",
       "  14.523133993148804,\n",
       "  14.104830980300903,\n",
       "  14.308544397354126,\n",
       "  13.963456988334656,\n",
       "  13.575583100318909,\n",
       "  14.03816270828247,\n",
       "  13.305662631988525,\n",
       "  13.375174760818481,\n",
       "  13.115581274032593,\n",
       "  12.866768956184387,\n",
       "  12.725746035575867,\n",
       "  12.694289088249207,\n",
       "  12.73124086856842,\n",
       "  12.415208220481873,\n",
       "  11.85884439945221,\n",
       "  11.984487414360046,\n",
       "  11.813671231269836,\n",
       "  11.820861577987671,\n",
       "  11.779666662216187,\n",
       "  11.68910527229309,\n",
       "  11.23646104335785,\n",
       "  10.974689960479736,\n",
       "  11.339393019676208,\n",
       "  11.231226205825806,\n",
       "  10.925596117973328,\n",
       "  11.103980779647827,\n",
       "  10.876532435417175,\n",
       "  10.692423701286316,\n",
       "  10.68445336818695,\n",
       "  10.392472982406616,\n",
       "  10.218769550323486,\n",
       "  10.354384183883667,\n",
       "  10.262977480888367,\n",
       "  10.428090453147888,\n",
       "  10.069273948669434,\n",
       "  9.986054301261902,\n",
       "  10.036126971244812,\n",
       "  10.24154543876648,\n",
       "  9.886629223823547,\n",
       "  9.88409972190857,\n",
       "  9.894542455673218,\n",
       "  9.819241285324097,\n",
       "  9.49451494216919,\n",
       "  9.620463132858276,\n",
       "  9.782084226608276,\n",
       "  9.278720736503601,\n",
       "  9.408573031425476,\n",
       "  9.533608198165894,\n",
       "  9.282196044921875,\n",
       "  9.386574149131775,\n",
       "  9.214739561080933,\n",
       "  9.243674159049988,\n",
       "  9.19844138622284,\n",
       "  9.282224774360657,\n",
       "  9.284988045692444,\n",
       "  8.784366130828857,\n",
       "  9.13969600200653,\n",
       "  8.999606490135193,\n",
       "  8.937295079231262,\n",
       "  8.892264723777771,\n",
       "  8.748728275299072,\n",
       "  8.780896067619324,\n",
       "  8.741178750991821,\n",
       "  8.674388766288757,\n",
       "  8.643970131874084,\n",
       "  8.619118571281433,\n",
       "  8.69912314414978,\n",
       "  8.509161472320557,\n",
       "  8.605970621109009,\n",
       "  8.49457061290741,\n",
       "  8.731178760528564,\n",
       "  8.652272820472717,\n",
       "  8.445675075054169,\n",
       "  7.528369963169098,\n",
       "  8.462934613227844,\n",
       "  8.5398451089859,\n",
       "  8.575816869735718,\n",
       "  8.306609869003296,\n",
       "  8.280268132686615,\n",
       "  8.352737605571747,\n",
       "  8.38648110628128,\n",
       "  8.27139937877655,\n",
       "  8.354432284832,\n",
       "  8.330206871032715,\n",
       "  8.255988776683807,\n",
       "  8.120338022708893,\n",
       "  8.155984938144684,\n",
       "  8.377875089645386,\n",
       "  8.084628105163574,\n",
       "  8.197769045829773,\n",
       "  8.242878913879395,\n",
       "  8.273401200771332,\n",
       "  7.953507900238037,\n",
       "  7.986338198184967,\n",
       "  8.14097785949707,\n",
       "  7.903916358947754,\n",
       "  8.033746719360352,\n",
       "  7.973791122436523,\n",
       "  8.011364161968231,\n",
       "  8.0023974776268,\n",
       "  8.002217471599579,\n",
       "  7.975315749645233,\n",
       "  8.344760000705719,\n",
       "  7.999037206172943,\n",
       "  7.865696132183075,\n",
       "  7.786661922931671,\n",
       "  7.81018590927124,\n",
       "  7.58171820640564,\n",
       "  7.453386902809143,\n",
       "  7.750313103199005,\n",
       "  7.849083423614502,\n",
       "  7.83814811706543,\n",
       "  7.751307487487793,\n",
       "  7.871744692325592,\n",
       "  7.896323621273041,\n",
       "  7.705917716026306,\n",
       "  7.792401432991028,\n",
       "  7.043137848377228,\n",
       "  7.881185173988342,\n",
       "  7.744402766227722,\n",
       "  7.880486845970154,\n",
       "  7.7009406089782715,\n",
       "  7.559234023094177,\n",
       "  7.549533724784851,\n",
       "  7.738476455211639,\n",
       "  7.689375221729279,\n",
       "  7.735783576965332,\n",
       "  7.569224238395691,\n",
       "  7.782960832118988,\n",
       "  7.573394179344177,\n",
       "  7.696650981903076,\n",
       "  7.762425661087036,\n",
       "  7.441642463207245,\n",
       "  7.676059663295746,\n",
       "  7.6455196142196655,\n",
       "  7.389989614486694,\n",
       "  7.615676701068878,\n",
       "  7.599680185317993,\n",
       "  7.5661962032318115,\n",
       "  7.517728328704834,\n",
       "  7.72370183467865,\n",
       "  7.500496208667755,\n",
       "  7.617976069450378,\n",
       "  7.41826719045639,\n",
       "  7.575459778308868,\n",
       "  7.539185166358948,\n",
       "  7.371894299983978,\n",
       "  7.592524290084839,\n",
       "  7.5344367027282715,\n",
       "  7.594118058681488,\n",
       "  7.608550548553467,\n",
       "  7.677003383636475,\n",
       "  7.514873385429382,\n",
       "  7.5429651737213135,\n",
       "  7.539272308349609,\n",
       "  7.608753502368927,\n",
       "  7.638948857784271,\n",
       "  7.5822173953056335,\n",
       "  7.676089704036713,\n",
       "  7.53504866361618,\n",
       "  7.442663073539734,\n",
       "  7.53112518787384,\n",
       "  7.592036128044128,\n",
       "  7.500458538532257,\n",
       "  7.151769042015076,\n",
       "  7.35325962305069,\n",
       "  7.619312584400177,\n",
       "  7.432175278663635,\n",
       "  7.373385190963745,\n",
       "  7.548990488052368,\n",
       "  7.220324814319611,\n",
       "  7.3560709953308105,\n",
       "  7.507583677768707,\n",
       "  7.434656262397766,\n",
       "  7.26617968082428,\n",
       "  7.567933797836304,\n",
       "  7.403351128101349,\n",
       "  7.364894688129425,\n",
       "  7.44015634059906,\n",
       "  7.416918039321899,\n",
       "  7.2804694175720215,\n",
       "  7.400542199611664,\n",
       "  7.514388382434845,\n",
       "  7.487114667892456,\n",
       "  7.600358486175537,\n",
       "  7.340465486049652,\n",
       "  7.561723709106445,\n",
       "  7.294753193855286,\n",
       "  7.4986706376075745,\n",
       "  7.608542859554291,\n",
       "  7.516564130783081,\n",
       "  7.526298403739929,\n",
       "  7.421631455421448,\n",
       "  7.40176260471344,\n",
       "  7.386593699455261,\n",
       "  7.6314016580581665,\n",
       "  7.488055527210236,\n",
       "  7.307274162769318,\n",
       "  7.362530827522278,\n",
       "  7.586111843585968,\n",
       "  7.644687652587891,\n",
       "  7.395692944526672,\n",
       "  7.36485081911087,\n",
       "  7.103367626667023,\n",
       "  7.342686176300049,\n",
       "  7.410712242126465,\n",
       "  7.437854766845703,\n",
       "  7.179525792598724,\n",
       "  7.4545533657073975,\n",
       "  7.437245666980743,\n",
       "  7.500276267528534,\n",
       "  7.573773920536041,\n",
       "  7.489062666893005,\n",
       "  6.653484463691711,\n",
       "  7.469536900520325,\n",
       "  7.246584057807922,\n",
       "  7.350325167179108,\n",
       "  7.180534064769745,\n",
       "  7.517434239387512,\n",
       "  7.418598294258118,\n",
       "  7.389244914054871,\n",
       "  7.281986236572266,\n",
       "  7.403852581977844,\n",
       "  7.430703639984131,\n",
       "  7.489344894886017,\n",
       "  7.507694661617279,\n",
       "  7.48869526386261,\n",
       "  7.427961349487305,\n",
       "  7.45024847984314,\n",
       "  7.1613433957099915,\n",
       "  7.147432267665863,\n",
       "  7.53951770067215,\n",
       "  6.7502873837947845,\n",
       "  7.415889620780945,\n",
       "  7.436227321624756,\n",
       "  7.312183916568756,\n",
       "  7.391109943389893,\n",
       "  7.432163178920746,\n",
       "  7.350797772407532,\n",
       "  7.344796597957611,\n",
       "  7.330563127994537,\n",
       "  7.541475832462311,\n",
       "  6.58280873298645,\n",
       "  7.211301863193512,\n",
       "  7.365877449512482,\n",
       "  7.293409466743469,\n",
       "  7.3166375160217285,\n",
       "  7.27869713306427,\n",
       "  7.438702940940857,\n",
       "  7.578359305858612,\n",
       "  7.496419608592987,\n",
       "  7.227981328964233,\n",
       "  7.54739773273468,\n",
       "  7.202870786190033,\n",
       "  7.3791263699531555,\n",
       "  7.481391966342926,\n",
       "  7.050494015216827,\n",
       "  7.460413098335266,\n",
       "  7.422332167625427,\n",
       "  7.542549312114716,\n",
       "  7.35899019241333,\n",
       "  7.358049511909485,\n",
       "  7.398635745048523,\n",
       "  7.2318554520606995,\n",
       "  6.608251139521599,\n",
       "  7.3125088810920715,\n",
       "  7.340418100357056,\n",
       "  7.355641305446625,\n",
       "  7.620379865169525,\n",
       "  7.426849067211151,\n",
       "  7.4166077971458435,\n",
       "  7.393912494182587,\n",
       "  7.178950369358063,\n",
       "  7.055226147174835,\n",
       "  7.099111497402191,\n",
       "  7.319700539112091,\n",
       "  7.295368194580078,\n",
       "  7.3666680455207825,\n",
       "  7.35551381111145,\n",
       "  7.164179444313049,\n",
       "  7.35211443901062,\n",
       "  7.611469864845276,\n",
       "  7.314692556858063,\n",
       "  7.51623272895813,\n",
       "  7.233769416809082,\n",
       "  7.376670062541962,\n",
       "  7.32567036151886,\n",
       "  7.293177783489227,\n",
       "  7.206861317157745,\n",
       "  7.306120574474335,\n",
       "  7.558419167995453,\n",
       "  7.257434010505676,\n",
       "  7.213972389698029,\n",
       "  7.315840840339661,\n",
       "  7.092023134231567,\n",
       "  7.119422972202301,\n",
       "  7.518542110919952,\n",
       "  7.237743139266968,\n",
       "  7.295147478580475,\n",
       "  7.487936496734619,\n",
       "  7.184063971042633,\n",
       "  7.509364306926727,\n",
       "  7.488882124423981,\n",
       "  7.37786465883255,\n",
       "  7.328265428543091,\n",
       "  7.294495642185211,\n",
       "  7.511511027812958,\n",
       "  7.501964330673218,\n",
       "  7.2939600348472595,\n",
       "  6.612920105457306,\n",
       "  7.202131509780884,\n",
       "  7.410756349563599,\n",
       "  7.423075318336487,\n",
       "  7.324426710605621,\n",
       "  7.335486590862274,\n",
       "  7.367114543914795,\n",
       "  7.527668833732605,\n",
       "  7.438220679759979,\n",
       "  7.3569347858428955,\n",
       "  7.163394272327423,\n",
       "  7.287820816040039,\n",
       "  7.186958014965057,\n",
       "  7.292815625667572,\n",
       "  7.207661688327789,\n",
       "  7.368656635284424,\n",
       "  7.323201894760132,\n",
       "  7.159155488014221,\n",
       "  7.035423099994659,\n",
       "  7.339913427829742,\n",
       "  7.177608668804169,\n",
       "  6.708857923746109,\n",
       "  7.175994455814362,\n",
       "  6.646795496344566,\n",
       "  7.362639904022217,\n",
       "  7.309994757175446,\n",
       "  7.3040772676467896,\n",
       "  7.315531313419342,\n",
       "  7.351564109325409,\n",
       "  7.039002180099487,\n",
       "  7.220401287078857,\n",
       "  7.368621110916138,\n",
       "  7.3292195200920105,\n",
       "  7.426938116550446,\n",
       "  7.334698259830475,\n",
       "  7.368048310279846,\n",
       "  7.288420915603638,\n",
       "  7.3241172432899475,\n",
       "  7.370963215827942,\n",
       "  7.482305109500885,\n",
       "  7.5721628069877625,\n",
       "  7.385331213474274,\n",
       "  7.302676796913147,\n",
       "  7.410029649734497,\n",
       "  7.2290709018707275,\n",
       "  7.240610599517822,\n",
       "  7.505516290664673,\n",
       "  7.312349557876587,\n",
       "  7.439233958721161,\n",
       "  7.159765899181366,\n",
       "  7.247889697551727,\n",
       "  7.19713169336319,\n",
       "  7.427327454090118,\n",
       "  6.94423508644104,\n",
       "  7.626213014125824,\n",
       "  7.436636388301849,\n",
       "  7.40086030960083,\n",
       "  7.440724313259125,\n",
       "  7.243244051933289,\n",
       "  7.458726227283478,\n",
       "  7.238308310508728,\n",
       "  7.523528873920441,\n",
       "  7.317684710025787,\n",
       "  7.310162365436554,\n",
       "  7.1235047578811646,\n",
       "  7.388720512390137,\n",
       "  7.379420697689056,\n",
       "  7.296239674091339,\n",
       "  7.247795224189758,\n",
       "  7.549346506595612,\n",
       "  7.120790362358093,\n",
       "  7.115918695926666,\n",
       "  7.302636563777924,\n",
       "  7.248770713806152,\n",
       "  7.300788879394531,\n",
       "  7.239360570907593,\n",
       "  7.3567699790000916,\n",
       "  7.165391564369202,\n",
       "  7.564194202423096,\n",
       "  7.480687916278839,\n",
       "  7.515333533287048,\n",
       "  7.385833382606506,\n",
       "  7.3080509305000305,\n",
       "  7.063702046871185,\n",
       "  7.501035511493683,\n",
       "  7.4410480260849,\n",
       "  7.279234766960144,\n",
       "  7.298071086406708,\n",
       "  7.337360441684723,\n",
       "  7.333395898342133,\n",
       "  7.333101809024811,\n",
       "  7.291688084602356,\n",
       "  7.384929537773132,\n",
       "  7.4621723890304565,\n",
       "  7.271357238292694,\n",
       "  7.436651349067688,\n",
       "  7.133052706718445,\n",
       "  7.273980498313904,\n",
       "  7.306700170040131,\n",
       "  7.539032161235809,\n",
       "  7.294548690319061,\n",
       "  7.50941789150238,\n",
       "  7.309046268463135,\n",
       "  7.235788702964783,\n",
       "  7.021716117858887,\n",
       "  7.120859503746033,\n",
       "  7.530286729335785,\n",
       "  7.200573444366455,\n",
       "  7.137637615203857,\n",
       "  7.26586252450943,\n",
       "  7.181018531322479,\n",
       "  7.353367030620575,\n",
       "  7.178906619548798,\n",
       "  7.224216103553772,\n",
       "  7.338948726654053,\n",
       "  7.344969987869263,\n",
       "  7.403936207294464,\n",
       "  7.203027129173279,\n",
       "  7.40300452709198,\n",
       "  7.280879497528076,\n",
       "  7.108875930309296,\n",
       "  7.31994765996933,\n",
       "  7.367003798484802,\n",
       "  6.864030420780182,\n",
       "  7.294059455394745,\n",
       "  7.335012257099152,\n",
       "  7.397557079792023,\n",
       "  7.298871099948883,\n",
       "  7.398421347141266,\n",
       "  7.411931931972504,\n",
       "  7.26480633020401,\n",
       "  7.267928183078766,\n",
       "  7.287001073360443,\n",
       "  7.001246690750122,\n",
       "  7.323870897293091,\n",
       "  7.147789120674133,\n",
       "  7.223928451538086,\n",
       "  7.3175742626190186,\n",
       "  7.247446000576019,\n",
       "  7.427430093288422,\n",
       "  7.101387798786163,\n",
       "  7.422089993953705,\n",
       "  7.126893639564514,\n",
       "  7.317407011985779,\n",
       "  7.291554391384125,\n",
       "  6.518429100513458,\n",
       "  7.377216875553131,\n",
       "  7.385026752948761,\n",
       "  7.395304560661316,\n",
       "  7.282791495323181,\n",
       "  7.552188158035278,\n",
       "  7.2892210483551025,\n",
       "  7.341394782066345,\n",
       "  7.486537277698517,\n",
       "  7.350311040878296,\n",
       "  7.296058237552643,\n",
       "  7.36061954498291,\n",
       "  7.406706929206848,\n",
       "  7.329909086227417,\n",
       "  7.071900427341461,\n",
       "  7.367893695831299,\n",
       "  7.315410912036896,\n",
       "  7.4280120730400085,\n",
       "  7.326568961143494,\n",
       "  7.312073230743408,\n",
       "  7.443532109260559,\n",
       "  7.4583505392074585,\n",
       "  7.361031591892242,\n",
       "  7.4161635637283325,\n",
       "  7.3674376010894775,\n",
       "  7.409329354763031,\n",
       "  7.118146181106567,\n",
       "  6.851305067539215,\n",
       "  7.444586455821991,\n",
       "  7.14120078086853,\n",
       "  6.8637822568416595,\n",
       "  7.413350820541382,\n",
       "  7.257643401622772,\n",
       "  7.3259013295173645,\n",
       "  7.2428611516952515,\n",
       "  7.379378378391266,\n",
       "  7.394975006580353,\n",
       "  7.128572225570679,\n",
       "  7.518612742424011,\n",
       "  7.456683278083801,\n",
       "  7.198914051055908,\n",
       "  7.299778759479523,\n",
       "  7.235566735267639,\n",
       "  7.438770651817322,\n",
       "  7.311604022979736,\n",
       "  7.25924426317215,\n",
       "  7.40163779258728,\n",
       "  7.150021910667419,\n",
       "  7.285498201847076,\n",
       "  7.22131085395813,\n",
       "  7.030173242092133,\n",
       "  7.315521836280823,\n",
       "  6.658489227294922,\n",
       "  7.0795891880989075,\n",
       "  7.479083836078644,\n",
       "  7.361544132232666,\n",
       "  7.5887874364852905,\n",
       "  7.301136612892151,\n",
       "  7.2611610889434814,\n",
       "  7.275336563587189,\n",
       "  7.264357805252075,\n",
       "  7.507010340690613,\n",
       "  7.485621273517609,\n",
       "  7.218377947807312,\n",
       "  7.277489721775055,\n",
       "  7.249136388301849,\n",
       "  7.392008602619171,\n",
       "  7.568898975849152,\n",
       "  7.389022409915924,\n",
       "  7.290503621101379,\n",
       "  7.039130747318268,\n",
       "  7.106330215930939,\n",
       "  7.467515408992767,\n",
       "  7.3060396909713745,\n",
       "  7.440836071968079,\n",
       "  7.286925435066223,\n",
       "  7.356866300106049,\n",
       "  7.314423561096191,\n",
       "  7.380209267139435,\n",
       "  7.031895816326141,\n",
       "  7.463144302368164,\n",
       "  7.255305588245392,\n",
       "  7.32115763425827,\n",
       "  7.314997375011444,\n",
       "  7.400246679782867,\n",
       "  7.382088720798492,\n",
       "  7.555073380470276,\n",
       "  7.185993254184723,\n",
       "  7.373929977416992,\n",
       "  6.883867144584656,\n",
       "  7.24234014749527,\n",
       "  7.36881160736084,\n",
       "  7.34341686964035,\n",
       "  7.043347954750061,\n",
       "  7.2073546051979065,\n",
       "  7.040065944194794,\n",
       "  7.100858688354492,\n",
       "  7.429029107093811,\n",
       "  7.445908308029175,\n",
       "  7.275725603103638,\n",
       "  7.261716663837433,\n",
       "  7.232485592365265,\n",
       "  7.412689447402954,\n",
       "  7.115813910961151,\n",
       "  7.236675798892975,\n",
       "  7.496853172779083,\n",
       "  6.580380663275719,\n",
       "  7.14191073179245,\n",
       "  7.4101755023002625,\n",
       "  7.141221404075623,\n",
       "  7.344436347484589,\n",
       "  7.355053067207336,\n",
       "  7.350353717803955,\n",
       "  7.022878468036652,\n",
       "  6.928735911846161,\n",
       "  7.220814764499664,\n",
       "  7.330021679401398,\n",
       "  7.113844215869904,\n",
       "  7.131426393985748,\n",
       "  7.146959900856018,\n",
       "  7.162978112697601,\n",
       "  7.18148547410965,\n",
       "  7.228663444519043,\n",
       "  6.9771318435668945,\n",
       "  7.168010354042053,\n",
       "  7.26627916097641,\n",
       "  7.120506942272186,\n",
       "  7.153289437294006,\n",
       "  7.318014204502106,\n",
       "  7.312409698963165,\n",
       "  7.427795171737671,\n",
       "  7.221342146396637,\n",
       "  7.157959461212158,\n",
       "  7.248237133026123,\n",
       "  7.044542670249939,\n",
       "  6.87730073928833,\n",
       "  7.210861921310425,\n",
       "  7.010869145393372,\n",
       "  7.256409168243408,\n",
       "  7.36140513420105,\n",
       "  7.324863612651825,\n",
       "  6.908107578754425,\n",
       "  7.356516599655151,\n",
       "  7.371903598308563,\n",
       "  7.462981581687927,\n",
       "  6.736827909946442,\n",
       "  7.254894852638245,\n",
       "  7.158555567264557,\n",
       "  7.191963791847229,\n",
       "  7.120098948478699,\n",
       "  7.074927926063538,\n",
       "  7.200146973133087,\n",
       "  6.521343842148781,\n",
       "  7.174831926822662,\n",
       "  7.207396745681763,\n",
       "  7.513025939464569,\n",
       "  7.476644933223724,\n",
       "  7.249642848968506,\n",
       "  6.938943326473236,\n",
       "  7.0279820561409,\n",
       "  7.078041195869446,\n",
       "  7.287352740764618,\n",
       "  7.0180553793907166,\n",
       "  7.397523522377014,\n",
       "  7.197050929069519,\n",
       "  7.223985373973846,\n",
       "  6.507880866527557,\n",
       "  7.16770076751709,\n",
       "  7.291200697422028,\n",
       "  7.270703315734863,\n",
       "  7.365400075912476,\n",
       "  7.2824360728263855,\n",
       "  7.15680992603302,\n",
       "  7.254667341709137,\n",
       "  7.307046473026276,\n",
       "  7.156759142875671,\n",
       "  7.234061002731323,\n",
       "  7.295292019844055,\n",
       "  7.104354023933411,\n",
       "  7.383157908916473,\n",
       "  7.391766965389252,\n",
       "  7.079114258289337,\n",
       "  6.935015916824341,\n",
       "  7.053284168243408,\n",
       "  7.219501197338104,\n",
       "  6.8280447125434875,\n",
       "  7.084622085094452,\n",
       "  7.29337739944458,\n",
       "  6.83222758769989,\n",
       "  7.237473368644714,\n",
       "  7.425435125827789,\n",
       "  7.277892470359802,\n",
       "  7.190684616565704,\n",
       "  7.284346878528595,\n",
       "  7.078912138938904,\n",
       "  7.188473761081696,\n",
       "  7.120284140110016,\n",
       "  7.416905164718628,\n",
       "  7.265574157238007,\n",
       "  7.223023772239685,\n",
       "  7.230450928211212,\n",
       "  7.1503143310546875,\n",
       "  7.280397534370422,\n",
       "  7.332463979721069,\n",
       "  7.05186539888382,\n",
       "  7.247607171535492,\n",
       "  7.344401001930237,\n",
       "  7.346997618675232,\n",
       "  7.16236424446106,\n",
       "  7.125749826431274,\n",
       "  7.115075349807739,\n",
       "  7.157331883907318,\n",
       "  7.35515820980072,\n",
       "  7.275904357433319,\n",
       "  7.251283884048462,\n",
       "  7.26488721370697,\n",
       "  7.069765388965607,\n",
       "  7.546237945556641,\n",
       "  7.215347349643707,\n",
       "  7.209562003612518,\n",
       "  6.962035834789276,\n",
       "  7.395524680614471,\n",
       "  7.174885332584381,\n",
       "  6.921665251255035,\n",
       "  7.218807876110077,\n",
       "  7.380342900753021,\n",
       "  7.042320191860199,\n",
       "  7.395902633666992,\n",
       "  7.261926114559174,\n",
       "  7.352740168571472,\n",
       "  7.144960939884186,\n",
       "  7.316575288772583,\n",
       "  6.987656235694885,\n",
       "  7.192342698574066,\n",
       "  6.939288377761841,\n",
       "  7.34410434961319,\n",
       "  7.313162982463837,\n",
       "  7.276544392108917,\n",
       "  7.316449046134949,\n",
       "  7.182901680469513,\n",
       "  7.063964605331421,\n",
       "  7.290956377983093,\n",
       "  7.265181481838226,\n",
       "  7.3153194189071655,\n",
       "  7.049184739589691,\n",
       "  7.3333335518836975,\n",
       "  7.294217169284821,\n",
       "  7.2538557052612305,\n",
       "  7.109247624874115,\n",
       "  7.387859642505646,\n",
       "  7.312149226665497,\n",
       "  7.411386549472809,\n",
       "  7.162175953388214,\n",
       "  7.136683940887451,\n",
       "  7.245921730995178,\n",
       "  6.984235465526581,\n",
       "  7.213274300098419,\n",
       "  7.189636826515198,\n",
       "  7.087817132472992,\n",
       "  7.26127290725708,\n",
       "  7.075149238109589,\n",
       "  6.950880944728851,\n",
       "  7.252619385719299,\n",
       "  7.144837021827698,\n",
       "  6.523684561252594,\n",
       "  6.982365131378174,\n",
       "  6.64586216211319,\n",
       "  7.2565776109695435,\n",
       "  6.131845064461231,\n",
       "  7.220697224140167,\n",
       "  7.419194161891937,\n",
       "  7.073601007461548,\n",
       "  7.133259475231171,\n",
       "  7.277367532253265,\n",
       "  7.044145703315735,\n",
       "  7.305367887020111,\n",
       "  7.375266671180725,\n",
       "  7.019149124622345,\n",
       "  7.224542915821075,\n",
       "  6.913235545158386,\n",
       "  6.886609077453613,\n",
       "  6.739023327827454,\n",
       "  7.039720177650452,\n",
       "  7.022793769836426,\n",
       "  7.242940783500671,\n",
       "  7.091956734657288,\n",
       "  6.991608202457428,\n",
       "  7.194262146949768,\n",
       "  6.913785398006439,\n",
       "  7.4250829219818115,\n",
       "  7.212641000747681,\n",
       "  7.3222410678863525,\n",
       "  7.129840552806854,\n",
       "  7.036667227745056,\n",
       "  7.197067856788635,\n",
       "  7.243467688560486,\n",
       "  6.735461354255676,\n",
       "  7.30643755197525,\n",
       "  6.983980476856232,\n",
       "  7.197704493999481,\n",
       "  7.134492337703705,\n",
       "  6.378064729273319,\n",
       "  7.302095293998718,\n",
       "  7.198005795478821,\n",
       "  7.192405819892883,\n",
       "  7.094396471977234,\n",
       "  7.0041375160217285,\n",
       "  6.940918326377869,\n",
       "  7.111226737499237,\n",
       "  7.280293941497803,\n",
       "  7.197618246078491,\n",
       "  6.89950692653656,\n",
       "  7.2835792899131775,\n",
       "  7.056459963321686,\n",
       "  7.171817660331726,\n",
       "  6.92497330904007,\n",
       "  7.024312853813171,\n",
       "  6.782052099704742,\n",
       "  7.05189722776413,\n",
       "  7.1947842836380005,\n",
       "  7.069864451885223,\n",
       "  7.270142614841461,\n",
       "  7.058033108711243,\n",
       "  6.967936635017395,\n",
       "  6.982751250267029,\n",
       "  6.9724708795547485,\n",
       "  7.17281186580658,\n",
       "  7.121401846408844,\n",
       "  7.063236832618713,\n",
       "  6.705254673957825,\n",
       "  6.823113799095154,\n",
       "  7.137285530567169,\n",
       "  7.178620934486389,\n",
       "  6.419649988412857,\n",
       "  7.066667556762695,\n",
       "  7.149006366729736,\n",
       "  7.048238158226013,\n",
       "  6.339042067527771,\n",
       "  7.130607545375824,\n",
       "  7.228333115577698,\n",
       "  7.129811704158783,\n",
       "  6.635489225387573,\n",
       "  7.160622596740723,\n",
       "  7.000665903091431,\n",
       "  7.190333664417267,\n",
       "  6.98550546169281,\n",
       "  6.898568511009216,\n",
       "  6.9243887066841125,\n",
       "  7.029711663722992,\n",
       "  6.986883044242859,\n",
       "  6.734805464744568,\n",
       "  6.459558606147766,\n",
       "  6.93003636598587,\n",
       "  6.799985945224762,\n",
       "  7.179239511489868,\n",
       "  7.0313021540641785,\n",
       "  7.179182708263397,\n",
       "  7.35795521736145,\n",
       "  6.645068466663361,\n",
       "  7.242737650871277,\n",
       "  7.055215001106262,\n",
       "  7.0121254324913025,\n",
       "  7.0674633383750916,\n",
       "  7.162679493427277,\n",
       "  7.167422652244568,\n",
       "  7.037965476512909,\n",
       "  7.050343036651611,\n",
       "  7.263216435909271,\n",
       "  7.101310312747955,\n",
       "  6.985771179199219,\n",
       "  7.079453110694885,\n",
       "  5.922541186213493,\n",
       "  7.005424857139587,\n",
       "  7.100051403045654,\n",
       "  6.996497392654419,\n",
       "  6.782349526882172,\n",
       "  7.292863965034485,\n",
       "  6.908961236476898,\n",
       "  7.1004486083984375,\n",
       "  6.654427886009216,\n",
       "  6.844179749488831,\n",
       "  7.085845649242401,\n",
       "  6.951325476169586,\n",
       "  6.8932711482048035,\n",
       "  6.971689224243164,\n",
       "  6.728852093219757,\n",
       "  7.184498965740204,\n",
       "  6.5627060532569885,\n",
       "  7.2024407386779785,\n",
       "  6.998068273067474,\n",
       "  6.932979226112366,\n",
       "  6.861680805683136,\n",
       "  6.99571305513382,\n",
       "  7.023089170455933,\n",
       "  6.9779582023620605,\n",
       "  7.167359352111816,\n",
       "  6.975539684295654,\n",
       "  6.828255891799927,\n",
       "  6.9828890562057495,\n",
       "  6.7396814823150635,\n",
       "  6.944553256034851,\n",
       "  6.760601699352264,\n",
       "  7.1356117725372314,\n",
       "  6.971385300159454,\n",
       "  6.608829975128174,\n",
       "  7.048838496208191,\n",
       "  6.997245907783508,\n",
       "  7.138847827911377,\n",
       "  7.106251239776611,\n",
       "  7.006926000118256,\n",
       "  6.507966935634613,\n",
       "  6.986833035945892,\n",
       "  6.8900187611579895,\n",
       "  7.010985493659973,\n",
       "  6.9482614398002625,\n",
       "  6.8548004031181335,\n",
       "  7.047466397285461,\n",
       "  6.62235814332962,\n",
       "  6.916146337985992,\n",
       "  6.770896315574646,\n",
       "  6.7782211899757385,\n",
       "  7.132558166980743,\n",
       "  7.033173978328705,\n",
       "  6.825645267963409,\n",
       "  7.072370767593384,\n",
       "  6.832369148731232,\n",
       "  6.7651777267456055,\n",
       "  6.8860756158828735,\n",
       "  6.671904385089874,\n",
       "  6.860196590423584,\n",
       "  7.110659718513489,\n",
       "  6.9460524916648865,\n",
       "  6.657534658908844,\n",
       "  6.983877062797546,\n",
       "  7.012190878391266,\n",
       "  6.661479413509369,\n",
       "  7.0385782122612,\n",
       "  6.924825012683868,\n",
       "  7.00462806224823,\n",
       "  6.820931375026703,\n",
       "  6.96268504858017,\n",
       "  7.013280391693115,\n",
       "  6.798773407936096,\n",
       "  6.777304172515869,\n",
       "  6.9693925976753235,\n",
       "  6.8380051255226135,\n",
       "  6.664772808551788,\n",
       "  6.884841561317444,\n",
       "  6.831960320472717,\n",
       "  6.783505737781525,\n",
       "  6.973216354846954,\n",
       "  6.516895949840546,\n",
       "  6.525019288063049,\n",
       "  7.072897136211395,\n",
       "  7.062162101268768,\n",
       "  6.923401236534119,\n",
       "  6.62516188621521,\n",
       "  6.825537621974945,\n",
       "  6.899448096752167,\n",
       "  6.871306121349335,\n",
       "  6.654354274272919,\n",
       "  6.49256157875061,\n",
       "  6.754416644573212,\n",
       "  6.888659834861755,\n",
       "  6.816255033016205,\n",
       "  6.700821697711945,\n",
       "  6.739688515663147,\n",
       "  6.656647443771362,\n",
       "  6.672500371932983,\n",
       "  6.858047842979431,\n",
       "  6.7464346289634705,\n",
       "  6.5302563309669495,\n",
       "  6.611455023288727,\n",
       "  6.532183110713959,\n",
       "  6.475403666496277,\n",
       "  6.758158922195435,\n",
       "  6.719345033168793,\n",
       "  6.81595778465271,\n",
       "  6.943917810916901,\n",
       "  6.979976773262024,\n",
       "  6.79720664024353,\n",
       "  6.527937889099121,\n",
       "  6.5956422090530396,\n",
       "  6.787570953369141,\n",
       "  6.59250408411026,\n",
       "  6.215314567089081,\n",
       "  6.839503467082977,\n",
       "  6.968848943710327,\n",
       "  6.876452922821045,\n",
       "  6.3164361119270325,\n",
       "  6.604846298694611,\n",
       "  6.875405490398407,\n",
       "  6.70478355884552,\n",
       "  6.655094265937805,\n",
       "  6.797870874404907,\n",
       "  7.080104887485504,\n",
       "  6.545773088932037,\n",
       "  6.667096793651581,\n",
       "  6.564234137535095,\n",
       "  6.840665519237518,\n",
       "  6.6961193680763245,\n",
       "  6.7018720507621765,\n",
       "  6.815166175365448,\n",
       "  6.173088550567627,\n",
       "  6.661763310432434,\n",
       "  6.819281101226807,\n",
       "  6.50656259059906,\n",
       "  6.611516237258911,\n",
       "  6.4579251408576965,\n",
       "  6.704672753810883,\n",
       "  6.911141395568848,\n",
       "  6.844570755958557,\n",
       "  6.643647134304047,\n",
       "  6.690779805183411,\n",
       "  6.715858578681946,\n",
       "  6.710184216499329,\n",
       "  6.7394373416900635,\n",
       "  6.763175129890442,\n",
       "  6.6004109382629395,\n",
       "  6.58008748292923,\n",
       "  6.683429956436157,\n",
       "  6.679262578487396,\n",
       "  6.735307574272156,\n",
       "  6.416068762540817,\n",
       "  6.1164767146110535,\n",
       "  6.626457929611206,\n",
       "  6.428334355354309,\n",
       "  6.726776361465454,\n",
       "  6.619041442871094,\n",
       "  6.750463366508484,\n",
       "  6.695129930973053,\n",
       "  6.380085289478302,\n",
       "  6.530819356441498,\n",
       "  6.5924153327941895,\n",
       "  6.332834839820862,\n",
       "  6.465321838855743,\n",
       "  6.721175193786621,\n",
       "  6.271600246429443,\n",
       "  ...])"
      ]
     },
     "execution_count": 986,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loop(iter_train, iter_test, steps_per_epoch, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ea6bb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses_test[100:], label=\"Test Loss\")\n",
    "plt.plot(losses_train[100:], label=\"Train Loss\")\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Losses during training\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "id": "f115cb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"<s>John is in the house. Sarah is in the garden.<q> Where is John?\"\n",
    "text = text.lower()\n",
    "tokens = tf.cast(tokenizer.encode(text), tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a6715b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547b0e4f43f7494eaeb01f01bc4c1f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', disabled=True, layout=Layout(height='20em', width='80ch'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_32616\\3811429441.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0mtopk_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m      \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mkth_value\u001b[0m         \u001b[1;33m=\u001b[0m \u001b[0mtopk_vals\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\krist\\Documents\\llm-basics\\src\\transformer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, tokens, training, logits)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mblock\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_blocks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munembed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\krist\\Documents\\llm-basics\\src\\transformer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, x_embeds, tokens, training)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_embeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mx_embeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_embeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m         \u001b[0mx_embeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mffnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_embeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx_embeds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\krist\\Documents\\llm-basics\\src\\transformer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, x_embeds, tokens, training)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0minner_masked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_embeds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m\"<s>\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_to_idx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m             \u001b[0minner_masked\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mput_block_diag_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minner_masked\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoken_to_idx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"<s>\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[0mdk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_embeds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\krist\\Documents\\llm-basics\\src\\transformer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(inner, tokens, start_token_id)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mput_block_diag_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_token_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m     \u001b[0mis_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_token_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    345\u001b[0m     \u001b[0msegment_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m     \u001b[0mseg_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msegment_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[0mseg_j\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msegment_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1174\u001b[0m       \u001b[1;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1175\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m         \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1919\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1920\u001b[0m   \u001b[0mRaises\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1921\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0mof\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mare\u001b[0m \u001b[0mincompatible\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1922\u001b[0m   \"\"\"\n\u001b[1;32m-> 1923\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(x, y, incompatible_shape_error, name)\u001b[0m\n\u001b[0;32m   3302\u001b[0m         incompatible_shape_error)\n\u001b[0;32m   3303\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3304\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3305\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3306\u001b[1;33m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3307\u001b[0m       \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3308\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3309\u001b[0m       return equal_eager_fallback(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    ".widget-textarea textarea {\n",
    "    background-color: #2e2e2e !important;\n",
    "    color: #f1f1f1 !important;\n",
    "    border: 1px solid #555 !important;\n",
    "}\n",
    "</style>\n",
    "\"\"\"))\n",
    "\n",
    "wrapper = textwrap.TextWrapper(width=80)\n",
    "\n",
    "# create a read-only text area\n",
    "ta = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    layout=widgets.Layout(width='80ch', height='20em'),\n",
    "    disabled=True\n",
    ")\n",
    "display(ta)\n",
    "\n",
    "text = \"<s>Link \"\n",
    "#text = \"<s>John is in the house. Sarah is in the garden.<q>Where is John?\"\n",
    "#text = \"<s>The apple is green. The banana is yellow. The tomato is red.<q> What color is the banana?\"\n",
    "#text = \"<s>The apple is green. The banana is yellow. The tomato is red.<q> What color is the banana?\"\n",
    "#text = \"<s><q>Who is Midna?<a>\"\n",
    "#text = \"<s><q>Name two presidents?<a>\"\n",
    "#text = \"<s><q>name two presidents?<a>george washington (1789-1832) and john adams.<q>tell me more about john adams.<a>\"\n",
    "#text = \"<s>Fred went to school. Then he went to church. Then he went home.<q>Where did Fred go after school?<a>\"\n",
    "#text = \"<s><q>Name a famous dictator.<a>\"\n",
    "#text = \"<s><q>Give me a long answer?<a>\"\n",
    "#text = \"<s><q>Who is the vocalist of coldplay?<a>\"\n",
    "#text = \"<s>cnn\"\n",
    "#text = \"<s><q>Why was hitler bad?<a>\"\n",
    "#text = \"<s><q>Name many colors?<a>\"\n",
    "#text = \"<s><q>Name a leader?<a>\"\n",
    "#text = \"<s><q>Name an illness?<a>police poisoning<q>what is that?<a>\"\n",
    "text = text.lower()\n",
    "tokens = tf.cast(tokenizer.encode(text), tf.int32)\n",
    "\n",
    "T = 1\n",
    "k = 50\n",
    "\n",
    "\n",
    "for i in range(1024):\n",
    "    logits = model.call(tokens)[0, -1:]\n",
    "    topk_vals, _      = tf.math.top_k(logits, k=k)\n",
    "    kth_value         = topk_vals[:,-1]\n",
    "\n",
    "    logits = tf.where(logits >= kth_value, logits, tf.constant(-np.inf, logits.dtype))\n",
    "\n",
    "    idx = tf.cast(\n",
    "        tf.random.categorical(logits / T, num_samples=1),\n",
    "        tf.int32\n",
    "    ) \n",
    "    tokens = tf.concat([tokens, idx], axis=1)\n",
    "\n",
    "    text_pred = (\n",
    "        tokenizer\n",
    "        .decode(tokens)\n",
    "        .numpy()[0]\n",
    "        .decode('utf-8')\n",
    "        .replace(\"\\n\", \" \")\n",
    "    )\n",
    "    ta.value = wrapper.fill(text_pred)  # this updates in-place\n",
    "\n",
    "    if idx[0, 0] == tokenizer.token_to_idx[\"</s>\"]:# or idx[0, 0] == tokenizer.token_to_idx[\"<q>\"]:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
