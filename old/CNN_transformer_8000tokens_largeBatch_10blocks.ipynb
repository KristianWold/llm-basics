{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04010a08",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa3d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from src.tokenizer import TokenizerBPE, word_split, normalize_to_ascii\n",
    "\n",
    "import os\n",
    "import time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm.notebook import tqdm\n",
    "from src.transformer import *\n",
    "from src.data_handling import read_first_n, sample_batch\n",
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b59898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce RTX 2080 SUPER, compute capability 7.5\n"
     ]
    }
   ],
   "source": [
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7afd4e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = pkl.load(open(\"tokenizers/tokenizer_CNN16000_lowercase.pkl\", 'rb'))\n",
    "tokenizer.create_hash()\n",
    "\n",
    "random.seed(42)\n",
    "corpus_indicies = pkl.load(open('corpus/CNN_tokenized16000_lowercase.pkl', 'rb'))\n",
    "random.shuffle(corpus_indicies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f830d881",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a564402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmUpThenDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self,\n",
    "                 initial_learning_rate: float,\n",
    "                 warmup_steps: int,\n",
    "                 decay_schedule_fn: tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "        \"\"\"\n",
    "        initial_learning_rate: peak LR reached at end of warmup\n",
    "        warmup_steps:      # of steps to ramp from 0 â†’ initial_learning_rate\n",
    "        decay_schedule_fn: a tf.keras schedule to apply *after* warmup\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.initial_lr = initial_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.decay_schedule_fn = decay_schedule_fn\n",
    "\n",
    "    def __call__(self, step):\n",
    "        # Cast to float32 for safety in graph mode\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
    "\n",
    "        # compute linear warmup: lr = initial_lr * (step / warmup_steps)\n",
    "        warmup_lr = self.initial_lr * (step / warmup_steps)\n",
    "\n",
    "        # after warmup_steps, switch to decay schedule (shift step count)\n",
    "        decay_step = step - warmup_steps\n",
    "        decay_lr = self.decay_schedule_fn(decay_step)\n",
    "\n",
    "        # if step < warmup_steps, pick warmup_lr, else decay_lr\n",
    "        return tf.cond(step < warmup_steps,\n",
    "                       lambda: warmup_lr,\n",
    "                       lambda: decay_lr)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a5a33ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lr = 1e-4\n",
    "decay_steps = 20000\n",
    "decay_rate = 0.5\n",
    "decay_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_lr,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=False)\n",
    "\n",
    "warmup_steps = 1000\n",
    "lr_schedule = WarmUpThenDecay(\n",
    "    initial_learning_rate=initial_lr,\n",
    "    warmup_steps=warmup_steps,\n",
    "    decay_schedule_fn=decay_schedule)\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "max_seq_len = 128\n",
    "embed_dim = 700\n",
    "tf_blocks = 10\n",
    "heads = 10\n",
    "ff_dim = 4*embed_dim\n",
    "weight_decay = 0.01\n",
    "dropout = 0.05\n",
    "\n",
    "unembed_dims = []\n",
    "\n",
    "model = Transformer(vocab_size=tokenizer.vocab_size,\n",
    "                    max_seq_len=max_seq_len,\n",
    "                    embed_dim=embed_dim,\n",
    "                    tf_blocks=tf_blocks,\n",
    "                    heads=heads,\n",
    "                    ff_dim = ff_dim,\n",
    "                    unembed_dims=unembed_dims,\n",
    "                    lr=lr_schedule,\n",
    "                    wd = weight_decay,\n",
    "                    dropout=dropout,\n",
    "                    )\n",
    "\n",
    "losses_train = []\n",
    "losses_test = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7970a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"model_16k_tokens_largeBatch_10blocks\"\n",
    "\n",
    "\n",
    "ckpt = tf.train.Checkpoint(\n",
    "    optimizer=model.opt,\n",
    "    model=model\n",
    ")\n",
    "ckpt_manager = tf.train.CheckpointManager(\n",
    "    ckpt, \n",
    "    directory=\"checkpoints/\" + name,      # folder where ckpts are saved\n",
    "    max_to_keep=5                         # only keep 5 latest checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88b34765",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "losses_train, losses_test = pkl.load(open(\"checkpoints/losses_\" + name + \".pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6527620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 70217670\n"
     ]
    }
   ],
   "source": [
    "total_params = 0\n",
    "for var in model.parameter_list:\n",
    "    shape = var.get_shape()\n",
    "    num_params = 1\n",
    "    for dim in shape:\n",
    "        num_params *= dim\n",
    "    total_params += num_params\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "93977cd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m ax2\u001b[38;5;241m.\u001b[39mgrid(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     66\u001b[0m plt\u001b[38;5;241m.\u001b[39mtight_layout()\n\u001b[1;32m---> 67\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\pyplot.py:614\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;124;03mDisplay all open figures.\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[38;5;124;03mexplicitly there.\u001b[39;00m\n\u001b[0;32m    612\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    613\u001b[0m _warn_if_gui_out_of_main_thread()\n\u001b[1;32m--> 614\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_backend_mod()\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib_inline\\backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(close, block)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m figure_manager \u001b[38;5;129;01min\u001b[39;00m Gcf\u001b[38;5;241m.\u001b[39mget_all_fig_managers():\n\u001b[1;32m---> 90\u001b[0m         \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fetch_figure_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     show\u001b[38;5;241m.\u001b[39m_to_draw \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\IPython\\core\\display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[1;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m     publish_display_data(data\u001b[38;5;241m=\u001b[39mobj, metadata\u001b[38;5;241m=\u001b[39mmetadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 298\u001b[0m     format_dict, md_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\IPython\\core\\formatters.py:238\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[1;34m(self, obj, include, exclude)\u001b[0m\n\u001b[0;32m    236\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 238\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\IPython\\core\\formatters.py:282\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[1;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     r \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\IPython\\core\\formatters.py:402\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[0;32m    404\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\IPython\\core\\pylabtools.py:170\u001b[0m, in \u001b[0;36mprint_figure\u001b[1;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[0;32m    168\u001b[0m     FigureCanvasBase(fig)\n\u001b[1;32m--> 170\u001b[0m fig\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mprint_figure(bytes_io, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    171\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\backend_bases.py:2184\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2181\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[0;32m   2182\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[0;32m   2183\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[1;32m-> 2184\u001b[0m         result \u001b[38;5;241m=\u001b[39m print_method(\n\u001b[0;32m   2185\u001b[0m             filename,\n\u001b[0;32m   2186\u001b[0m             facecolor\u001b[38;5;241m=\u001b[39mfacecolor,\n\u001b[0;32m   2187\u001b[0m             edgecolor\u001b[38;5;241m=\u001b[39medgecolor,\n\u001b[0;32m   2188\u001b[0m             orientation\u001b[38;5;241m=\u001b[39morientation,\n\u001b[0;32m   2189\u001b[0m             bbox_inches_restore\u001b[38;5;241m=\u001b[39m_bbox_inches_restore,\n\u001b[0;32m   2190\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2191\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   2192\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\backend_bases.py:2040\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2036\u001b[0m     optional_kws \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[0;32m   2037\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2038\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_inches_restore\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m   2039\u001b[0m     skip \u001b[38;5;241m=\u001b[39m optional_kws \u001b[38;5;241m-\u001b[39m {\u001b[38;5;241m*\u001b[39minspect\u001b[38;5;241m.\u001b[39msignature(meth)\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[1;32m-> 2040\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: meth(\n\u001b[0;32m   2041\u001b[0m         \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m skip}))\n\u001b[0;32m   2042\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[0;32m   2043\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m meth\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:481\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[1;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    435\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 481\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_print_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:429\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[1;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_print_pil\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, fmt, pil_kwargs, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    425\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;124;03m    Draw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;124;03m    *pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 429\u001b[0m     \u001b[43mFigureCanvasAgg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    430\u001b[0m     mpl\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mimsave(\n\u001b[0;32m    431\u001b[0m         filename_or_obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuffer_rgba(), \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39mfmt, origin\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupper\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    432\u001b[0m         dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure\u001b[38;5;241m.\u001b[39mdpi, metadata\u001b[38;5;241m=\u001b[39mmetadata, pil_kwargs\u001b[38;5;241m=\u001b[39mpil_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:382\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;66;03m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\u001b[38;5;241m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtoolbar\n\u001b[0;32m    381\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m nullcontext()):\n\u001b[1;32m--> 382\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;66;03m# A GUI class may be need to update a window using this draw, so\u001b[39;00m\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;66;03m# don't forget to call the superclass.\u001b[39;00m\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mdraw()\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\artist.py:94\u001b[0m, in \u001b[0;36m_finalize_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer, *args, **kwargs)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(draw)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdraw_wrapper\u001b[39m(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 94\u001b[0m     result \u001b[38;5;241m=\u001b[39m draw(artist, renderer, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m renderer\u001b[38;5;241m.\u001b[39m_rasterizing:\n\u001b[0;32m     96\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstop_rasterizing()\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\artist.py:71\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\figure.py:3257\u001b[0m, in \u001b[0;36mFigure.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   3254\u001b[0m             \u001b[38;5;66;03m# ValueError can occur when resizing a window.\u001b[39;00m\n\u001b[0;32m   3256\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch\u001b[38;5;241m.\u001b[39mdraw(renderer)\n\u001b[1;32m-> 3257\u001b[0m     \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3260\u001b[0m     renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigure\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   3261\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\image.py:134\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[1;32m--> 134\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\artist.py:71\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\axes\\_base.py:3210\u001b[0m, in \u001b[0;36m_AxesBase.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m   3207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m artists_rasterized:\n\u001b[0;32m   3208\u001b[0m     _draw_rasterized(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_figure(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), artists_rasterized, renderer)\n\u001b[1;32m-> 3210\u001b[0m \u001b[43mmimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_draw_list_compositing_images\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martists\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_figure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuppressComposite\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3213\u001b[0m renderer\u001b[38;5;241m.\u001b[39mclose_group(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxes\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   3214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\image.py:134\u001b[0m, in \u001b[0;36m_draw_list_compositing_images\u001b[1;34m(renderer, parent, artists, suppress_composite)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m not_composite \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_images:\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m artists:\n\u001b[1;32m--> 134\u001b[0m         \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# Composite any adjacent images together\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     image_group \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\artist.py:71\u001b[0m, in \u001b[0;36mallow_rasterization.<locals>.draw_wrapper\u001b[1;34m(artist, renderer)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m         renderer\u001b[38;5;241m.\u001b[39mstart_filter()\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdraw\u001b[49m\u001b[43m(\u001b[49m\u001b[43martist\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrenderer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m artist\u001b[38;5;241m.\u001b[39mget_agg_filter() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\lines.py:807\u001b[0m, in \u001b[0;36mLine2D.draw\u001b[1;34m(self, renderer)\u001b[0m\n\u001b[0;32m    804\u001b[0m         gc\u001b[38;5;241m.\u001b[39mset_foreground(lc_rgba, isRGBA\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    806\u001b[0m         gc\u001b[38;5;241m.\u001b[39mset_dashes(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dash_pattern)\n\u001b[1;32m--> 807\u001b[0m         \u001b[43mrenderer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraw_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maffine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrozen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    808\u001b[0m         gc\u001b[38;5;241m.\u001b[39mrestore()\n\u001b[0;32m    810\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_marker \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_markersize \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "for i in tqdm(range(100000)):\n",
    "    indices, y_true = sample_batch(corpus_indicies[:80000], \n",
    "                                   batch_size, \n",
    "                                   tokenizer, \n",
    "                                   max_seq_len)\n",
    "    \n",
    "    weight_norm = model.get_weight_norm()\n",
    "    print(\"Weight Norm: \", weight_norm)\n",
    "    loss_train = model.train_step(indices, y_true).numpy()\n",
    "    losses_train.append(loss_train)\n",
    "    print(\"Step: \", i, \"Train Loss: \", loss_train)\n",
    "\n",
    "\n",
    "    indices, y_true = sample_batch(corpus_indicies[80000:], \n",
    "                                batch_size//4, \n",
    "                                tokenizer, \n",
    "                                max_seq_len)\n",
    "    \n",
    "    loss_test = model.evaluate(indices, y_true).numpy()\n",
    "        \n",
    "    losses_test.append(loss_test)\n",
    "\n",
    "    if (i+1) % 1000 == 0:\n",
    "        ckpt_manager.save()\n",
    "        pkl.dump([losses_train, losses_test], open(\"checkpoints/losses_\" + name + \".pkl\", 'wb'))\n",
    "\n",
    "\n",
    "    lr = model.opt.inner_optimizer._decayed_lr(tf.float32).numpy()\n",
    "    #\"\"\"\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # prepare x-axis for the last 400 steps\n",
    "    start = max(0, len(losses_train) - 1000)\n",
    "    x_zoom = np.arange(start, len(losses_train))\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 8), sharex=False)\n",
    "\n",
    "    # Top subplot: zoom on last 400 steps\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(x_zoom, losses_test[-1000:], label=\"Test Loss\")\n",
    "    ax1.plot(x_zoom, losses_train[-1000:], label=\"Train Loss\")\n",
    "\n",
    "    _min = min(losses_train[-1000:] + losses_test[-1000:])\n",
    "    _max = max(losses_train[-1000:] + losses_test[-1000:])\n",
    "    delta = _max - _min\n",
    "    #ax1.set_ylim(_min - 0.1 * delta, _max + 0.1 * delta)\n",
    "\n",
    "    ax1.set_title(\"Training Loss (Last 1000 Steps)\")\n",
    "    ax1.set_xlabel(\"Step\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Bottom subplot: full series\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(losses_test[10:], label=\"Test Loss\")\n",
    "    ax2.plot(losses_train[10:], label=\"Train Loss, lr = {:.2e}\".format(lr))\n",
    "\n",
    "    ax2.set_title(\"Training Loss (Full Series)\")\n",
    "    ax2.set_xlabel(\"Step\")\n",
    "    ax2.set_ylabel(\"Loss\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    #\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2797e257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44000\n",
      "2.295964\n"
     ]
    }
   ],
   "source": [
    "print(len(losses_train))\n",
    "print(np.mean(losses_train[43000:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58649737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(indices, merge_list):\n",
    "    indices = np.array(indices)\n",
    "    for pair, new_idx in merge_list:\n",
    "        slice = np.where(np.logical_and(indices[:-1] == pair[0],  indices[1:] == pair[1]))\n",
    "        if len(slice[0]) > 0:\n",
    "            indices[:-1][slice] = new_idx\n",
    "            indices = np.delete(indices, (slice[0]+1))\n",
    "\n",
    "    return tf.expand_dims(tf.convert_to_tensor(indices, dtype=tf.int32), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e216ce01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[359   1 479  27]], shape=(1, 4), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "text = \"it's official:\"\n",
    "text = text.lower()\n",
    "\n",
    "indices = tf.cast(tokenizer.tokenizer.tokenize(text), tf.int32)\n",
    "indices = tokenize(indices, tokenizer.merge_list)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "74a017b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it's official: \"i think it's a very important thing to do with the people of the united states and the world.\"  @highlight  the u.s. is trying to get a new job in the u.s.  @highlight  the u.s. has been working with the u.s. government since the 1960s  @highlight  the u.s. has been \r"
     ]
    }
   ],
   "source": [
    "T = 0.01\n",
    "tf.random.set_seed(43)\n",
    "for i in range(128):\n",
    "    logits = model.call(indices)[0,-1:]\n",
    "    idx = tf.cast(tf.random.categorical(logits/T, num_samples=1), tf.int32)\n",
    "    indices = tf.concat([indices, idx], axis=1)\n",
    "    text_pred = tokenizer.detokenize(indices)\n",
    "    text_pred = text_pred.numpy()[0].decode('utf-8').replace(\"\\n\", \" \")\n",
    "    print(text_pred, end='\\r', flush=True)\n",
    "    #time.sleep(0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "680d4eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[553]], shape=(1, 1), dtype=int32)\n",
      "obama said he was \"deeply concerned\" about the issue of the issue.  \"i think the president has a very strong desire to do something that is not a good thing,\" he said. \"he has a great opportunity to be able to do it for the president to do it.\"  @highlight  obama has been in office since he was\r"
     ]
    }
   ],
   "source": [
    "text = \"obama\"\n",
    "text = text.lower()\n",
    "\n",
    "indices = tf.cast(tokenizer.tokenizer.tokenize(text), tf.int32)\n",
    "indices = tokenize(indices, tokenizer.merge_list)\n",
    "print(indices)\n",
    "\n",
    "T = 0.2\n",
    "tf.random.set_seed(43)\n",
    "for i in range(128):\n",
    "    logits = model.call(indices)[0,-1:]\n",
    "    idx = tf.cast(tf.random.categorical(logits/T, num_samples=1), tf.int32)\n",
    "    indices = tf.concat([indices, idx], axis=1)\n",
    "    text_pred = tokenizer.detokenize(indices)\n",
    "    text_pred = text_pred.numpy()[0].decode('utf-8').replace(\"\\n\", \" \")\n",
    "    print(text_pred, end='\\r', flush=True)\n",
    "    #time.sleep(0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5256b899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[10094]], shape=(1, 1), dtype=int32)\n",
      "trump is a \"very nice guy\" who has a great relationship with his wife and a daughter.  \"i think it's a great deal for him to be a part of the world,\" he said. \"he's a great person and he's a great person. he's a great person.\"  @highlight  the former world no. 1 has been a regular\r"
     ]
    }
   ],
   "source": [
    "text = \"trump\"\n",
    "text = text.lower()\n",
    "\n",
    "indices = tf.cast(tokenizer.tokenizer.tokenize(text), tf.int32)\n",
    "indices = tokenize(indices, tokenizer.merge_list)\n",
    "print(indices)\n",
    "\n",
    "T = 0.3\n",
    "tf.random.set_seed(43)\n",
    "for i in range(128):\n",
    "    logits = model.call(indices)[0,-1:]\n",
    "    idx = tf.cast(tf.random.categorical(logits/T, num_samples=1), tf.int32)\n",
    "    indices = tf.concat([indices, idx], axis=1)\n",
    "    text_pred = tokenizer.detokenize(indices)\n",
    "    text_pred = text_pred.numpy()[0].decode('utf-8').replace(\"\\n\", \" \")\n",
    "    print(text_pred, end='\\r', flush=True)\n",
    "    #time.sleep(0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e77fb01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def cosine_similarity(embed_a, embed_b):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two vectors.\n",
    "    \"\"\"\n",
    "    embed_b_T = tf.transpose(embed_b)\n",
    "    dot_product = embed_a@embed_b_T\n",
    "    \n",
    "    norm_a = tf.linalg.norm(embed_a, axis=1, keepdims=True)\n",
    "    norm_b = tf.linalg.norm(embed_b_T, axis=0, keepdims=True)\n",
    "\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "\n",
    "def cluster(X, n_clusters, normalize=True):\n",
    "    if normalize:\n",
    "        X = X/np.linalg.norm(X, axis=1, keepdims=True)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n",
    "    inertia = kmeans.inertia_\n",
    "    labels = kmeans.labels_\n",
    "    clusters = kmeans.cluster_centers_\n",
    "\n",
    "    return inertia, labels, clusters\n",
    "\n",
    "\n",
    "class EmbeddingClustering:\n",
    "    def __init__(self, tokenizer, n_clusters=10):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_clusters = n_clusters\n",
    "\n",
    "    def fit(self, word_embed, normalize=True):\n",
    "        inertia, labels, clusters = cluster(word_embed, self.n_clusters, normalize)\n",
    "        self.word_embed = word_embed\n",
    "        self.inertia = inertia\n",
    "        self.labels = labels\n",
    "        self.clusters = tf.convert_to_tensor(clusters, dtype=tf.float32)\n",
    "\n",
    "        cos_sim = cosine_similarity(self.clusters, word_embed, normalize)\n",
    "        self.idx_list =  tf.argsort(cos_sim, axis=-1, direction='DESCENDING', stable=False, name=None)\n",
    "\n",
    "    def print_clusters(self, n_words=10):\n",
    "        for idx in self.idx_list:\n",
    "            for i in idx[:n_words]:\n",
    "                word = self.tokenizer.detokenize(tf.expand_dims(tf.cast(i, tf.int32), axis=0))\n",
    "                word = word.numpy().decode('utf-8')\n",
    "                print(word)\n",
    "            print(\"\\n\")\n",
    "\n",
    "\n",
    "def cosine_similarity(embed_a, embed_b, normalize=True):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two vectors.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        embed_a = tf.nn.l2_normalize(embed_a, axis=1)\n",
    "        embed_b = tf.nn.l2_normalize(embed_b, axis=1)\n",
    "    dot_product = embed_a@tf.transpose(embed_b)\n",
    "\n",
    "\n",
    "    return dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48583bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "afgh\n",
      "infrast\n",
      "fahren\n",
      "attem\n",
      "riculum\n",
      "tournam\n",
      "satell\n",
      "negot\n",
      "onsored\n",
      "secutive\n",
      "\n",
      "\n",
      "egypt's\n",
      "ukraine's\n",
      "indonesia's\n",
      "turkey's\n",
      "libya's\n",
      "lebanon's\n",
      "cuba's\n",
      "somalia's\n",
      "russia's\n",
      "fifa's\n",
      "\n",
      "\n",
      "uguay\n",
      "afgh\n",
      "oppon\n",
      "claugh\n",
      "avez\n",
      "infrast\n",
      "espion\n",
      "satell\n",
      "provin\n",
      "secutive\n",
      "\n",
      "\n",
      "cindy\n",
      "deborah\n",
      "christina\n",
      "candi\n",
      "neth\n",
      "melissa\n",
      "denise\n",
      "kanye\n",
      "debbie\n",
      "patricia\n",
      "\n",
      "\n",
      "athle\n",
      "fahren\n",
      "enthus\n",
      "theless\n",
      "avez\n",
      "ultane\n",
      "glary\n",
      "includ\n",
      "emerg\n",
      "0s\n",
      "\n",
      "\n",
      "gior\n",
      "dmit\n",
      "syl\n",
      "stef\n",
      "fitz\n",
      "gian\n",
      "christop\n",
      "benjam\n",
      "charlot\n",
      "laim\n",
      "\n",
      "\n",
      "toug\n",
      "enz\n",
      "juvent\n",
      "koso\n",
      "guang\n",
      "benjam\n",
      "cinc\n",
      "unh\n",
      "abun\n",
      "sni\n",
      "\n",
      "\n",
      "afgh\n",
      "indust\n",
      "oppon\n",
      "athle\n",
      "helicop\n",
      "magaz\n",
      "entreprene\n",
      "fahren\n",
      "environ\n",
      "invol\n",
      "\n",
      "\n",
      "climb\n",
      "pull\n",
      "throw\n",
      "walk\n",
      "bounce\n",
      "blow\n",
      "break\n",
      "shake\n",
      "fill\n",
      "catch\n",
      "\n",
      "\n",
      "kathle\n",
      "tobac\n",
      "afgh\n",
      "disast\n",
      "oppon\n",
      "propof\n",
      "proble\n",
      "toug\n",
      "salmone\n",
      "mingham\n",
      "\n",
      "\n",
      "somebody\n",
      "everybody\n",
      "anybody\n",
      "anything\n",
      "everything\n",
      "everyone\n",
      "someone's\n",
      "someone\n",
      "what's\n",
      "anyone\n",
      "\n",
      "\n",
      "perpe\n",
      "territor\n",
      "avez\n",
      "charac\n",
      "possib\n",
      "performan\n",
      "secutive\n",
      "determ\n",
      "subsequ\n",
      "uguay\n",
      "\n",
      "\n",
      "avez\n",
      "onsored\n",
      "theless\n",
      "afgh\n",
      "satell\n",
      "includ\n",
      "assage\n",
      "surpris\n",
      "signific\n",
      "provin\n",
      "\n",
      "\n",
      "behave\n",
      "contribute\n",
      "interact\n",
      "thrive\n",
      "disappear\n",
      "endure\n",
      "participate\n",
      "survive\n",
      "react\n",
      "emerge\n",
      "\n",
      "\n",
      "infrast\n",
      "afgh\n",
      "onsored\n",
      "craigs\n",
      "signific\n",
      "surpris\n",
      "propof\n",
      "yugo\n",
      "theless\n",
      "unh\n",
      "\n",
      "\n",
      "injunction\n",
      "inspe\n",
      "statute\n",
      "subcommittee\n",
      "directive\n",
      "fahren\n",
      "environ\n",
      "ultane\n",
      "glary\n",
      "athle\n",
      "\n",
      "\n",
      "adjac\n",
      "recomm\n",
      "preh\n",
      "discrimin\n",
      "expl\n",
      "ingred\n",
      "appl\n",
      "desc\n",
      "kidn\n",
      "erad\n",
      "\n",
      "\n",
      "unh\n",
      "toug\n",
      "espion\n",
      "liby\n",
      "indust\n",
      "ailand\n",
      "athle\n",
      "occas\n",
      "propof\n",
      "weren\n",
      "\n",
      "\n",
      "fahren\n",
      "afgh\n",
      "indust\n",
      "onsored\n",
      "assage\n",
      "avez\n",
      "infrast\n",
      "helicop\n",
      "surger\n",
      "riculum\n",
      "\n",
      "\n",
      "reasonably\n",
      "substantially\n",
      "utterly\n",
      "overly\n",
      "economically\n",
      "fundamentally\n",
      "unfairly\n",
      "socially\n",
      "dramatically\n",
      "adequately\n",
      "\n",
      "\n",
      "organization's\n",
      "department's\n",
      "school's\n",
      "band's\n",
      "hotel's\n",
      "fbi's\n",
      "agency's\n",
      "regime's\n",
      "military's\n",
      "game's\n",
      "\n",
      "\n",
      "unh\n",
      "infrast\n",
      "includ\n",
      "helicop\n",
      "craigs\n",
      "riculum\n",
      "schalke\n",
      "theless\n",
      "portugu\n",
      "assage\n",
      "\n",
      "\n",
      "good\n",
      "big\n",
      "very\n",
      "great\n",
      "huge\n",
      "tough\n",
      "pretty\n",
      "sweet\n",
      "small\n",
      "simple\n",
      "\n",
      "\n",
      "characterized\n",
      "attributed\n",
      "dealt\n",
      "portrayed\n",
      "interpreted\n",
      "spawned\n",
      "crafted\n",
      "depicted\n",
      "influenced\n",
      "relied\n",
      "\n",
      "\n",
      "thankful\n",
      "wondered\n",
      "pleased\n",
      "hoped\n",
      "doubted\n",
      "excited\n",
      "wished\n",
      "convinced\n",
      "delighted\n",
      "thrilled\n",
      "\n",
      "\n",
      "give\n",
      "tell\n",
      "ask\n",
      "make\n",
      "bring\n",
      "want\n",
      "teach\n",
      "see\n",
      "learn\n",
      "do\n",
      "\n",
      "\n",
      "afgh\n",
      "tournam\n",
      "fahren\n",
      "infrast\n",
      "oppon\n",
      "palestin\n",
      "indust\n",
      "mingham\n",
      "usalem\n",
      "includ\n",
      "\n",
      "\n",
      "denounced\n",
      "reiterated\n",
      "applauded\n",
      "defended\n",
      "thanked\n",
      "affirmed\n",
      "praised\n",
      "persuaded\n",
      "avez\n",
      "emphasized\n",
      "\n",
      "\n",
      "most\n",
      "more\n",
      "three\n",
      "last\n",
      "many\n",
      "all\n",
      "two\n",
      "some\n",
      "much\n",
      "four\n",
      "\n",
      "\n",
      "hum\n",
      "ou\n",
      "ag\n",
      "resur\n",
      "cou\n",
      "fl\n",
      "tum\n",
      "dis\n",
      "ere\n",
      "cu\n",
      "\n",
      "\n",
      "lt\n",
      "maj\n",
      "sgt\n",
      "capt\n",
      "dr\n",
      "adm\n",
      "mr\n",
      "brig\n",
      "sen\n",
      "rev\n",
      "\n",
      "\n",
      "700\n",
      "250\n",
      "750\n",
      "220\n",
      "400\n",
      "550\n",
      "600\n",
      "650\n",
      "150\n",
      "240\n",
      "\n",
      "\n",
      "algerian\n",
      "tunisian\n",
      "yemeni\n",
      "turkish\n",
      "indonesian\n",
      "pakistani\n",
      "colombian\n",
      "saudi\n",
      "philippine\n",
      "egyptian\n",
      "\n",
      "\n",
      "empathy\n",
      "adjac\n",
      "assage\n",
      "entreprene\n",
      "provin\n",
      "afgh\n",
      "infrast\n",
      "secutive\n",
      "negot\n",
      "glary\n",
      "\n",
      "\n",
      "accusations\n",
      "revelations\n",
      "allegations\n",
      "doubts\n",
      "decisions\n",
      "objections\n",
      "lawsuits\n",
      "complaints\n",
      "assurances\n",
      "suspicions\n",
      "\n",
      "\n",
      "trips\n",
      "bids\n",
      "agreements\n",
      "repairs\n",
      "transactions\n",
      "contracts\n",
      "visas\n",
      "arrangements\n",
      "transfers\n",
      "inspe\n",
      "\n",
      "\n",
      "mar\n",
      "sha\n",
      "mu\n",
      "mo\n",
      "cal\n",
      "cor\n",
      "gian\n",
      "bu\n",
      "mag\n",
      "sar\n",
      "\n",
      "\n",
      "certainly\n",
      "undoubtedly\n",
      "definitely\n",
      "rarely\n",
      "inevitably\n",
      "surely\n",
      "hardly\n",
      "occasionally\n",
      "likewise\n",
      "nevertheless\n",
      "\n",
      "\n",
      "rocked\n",
      "devastated\n",
      "hampered\n",
      "ravaged\n",
      "overwhelmed\n",
      "plagued\n",
      "marred\n",
      "touched\n",
      "angered\n",
      "impacted\n",
      "\n",
      "\n",
      "thrilling\n",
      "fascinating\n",
      "wonderful\n",
      "exciting\n",
      "memorable\n",
      "daunting\n",
      "magical\n",
      "magnificent\n",
      "delicious\n",
      "frightening\n",
      "\n",
      "\n",
      "slayings\n",
      "kidnappings\n",
      "shootings\n",
      "afgh\n",
      "rapes\n",
      "killings\n",
      "bombings\n",
      "homicides\n",
      "infrast\n",
      "grenades\n",
      "\n",
      "\n",
      "portrait\n",
      "exhibition\n",
      "novels\n",
      "essay\n",
      "sculpture\n",
      "costume\n",
      "biography\n",
      "album\n",
      "photograph\n",
      "genre\n",
      "\n",
      "\n",
      "evaluate\n",
      "minimize\n",
      "assure\n",
      "eliminate\n",
      "educate\n",
      "convince\n",
      "avez\n",
      "emphasize\n",
      "uphold\n",
      "emerg\n",
      "\n",
      "\n",
      "steven\n",
      "darren\n",
      "nicolas\n",
      "jason\n",
      "brett\n",
      "larry\n",
      "colin\n",
      "doug\n",
      "timothy\n",
      "bobby\n",
      "\n",
      "\n",
      "altercation\n",
      "confrontation\n",
      "conversation\n",
      "standoff\n",
      "briefing\n",
      "announcement\n",
      "interview\n",
      "ordeal\n",
      "collision\n",
      "discussion\n",
      "\n",
      "\n",
      "subur\n",
      "proble\n",
      "tobac\n",
      "disast\n",
      "indust\n",
      "expl\n",
      "eleph\n",
      "usalem\n",
      "espion\n",
      "genu\n",
      "\n",
      "\n",
      "afgh\n",
      "barcelon\n",
      "occas\n",
      "oppon\n",
      "athle\n",
      "unh\n",
      "mingham\n",
      "toug\n",
      "strengthe\n",
      "tournam\n",
      "\n",
      "\n",
      "tends\n",
      "takes\n",
      "pulls\n",
      "teaches\n",
      "brings\n",
      "attracts\n",
      "extends\n",
      "relies\n",
      "delivers\n",
      "gets\n",
      "\n",
      "\n",
      "when\n",
      "at\n",
      "after\n",
      "during\n",
      "by\n",
      "from\n",
      "before\n",
      "until\n",
      "to\n",
      "in\n",
      "\n",
      "\n",
      "indust\n",
      "controver\n",
      "ingred\n",
      "oppon\n",
      "consid\n",
      "athle\n",
      "tournam\n",
      "glary\n",
      "expl\n",
      "usalem\n",
      "\n",
      "\n",
      "infrast\n",
      "afgh\n",
      "ultane\n",
      "glary\n",
      "secutive\n",
      "palestin\n",
      "portra\n",
      "avez\n",
      "helicop\n",
      "provin\n",
      "\n",
      "\n",
      "2006\n",
      "1997\n",
      "2005\n",
      "2003\n",
      "1999\n",
      "2004\n",
      "1995\n",
      "1991\n",
      "1993\n",
      "1996\n",
      "\n",
      "\n",
      "situation\n",
      "position\n",
      "challenge\n",
      "approach\n",
      "role\n",
      "decision\n",
      "effort\n",
      "response\n",
      "strategy\n",
      "threat\n",
      "\n",
      "\n",
      "taking\n",
      "trying\n",
      "getting\n",
      "turning\n",
      "attempting\n",
      "giving\n",
      "wanting\n",
      "returning\n",
      "preparing\n",
      "going\n",
      "\n",
      "\n",
      "tour's\n",
      "sport's\n",
      "ona's\n",
      "fifa's\n",
      "side's\n",
      "infrast\n",
      "pga\n",
      "football's\n",
      "game's\n",
      "season's\n",
      "\n",
      "\n",
      "haqqani\n",
      "isaf\n",
      "environ\n",
      "infrast\n",
      "experi\n",
      "assage\n",
      "glary\n",
      "aqap\n",
      "fundam\n",
      "shabaab\n",
      "\n",
      "\n",
      "drove\n",
      "popped\n",
      "walked\n",
      "poured\n",
      "jumped\n",
      "raced\n",
      "bounced\n",
      "chased\n",
      "grabbed\n",
      "climbed\n",
      "\n",
      "\n",
      "inspir\n",
      "modif\n",
      "excl\n",
      "colleg\n",
      "acknowled\n",
      "reim\n",
      "explan\n",
      "preh\n",
      "apprec\n",
      "scrut\n",
      "\n",
      "\n",
      "indiana\n",
      "delaware\n",
      "oklahoma\n",
      "kansas\n",
      "idaho\n",
      "utah\n",
      "arkansas\n",
      "tennessee\n",
      "connecticut\n",
      "wyoming\n",
      "\n",
      "\n",
      "ninth\n",
      "12th\n",
      "eighth\n",
      "11th\n",
      "10th\n",
      "sixth\n",
      "15th\n",
      "16th\n",
      "seventh\n",
      "19th\n",
      "\n",
      "\n",
      "environ\n",
      "espion\n",
      "usalem\n",
      "athle\n",
      "hrir\n",
      "palestin\n",
      "attem\n",
      "apova\n",
      "strengthe\n",
      "assage\n",
      "\n",
      "\n",
      "commentator\n",
      "therapist\n",
      "psychologist\n",
      "grapher\n",
      "counselor\n",
      "historian\n",
      "consultant\n",
      "strategist\n",
      "filmmaker\n",
      "songwriter\n",
      "\n",
      "\n",
      "stating\n",
      "accusing\n",
      "insisting\n",
      "condemning\n",
      "acknowledging\n",
      "describing\n",
      "urging\n",
      "suggesting\n",
      "explaining\n",
      "recognizing\n",
      "\n",
      "\n",
      "preserving\n",
      "capturing\n",
      "letting\n",
      "delivering\n",
      "ignoring\n",
      "reducing\n",
      "enforcing\n",
      "destroying\n",
      "restoring\n",
      "gaining\n",
      "\n",
      "\n",
      "emails\n",
      "mails\n",
      "messages\n",
      "photographs\n",
      "tapes\n",
      "videos\n",
      "recordings\n",
      "responses\n",
      "inquiries\n",
      "portraits\n",
      "\n",
      "\n",
      "raided\n",
      "assaulted\n",
      "invaded\n",
      "abducted\n",
      "expelled\n",
      "escorted\n",
      "deported\n",
      "occas\n",
      "theless\n",
      "attem\n",
      "\n",
      "\n",
      "fundraiser\n",
      "craigs\n",
      "mingham\n",
      "includ\n",
      "invol\n",
      "eday\n",
      "entreprene\n",
      "riculum\n",
      "munic\n",
      "massachu\n",
      "\n",
      "\n",
      "palestin\n",
      "afgh\n",
      "attem\n",
      "usalem\n",
      "tournam\n",
      "environ\n",
      "helicop\n",
      "infrast\n",
      "fahren\n",
      "zimbab\n",
      "\n",
      "\n",
      "crowned\n",
      "includ\n",
      "infrast\n",
      "surpris\n",
      "assage\n",
      "onsored\n",
      "riculum\n",
      "emerg\n",
      "fahren\n",
      "resul\n",
      "\n",
      "\n",
      "monday\n",
      "tuesday\n",
      "wednesday\n",
      "saturday\n",
      "sunday\n",
      "weeks\n",
      "thursday\n",
      "friday\n",
      "months\n",
      "days\n",
      "\n",
      "\n",
      "a\n",
      "the\n",
      "his\n",
      "it\n",
      "their\n",
      "an\n",
      "is\n",
      "has\n",
      "was\n",
      "he\n",
      "\n",
      "\n",
      "riculum\n",
      "mingham\n",
      "palestin\n",
      "eday\n",
      "afgh\n",
      "prede\n",
      "oppon\n",
      "attem\n",
      "environ\n",
      "theless\n",
      "\n",
      "\n",
      "morocco\n",
      "tunisia\n",
      "guatemala\n",
      "finland\n",
      "algeria\n",
      "denmark\n",
      "peru\n",
      "romania\n",
      "bolivia\n",
      "mali\n",
      "\n",
      "\n",
      "retailers\n",
      "gamers\n",
      "designers\n",
      "enthusiasts\n",
      "publishers\n",
      "filmmakers\n",
      "developers\n",
      "chefs\n",
      "entrepreneurs\n",
      "manufacturers\n",
      "\n",
      "\n",
      "integral\n",
      "cheaper\n",
      "tougher\n",
      "smarter\n",
      "healthier\n",
      "shorter\n",
      "slower\n",
      "deeper\n",
      "fewer\n",
      "accustomed\n",
      "\n",
      "\n",
      "rugged\n",
      "mountainous\n",
      "colorful\n",
      "vibrant\n",
      "sprawling\n",
      "dense\n",
      "tiny\n",
      "distinctive\n",
      "northeastern\n",
      "medieval\n",
      "\n",
      "\n",
      "christop\n",
      "toug\n",
      "benjam\n",
      "tobac\n",
      "disast\n",
      "genu\n",
      "proble\n",
      "hurrican\n",
      "usalem\n",
      "assaul\n",
      "\n",
      "\n",
      "argued\n",
      "insisted\n",
      "acknowledged\n",
      "explained\n",
      "joked\n",
      "cautioned\n",
      "argues\n",
      "conceded\n",
      "asserted\n",
      "warned\n",
      "\n",
      "\n",
      "objectives\n",
      "realities\n",
      "adjac\n",
      "strategies\n",
      "initiatives\n",
      "sched\n",
      "inspe\n",
      "riculum\n",
      "afgh\n",
      "norms\n",
      "\n",
      "\n",
      "evaluation\n",
      "rehabilitation\n",
      "preservation\n",
      "inspections\n",
      "supervision\n",
      "examination\n",
      "inspection\n",
      "infrast\n",
      "claugh\n",
      "deployment\n",
      "\n",
      "\n",
      "consid\n",
      "benjam\n",
      "ingred\n",
      "controver\n",
      "determ\n",
      "theless\n",
      "cuis\n",
      "genu\n",
      "indust\n",
      "volunt\n",
      "\n",
      "\n",
      "secutive\n",
      "inspe\n",
      "adjac\n",
      "sched\n",
      "likelihood\n",
      "reminder\n",
      "dort\n",
      "shakespe\n",
      "ultane\n",
      "glary\n",
      "\n",
      "\n",
      "resu\n",
      "resur\n",
      "stub\n",
      "chry\n",
      "stair\n",
      "satis\n",
      "electro\n",
      "subur\n",
      "anthro\n",
      "reci\n",
      "\n",
      "\n",
      "afgh\n",
      "particles\n",
      "fahren\n",
      "riculum\n",
      "artific\n",
      "infrast\n",
      "secutive\n",
      "firs\n",
      "glary\n",
      "indust\n",
      "\n",
      "\n",
      "sectors\n",
      "mosques\n",
      "venues\n",
      "museums\n",
      "embassies\n",
      "regions\n",
      "departments\n",
      "institutions\n",
      "destinations\n",
      "districts\n",
      "\n",
      "\n",
      "investigators\n",
      "officials\n",
      "employees\n",
      "rescuers\n",
      "officers\n",
      "residents\n",
      "commanders\n",
      "lawyers\n",
      "attorneys\n",
      "staffers\n",
      "\n",
      "\n",
      "contagious\n",
      "respiratory\n",
      "diarrhea\n",
      "onsored\n",
      "propof\n",
      "infrast\n",
      "avez\n",
      "afgh\n",
      "yugo\n",
      "signific\n",
      "\n",
      "\n",
      "27\n",
      "25\n",
      "28\n",
      "26\n",
      "29\n",
      "40\n",
      "34\n",
      "33\n",
      "22\n",
      "35\n",
      "\n",
      "\n",
      "technological\n",
      "renewable\n",
      "atmospheric\n",
      "cognitive\n",
      "greenhouse\n",
      "nasa's\n",
      "silicon\n",
      "computing\n",
      "infrast\n",
      "solar\n",
      "\n",
      "\n",
      "convin\n",
      "applau\n",
      "injun\n",
      "presu\n",
      "refle\n",
      "introdu\n",
      "reminis\n",
      "produ\n",
      "photograp\n",
      "redu\n",
      "\n",
      "\n",
      "gr\n",
      "l\n",
      "r\n",
      "re\n",
      "d\n",
      "co\n",
      "tr\n",
      "-\n",
      "dis\n",
      "sc\n",
      "\n",
      "\n",
      "sunnis\n",
      "afgh\n",
      "shiites\n",
      "saudis\n",
      "syrians\n",
      "fahren\n",
      "bians\n",
      "mexicans\n",
      "strengthe\n",
      "egyptians\n",
      "\n",
      "\n",
      "submitted\n",
      "provided\n",
      "delivered\n",
      "brought\n",
      "presented\n",
      "bought\n",
      "issued\n",
      "conducted\n",
      "sent\n",
      "offered\n",
      "\n",
      "\n",
      "baby's\n",
      "friend's\n",
      "newborn\n",
      "underage\n",
      "daughter's\n",
      "wife's\n",
      "parents'\n",
      "brother's\n",
      "couple's\n",
      "child's\n",
      "\n",
      "\n",
      "secutive\n",
      "quarterfinals\n",
      "riculum\n",
      "assage\n",
      "afgh\n",
      "infrast\n",
      "inspe\n",
      "sched\n",
      "includ\n",
      "perpe\n",
      "\n",
      "\n",
      "revenues\n",
      "loans\n",
      "inflation\n",
      "gdp\n",
      "subsidies\n",
      "debt\n",
      "bailout\n",
      "salaries\n",
      "productivity\n",
      "deficits\n",
      "\n",
      "\n",
      "i'll\n",
      "we'll\n",
      "we'd\n",
      "i'd\n",
      "won't\n",
      "you'll\n",
      "you'd\n",
      "it'll\n",
      "wouldn't\n",
      "they'll\n",
      "\n",
      "\n",
      "reminds\n",
      "encourages\n",
      "promotes\n",
      "attracts\n",
      "contains\n",
      "teaches\n",
      "recognizes\n",
      "tells\n",
      "welcomes\n",
      "creates\n",
      "\n",
      "\n",
      "social\n",
      "federal\n",
      "national\n",
      "civil\n",
      "foreign\n",
      "political\n",
      "public\n",
      "health\n",
      "government\n",
      "prime\n",
      "\n",
      "\n",
      "theless\n",
      "mingham\n",
      "infrast\n",
      "palestin\n",
      "perpe\n",
      "ailand\n",
      "riculum\n",
      "drawal\n",
      "includ\n",
      "tournam\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_embed = model.word_embed\n",
    "embedding_clustering = EmbeddingClustering(tokenizer, n_clusters=100)\n",
    "embedding_clustering.fit(word_embed, normalize=True)\n",
    "embedding_clustering.print_clusters(n_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc968e2d",
   "metadata": {},
   "source": [
    "# Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0de0e293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1602]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([[3512]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([[5393]], shape=(1, 1), dtype=int32)\n",
      "netanyahu\n",
      "russia\n",
      "israel\n",
      "hamas\n",
      "israelis\n",
      "jerusalem\n",
      "tehran\n",
      "kiev\n",
      "gaza\n",
      "palestinians\n",
      "democr\n",
      "beirut\n",
      "azer\n",
      "syria\n",
      "egypt\n",
      "idf\n",
      "iran\n",
      "britain\n",
      "palestinian\n",
      "abbas\n",
      "tunisia\n",
      "alger\n",
      "lebanon\n",
      "perpe\n",
      "israeli\n",
      "davos\n",
      "brahim\n",
      "controver\n",
      "hezbollah\n",
      "hagel\n",
      "jevich\n",
      "norway\n",
      "fah\n",
      "guinea\n",
      "khamenei\n",
      "cuba\n",
      "anbar\n",
      "utt\n",
      "khamene\n",
      "hezbol\n",
      "weren\n",
      "canada\n",
      "sunnis\n",
      "dipl\n",
      "stoke\n",
      "lavrov\n",
      "aviv\n",
      "karzai\n",
      "israel's\n",
      "arct\n",
      "cambodia\n",
      "zuckerberg\n",
      "yanukov\n",
      "yad\n",
      "ukraine\n",
      "pakistan\n",
      "carney\n",
      "netherlands\n",
      "cairo\n",
      "lieberman\n",
      "panetta\n",
      "homs\n",
      "zawah\n",
      "austria\n",
      "espion\n",
      "wawrink\n",
      "vinc\n",
      "libertar\n",
      "libya\n",
      "poland\n",
      "indonesia\n",
      "liby\n",
      "merkel\n",
      "pyongyang\n",
      "tik\n",
      "airstrikes\n",
      "ibrahimovic\n",
      "abe\n",
      "iran's\n",
      "yugo\n",
      "kass\n",
      "mosul\n",
      "galax\n",
      "yemen\n",
      "scotland\n",
      "settlements\n",
      "sudan\n",
      "nuri\n",
      "niger\n",
      "palestine\n",
      "tsvangira\n",
      "ieval\n",
      "iaea\n",
      "denmark\n",
      "hmer\n",
      "tahrir\n",
      "nusra\n",
      "sarkoz\n",
      "ukrain\n",
      "bolivia\n"
     ]
    }
   ],
   "source": [
    "word_embed = model.word_embed\n",
    "\n",
    "text = \"russia\"\n",
    "text = text.lower()\n",
    "\n",
    "idx = tf.cast(tokenizer.tokenizer.tokenize(text), tf.int32)\n",
    "idx = tokenize(idx, tokenizer.merge_list)\n",
    "print(idx)\n",
    "embed1 = tf.expand_dims(word_embed[idx[0][0]], axis=0)\n",
    "\n",
    "\n",
    "text = \"putin\"\n",
    "text = text.lower()\n",
    "\n",
    "idx = tf.cast(tokenizer.tokenizer.tokenize(text), tf.int32)\n",
    "idx = tokenize(idx, tokenizer.merge_list)\n",
    "print(idx)\n",
    "embed2 = tf.expand_dims(word_embed[idx[0][0]], axis=0)\n",
    "\n",
    "text = \"netanyahu\"\n",
    "text = text.lower()\n",
    "\n",
    "idx = tf.cast(tokenizer.tokenizer.tokenize(text), tf.int32)\n",
    "idx = tokenize(idx, tokenizer.merge_list)\n",
    "print(idx)\n",
    "embed3 = tf.expand_dims(word_embed[idx[0][0]], axis=0)\n",
    "\n",
    "embed = embed1 - embed2 + embed3\n",
    "\n",
    "cosine_sim = cosine_similarity(embed, word_embed, normalize=False)\n",
    "idx = tf.argsort(cosine_sim, axis=-1, \n",
    "                 direction='DESCENDING',\n",
    "                 #direction='ASCENDING', \n",
    "                 stable=False, name=None)[0]\n",
    "\n",
    "for i in idx[:100]:\n",
    "    i = tf.expand_dims(i, axis=0)\n",
    "    print(tokenizer.detokenize(i).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "54734624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1617]], shape=(1, 1), dtype=int32)\n",
      "israel\n",
      "gaza\n",
      "hamas\n",
      "netanyahu\n",
      "israel's\n",
      "israeli\n",
      "jerusalem\n",
      "israelis\n",
      "idf\n",
      "lebanon\n",
      "palestinians\n",
      "iran\n",
      "egypt\n",
      "palestinian\n",
      "palestine\n",
      "tehran\n",
      "hezbollah\n",
      "syria\n",
      "cairo\n",
      "pakistan\n",
      "bolivia\n",
      "iranians\n",
      "iraq\n",
      "sinai\n",
      "turkey\n",
      "britain\n",
      "venezuela\n",
      "russia\n",
      "ukraine\n",
      "tunnels\n",
      "egyptian\n",
      "misrata\n",
      "norway\n",
      "sudan\n",
      "cuba\n",
      "pyongyang\n",
      "egypt's\n",
      "canada\n",
      "moscow\n",
      "beirut\n",
      "nigeria\n",
      "germany\n",
      "kenya\n",
      "allah\n",
      "sarkoz\n",
      "libya\n",
      "travolta\n",
      "japan\n",
      "nusra\n",
      "assad's\n",
      "aq\n",
      "mubarak\n",
      "isis\n",
      "tunisia\n",
      "france\n",
      "greece\n",
      "aviv\n",
      "1967\n",
      "ahmadinejad\n",
      "damascus\n",
      "jewish\n",
      "myanmar\n",
      "syrians\n",
      "abbas\n",
      "iranian\n",
      "isaf\n",
      "khamenei\n",
      "merkel\n",
      "baghdad\n",
      "croatia\n",
      "kiev\n",
      "lebanese\n",
      "sudan's\n",
      "tibet\n",
      "spain\n",
      "iran's\n",
      "afghanistan\n",
      "chile\n",
      "rouhani\n",
      "america\n",
      "annan\n",
      "algeria\n",
      "kurdistan\n",
      "turkish\n",
      "ireland\n",
      "khamene\n",
      "aleppo\n",
      "hani\n",
      "jews\n",
      "sweden\n",
      "egyptians\n",
      "colombia\n",
      "morsy\n",
      "nato\n",
      "awlaki\n",
      "gbagbo\n",
      "yemeni\n",
      "fah\n",
      "burma\n",
      "gadhafi's\n"
     ]
    }
   ],
   "source": [
    "word_embed = model.word_embed\n",
    "\n",
    "text = \"israel\"\n",
    "text = text.lower()\n",
    "\n",
    "idx = tf.cast(tokenizer.tokenizer.tokenize(text), tf.int32)\n",
    "idx = tokenize(idx, tokenizer.merge_list)\n",
    "print(idx)\n",
    "embed = tf.expand_dims(word_embed[idx[0][0]], axis=0)\n",
    "\n",
    "cosine_sim = embed@tf.transpose(word_embed)\n",
    "idx = tf.argsort(cosine_sim, axis=-1, \n",
    "                 direction='DESCENDING',\n",
    "                 #direction='ASCENDING', \n",
    "                 stable=False, name=None)[0]\n",
    "\n",
    "for i in idx[:100]:\n",
    "    i = tf.expand_dims(i, axis=0)\n",
    "    print(tokenizer.detokenize(i).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff23330f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[5393]], shape=(1, 1), dtype=int32)\n",
      "netanyahu\n",
      "abulary\n",
      "hagel\n",
      "maduro\n",
      "espion\n",
      "yingluck\n",
      "onsored\n",
      "nandez\n",
      "saleh\n",
      "natur\n",
      "ailand\n",
      "hezbol\n",
      "panetta\n",
      "biden\n",
      "shinse\n",
      "kerry\n",
      "gibbs\n",
      "sarkozy\n",
      "fundam\n",
      "hift\n",
      "patro\n",
      "signific\n",
      "anonymity\n",
      "putin\n",
      "mugabe\n",
      "lades\n",
      "boehner\n",
      "pelosi\n",
      "medvedev\n",
      "ahmadinejad\n",
      "warri\n",
      "thaksin\n",
      "landrieu\n",
      "shaba\n",
      "gbag\n",
      "accust\n",
      "charac\n",
      "fahren\n",
      "liby\n",
      "peninsu\n",
      "helicop\n",
      "zuma\n",
      "traged\n",
      "portugu\n",
      "morsy\n",
      "publ\n",
      "enjo\n",
      "ilight\n",
      "abbas\n",
      "erdogan\n",
      "ieval\n",
      "bachmann\n",
      "yanukovych\n",
      "leep\n",
      "confir\n",
      "rodrigue\n",
      "secutive\n",
      "provin\n",
      "mccain's\n",
      "moil\n",
      "subsequ\n",
      "abled\n",
      "juvent\n",
      "o'ne\n",
      "guardiola\n",
      "lieberman\n",
      "karzai\n",
      "catastro\n",
      "ouatt\n",
      "zardari\n",
      "possib\n",
      "toug\n",
      "theless\n",
      "burma\n",
      "carney\n",
      "ricul\n",
      "zhok\n",
      "barcelon\n",
      "dort\n",
      "sunnis\n",
      "lomb\n",
      "snowden\n",
      "avez\n",
      "diffic\n",
      "sess\n",
      "khamenei\n",
      "exer\n",
      "golese\n",
      "copen\n",
      "rouhani\n",
      "ipal\n",
      "transparen\n",
      "ultane\n",
      "mccain\n",
      "boeh\n",
      "diox\n",
      "citiz\n",
      "adjac\n",
      "nieto\n",
      "lavrov\n"
     ]
    }
   ],
   "source": [
    "word_embed = model.word_embed\n",
    "\n",
    "text = \"netanyahu\"\n",
    "text = text.lower()\n",
    "\n",
    "idx = tf.cast(tokenizer.tokenizer.tokenize(text), tf.int32)\n",
    "idx = tokenize(idx, tokenizer.merge_list)\n",
    "print(idx)\n",
    "embed = tf.expand_dims(word_embed[idx[0][0]], axis=0)\n",
    "\n",
    "cosine_sim = embed@tf.transpose(word_embed)\n",
    "idx = tf.argsort(cosine_sim, axis=-1, \n",
    "                 direction='DESCENDING',\n",
    "                 #direction='ASCENDING', \n",
    "                 stable=False, name=None)[0]\n",
    "\n",
    "for i in idx[:100]:\n",
    "    i = tf.expand_dims(i, axis=0)\n",
    "    print(tokenizer.detokenize(i).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a5f20b",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
