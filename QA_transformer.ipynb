{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04010a08",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8fa3d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from src.tokenizer import TokenizerBPE, fuse_tokenized_corpus, chunk_corpus\n",
    "\n",
    "import os\n",
    "import time\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm.notebook import tqdm\n",
    "from src.transformer import *\n",
    "from src.data_handling import read_first_n, sample_batch\n",
    "\n",
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b2b59898",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "576256a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_tokenized_corpus(corpus, tokenizer):\n",
    "    SOS = tf.convert_to_tensor([[tokenizer.token_to_idx[\"<s>\"]]])\n",
    "    EOS = tf.convert_to_tensor([[tokenizer.token_to_idx[\"</s>\"]]])\n",
    "\n",
    "    corpus_list = [SOS]\n",
    "    for line in tqdm(corpus):\n",
    "        corpus_list.append(line)\n",
    "        corpus_list.append(EOS)\n",
    "        corpus_list.append(SOS)\n",
    "\n",
    "    corpus = tf.concat(corpus_list[:-1], axis=1)\n",
    "    return corpus\n",
    "\n",
    "\n",
    "def chunk_and_batch(corpus, chunk_size, batch_size, shuffle=True, repeat=False):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(corpus)\n",
    "    ds = ds.batch(chunk_size, drop_remainder=True)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=100*chunk_size, reshuffle_each_iteration=True)\n",
    "        \n",
    "    ds = ds.batch(batch_size, drop_remainder=True)\n",
    "    if repeat:\n",
    "        ds = ds.repeat()\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7afd4e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 128\n",
    "\n",
    "tokenizer = pkl.load(open(\"tokenizer_QA8000.pkl\", 'rb'))\n",
    "tokenizer.create_hash()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8751dd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pkl.load(open('corpus/QA_8k.pkl', 'rb'))[0]\n",
    "\n",
    "length = corpus.shape[0]\n",
    "train_corpus = corpus[:int(length*0.8)]\n",
    "test_corpus = corpus[int(length*0.8):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e38fb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = chunk_and_batch(train_corpus, chunk_size=max_seq_len, batch_size=16, shuffle=True, repeat=True)\n",
    "ds_test = chunk_and_batch(test_corpus, chunk_size=max_seq_len, batch_size=8, repeat=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f830d881",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a564402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WarmUpThenDecay(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self,\n",
    "                 initial_learning_rate: float,\n",
    "                 warmup_steps: int,\n",
    "                 decay_schedule_fn: tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "        \"\"\"\n",
    "        initial_learning_rate: peak LR reached at end of warmup\n",
    "        warmup_steps:      # of steps to ramp from 0 â†’ initial_learning_rate\n",
    "        decay_schedule_fn: a tf.keras schedule to apply *after* warmup\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.initial_lr = initial_learning_rate\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.decay_schedule_fn = decay_schedule_fn\n",
    "\n",
    "    def __call__(self, step):\n",
    "        # Cast to float32 for safety in graph mode\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        warmup_steps = tf.cast(self.warmup_steps, tf.float32)\n",
    "\n",
    "        # compute linear warmup: lr = initial_lr * (step / warmup_steps)\n",
    "        warmup_lr = self.initial_lr * (step / warmup_steps)\n",
    "\n",
    "        # after warmup_steps, switch to decay schedule (shift step count)\n",
    "        decay_step = step - warmup_steps\n",
    "        decay_lr = self.decay_schedule_fn(decay_step)\n",
    "\n",
    "        # if step < warmup_steps, pick warmup_lr, else decay_lr\n",
    "        return tf.cond(step < warmup_steps,\n",
    "                       lambda: warmup_lr,\n",
    "                       lambda: decay_lr)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a5a33ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lr = 1e-4\n",
    "decay_steps = 20000\n",
    "decay_rate = 0.5\n",
    "decay_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_lr,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=False)\n",
    "\n",
    "warmup_steps = 1000\n",
    "lr_schedule = WarmUpThenDecay(\n",
    "    initial_learning_rate=initial_lr,\n",
    "    warmup_steps=warmup_steps,\n",
    "    decay_schedule_fn=decay_schedule)\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "max_seq_len = 128\n",
    "embed_dim = 512\n",
    "tf_blocks = 8\n",
    "heads = 8\n",
    "ff_dim = 4*embed_dim\n",
    "weight_decay = 0.01\n",
    "dropout = 0.05\n",
    "\n",
    "unembed_dims = []\n",
    "\n",
    "model = Transformer(vocab_size=tokenizer.vocab_size,\n",
    "                    max_seq_len=max_seq_len,\n",
    "                    embed_dim=embed_dim,\n",
    "                    tf_blocks=tf_blocks,\n",
    "                    heads=heads,\n",
    "                    ff_dim = ff_dim,\n",
    "                    unembed_dims=unembed_dims,\n",
    "                    lr=lr_schedule,\n",
    "                    wd = weight_decay,\n",
    "                    dropout=dropout,\n",
    "                    )\n",
    "\n",
    "losses_train = []\n",
    "losses_test = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7970a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"model_qa_8k\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(\n",
    "    optimizer=model.opt,\n",
    "    model=model\n",
    ")\n",
    "ckpt_manager = tf.train.CheckpointManager(\n",
    "    ckpt, \n",
    "    directory=\"checkpoints/\" + name,      # folder where ckpts are saved\n",
    "    max_to_keep=5                         # only keep 5 latest checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "88b34765",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "losses_train, losses_test = pkl.load(open(\"checkpoints/losses_\" + name + \".pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6527620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 29546131\n"
     ]
    }
   ],
   "source": [
    "total_params = 0\n",
    "for var in model.parameter_list:\n",
    "    shape = var.get_shape()\n",
    "    num_params = 1\n",
    "    for dim in shape:\n",
    "        num_params *= dim\n",
    "    total_params += num_params\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "93977cd2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\PIL\\ImageFile.py:643\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 643\u001b[0m     fh \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileno\u001b[49m()\n\u001b[0;32m    644\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_idat' object has no attribute 'fileno'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m ax2\u001b[38;5;241m.\u001b[39mgrid(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     52\u001b[0m plt\u001b[38;5;241m.\u001b[39mtight_layout()\n\u001b[1;32m---> 53\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\pyplot.py:614\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;124;03mDisplay all open figures.\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[38;5;124;03mexplicitly there.\u001b[39;00m\n\u001b[0;32m    612\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    613\u001b[0m _warn_if_gui_out_of_main_thread()\n\u001b[1;32m--> 614\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _get_backend_mod()\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib_inline\\backend_inline.py:90\u001b[0m, in \u001b[0;36mshow\u001b[1;34m(close, block)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m figure_manager \u001b[38;5;129;01min\u001b[39;00m Gcf\u001b[38;5;241m.\u001b[39mget_all_fig_managers():\n\u001b[1;32m---> 90\u001b[0m         \u001b[43mdisplay\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_fetch_figure_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfigure_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcanvas\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     show\u001b[38;5;241m.\u001b[39m_to_draw \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\IPython\\core\\display_functions.py:298\u001b[0m, in \u001b[0;36mdisplay\u001b[1;34m(include, exclude, metadata, transient, display_id, raw, clear, *objs, **kwargs)\u001b[0m\n\u001b[0;32m    296\u001b[0m     publish_display_data(data\u001b[38;5;241m=\u001b[39mobj, metadata\u001b[38;5;241m=\u001b[39mmetadata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 298\u001b[0m     format_dict, md_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m format_dict:\n\u001b[0;32m    300\u001b[0m         \u001b[38;5;66;03m# nothing to display (e.g. _ipython_display_ took over)\u001b[39;00m\n\u001b[0;32m    301\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\IPython\\core\\formatters.py:238\u001b[0m, in \u001b[0;36mDisplayFormatter.format\u001b[1;34m(self, obj, include, exclude)\u001b[0m\n\u001b[0;32m    236\u001b[0m md \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 238\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m# FIXME: log the exception\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\decorator.py:232\u001b[0m, in \u001b[0;36mdecorate.<locals>.fun\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwsyntax:\n\u001b[0;32m    231\u001b[0m     args, kw \u001b[38;5;241m=\u001b[39m fix(args, kw, sig)\n\u001b[1;32m--> 232\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m caller(func, \u001b[38;5;241m*\u001b[39m(extras \u001b[38;5;241m+\u001b[39m args), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\IPython\\core\\formatters.py:282\u001b[0m, in \u001b[0;36mcatch_format_error\u001b[1;34m(method, self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"show traceback on failed format call\"\"\"\u001b[39;00m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     r \u001b[38;5;241m=\u001b[39m method(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;66;03m# don't warn on NotImplementedErrors\u001b[39;00m\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_return(\u001b[38;5;28;01mNone\u001b[39;00m, args[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\IPython\\core\\formatters.py:402\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 402\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprinter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;66;03m# Finally look for special method names\u001b[39;00m\n\u001b[0;32m    404\u001b[0m method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\IPython\\core\\pylabtools.py:170\u001b[0m, in \u001b[0;36mprint_figure\u001b[1;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_bases\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FigureCanvasBase\n\u001b[0;32m    168\u001b[0m     FigureCanvasBase(fig)\n\u001b[1;32m--> 170\u001b[0m fig\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mprint_figure(bytes_io, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    171\u001b[0m data \u001b[38;5;241m=\u001b[39m bytes_io\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fmt \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msvg\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\backend_bases.py:2184\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[1;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[0;32m   2180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2181\u001b[0m     \u001b[38;5;66;03m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[0;32m   2182\u001b[0m     \u001b[38;5;66;03m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[0;32m   2183\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cbook\u001b[38;5;241m.\u001b[39m_setattr_cm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfigure, dpi\u001b[38;5;241m=\u001b[39mdpi):\n\u001b[1;32m-> 2184\u001b[0m         result \u001b[38;5;241m=\u001b[39m print_method(\n\u001b[0;32m   2185\u001b[0m             filename,\n\u001b[0;32m   2186\u001b[0m             facecolor\u001b[38;5;241m=\u001b[39mfacecolor,\n\u001b[0;32m   2187\u001b[0m             edgecolor\u001b[38;5;241m=\u001b[39medgecolor,\n\u001b[0;32m   2188\u001b[0m             orientation\u001b[38;5;241m=\u001b[39morientation,\n\u001b[0;32m   2189\u001b[0m             bbox_inches_restore\u001b[38;5;241m=\u001b[39m_bbox_inches_restore,\n\u001b[0;32m   2190\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2191\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   2192\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bbox_inches \u001b[38;5;129;01mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\backend_bases.py:2040\u001b[0m, in \u001b[0;36mFigureCanvasBase._switch_canvas_and_return_print_method.<locals>.<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   2036\u001b[0m     optional_kws \u001b[38;5;241m=\u001b[39m {  \u001b[38;5;66;03m# Passed by print_figure for other renderers.\u001b[39;00m\n\u001b[0;32m   2037\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medgecolor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morientation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2038\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbbox_inches_restore\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m   2039\u001b[0m     skip \u001b[38;5;241m=\u001b[39m optional_kws \u001b[38;5;241m-\u001b[39m {\u001b[38;5;241m*\u001b[39minspect\u001b[38;5;241m.\u001b[39msignature(meth)\u001b[38;5;241m.\u001b[39mparameters}\n\u001b[1;32m-> 2040\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mwraps(meth)(\u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: meth(\n\u001b[0;32m   2041\u001b[0m         \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m skip}))\n\u001b[0;32m   2042\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Let third-parties do as they see fit.\u001b[39;00m\n\u001b[0;32m   2043\u001b[0m     print_method \u001b[38;5;241m=\u001b[39m meth\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:481\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[1;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprint_png\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename_or_obj, \u001b[38;5;241m*\u001b[39m, metadata\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, pil_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    435\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;124;03m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;124;03m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 481\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_print_pil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\backends\\backend_agg.py:430\u001b[0m, in \u001b[0;36mFigureCanvasAgg._print_pil\u001b[1;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;124;03mDraw the canvas, then save it using `.image.imsave` (to which\u001b[39;00m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;124;03m*pil_kwargs* and *metadata* are forwarded).\u001b[39;00m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    429\u001b[0m FigureCanvasAgg\u001b[38;5;241m.\u001b[39mdraw(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 430\u001b[0m \u001b[43mmpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimsave\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer_rgba\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morigin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mupper\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfigure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpil_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpil_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\matplotlib\\image.py:1644\u001b[0m, in \u001b[0;36mimsave\u001b[1;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[0;32m   1642\u001b[0m pil_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m   1643\u001b[0m pil_kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdpi\u001b[39m\u001b[38;5;124m\"\u001b[39m, (dpi, dpi))\n\u001b[1;32m-> 1644\u001b[0m image\u001b[38;5;241m.\u001b[39msave(fname, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpil_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\PIL\\Image.py:2581\u001b[0m, in \u001b[0;36mImage.save\u001b[1;34m(self, fp, format, **params)\u001b[0m\n\u001b[0;32m   2578\u001b[0m     fp \u001b[38;5;241m=\u001b[39m cast(IO[\u001b[38;5;28mbytes\u001b[39m], fp)\n\u001b[0;32m   2580\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2581\u001b[0m     \u001b[43msave_handler\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2582\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   2583\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m open_fp:\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\PIL\\PngImagePlugin.py:1492\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, filename, chunk, save_all)\u001b[0m\n\u001b[0;32m   1488\u001b[0m     single_im \u001b[38;5;241m=\u001b[39m _write_multiple_frames(\n\u001b[0;32m   1489\u001b[0m         im, fp, chunk, mode, rawmode, default_image, append_images\n\u001b[0;32m   1490\u001b[0m     )\n\u001b[0;32m   1491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m single_im:\n\u001b[1;32m-> 1492\u001b[0m     \u001b[43mImageFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1493\u001b[0m \u001b[43m        \u001b[49m\u001b[43msingle_im\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mIO\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_idat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1495\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mImageFile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Tile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mzip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msingle_im\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1496\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info:\n\u001b[0;32m   1499\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m info_chunk \u001b[38;5;129;01min\u001b[39;00m info\u001b[38;5;241m.\u001b[39mchunks:\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\PIL\\ImageFile.py:647\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(im, fp, tile, bufsize)\u001b[0m\n\u001b[0;32m    645\u001b[0m     _encode_tile(im, fp, tile, bufsize, fh)\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, io\u001b[38;5;241m.\u001b[39mUnsupportedOperation) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 647\u001b[0m     \u001b[43m_encode_tile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fp, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflush\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    649\u001b[0m     fp\u001b[38;5;241m.\u001b[39mflush()\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\PIL\\ImageFile.py:673\u001b[0m, in \u001b[0;36m_encode_tile\u001b[1;34m(im, fp, tile, bufsize, fh, exc)\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exc:\n\u001b[0;32m    671\u001b[0m     \u001b[38;5;66;03m# compress to Python file-compatible object\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 673\u001b[0m         errcode, data \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbufsize\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    674\u001b[0m         fp\u001b[38;5;241m.\u001b[39mwrite(data)\n\u001b[0;32m    675\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m errcode:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, (batch_train, batch_test) in enumerate(zip(ds_train, ds_test)):\n",
    "    \n",
    "\n",
    "    loss_train = model.train_step(batch_train).numpy()\n",
    "    losses_train.append(loss_train)\n",
    "    \n",
    "    loss_test = model.evaluate(batch_test).numpy()\n",
    "    losses_test.append(loss_test)\n",
    "\n",
    "    if (i+1) % 1000 == 0:\n",
    "        ckpt_manager.save()\n",
    "        pkl.dump([losses_train, losses_test], open(\"checkpoints/losses_\" + name + \".pkl\", 'wb'))\n",
    "\n",
    "\n",
    "    lr = model.opt.inner_optimizer._decayed_lr(tf.float32).numpy()\n",
    "    #\"\"\"\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    # prepare x-axis for the last 400 steps\n",
    "    start = max(0, len(losses_train) - 1000)\n",
    "    x_zoom = np.arange(start, len(losses_train))\n",
    "\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 8), sharex=False)\n",
    "\n",
    "    # Top subplot: zoom on last 400 steps\n",
    "    ax1 = axes[0]\n",
    "    ax1.plot(x_zoom, losses_test[-1000:], label=\"Test Loss\")\n",
    "    ax1.plot(x_zoom, losses_train[-1000:], label=\"Train Loss\")\n",
    "\n",
    "    _min = min(losses_train[-1000:] + losses_test[-1000:])\n",
    "    _max = max(losses_train[-1000:] + losses_test[-1000:])\n",
    "    delta = _max - _min\n",
    "    #ax1.set_ylim(_min - 0.1 * delta, _max + 0.1 * delta)\n",
    "\n",
    "    ax1.set_title(\"Training Loss (Last 1000 Steps)\")\n",
    "    ax1.set_xlabel(\"Step\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    # Bottom subplot: full series\n",
    "    ax2 = axes[1]\n",
    "    ax2.plot(losses_test[10:], label=\"Test Loss\")\n",
    "    ax2.plot(losses_train[10:], label=\"Train Loss, lr = {:.2e}\".format(lr))\n",
    "\n",
    "    ax2.set_title(\"Training Loss (Full Series)\")\n",
    "    ax2.set_xlabel(\"Step\")\n",
    "    ax2.set_ylabel(\"Loss\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    #\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "74a017b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[8335 8337  346    0  741 2539    0  524    0  374    0  532    0  358\n",
      "     0  773    0 4537    0 3237   31]], shape=(1, 21), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5794c9b6f16414e92a6814958f6eb70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', disabled=True, layout=Layout(height='20em', width='80ch'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = \"What consoles can be used to play Twilight Princess?\"\n",
    "text = text.lower()\n",
    "sos = tf.convert_to_tensor([[tokenizer.token_to_idx[\"<s>\"]]])\n",
    "q = tf.convert_to_tensor([[tokenizer.token_to_idx[\"<q>\"]]])\n",
    "indices = tf.cast(tokenizer.tokenize(text), tf.int32)\n",
    "indices = tf.concat([sos, q, indices], axis=1)\n",
    "print(indices)\n",
    "\n",
    "\n",
    "import textwrap\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "T = 0.2\n",
    "#tf.random.set_seed(43)\n",
    "wrapper = textwrap.TextWrapper(width=80)\n",
    "\n",
    "# create a read-only text area\n",
    "ta = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    layout=widgets.Layout(width='80ch', height='20em'),\n",
    "    disabled=True\n",
    ")\n",
    "display(ta)\n",
    "\n",
    "for i in range(512):\n",
    "    logits = model.call(indices)[0, -1:]\n",
    "    idx = tf.cast(\n",
    "        tf.random.categorical(logits / T, num_samples=1),\n",
    "        tf.int32\n",
    "    )\n",
    "    indices = tf.concat([indices, idx], axis=1)\n",
    "\n",
    "    text_pred = (\n",
    "        tokenizer\n",
    "        .detokenize(indices)\n",
    "        .numpy()[0]\n",
    "        .decode('utf-8')\n",
    "        .replace(\"\\n\", \" \")\n",
    "    )\n",
    "    ta.value = wrapper.fill(text_pred)  # this updates in-place\n",
    "\n",
    "    if idx[0, 0] == tokenizer.token_to_idx[\"</s>\"]:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e77fb01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def cosine_similarity(embed_a, embed_b):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two vectors.\n",
    "    \"\"\"\n",
    "    embed_b_T = tf.transpose(embed_b)\n",
    "    dot_product = embed_a@embed_b_T\n",
    "    \n",
    "    norm_a = tf.linalg.norm(embed_a, axis=1, keepdims=True)\n",
    "    norm_b = tf.linalg.norm(embed_b_T, axis=0, keepdims=True)\n",
    "\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "\n",
    "def cluster(X, n_clusters, normalize=True):\n",
    "    if normalize:\n",
    "        X = X/np.linalg.norm(X, axis=1, keepdims=True)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n",
    "    inertia = kmeans.inertia_\n",
    "    labels = kmeans.labels_\n",
    "    clusters = kmeans.cluster_centers_\n",
    "\n",
    "    return inertia, labels, clusters\n",
    "\n",
    "\n",
    "class EmbeddingClustering:\n",
    "    def __init__(self, tokenizer, n_clusters=10):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_clusters = n_clusters\n",
    "\n",
    "    def fit(self, word_embed, normalize=True):\n",
    "        inertia, labels, clusters = cluster(word_embed, self.n_clusters, normalize)\n",
    "        self.word_embed = word_embed\n",
    "        self.inertia = inertia\n",
    "        self.labels = labels\n",
    "        self.clusters = tf.convert_to_tensor(clusters, dtype=tf.float32)\n",
    "\n",
    "        cos_sim = cosine_similarity(self.clusters, word_embed, normalize)\n",
    "        self.idx_list =  tf.argsort(cos_sim, axis=-1, direction='DESCENDING', stable=False, name=None)\n",
    "\n",
    "    def print_clusters(self, n_words=10):\n",
    "        for idx in self.idx_list:\n",
    "            for i in idx[:n_words]:\n",
    "                word = self.tokenizer.detokenize(tf.expand_dims(tf.cast(i, tf.int32), axis=0))\n",
    "                word = word.numpy().decode('utf-8')\n",
    "                print(word)\n",
    "            print(\"\\n\")\n",
    "\n",
    "\n",
    "def cosine_similarity(embed_a, embed_b, normalize=True):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two vectors.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        embed_a = tf.nn.l2_normalize(embed_a, axis=1)\n",
    "        embed_b = tf.nn.l2_normalize(embed_b, axis=1)\n",
    "    dot_product = embed_a@tf.transpose(embed_b)\n",
    "\n",
    "\n",
    "    return dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "48583bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debbie\n",
      "denise\n",
      "katherine\n",
      "stephanie\n",
      "randy\n",
      "jason\n",
      "patricia\n",
      "kathy\n",
      "jesse\n",
      "sergio\n",
      "\n",
      "\n",
      "deficits\n",
      "revenues\n",
      "reductions\n",
      "subsidies\n",
      "incentives\n",
      "loans\n",
      "exports\n",
      "budgets\n",
      "salaries\n",
      "bonuses\n",
      "\n",
      "\n",
      "attem\n",
      "theless\n",
      "occas\n",
      "ieval\n",
      "theastern\n",
      "enthusia\n",
      "negot\n",
      "lesterol\n",
      "subsequ\n",
      "fahren\n",
      "\n",
      "\n",
      "occas\n",
      "afgh\n",
      "negot\n",
      "includ\n",
      "delaw\n",
      "glary\n",
      "ifics\n",
      "theast\n",
      "salad\n",
      "patro\n",
      "\n",
      "\n",
      "theastern\n",
      "experi\n",
      "delaw\n",
      "theast\n",
      "lesterol\n",
      "dort\n",
      "semif\n",
      "attem\n",
      "surpris\n",
      "compon\n",
      "\n",
      "\n",
      "ffed\n",
      "strugg\n",
      "portra\n",
      "athle\n",
      "massachu\n",
      "ifics\n",
      "pered\n",
      "experi\n",
      "arct\n",
      "thered\n",
      "\n",
      "\n",
      "subsequ\n",
      "theast\n",
      "prede\n",
      "occas\n",
      "theastern\n",
      "ieval\n",
      "lesterol\n",
      "experi\n",
      "arct\n",
      "ailand\n",
      "\n",
      "\n",
      "pretty\n",
      "fairly\n",
      "reasonably\n",
      "utterly\n",
      "remarkably\n",
      "substantially\n",
      "fundamentally\n",
      "relatively\n",
      "totally\n",
      "unusually\n",
      "\n",
      "\n",
      "deal\n",
      "challenge\n",
      "talk\n",
      "question\n",
      "move\n",
      "answer\n",
      "appeal\n",
      "attempt\n",
      "approach\n",
      "decision\n",
      "\n",
      "\n",
      "sending\n",
      "helping\n",
      "letting\n",
      "handing\n",
      "putting\n",
      "taking\n",
      "giving\n",
      "bringing\n",
      "pulling\n",
      "throwing\n",
      "\n",
      "\n",
      "tens\n",
      "hundreds\n",
      "thousands\n",
      "dozens\n",
      "millions\n",
      "billions\n",
      "plenty\n",
      "sort\n",
      "there\n",
      "none\n",
      "\n",
      "\n",
      "uguay\n",
      "theid\n",
      "lesterol\n",
      "negot\n",
      "theastern\n",
      "delaw\n",
      "ailand\n",
      "occas\n",
      "guate\n",
      "arct\n",
      "\n",
      "\n",
      "convince\n",
      "impose\n",
      "abide\n",
      "eliminate\n",
      "undermine\n",
      "prohibit\n",
      "tolerate\n",
      "educate\n",
      "satisfy\n",
      "violate\n",
      "\n",
      "\n",
      "two\n",
      "three\n",
      "four\n",
      "the\n",
      "six\n",
      "a\n",
      "several\n",
      "some\n",
      "no\n",
      "more\n",
      "\n",
      "\n",
      "christop\n",
      "djok\n",
      "berdy\n",
      "tember\n",
      "benjam\n",
      "whate\n",
      "aragu\n",
      "petrole\n",
      "dono\n",
      "bundes\n",
      "\n",
      "\n",
      "gives\n",
      "helps\n",
      "provides\n",
      "encourages\n",
      "relies\n",
      "deserves\n",
      "refuses\n",
      "enjoys\n",
      "seeks\n",
      "tends\n",
      "\n",
      "\n",
      "renewable\n",
      "greenhouse\n",
      "geological\n",
      "tropical\n",
      "mountainous\n",
      "carbon\n",
      "saharan\n",
      "rugged\n",
      "atomic\n",
      "fukushima\n",
      "\n",
      "\n",
      "encouraged\n",
      "instructed\n",
      "persuaded\n",
      "refused\n",
      "urged\n",
      "subjected\n",
      "managed\n",
      "referred\n",
      "helped\n",
      "unable\n",
      "\n",
      "\n",
      "glary\n",
      "gbag\n",
      "strugg\n",
      "espion\n",
      "risings\n",
      "beliefs\n",
      "guate\n",
      "transparen\n",
      "athle\n",
      "usalem\n",
      "\n",
      "\n",
      "behavioral\n",
      "psychiatric\n",
      "reproductive\n",
      "cognitive\n",
      "bodily\n",
      "preventive\n",
      "genetic\n",
      "spinal\n",
      "traumatic\n",
      "infectious\n",
      "\n",
      "\n",
      "1972\n",
      "2002\n",
      "1992\n",
      "2001\n",
      "1993\n",
      "2003\n",
      "1969\n",
      "1995\n",
      "1990\n",
      "2005\n",
      "\n",
      "\n",
      "glary\n",
      "burglary\n",
      "interrogation\n",
      "gbag\n",
      "portra\n",
      "prede\n",
      "afgh\n",
      "fahren\n",
      "environ\n",
      "lesterol\n",
      "\n",
      "\n",
      "bolivia\n",
      "tunisia\n",
      "belarus\n",
      "croatia\n",
      "kazakhstan\n",
      "portugal\n",
      "serbia\n",
      "uzbek\n",
      "paraguay\n",
      "algeria\n",
      "\n",
      "\n",
      "indicted\n",
      "accuses\n",
      "sentenced\n",
      "pleaded\n",
      "criticized\n",
      "denounced\n",
      "denied\n",
      "hailed\n",
      "dismissed\n",
      "acquitted\n",
      "\n",
      "\n",
      "delicious\n",
      "magical\n",
      "exciting\n",
      "fascinating\n",
      "vibrant\n",
      "formidable\n",
      "thoughtful\n",
      "lovely\n",
      "beneficial\n",
      "profound\n",
      "\n",
      "\n",
      "nostal\n",
      "norwe\n",
      "twel\n",
      "archite\n",
      "aero\n",
      "nove\n",
      "reci\n",
      "engul\n",
      "whate\n",
      "anthro\n",
      "\n",
      "\n",
      "oldest\n",
      "longest\n",
      "lowest\n",
      "fastest\n",
      "tallest\n",
      "busiest\n",
      "hottest\n",
      "12th\n",
      "youngest\n",
      "16th\n",
      "\n",
      "\n",
      "unsure\n",
      "wondering\n",
      "convinced\n",
      "insisting\n",
      "wondered\n",
      "reminded\n",
      "complaining\n",
      "excited\n",
      "arguing\n",
      "hoping\n",
      "\n",
      "\n",
      "ag\n",
      " \n",
      "\n",
      "\n",
      "leng\n",
      "scand\n",
      "ig\n",
      "uc\n",
      "em\n",
      "ud\n",
      "mar\n",
      "\n",
      "\n",
      "inno\n",
      "spon\n",
      "mber\n",
      "lene\n",
      "resu\n",
      "mediterran\n",
      "uke\n",
      "tember\n",
      "bam\n",
      "kyr\n",
      "\n",
      "\n",
      "mo\n",
      "lo\n",
      "con\n",
      "mu\n",
      "de\n",
      "sha\n",
      "du\n",
      "su\n",
      "tu\n",
      "hu\n",
      "\n",
      "\n",
      "ailand\n",
      "zimbab\n",
      "occas\n",
      "immen\n",
      "includ\n",
      "delaw\n",
      "athle\n",
      "experi\n",
      "negot\n",
      "hrir\n",
      "\n",
      "\n",
      "grandmother\n",
      "roommate\n",
      "boyfriend\n",
      "girlfriend\n",
      "fiance\n",
      "aunt\n",
      "cousin\n",
      "fiancee\n",
      "nephew\n",
      "counselor\n",
      "\n",
      "\n",
      "ised\n",
      "ipped\n",
      "aded\n",
      "ered\n",
      "uted\n",
      "ized\n",
      "ilized\n",
      "oned\n",
      "ished\n",
      "aled\n",
      "\n",
      "\n",
      "28\n",
      "15\n",
      "36\n",
      "29\n",
      "25\n",
      "23\n",
      "39\n",
      "40\n",
      "27\n",
      "38\n",
      "\n",
      "\n",
      "popu\n",
      "magist\n",
      "nove\n",
      "theat\n",
      "injun\n",
      "frust\n",
      "juris\n",
      "convin\n",
      "immig\n",
      "anthro\n",
      "\n",
      "\n",
      "spacecraft\n",
      "telescope\n",
      "airliner\n",
      "dreamliner\n",
      "tanker\n",
      "delaw\n",
      "submarine\n",
      "airbus\n",
      "occas\n",
      "plane's\n",
      "\n",
      "\n",
      "theless\n",
      "lesterol\n",
      "occas\n",
      "subsequ\n",
      "assage\n",
      "fahren\n",
      "ailand\n",
      "secutive\n",
      "luscon\n",
      "theid\n",
      "\n",
      "\n",
      "atp\n",
      "paralympic\n",
      "tennis\n",
      "wta\n",
      "basketball\n",
      "rugby\n",
      "athletics\n",
      "wimbledon\n",
      "jazz\n",
      "players'\n",
      "\n",
      "\n",
      "civil\n",
      "foreign\n",
      "political\n",
      "supreme\n",
      "financial\n",
      "constitutional\n",
      "legislative\n",
      "social\n",
      "economic\n",
      "immigration\n",
      "\n",
      "\n",
      "i'd\n",
      "we'd\n",
      "you'd\n",
      "we'll\n",
      "we've\n",
      "i'll\n",
      "you've\n",
      "i've\n",
      "hasn't\n",
      "they'd\n",
      "\n",
      "\n",
      "investigators\n",
      "officials\n",
      "residents\n",
      "doctors\n",
      "firefighters\n",
      "authorities\n",
      "students\n",
      "experts\n",
      "activists\n",
      "attorneys\n",
      "\n",
      "\n",
      "uguay\n",
      "negot\n",
      "rouhani\n",
      "maduro\n",
      "athle\n",
      "occas\n",
      "guate\n",
      "gbag\n",
      "afgh\n",
      "theastern\n",
      "\n",
      "\n",
      "injunction\n",
      "rulings\n",
      "athle\n",
      "experi\n",
      "tribunal\n",
      "guate\n",
      "arct\n",
      "theastern\n",
      "itored\n",
      "gbag\n",
      "\n",
      "\n",
      "scrut\n",
      "inflam\n",
      "nutr\n",
      "reim\n",
      "symp\n",
      "sophist\n",
      "inef\n",
      "frust\n",
      "erad\n",
      "scand\n",
      "\n",
      "\n",
      "enthusia\n",
      "immen\n",
      "usalem\n",
      "surpris\n",
      "fahren\n",
      "delaw\n",
      "assage\n",
      "theless\n",
      "subsequ\n",
      "theid\n",
      "\n",
      "\n",
      "strategist\n",
      "coordinator\n",
      "commentator\n",
      "columnist\n",
      "chairwoman\n",
      "mogul\n",
      "grapher\n",
      "adviser\n",
      "contributor\n",
      "historian\n",
      "\n",
      "\n",
      "hosni\n",
      "christiane\n",
      "jethro\n",
      "cristiano\n",
      "elise\n",
      "kanye\n",
      "saad\n",
      "udad\n",
      "rory\n",
      "charac\n",
      "\n",
      "\n",
      "concerns\n",
      "doubts\n",
      "commitment\n",
      "distinction\n",
      "sympathy\n",
      "concern\n",
      "efforts\n",
      "frustration\n",
      "connections\n",
      "questions\n",
      "\n",
      "\n",
      "teed\n",
      "descended\n",
      "poured\n",
      "climbed\n",
      "bounced\n",
      "slipped\n",
      "tossed\n",
      "wiped\n",
      "ripped\n",
      "edged\n",
      "\n",
      "\n",
      "fbi's\n",
      "singer's\n",
      "tour's\n",
      "organization's\n",
      "military's\n",
      "couple's\n",
      "agency's\n",
      "minister's\n",
      "army's\n",
      "show's\n",
      "\n",
      "\n",
      "175\n",
      "450\n",
      "550\n",
      "650\n",
      "120\n",
      "250\n",
      "750\n",
      "260\n",
      "270\n",
      "240\n",
      "\n",
      "\n",
      "hampered\n",
      "governed\n",
      "enriched\n",
      "administered\n",
      "transmitted\n",
      "traced\n",
      "regulated\n",
      "renewable\n",
      "populated\n",
      "resistant\n",
      "\n",
      "\n",
      "involuntary\n",
      "impending\n",
      "lucrative\n",
      "immediate\n",
      "deepwater\n",
      "prolonged\n",
      "unnamed\n",
      "extensive\n",
      "broader\n",
      "lengthy\n",
      "\n",
      "\n",
      "photograp\n",
      "moroc\n",
      "leng\n",
      "reim\n",
      "resur\n",
      "refres\n",
      "phis\n",
      "inflam\n",
      "detro\n",
      "resor\n",
      "\n",
      "\n",
      "generated\n",
      "influenced\n",
      "benefited\n",
      "initiated\n",
      "supported\n",
      "supplied\n",
      "aided\n",
      "attracted\n",
      "reviewed\n",
      "acquired\n",
      "\n",
      "\n",
      "theless\n",
      "negot\n",
      "compon\n",
      "ivid\n",
      "enthusia\n",
      "assage\n",
      "immen\n",
      "usalem\n",
      "dort\n",
      "itored\n",
      "\n",
      "\n",
      "scrut\n",
      "fict\n",
      "confis\n",
      "insurg\n",
      "proxim\n",
      "accompan\n",
      "obst\n",
      "o'ne\n",
      "itored\n",
      "dys\n",
      "\n",
      "\n",
      "profound\n",
      "dense\n",
      "immense\n",
      "substantial\n",
      "vague\n",
      "fierce\n",
      "fragile\n",
      "tremendous\n",
      "neat\n",
      "enormous\n",
      "\n",
      "\n",
      "kansas\n",
      "connecticut\n",
      "illinois\n",
      "pennsylvania\n",
      "arkansas\n",
      "louisiana\n",
      "maryland\n",
      "missouri\n",
      "tampa\n",
      "wisconsin\n",
      "\n",
      "\n",
      "ley's\n",
      "er's\n",
      "i's\n",
      "e's\n",
      "an's\n",
      "ton's\n",
      "on's\n",
      "ie's\n",
      "es'\n",
      "ler's\n",
      "\n",
      "\n",
      "espion\n",
      "explos\n",
      "unbeliev\n",
      "mclaugh\n",
      "inevit\n",
      "gbag\n",
      "ieval\n",
      "strugg\n",
      "itored\n",
      "glary\n",
      "\n",
      "\n",
      "athle\n",
      "drivers'\n",
      "quarterfinals\n",
      "standings\n",
      "occas\n",
      "espion\n",
      "liga\n",
      "couver\n",
      "glary\n",
      "bundesliga\n",
      "\n",
      "\n",
      "higher\n",
      "better\n",
      "harder\n",
      "faster\n",
      "bigger\n",
      "deeper\n",
      "worse\n",
      "greater\n",
      "cheaper\n",
      "stronger\n",
      "\n",
      "\n",
      "whate\n",
      "cuis\n",
      "manh\n",
      "accompan\n",
      "espion\n",
      "demp\n",
      "delaw\n",
      "o'ne\n",
      "enthusia\n",
      "theid\n",
      "\n",
      "\n",
      "anbar\n",
      "fallu\n",
      "occas\n",
      "homs\n",
      "environ\n",
      "mosul\n",
      "daraa\n",
      "lesterol\n",
      "ttp\n",
      "ifics\n",
      "\n",
      "\n",
      "confis\n",
      "dort\n",
      "compon\n",
      "attem\n",
      "ieval\n",
      "enrich\n",
      "incre\n",
      "surpris\n",
      "includ\n",
      "subsequ\n",
      "\n",
      "\n",
      "bullied\n",
      "depressed\n",
      "ashamed\n",
      "raped\n",
      "saddened\n",
      "gbag\n",
      "charac\n",
      "obese\n",
      "terrified\n",
      "ifics\n",
      "\n",
      "\n",
      "rehear\n",
      "engul\n",
      "resur\n",
      "eclip\n",
      "reim\n",
      "glimp\n",
      "moroc\n",
      "suc\n",
      "spear\n",
      "popu\n",
      "\n",
      "\n",
      "year\n",
      "friday\n",
      "week\n",
      "month\n",
      "sunday\n",
      "thursday\n",
      "monday\n",
      "saturday\n",
      "tuesday\n",
      "wednesday\n",
      "\n",
      "\n",
      "rampage\n",
      "bombings\n",
      "clashes\n",
      "shootings\n",
      "killings\n",
      "massacre\n",
      "slayings\n",
      "explosions\n",
      "altercation\n",
      "siege\n",
      "\n",
      "\n",
      "requiring\n",
      "letting\n",
      "violating\n",
      "distributing\n",
      "implementing\n",
      "introducing\n",
      "reducing\n",
      "ordering\n",
      "enforcing\n",
      "eliminating\n",
      "\n",
      "\n",
      "odox\n",
      "athle\n",
      "ifics\n",
      "massachu\n",
      "secutive\n",
      "charac\n",
      "mbley\n",
      "anmen\n",
      "tournam\n",
      "hrir\n",
      "\n",
      "\n",
      "museum\n",
      "institution\n",
      "facility\n",
      "library\n",
      "institute\n",
      "corporation\n",
      "organization\n",
      "council\n",
      "foundation\n",
      "ministry\n",
      "\n",
      "\n",
      "dys\n",
      "distr\n",
      "retr\n",
      "desc\n",
      "contr\n",
      "cont\n",
      "extr\n",
      "appreh\n",
      "videot\n",
      "transc\n",
      "\n",
      "\n",
      "insisted\n",
      "explained\n",
      "argued\n",
      "cautioned\n",
      "replied\n",
      "joked\n",
      "testified\n",
      "acknowledged\n",
      "wondered\n",
      "vowed\n",
      "\n",
      "\n",
      "independents\n",
      "hispanics\n",
      "colleges\n",
      "economists\n",
      "liberals\n",
      "households\n",
      "respondents\n",
      "entrepreneurs\n",
      "conservatives\n",
      "governors\n",
      "\n",
      "\n",
      "scand\n",
      "traged\n",
      "dort\n",
      "strug\n",
      "yose\n",
      "jere\n",
      "compon\n",
      "surpris\n",
      "califor\n",
      "theless\n",
      "\n",
      "\n",
      "detonated\n",
      "stormed\n",
      "grabbed\n",
      "engulfed\n",
      "transported\n",
      "raided\n",
      "chased\n",
      "entered\n",
      "collided\n",
      "invaded\n",
      "\n",
      "\n",
      "albums\n",
      "singers\n",
      "festivals\n",
      "musicians\n",
      "novels\n",
      "uguay\n",
      "espion\n",
      "glary\n",
      "strugg\n",
      "oscars\n",
      "\n",
      "\n",
      "in\n",
      "at\n",
      "from\n",
      "by\n",
      "when\n",
      "during\n",
      "after\n",
      "for\n",
      "on\n",
      "with\n",
      "\n",
      "\n",
      "certainly\n",
      "probably\n",
      "definitely\n",
      "never\n",
      "obviously\n",
      "hardly\n",
      "surely\n",
      "actually\n",
      "always\n",
      "usually\n",
      "\n",
      "\n",
      "suites\n",
      "floors\n",
      "shelters\n",
      "beaches\n",
      "trees\n",
      "pools\n",
      "bottles\n",
      "shops\n",
      "delaw\n",
      "glary\n",
      "\n",
      "\n",
      "boko\n",
      "yemeni\n",
      "somali\n",
      "somalia's\n",
      "saudi\n",
      "lebanese\n",
      "bosnian\n",
      "sri\n",
      "sudanese\n",
      "transitional\n",
      "\n",
      "\n",
      "responses\n",
      "attempts\n",
      "lawsuits\n",
      "decisions\n",
      "statements\n",
      "inquiries\n",
      "agreements\n",
      "proposals\n",
      "accusations\n",
      "discussions\n",
      "\n",
      "\n",
      "islamists\n",
      "militias\n",
      "shiites\n",
      "gunmen\n",
      "kurds\n",
      "militants\n",
      "egyptians\n",
      "insurgents\n",
      "houthis\n",
      "sunnis\n",
      "\n",
      "\n",
      "couver\n",
      "ailand\n",
      "inals\n",
      "glary\n",
      "ences\n",
      "ivid\n",
      "espion\n",
      "hrir\n",
      "theid\n",
      "negot\n",
      "\n",
      "\n",
      "mashable\n",
      "android\n",
      "verizon\n",
      "itunes\n",
      "playstation\n",
      "samsung\n",
      "nintend\n",
      "nintendo\n",
      "silicon\n",
      "netflix\n",
      "\n",
      "\n",
      "rainfall\n",
      "floods\n",
      "diarrhea\n",
      "devastation\n",
      "outages\n",
      "rains\n",
      "earthquakes\n",
      "vomiting\n",
      "espion\n",
      "snowfall\n",
      "\n",
      "\n",
      "scrut\n",
      "testim\n",
      "rele\n",
      "nutr\n",
      "proto\n",
      "leng\n",
      "scand\n",
      "obst\n",
      "accompan\n",
      "whate\n",
      "\n",
      "\n",
      "f\n",
      "l\n",
      "g\n",
      "d\n",
      "r\n",
      "t\n",
      " \n",
      "v\n",
      "b\n",
      "k\n",
      "\n",
      "\n",
      "lebanon's\n",
      "london's\n",
      "yemen's\n",
      "football's\n",
      "ukraine's\n",
      "egypt's\n",
      "afghanistan's\n",
      "libya's\n",
      "somalia's\n",
      "greece's\n",
      "\n",
      "\n",
      "sectors\n",
      "technologies\n",
      "sensors\n",
      "industries\n",
      "providers\n",
      "environments\n",
      "techniques\n",
      "entities\n",
      "installations\n",
      "tasks\n",
      "\n",
      "\n",
      "hrir\n",
      "lesterol\n",
      "glary\n",
      "icting\n",
      "charac\n",
      "fahren\n",
      "mbley\n",
      "negot\n",
      "couver\n",
      "oking\n",
      "\n",
      "\n",
      "assumption\n",
      "acknowled\n",
      "reminder\n",
      "notion\n",
      "mechanism\n",
      "incentive\n",
      "espion\n",
      "strugg\n",
      "obstacle\n",
      "inevit\n",
      "\n",
      "\n",
      "promptly\n",
      "swiftly\n",
      "safely\n",
      "adequately\n",
      "broadly\n",
      "properly\n",
      "readily\n",
      "thoroughly\n",
      "instantly\n",
      "voluntarily\n",
      "\n",
      "\n",
      "give\n",
      "make\n",
      "bring\n",
      "get\n",
      "want\n",
      "take\n",
      "tell\n",
      "subscribe\n",
      "learn\n",
      "keep\n",
      "\n",
      "\n",
      "assage\n",
      "enthusia\n",
      "theid\n",
      "uguay\n",
      "usalem\n",
      "ieval\n",
      "negot\n",
      "espion\n",
      "moil\n",
      "performan\n",
      "\n",
      "\n",
      "confis\n",
      "compon\n",
      "surpris\n",
      "notor\n",
      "dort\n",
      "attem\n",
      "scrut\n",
      "insurg\n",
      "refres\n",
      "accompan\n",
      "\n",
      "\n",
      "went\n",
      "took\n",
      "brought\n",
      "came\n",
      "pulled\n",
      "turned\n",
      "moved\n",
      "gave\n",
      "jumped\n",
      "walked\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_embed = model.word_embed\n",
    "embedding_clustering = EmbeddingClustering(tokenizer, n_clusters=100)\n",
    "embedding_clustering.fit(word_embed, normalize=True)\n",
    "embedding_clustering.print_clusters(n_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc968e2d",
   "metadata": {},
   "source": [
    "# Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0de0e293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[1602]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([[3512]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor([[5393]], shape=(1, 1), dtype=int32)\n",
      "netanyahu\n",
      "russia\n",
      "israel\n",
      "hamas\n",
      "israelis\n",
      "jerusalem\n",
      "tehran\n",
      "kiev\n",
      "gaza\n",
      "palestinians\n",
      "democr\n",
      "beirut\n",
      "azer\n",
      "syria\n",
      "egypt\n",
      "idf\n",
      "iran\n",
      "britain\n",
      "palestinian\n",
      "abbas\n",
      "tunisia\n",
      "alger\n",
      "lebanon\n",
      "perpe\n",
      "israeli\n",
      "davos\n",
      "brahim\n",
      "controver\n",
      "hezbollah\n",
      "hagel\n",
      "jevich\n",
      "norway\n",
      "fah\n",
      "guinea\n",
      "khamenei\n",
      "cuba\n",
      "anbar\n",
      "utt\n",
      "khamene\n",
      "hezbol\n",
      "weren\n",
      "canada\n",
      "sunnis\n",
      "dipl\n",
      "stoke\n",
      "lavrov\n",
      "aviv\n",
      "karzai\n",
      "israel's\n",
      "arct\n",
      "cambodia\n",
      "zuckerberg\n",
      "yanukov\n",
      "yad\n",
      "ukraine\n",
      "pakistan\n",
      "carney\n",
      "netherlands\n",
      "cairo\n",
      "lieberman\n",
      "panetta\n",
      "homs\n",
      "zawah\n",
      "austria\n",
      "espion\n",
      "wawrink\n",
      "vinc\n",
      "libertar\n",
      "libya\n",
      "poland\n",
      "indonesia\n",
      "liby\n",
      "merkel\n",
      "pyongyang\n",
      "tik\n",
      "airstrikes\n",
      "ibrahimovic\n",
      "abe\n",
      "iran's\n",
      "yugo\n",
      "kass\n",
      "mosul\n",
      "galax\n",
      "yemen\n",
      "scotland\n",
      "settlements\n",
      "sudan\n",
      "nuri\n",
      "niger\n",
      "palestine\n",
      "tsvangira\n",
      "ieval\n",
      "iaea\n",
      "denmark\n",
      "hmer\n",
      "tahrir\n",
      "nusra\n",
      "sarkoz\n",
      "ukrain\n",
      "bolivia\n"
     ]
    }
   ],
   "source": [
    "word_embed = model.word_embed\n",
    "\n",
    "text = \"russia\"\n",
    "text = text.lower()\n",
    "\n",
    "idx = tf.cast(tokenizer.tokenizer.tokenize(text), tf.int32)\n",
    "idx = tokenize(idx, tokenizer.merge_list)\n",
    "print(idx)\n",
    "embed1 = tf.expand_dims(word_embed[idx[0][0]], axis=0)\n",
    "\n",
    "\n",
    "text = \"putin\"\n",
    "text = text.lower()\n",
    "\n",
    "idx = tf.cast(tokenizer.tokenizer.tokenize(text), tf.int32)\n",
    "idx = tokenize(idx, tokenizer.merge_list)\n",
    "print(idx)\n",
    "embed2 = tf.expand_dims(word_embed[idx[0][0]], axis=0)\n",
    "\n",
    "text = \"netanyahu\"\n",
    "text = text.lower()\n",
    "\n",
    "idx = tf.cast(tokenizer.tokenizer.tokenize(text), tf.int32)\n",
    "idx = tokenize(idx, tokenizer.merge_list)\n",
    "print(idx)\n",
    "embed3 = tf.expand_dims(word_embed[idx[0][0]], axis=0)\n",
    "\n",
    "embed = embed1 - embed2 + embed3\n",
    "\n",
    "cosine_sim = cosine_similarity(embed, word_embed, normalize=False)\n",
    "idx = tf.argsort(cosine_sim, axis=-1, \n",
    "                 direction='DESCENDING',\n",
    "                 #direction='ASCENDING', \n",
    "                 stable=False, name=None)[0]\n",
    "\n",
    "for i in idx[:100]:\n",
    "    i = tf.expand_dims(i, axis=0)\n",
    "    print(tokenizer.detokenize(i).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "54734624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[553]], shape=(1, 1), dtype=int32)\n",
      "obama\n",
      "obama's\n",
      "clinton\n",
      "romney\n",
      "republicans\n",
      "bush\n",
      "boehner\n",
      "sen\n",
      "reagan\n",
      "democrats\n",
      "barack\n",
      "mccain\n",
      "congressional\n",
      "sarkozy\n",
      "pentagon\n",
      "putin\n",
      "u\n",
      "assad\n",
      "liberals\n",
      "afghans\n",
      "calderon\n",
      "washington\n",
      "bush's\n",
      "conservatives\n",
      "president\n",
      "obamacare\n",
      "iraqis\n",
      "panetta\n",
      "snowden\n",
      "mcconnell\n",
      "clinton's\n",
      "chavez\n",
      "gop\n",
      "palin\n",
      "americans\n",
      "senate\n",
      "christie\n",
      "isis\n",
      "veterans\n",
      "he\n",
      "voters\n",
      "petraeus\n",
      "pelosi\n",
      "secretary\n",
      "jindal\n",
      "george\n",
      "mandela\n",
      "republican\n",
      "kerry\n",
      "karzai\n",
      "biden\n",
      "gop's\n",
      "francis\n",
      "economists\n",
      "lawmakers\n",
      "jeb\n",
      "we've\n",
      "congressman\n",
      "rouhani\n",
      "navarrette\n",
      "congress\n",
      "netanyahu\n",
      "latinos\n",
      "nixon\n",
      "aides\n",
      "iraqi\n",
      "nra\n",
      "pelos\n",
      "richard\n",
      "white\n",
      "clint\n",
      "gov\n",
      "lincoln\n",
      "romney's\n",
      "gingrich\n",
      "taxpayers\n",
      "nato\n",
      "presidents\n",
      "vietnam\n",
      "nieto\n",
      "analysts\n",
      "president's\n",
      "ryan\n",
      "gupta\n",
      "senators\n",
      "cdc\n",
      "shinse\n",
      "brennan\n",
      "next\n",
      "capitol\n",
      "legislators\n",
      "ahmadinejad\n",
      "elect\n",
      "erdogan\n",
      "afghanistan\n",
      "iraq\n",
      "gadhafi\n",
      "roosevelt\n",
      "cheney\n",
      "santorum\n"
     ]
    }
   ],
   "source": [
    "word_embed = model.word_embed\n",
    "\n",
    "text = \"obama\"\n",
    "text = text.lower()\n",
    "\n",
    "idx = tf.cast(tokenizer.tokenize(text), tf.int32)\n",
    "print(idx)\n",
    "embed = tf.expand_dims(word_embed[idx[0][0]], axis=0)\n",
    "\n",
    "cosine_sim = embed@tf.transpose(word_embed)\n",
    "idx = tf.argsort(cosine_sim, axis=-1, \n",
    "                 direction='DESCENDING',\n",
    "                 #direction='ASCENDING', \n",
    "                 stable=False, name=None)[0]\n",
    "\n",
    "for i in idx[:100]:\n",
    "    i = tf.expand_dims(i, axis=0)\n",
    "    print(tokenizer.detokenize(i).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ff23330f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[5393]], shape=(1, 1), dtype=int32)\n",
      "netanyahu\n",
      "abulary\n",
      "hagel\n",
      "maduro\n",
      "espion\n",
      "yingluck\n",
      "onsored\n",
      "nandez\n",
      "saleh\n",
      "natur\n",
      "ailand\n",
      "hezbol\n",
      "panetta\n",
      "biden\n",
      "shinse\n",
      "kerry\n",
      "gibbs\n",
      "sarkozy\n",
      "fundam\n",
      "hift\n",
      "patro\n",
      "signific\n",
      "anonymity\n",
      "putin\n",
      "mugabe\n",
      "lades\n",
      "boehner\n",
      "pelosi\n",
      "medvedev\n",
      "ahmadinejad\n",
      "warri\n",
      "thaksin\n",
      "landrieu\n",
      "shaba\n",
      "gbag\n",
      "accust\n",
      "charac\n",
      "fahren\n",
      "liby\n",
      "peninsu\n",
      "helicop\n",
      "zuma\n",
      "traged\n",
      "portugu\n",
      "morsy\n",
      "publ\n",
      "enjo\n",
      "ilight\n",
      "abbas\n",
      "erdogan\n",
      "ieval\n",
      "bachmann\n",
      "yanukovych\n",
      "leep\n",
      "confir\n",
      "rodrigue\n",
      "secutive\n",
      "provin\n",
      "mccain's\n",
      "moil\n",
      "subsequ\n",
      "abled\n",
      "juvent\n",
      "o'ne\n",
      "guardiola\n",
      "lieberman\n",
      "karzai\n",
      "catastro\n",
      "ouatt\n",
      "zardari\n",
      "possib\n",
      "toug\n",
      "theless\n",
      "burma\n",
      "carney\n",
      "ricul\n",
      "zhok\n",
      "barcelon\n",
      "dort\n",
      "sunnis\n",
      "lomb\n",
      "snowden\n",
      "avez\n",
      "diffic\n",
      "sess\n",
      "khamenei\n",
      "exer\n",
      "golese\n",
      "copen\n",
      "rouhani\n",
      "ipal\n",
      "transparen\n",
      "ultane\n",
      "mccain\n",
      "boeh\n",
      "diox\n",
      "citiz\n",
      "adjac\n",
      "nieto\n",
      "lavrov\n"
     ]
    }
   ],
   "source": [
    "word_embed = model.word_embed\n",
    "\n",
    "text = \"netanyahu\"\n",
    "text = text.lower()\n",
    "\n",
    "idx = tf.cast(tokenizer.tokenizer.tokenize(text), tf.int32)\n",
    "idx = tokenize(idx, tokenizer.merge_list)\n",
    "print(idx)\n",
    "embed = tf.expand_dims(word_embed[idx[0][0]], axis=0)\n",
    "\n",
    "cosine_sim = embed@tf.transpose(word_embed)\n",
    "idx = tf.argsort(cosine_sim, axis=-1, \n",
    "                 direction='DESCENDING',\n",
    "                 #direction='ASCENDING', \n",
    "                 stable=False, name=None)[0]\n",
    "\n",
    "for i in idx[:100]:\n",
    "    i = tf.expand_dims(i, axis=0)\n",
    "    print(tokenizer.detokenize(i).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ff28a5",
   "metadata": {},
   "source": [
    "## Mean Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b71e4c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 62)\n",
      " \n",
      ".\n",
      "-\n",
      "\"\n",
      ",\n",
      "a\n",
      "in\n",
      "\n",
      "\n",
      "and\n",
      "the\n",
      "on\n",
      "to\n",
      "at\n",
      "'\n",
      "an\n",
      "or\n",
      "u\n",
      "by\n",
      "that\n",
      ":\n",
      "as\n",
      "'s\n",
      "s\n",
      "al\n",
      "of\n",
      "it\n",
      "he\n",
      "for\n",
      "un\n",
      "over\n",
      "e\n",
      "about\n",
      "is\n",
      "with\n",
      "after\n",
      "up\n",
      "not\n",
      "last\n",
      "more\n",
      "may\n",
      "?\n",
      "re\n",
      "from\n",
      "ad\n",
      "(\n",
      "state\n",
      "be\n",
      "just\n",
      "so\n",
      "was\n",
      "one\n",
      "/\n",
      "ed\n",
      "no\n",
      "war\n",
      "while\n",
      "security\n",
      ";\n",
      "but\n",
      "1\n",
      "en\n",
      "n\n",
      "man\n",
      "house\n",
      "i\n",
      "north\n",
      "m\n",
      "first\n",
      "ar\n",
      "l\n",
      "f\n",
      "c\n",
      "er\n",
      "there\n",
      "out\n",
      "o\n",
      "do\n",
      "two\n",
      "when\n",
      "less\n",
      "had\n",
      "air\n",
      "k\n",
      "v\n",
      "h\n",
      "near\n",
      "they\n",
      "2\n",
      "his\n",
      "some\n",
      "de\n",
      "back\n",
      "we\n",
      "field\n",
      "fire\n",
      "if\n",
      "this\n",
      "under\n",
      "p\n",
      "right\n"
     ]
    }
   ],
   "source": [
    "word_embed = model.word_embed\n",
    "\n",
    "text = \"Obama's remarks came shortly after U.N. inspectors left Syria, carrying evidence that will determine whether chemical weapons were used in an attack early last week in a Damascus suburb.\"\n",
    "text = text.lower()\n",
    "\n",
    "idx = tf.cast(tokenizer.tokenizer.tokenize(text), tf.int32)\n",
    "idx = tokenize(idx, tokenizer.merge_list)\n",
    "print(idx.shape)\n",
    "embed_final = model.call(idx, logits=False)\n",
    "#embed_mean = embed_final[:,-1,:]\n",
    "embed_mean = tf.reduce_mean(embed_final, axis=1)\n",
    "embed_mean = tf.cast(embed_mean, dtype=tf.float32) \n",
    "\n",
    "cosine_sim = cosine_similarity(embed_mean, word_embed, normalize=False)\n",
    "#cosine_sim = cosine_similarity(embed_mean, word_embed, normalize=True)\n",
    "\n",
    "idx = tf.argsort(cosine_sim, axis=-1, \n",
    "                 direction='DESCENDING',\n",
    "                 #direction='ASCENDING', \n",
    "                 stable=False, name=None)[0]\n",
    "\n",
    "for i in idx[:100]:\n",
    "    i = tf.expand_dims(i, axis=0)\n",
    "    print(tokenizer.detokenize(i).numpy().decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d6f1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[553]], shape=(1, 1), dtype=int32)\n",
      "tf.Tensor(0.08745351, shape=(), dtype=float32)\n",
      "tf.Tensor([[    1    13    15 ... 15466  9736 15505]], shape=(1, 16070), dtype=int32)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m i \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(i, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "word_embed = model.word_embed\n",
    "\n",
    "text = \"Obama\"\n",
    "text = text.lower()\n",
    "\n",
    "idx = tf.cast(tokenizer.tokenizer.tokenize(text), tf.int32)\n",
    "idx = tokenize(idx, tokenizer.merge_list)\n",
    "print(idx)\n",
    "b = model.unembed_b[idx[0][0]]\n",
    "print(b)\n",
    "logits = model.call(idx, logits=True) \n",
    "#embed_mean = embed_final[:,-1,:]\n",
    "embed_mean = tf.reduce_mean(embed_final, axis=1)\n",
    "embed_mean = tf.cast(embed_mean, dtype=tf.float32)\n",
    "\n",
    "cosine_sim = cosine_similarity(embed_mean, word_embed, normalize=False)\n",
    "#cosine_sim = cosine_similarity(embed_mean, word_embed, normalize=True)\n",
    "\n",
    "idx = tf.argsort(logits, axis=-1, \n",
    "                 direction='DESCENDING',\n",
    "                 #direction='ASCENDING', \n",
    "                 stable=False, name=None)[0]\n",
    "\n",
    "for i in idx[:100]:\n",
    "    i = tf.expand_dims(i, axis=0)\n",
    "    print(i)\n",
    "    print(tokenizer.detokenize(i).numpy().decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a5f20b",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
