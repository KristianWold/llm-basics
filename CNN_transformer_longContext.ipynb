{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04010a08",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8fa3d166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from src.tokenizer import TokenizerBPE, fuse_tokenized_corpus, chunk_corpus\n",
    "\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "from tqdm.notebook import tqdm\n",
    "from src.transformer import Transformer, WarmUpThenDecay\n",
    "from src.data_handling import read_first_n, sample_batch\n",
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b2b59898",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7afd4e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 1024\n",
    "\n",
    "tokenizer = pkl.load(open(\"tokenizers/tokenizer_CNN_24k_whitespace.pkl\", 'rb'))\n",
    "tokenizer.create_hash()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e252544a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24072\n",
      "24070\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.vocab_size)\n",
    "print(tokenizer.token_to_idx[\"<s>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e38fb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(corpus, batch_size=32, offset=0):\n",
    "    corpus = corpus[offset:]\n",
    "    length = corpus.shape[0]\n",
    "\n",
    "    batches = length // batch_size\n",
    "\n",
    "    corpus = corpus[:batches * batch_size]\n",
    "\n",
    "    corpus = tf.reshape(corpus, [-1, batch_size])\n",
    "    return corpus\n",
    "\n",
    "corpus_1 = pkl.load(open('corpus/corpus_CNN_24k_whitespace_1', 'rb'))[0]\n",
    "corpus_2 = pkl.load(open('corpus/corpus_CNN_24k_whitespace_1', 'rb'))[0]\n",
    "corpus = tf.concat([corpus_1, corpus_2], axis=0)\n",
    "\n",
    "length = len(corpus)\n",
    "\n",
    "train_size = int(length * 0.95)\n",
    "corpus_train = corpus[:train_size]\n",
    "corpus_test = corpus[train_size:]\n",
    "\n",
    "corpus_train_batch = batch(corpus_train, batch_size = max_seq_len, offset=0)\n",
    "corpus_train_offset = batch(corpus_train, batch_size = max_seq_len, offset=max_seq_len//2)\n",
    "corpus_train = tf.concat([corpus_train_batch, corpus_train_offset], axis=0)\n",
    "\n",
    "corpus_test = batch(corpus_test, batch_size = max_seq_len, offset=0)\n",
    "corpus_test_offset = batch(corpus_test, batch_size = max_seq_len, offset=max_seq_len//2)\n",
    "corpus_test = tf.concat([corpus_test, corpus_test_offset], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c88cc00",
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl.dump(np.array(corpus_train), open('corpus/corpus_CNN_24k_whitespace_train_numpy', 'wb'))\n",
    "pkl.dump(np.array(corpus_test), open('corpus/corpus_CNN_24k_whitespace_test_numpy', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a70bdc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159109120\n"
     ]
    }
   ],
   "source": [
    "print(corpus_train.shape[0]*corpus_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6deeea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pipeline(corpus, batch_size=32):\n",
    "    samples = corpus.shape[0]\n",
    "\n",
    "    steps_per_epoch = samples // batch_size\n",
    "    \n",
    "    ds = tf.data.Dataset.from_tensor_slices(corpus)\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.shuffle(buffer_size=100*batch_size, reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(batch_size, drop_remainder=True).prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    return ds, steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2f84497",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, steps_per_epoch = data_pipeline(corpus_train, batch_size=8)\n",
    "ds_test,_ = data_pipeline(corpus_test, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f830d881",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2d47b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_lr = 1e-4\n",
    "decay_steps = 20000\n",
    "decay_rate = 1\n",
    "decay_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=initial_lr,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    staircase=False)\n",
    "\n",
    "warmup_steps = 1000\n",
    "lr_schedule = WarmUpThenDecay(\n",
    "    initial_learning_rate=initial_lr,\n",
    "    warmup_steps=warmup_steps,\n",
    "    decay_schedule_fn=decay_schedule)\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "max_seq_len = 1024\n",
    "embed_dim = 64*10\n",
    "tf_blocks = 10\n",
    "heads = 10\n",
    "ff_dim = 4*embed_dim\n",
    "weight_decay = 0.025\n",
    "dropout = 0.1\n",
    "accum_steps = 8\n",
    "\n",
    "unembed_dims = []\n",
    "\n",
    "model = Transformer(vocab_size=tokenizer.vocab_size,\n",
    "                    max_seq_len=max_seq_len,\n",
    "                    embed_dim=embed_dim,\n",
    "                    tf_blocks=tf_blocks,\n",
    "                    heads=heads,\n",
    "                    ff_dim = ff_dim,\n",
    "                    unembed_dims=unembed_dims,\n",
    "                    tokenizer=tokenizer,\n",
    "                    lr=lr_schedule,\n",
    "                    wd = weight_decay,\n",
    "                    dropout=dropout,\n",
    "                    accum_steps=tf.constant(accum_steps, dtype=tf.int32)\n",
    "                    )\n",
    "\n",
    "losses_train = []\n",
    "losses_test = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7970a401",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"model_24k_CNN\"\n",
    "\n",
    "\n",
    "ckpt = tf.train.Checkpoint(\n",
    "    optimizer=model.opt,\n",
    "    model=model\n",
    ")\n",
    "ckpt_manager = tf.train.CheckpointManager(\n",
    "    ckpt, \n",
    "    directory=\"checkpoints/\" + name,      # folder where ckpts are saved\n",
    "    max_to_keep=5                         # only keep 5 latest checkpoints\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88b34765",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'checkpoints/losses_model_24k_CNN.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m ckpt\u001b[38;5;241m.\u001b[39mrestore(ckpt_manager\u001b[38;5;241m.\u001b[39mlatest_checkpoint)\n\u001b[1;32m----> 2\u001b[0m losses_train, losses_test \u001b[38;5;241m=\u001b[39m pkl\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcheckpoints/losses_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'checkpoints/losses_model_24k_CNN.pkl'"
     ]
    }
   ],
   "source": [
    "ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "losses_train, losses_test = pkl.load(open(\"checkpoints/losses_\" + name + \".pkl\", 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6527620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 65295112\n"
     ]
    }
   ],
   "source": [
    "total_params = 0\n",
    "for var in model.parameter_list:\n",
    "    shape = var.get_shape()\n",
    "    num_params = 1\n",
    "    for dim in shape:\n",
    "        num_params *= dim\n",
    "    total_params += num_params\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8a0126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(iter_train, iter_test, steps_per_epoch, epochs):\n",
    "    for i in tqdm(range(steps_per_epoch//accum_steps*epochs)):\n",
    "        loss_train_temp = 0\n",
    "        loss_test_temp = 0\n",
    "        for _ in range(accum_steps):\n",
    "            batch_train = next(iter_train)\n",
    "            batch_test = next(iter_test)\n",
    "            \n",
    "            loss_train_temp += model.train_step(batch_train).numpy()\n",
    "            \n",
    "            if i % 25 == 0:\n",
    "                loss_test_temp += model.evaluate(batch_test).numpy()\n",
    "            \n",
    "        losses_train.append(loss_train_temp/accum_steps)\n",
    "\n",
    "        if i % 25 == 0:\n",
    "            losses_test.append(loss_test_temp/accum_steps)\n",
    "        else:\n",
    "            losses_test.append(losses_test[-1])\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            ckpt_manager.save()\n",
    "            pkl.dump([losses_train, losses_test], open(\"checkpoints/losses_\" + name + \".pkl\", 'wb'))\n",
    "\n",
    "    \n",
    "        lr = model.opt.inner_optimizer._decayed_lr(tf.float32).numpy()\n",
    "        print(f\"Step {i+1}, Loss Train: {losses_train[-1]:.4f}, Loss Test: {losses_test[-1]:.4f}, LR: {lr:.6f}\")\n",
    "    ckpt_manager.save()\n",
    "    pkl.dump([losses_train, losses_test], open(\"checkpoints/losses_\" + name + \".pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "25d60938",
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_train = iter(ds_train)\n",
    "iter_test = iter(iter_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1858201f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3b7b0a9919640a69b707ade9d9e970d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7281 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Tracing train_step; token shape: (None, None)\n",
      "🔄 Tracing train_step; token shape: (None, None)\n",
      "Step 1, Loss Train: 13.4090, Loss Test: 13.4437, LR: 0.000000\n",
      "Step 2, Loss Train: 13.4081, Loss Test: 13.4437, LR: 0.000000\n",
      "Step 3, Loss Train: 13.3730, Loss Test: 13.4437, LR: 0.000000\n",
      "Step 4, Loss Train: 13.3279, Loss Test: 13.4437, LR: 0.000000\n",
      "Step 5, Loss Train: 13.3163, Loss Test: 13.4437, LR: 0.000000\n",
      "Step 6, Loss Train: 13.2731, Loss Test: 13.4437, LR: 0.000001\n",
      "Step 7, Loss Train: 13.2564, Loss Test: 13.4437, LR: 0.000001\n",
      "Step 8, Loss Train: 13.2251, Loss Test: 13.4437, LR: 0.000001\n",
      "Step 9, Loss Train: 13.1363, Loss Test: 13.4437, LR: 0.000001\n",
      "Step 10, Loss Train: 13.0866, Loss Test: 13.4437, LR: 0.000001\n",
      "Step 11, Loss Train: 13.0335, Loss Test: 13.4437, LR: 0.000001\n",
      "Step 12, Loss Train: 12.9482, Loss Test: 13.4437, LR: 0.000001\n",
      "Step 13, Loss Train: 12.8522, Loss Test: 13.4437, LR: 0.000001\n",
      "Step 14, Loss Train: 12.7431, Loss Test: 13.4437, LR: 0.000001\n",
      "Step 15, Loss Train: 12.6588, Loss Test: 13.4437, LR: 0.000001\n",
      "Step 16, Loss Train: 12.5297, Loss Test: 13.4437, LR: 0.000002\n",
      "Step 17, Loss Train: 12.4636, Loss Test: 13.4437, LR: 0.000002\n",
      "Step 18, Loss Train: 12.3395, Loss Test: 13.4437, LR: 0.000002\n",
      "Step 19, Loss Train: 12.2317, Loss Test: 13.4437, LR: 0.000002\n",
      "Step 20, Loss Train: 12.1057, Loss Test: 13.4437, LR: 0.000002\n",
      "Step 21, Loss Train: 11.9938, Loss Test: 13.4437, LR: 0.000002\n",
      "Step 22, Loss Train: 11.8904, Loss Test: 13.4437, LR: 0.000002\n",
      "Step 23, Loss Train: 11.7239, Loss Test: 13.4437, LR: 0.000002\n",
      "Step 24, Loss Train: 11.5760, Loss Test: 13.4437, LR: 0.000002\n",
      "Step 25, Loss Train: 11.4555, Loss Test: 13.4437, LR: 0.000002\n",
      "Step 26, Loss Train: 11.3823, Loss Test: 11.1480, LR: 0.000003\n",
      "Step 27, Loss Train: 11.2502, Loss Test: 11.1480, LR: 0.000003\n",
      "Step 28, Loss Train: 11.1159, Loss Test: 11.1480, LR: 0.000003\n",
      "Step 29, Loss Train: 11.0310, Loss Test: 11.1480, LR: 0.000003\n",
      "Step 30, Loss Train: 10.9532, Loss Test: 11.1480, LR: 0.000003\n",
      "Step 31, Loss Train: 10.8627, Loss Test: 11.1480, LR: 0.000003\n",
      "Step 32, Loss Train: 10.7867, Loss Test: 11.1480, LR: 0.000003\n",
      "Step 33, Loss Train: 10.6557, Loss Test: 11.1480, LR: 0.000003\n",
      "Step 34, Loss Train: 10.6154, Loss Test: 11.1480, LR: 0.000003\n",
      "Step 35, Loss Train: 10.4921, Loss Test: 11.1480, LR: 0.000003\n",
      "Step 36, Loss Train: 10.4102, Loss Test: 11.1480, LR: 0.000004\n",
      "Step 37, Loss Train: 10.3557, Loss Test: 11.1480, LR: 0.000004\n",
      "Step 38, Loss Train: 10.2728, Loss Test: 11.1480, LR: 0.000004\n",
      "Step 39, Loss Train: 10.2389, Loss Test: 11.1480, LR: 0.000004\n",
      "Step 40, Loss Train: 10.1829, Loss Test: 11.1480, LR: 0.000004\n",
      "Step 41, Loss Train: 10.1491, Loss Test: 11.1480, LR: 0.000004\n",
      "Step 42, Loss Train: 10.0850, Loss Test: 11.1480, LR: 0.000004\n",
      "Step 43, Loss Train: 10.0139, Loss Test: 11.1480, LR: 0.000004\n",
      "Step 44, Loss Train: 9.9518, Loss Test: 11.1480, LR: 0.000004\n",
      "Step 45, Loss Train: 9.9006, Loss Test: 11.1480, LR: 0.000005\n",
      "Step 46, Loss Train: 9.8456, Loss Test: 11.1480, LR: 0.000005\n",
      "Step 47, Loss Train: 9.8182, Loss Test: 11.1480, LR: 0.000005\n",
      "Step 48, Loss Train: 9.7212, Loss Test: 11.1480, LR: 0.000005\n",
      "Step 49, Loss Train: 9.7201, Loss Test: 11.1480, LR: 0.000005\n",
      "Step 50, Loss Train: 9.6544, Loss Test: 11.1480, LR: 0.000005\n",
      "Step 51, Loss Train: 9.6329, Loss Test: 9.3539, LR: 0.000005\n",
      "Step 52, Loss Train: 9.5339, Loss Test: 9.3539, LR: 0.000005\n",
      "Step 53, Loss Train: 9.5786, Loss Test: 9.3539, LR: 0.000005\n",
      "Step 54, Loss Train: 9.4695, Loss Test: 9.3539, LR: 0.000005\n",
      "Step 55, Loss Train: 9.4050, Loss Test: 9.3539, LR: 0.000006\n",
      "Step 56, Loss Train: 9.4061, Loss Test: 9.3539, LR: 0.000006\n",
      "Step 57, Loss Train: 9.3580, Loss Test: 9.3539, LR: 0.000006\n",
      "Step 58, Loss Train: 9.2991, Loss Test: 9.3539, LR: 0.000006\n",
      "Step 59, Loss Train: 9.2516, Loss Test: 9.3539, LR: 0.000006\n",
      "Step 60, Loss Train: 9.2532, Loss Test: 9.3539, LR: 0.000006\n",
      "Step 61, Loss Train: 9.1765, Loss Test: 9.3539, LR: 0.000006\n",
      "Step 62, Loss Train: 9.1646, Loss Test: 9.3539, LR: 0.000006\n",
      "Step 63, Loss Train: 9.1418, Loss Test: 9.3539, LR: 0.000006\n",
      "Step 64, Loss Train: 9.1226, Loss Test: 9.3539, LR: 0.000006\n",
      "Step 65, Loss Train: 9.1154, Loss Test: 9.3539, LR: 0.000006\n",
      "Step 66, Loss Train: 8.9775, Loss Test: 9.3539, LR: 0.000007\n",
      "Step 67, Loss Train: 8.9871, Loss Test: 9.3539, LR: 0.000007\n",
      "Step 68, Loss Train: 8.9591, Loss Test: 9.3539, LR: 0.000007\n",
      "Step 69, Loss Train: 8.9587, Loss Test: 9.3539, LR: 0.000007\n",
      "Step 70, Loss Train: 8.8710, Loss Test: 9.3539, LR: 0.000007\n",
      "Step 71, Loss Train: 8.8724, Loss Test: 9.3539, LR: 0.000007\n",
      "Step 72, Loss Train: 8.8728, Loss Test: 9.3539, LR: 0.000007\n",
      "Step 73, Loss Train: 8.8189, Loss Test: 9.3539, LR: 0.000007\n",
      "Step 74, Loss Train: 8.7353, Loss Test: 9.3539, LR: 0.000007\n",
      "Step 75, Loss Train: 8.7390, Loss Test: 9.3539, LR: 0.000008\n",
      "Step 76, Loss Train: 8.7018, Loss Test: 8.4925, LR: 0.000008\n",
      "Step 77, Loss Train: 8.6755, Loss Test: 8.4925, LR: 0.000008\n",
      "Step 78, Loss Train: 8.6794, Loss Test: 8.4925, LR: 0.000008\n",
      "Step 79, Loss Train: 8.6350, Loss Test: 8.4925, LR: 0.000008\n",
      "Step 80, Loss Train: 8.6243, Loss Test: 8.4925, LR: 0.000008\n",
      "Step 81, Loss Train: 8.6495, Loss Test: 8.4925, LR: 0.000008\n",
      "Step 82, Loss Train: 8.5840, Loss Test: 8.4925, LR: 0.000008\n",
      "Step 83, Loss Train: 8.5462, Loss Test: 8.4925, LR: 0.000008\n",
      "Step 84, Loss Train: 8.5740, Loss Test: 8.4925, LR: 0.000008\n",
      "Step 85, Loss Train: 8.4877, Loss Test: 8.4925, LR: 0.000009\n",
      "Step 86, Loss Train: 8.4809, Loss Test: 8.4925, LR: 0.000009\n",
      "Step 87, Loss Train: 8.4353, Loss Test: 8.4925, LR: 0.000009\n",
      "Step 88, Loss Train: 8.4815, Loss Test: 8.4925, LR: 0.000009\n",
      "Step 89, Loss Train: 8.4130, Loss Test: 8.4925, LR: 0.000009\n",
      "Step 90, Loss Train: 8.3967, Loss Test: 8.4925, LR: 0.000009\n",
      "Step 91, Loss Train: 8.3761, Loss Test: 8.4925, LR: 0.000009\n",
      "Step 92, Loss Train: 8.3809, Loss Test: 8.4925, LR: 0.000009\n",
      "Step 93, Loss Train: 8.3214, Loss Test: 8.4925, LR: 0.000009\n",
      "Step 94, Loss Train: 8.3041, Loss Test: 8.4925, LR: 0.000009\n",
      "Step 95, Loss Train: 8.2923, Loss Test: 8.4925, LR: 0.000009\n",
      "Step 96, Loss Train: 8.2753, Loss Test: 8.4925, LR: 0.000010\n",
      "Step 97, Loss Train: 8.2505, Loss Test: 8.4925, LR: 0.000010\n",
      "Step 98, Loss Train: 8.2445, Loss Test: 8.4925, LR: 0.000010\n",
      "Step 99, Loss Train: 8.1985, Loss Test: 8.4925, LR: 0.000010\n",
      "Step 100, Loss Train: 8.2081, Loss Test: 8.4925, LR: 0.000010\n",
      "Step 101, Loss Train: 8.1894, Loss Test: 7.9536, LR: 0.000010\n",
      "Step 102, Loss Train: 8.1946, Loss Test: 7.9536, LR: 0.000010\n",
      "Step 103, Loss Train: 8.1556, Loss Test: 7.9536, LR: 0.000010\n",
      "Step 104, Loss Train: 8.1548, Loss Test: 7.9536, LR: 0.000010\n",
      "Step 105, Loss Train: 8.0771, Loss Test: 7.9536, LR: 0.000010\n",
      "Step 106, Loss Train: 8.0834, Loss Test: 7.9536, LR: 0.000011\n",
      "Step 107, Loss Train: 8.1022, Loss Test: 7.9536, LR: 0.000011\n",
      "Step 108, Loss Train: 8.0283, Loss Test: 7.9536, LR: 0.000011\n",
      "Step 109, Loss Train: 8.0557, Loss Test: 7.9536, LR: 0.000011\n",
      "Step 110, Loss Train: 8.0149, Loss Test: 7.9536, LR: 0.000011\n",
      "Step 111, Loss Train: 7.9833, Loss Test: 7.9536, LR: 0.000011\n",
      "Step 112, Loss Train: 8.0124, Loss Test: 7.9536, LR: 0.000011\n",
      "Step 113, Loss Train: 7.9900, Loss Test: 7.9536, LR: 0.000011\n",
      "Step 114, Loss Train: 7.9872, Loss Test: 7.9536, LR: 0.000011\n",
      "Step 115, Loss Train: 7.9558, Loss Test: 7.9536, LR: 0.000012\n",
      "Step 116, Loss Train: 7.8786, Loss Test: 7.9536, LR: 0.000012\n",
      "Step 117, Loss Train: 7.8849, Loss Test: 7.9536, LR: 0.000012\n",
      "Step 118, Loss Train: 7.8840, Loss Test: 7.9536, LR: 0.000012\n",
      "Step 119, Loss Train: 7.8903, Loss Test: 7.9536, LR: 0.000012\n",
      "Step 120, Loss Train: 7.8812, Loss Test: 7.9536, LR: 0.000012\n",
      "Step 121, Loss Train: 7.8672, Loss Test: 7.9536, LR: 0.000012\n",
      "Step 122, Loss Train: 7.8402, Loss Test: 7.9536, LR: 0.000012\n",
      "Step 123, Loss Train: 7.8274, Loss Test: 7.9536, LR: 0.000012\n",
      "Step 124, Loss Train: 7.8221, Loss Test: 7.9536, LR: 0.000012\n",
      "Step 125, Loss Train: 7.7844, Loss Test: 7.9536, LR: 0.000012\n",
      "Step 126, Loss Train: 7.7644, Loss Test: 7.6215, LR: 0.000013\n",
      "Step 127, Loss Train: 7.7872, Loss Test: 7.6215, LR: 0.000013\n",
      "Step 128, Loss Train: 7.7545, Loss Test: 7.6215, LR: 0.000013\n",
      "Step 129, Loss Train: 7.7380, Loss Test: 7.6215, LR: 0.000013\n",
      "Step 130, Loss Train: 7.7161, Loss Test: 7.6215, LR: 0.000013\n",
      "Step 131, Loss Train: 7.7174, Loss Test: 7.6215, LR: 0.000013\n",
      "Step 132, Loss Train: 7.7146, Loss Test: 7.6215, LR: 0.000013\n",
      "Step 133, Loss Train: 7.6898, Loss Test: 7.6215, LR: 0.000013\n",
      "Step 134, Loss Train: 7.6803, Loss Test: 7.6215, LR: 0.000013\n",
      "Step 135, Loss Train: 7.6587, Loss Test: 7.6215, LR: 0.000014\n",
      "Step 136, Loss Train: 7.6426, Loss Test: 7.6215, LR: 0.000014\n",
      "Step 137, Loss Train: 7.6702, Loss Test: 7.6215, LR: 0.000014\n",
      "Step 138, Loss Train: 7.6549, Loss Test: 7.6215, LR: 0.000014\n",
      "Step 139, Loss Train: 7.6575, Loss Test: 7.6215, LR: 0.000014\n",
      "Step 140, Loss Train: 7.6625, Loss Test: 7.6215, LR: 0.000014\n",
      "Step 141, Loss Train: 7.6114, Loss Test: 7.6215, LR: 0.000014\n",
      "Step 142, Loss Train: 7.5886, Loss Test: 7.6215, LR: 0.000014\n",
      "Step 143, Loss Train: 7.6159, Loss Test: 7.6215, LR: 0.000014\n",
      "Step 144, Loss Train: 7.5960, Loss Test: 7.6215, LR: 0.000014\n",
      "Step 145, Loss Train: 7.6165, Loss Test: 7.6215, LR: 0.000014\n",
      "Step 146, Loss Train: 7.6094, Loss Test: 7.6215, LR: 0.000015\n",
      "Step 147, Loss Train: 7.5616, Loss Test: 7.6215, LR: 0.000015\n",
      "Step 148, Loss Train: 7.5974, Loss Test: 7.6215, LR: 0.000015\n",
      "Step 149, Loss Train: 7.5956, Loss Test: 7.6215, LR: 0.000015\n",
      "Step 150, Loss Train: 7.5832, Loss Test: 7.6215, LR: 0.000015\n",
      "Step 151, Loss Train: 7.5798, Loss Test: 7.3278, LR: 0.000015\n",
      "Step 152, Loss Train: 7.5116, Loss Test: 7.3278, LR: 0.000015\n",
      "Step 153, Loss Train: 7.5473, Loss Test: 7.3278, LR: 0.000015\n",
      "Step 154, Loss Train: 7.5130, Loss Test: 7.3278, LR: 0.000015\n",
      "Step 155, Loss Train: 7.5537, Loss Test: 7.3278, LR: 0.000015\n",
      "Step 156, Loss Train: 7.5040, Loss Test: 7.3278, LR: 0.000016\n",
      "Step 157, Loss Train: 7.5152, Loss Test: 7.3278, LR: 0.000016\n",
      "Step 158, Loss Train: 7.5211, Loss Test: 7.3278, LR: 0.000016\n",
      "Step 159, Loss Train: 7.4991, Loss Test: 7.3278, LR: 0.000016\n",
      "Step 160, Loss Train: 7.5298, Loss Test: 7.3278, LR: 0.000016\n",
      "Step 161, Loss Train: 7.4869, Loss Test: 7.3278, LR: 0.000016\n",
      "Step 162, Loss Train: 7.4706, Loss Test: 7.3278, LR: 0.000016\n",
      "Step 163, Loss Train: 7.4686, Loss Test: 7.3278, LR: 0.000016\n",
      "Step 164, Loss Train: 7.4892, Loss Test: 7.3278, LR: 0.000016\n",
      "Step 165, Loss Train: 7.4881, Loss Test: 7.3278, LR: 0.000017\n",
      "Step 166, Loss Train: 7.4653, Loss Test: 7.3278, LR: 0.000017\n",
      "Step 167, Loss Train: 7.4434, Loss Test: 7.3278, LR: 0.000017\n",
      "Step 168, Loss Train: 7.4389, Loss Test: 7.3278, LR: 0.000017\n",
      "Step 169, Loss Train: 7.4314, Loss Test: 7.3278, LR: 0.000017\n",
      "Step 170, Loss Train: 7.4002, Loss Test: 7.3278, LR: 0.000017\n",
      "Step 171, Loss Train: 7.4669, Loss Test: 7.3278, LR: 0.000017\n",
      "Step 172, Loss Train: 7.4087, Loss Test: 7.3278, LR: 0.000017\n",
      "Step 173, Loss Train: 7.4539, Loss Test: 7.3278, LR: 0.000017\n",
      "Step 174, Loss Train: 7.4209, Loss Test: 7.3278, LR: 0.000017\n",
      "Step 175, Loss Train: 7.4292, Loss Test: 7.3278, LR: 0.000017\n",
      "Step 176, Loss Train: 7.3941, Loss Test: 7.2144, LR: 0.000018\n",
      "Step 177, Loss Train: 7.4410, Loss Test: 7.2144, LR: 0.000018\n",
      "Step 178, Loss Train: 7.3969, Loss Test: 7.2144, LR: 0.000018\n",
      "Step 179, Loss Train: 7.4390, Loss Test: 7.2144, LR: 0.000018\n",
      "Step 180, Loss Train: 7.4502, Loss Test: 7.2144, LR: 0.000018\n",
      "Step 181, Loss Train: 7.4071, Loss Test: 7.2144, LR: 0.000018\n",
      "Step 182, Loss Train: 7.3798, Loss Test: 7.2144, LR: 0.000018\n",
      "Step 183, Loss Train: 7.3951, Loss Test: 7.2144, LR: 0.000018\n",
      "Step 184, Loss Train: 7.3664, Loss Test: 7.2144, LR: 0.000018\n",
      "Step 185, Loss Train: 7.4009, Loss Test: 7.2144, LR: 0.000018\n",
      "Step 186, Loss Train: 7.4118, Loss Test: 7.2144, LR: 0.000019\n",
      "Step 187, Loss Train: 7.3960, Loss Test: 7.2144, LR: 0.000019\n",
      "Step 188, Loss Train: 7.3951, Loss Test: 7.2144, LR: 0.000019\n",
      "Step 189, Loss Train: 7.4018, Loss Test: 7.2144, LR: 0.000019\n",
      "Step 190, Loss Train: 7.3665, Loss Test: 7.2144, LR: 0.000019\n",
      "Step 191, Loss Train: 7.3985, Loss Test: 7.2144, LR: 0.000019\n",
      "Step 192, Loss Train: 7.3364, Loss Test: 7.2144, LR: 0.000019\n",
      "Step 193, Loss Train: 7.4164, Loss Test: 7.2144, LR: 0.000019\n",
      "Step 194, Loss Train: 7.3489, Loss Test: 7.2144, LR: 0.000019\n",
      "Step 195, Loss Train: 7.3804, Loss Test: 7.2144, LR: 0.000019\n",
      "Step 196, Loss Train: 7.3930, Loss Test: 7.2144, LR: 0.000020\n",
      "Step 197, Loss Train: 7.3686, Loss Test: 7.2144, LR: 0.000020\n",
      "Step 198, Loss Train: 7.3887, Loss Test: 7.2144, LR: 0.000020\n",
      "Step 199, Loss Train: 7.3678, Loss Test: 7.2144, LR: 0.000020\n",
      "Step 200, Loss Train: 7.3917, Loss Test: 7.2144, LR: 0.000020\n",
      "Step 201, Loss Train: 7.3467, Loss Test: 7.1869, LR: 0.000020\n",
      "Step 202, Loss Train: 7.3807, Loss Test: 7.1869, LR: 0.000020\n",
      "Step 203, Loss Train: 7.3522, Loss Test: 7.1869, LR: 0.000020\n",
      "Step 204, Loss Train: 7.3558, Loss Test: 7.1869, LR: 0.000020\n",
      "Step 205, Loss Train: 7.3644, Loss Test: 7.1869, LR: 0.000020\n",
      "Step 206, Loss Train: 7.3789, Loss Test: 7.1869, LR: 0.000021\n",
      "Step 207, Loss Train: 7.3701, Loss Test: 7.1869, LR: 0.000021\n",
      "Step 208, Loss Train: 7.3539, Loss Test: 7.1869, LR: 0.000021\n",
      "Step 209, Loss Train: 7.3576, Loss Test: 7.1869, LR: 0.000021\n",
      "Step 210, Loss Train: 7.3676, Loss Test: 7.1869, LR: 0.000021\n",
      "Step 211, Loss Train: 7.3605, Loss Test: 7.1869, LR: 0.000021\n",
      "Step 212, Loss Train: 7.3423, Loss Test: 7.1869, LR: 0.000021\n",
      "Step 213, Loss Train: 7.3323, Loss Test: 7.1869, LR: 0.000021\n",
      "Step 214, Loss Train: 7.3761, Loss Test: 7.1869, LR: 0.000021\n",
      "Step 215, Loss Train: 7.3222, Loss Test: 7.1869, LR: 0.000022\n",
      "Step 216, Loss Train: 7.3257, Loss Test: 7.1869, LR: 0.000022\n",
      "Step 217, Loss Train: 7.3666, Loss Test: 7.1869, LR: 0.000022\n",
      "Step 218, Loss Train: 7.3409, Loss Test: 7.1869, LR: 0.000022\n",
      "Step 219, Loss Train: 7.3456, Loss Test: 7.1869, LR: 0.000022\n",
      "Step 220, Loss Train: 7.2891, Loss Test: 7.1869, LR: 0.000022\n",
      "Step 221, Loss Train: 7.2924, Loss Test: 7.1869, LR: 0.000022\n",
      "Step 222, Loss Train: 7.3080, Loss Test: 7.1869, LR: 0.000022\n",
      "Step 223, Loss Train: 7.3475, Loss Test: 7.1869, LR: 0.000022\n",
      "Step 224, Loss Train: 7.3140, Loss Test: 7.1869, LR: 0.000022\n",
      "Step 225, Loss Train: 7.3050, Loss Test: 7.1869, LR: 0.000022\n",
      "Step 226, Loss Train: 7.3442, Loss Test: 7.2089, LR: 0.000023\n",
      "Step 227, Loss Train: 7.3391, Loss Test: 7.2089, LR: 0.000023\n",
      "Step 228, Loss Train: 7.3354, Loss Test: 7.2089, LR: 0.000023\n",
      "Step 229, Loss Train: 7.3401, Loss Test: 7.2089, LR: 0.000023\n",
      "Step 230, Loss Train: 7.3138, Loss Test: 7.2089, LR: 0.000023\n",
      "Step 231, Loss Train: 7.3009, Loss Test: 7.2089, LR: 0.000023\n",
      "Step 232, Loss Train: 7.3283, Loss Test: 7.2089, LR: 0.000023\n",
      "Step 233, Loss Train: 7.3342, Loss Test: 7.2089, LR: 0.000023\n",
      "Step 234, Loss Train: 7.3591, Loss Test: 7.2089, LR: 0.000023\n",
      "Step 235, Loss Train: 7.3243, Loss Test: 7.2089, LR: 0.000023\n",
      "Step 236, Loss Train: 7.3412, Loss Test: 7.2089, LR: 0.000024\n",
      "Step 237, Loss Train: 7.2872, Loss Test: 7.2089, LR: 0.000024\n",
      "Step 238, Loss Train: 7.3427, Loss Test: 7.2089, LR: 0.000024\n",
      "Step 239, Loss Train: 7.3176, Loss Test: 7.2089, LR: 0.000024\n",
      "Step 240, Loss Train: 7.3039, Loss Test: 7.2089, LR: 0.000024\n",
      "Step 241, Loss Train: 7.3208, Loss Test: 7.2089, LR: 0.000024\n",
      "Step 242, Loss Train: 7.3254, Loss Test: 7.2089, LR: 0.000024\n",
      "Step 243, Loss Train: 7.3422, Loss Test: 7.2089, LR: 0.000024\n",
      "Step 244, Loss Train: 7.2909, Loss Test: 7.2089, LR: 0.000024\n",
      "Step 245, Loss Train: 7.3194, Loss Test: 7.2089, LR: 0.000024\n",
      "Step 246, Loss Train: 7.3627, Loss Test: 7.2089, LR: 0.000025\n",
      "Step 247, Loss Train: 7.2847, Loss Test: 7.2089, LR: 0.000025\n",
      "Step 248, Loss Train: 7.3666, Loss Test: 7.2089, LR: 0.000025\n",
      "Step 249, Loss Train: 7.2409, Loss Test: 7.2089, LR: 0.000025\n",
      "Step 250, Loss Train: 7.3582, Loss Test: 7.2089, LR: 0.000025\n",
      "Step 251, Loss Train: 7.3317, Loss Test: 7.1602, LR: 0.000025\n",
      "Step 252, Loss Train: 7.3547, Loss Test: 7.1602, LR: 0.000025\n",
      "Step 253, Loss Train: 7.3884, Loss Test: 7.1602, LR: 0.000025\n",
      "Step 254, Loss Train: 7.3425, Loss Test: 7.1602, LR: 0.000025\n",
      "Step 255, Loss Train: 7.3563, Loss Test: 7.1602, LR: 0.000025\n",
      "Step 256, Loss Train: 7.3199, Loss Test: 7.1602, LR: 0.000026\n",
      "Step 257, Loss Train: 7.3208, Loss Test: 7.1602, LR: 0.000026\n",
      "Step 258, Loss Train: 7.3133, Loss Test: 7.1602, LR: 0.000026\n",
      "Step 259, Loss Train: 7.3205, Loss Test: 7.1602, LR: 0.000026\n",
      "Step 260, Loss Train: 7.3417, Loss Test: 7.1602, LR: 0.000026\n",
      "Step 261, Loss Train: 7.3145, Loss Test: 7.1602, LR: 0.000026\n",
      "Step 262, Loss Train: 7.3279, Loss Test: 7.1602, LR: 0.000026\n",
      "Step 263, Loss Train: 7.2894, Loss Test: 7.1602, LR: 0.000026\n",
      "Step 264, Loss Train: 7.2843, Loss Test: 7.1602, LR: 0.000026\n",
      "Step 265, Loss Train: 7.2853, Loss Test: 7.1602, LR: 0.000026\n",
      "Step 266, Loss Train: 7.2912, Loss Test: 7.1602, LR: 0.000027\n",
      "Step 267, Loss Train: 7.2749, Loss Test: 7.1602, LR: 0.000027\n",
      "Step 268, Loss Train: 7.2708, Loss Test: 7.1602, LR: 0.000027\n",
      "Step 269, Loss Train: 7.3416, Loss Test: 7.1602, LR: 0.000027\n",
      "Step 270, Loss Train: 7.2915, Loss Test: 7.1602, LR: 0.000027\n",
      "Step 271, Loss Train: 7.3500, Loss Test: 7.1602, LR: 0.000027\n",
      "Step 272, Loss Train: 7.2958, Loss Test: 7.1602, LR: 0.000027\n",
      "Step 273, Loss Train: 7.3181, Loss Test: 7.1602, LR: 0.000027\n",
      "Step 274, Loss Train: 7.2677, Loss Test: 7.1602, LR: 0.000027\n",
      "Step 275, Loss Train: 7.3012, Loss Test: 7.1602, LR: 0.000028\n",
      "Step 276, Loss Train: 7.3339, Loss Test: 7.1638, LR: 0.000028\n",
      "Step 277, Loss Train: 7.2863, Loss Test: 7.1638, LR: 0.000028\n",
      "Step 278, Loss Train: 7.2711, Loss Test: 7.1638, LR: 0.000028\n",
      "Step 279, Loss Train: 7.3507, Loss Test: 7.1638, LR: 0.000028\n",
      "Step 280, Loss Train: 7.3239, Loss Test: 7.1638, LR: 0.000028\n",
      "Step 281, Loss Train: 7.2877, Loss Test: 7.1638, LR: 0.000028\n",
      "Step 282, Loss Train: 7.2956, Loss Test: 7.1638, LR: 0.000028\n",
      "Step 283, Loss Train: 7.3099, Loss Test: 7.1638, LR: 0.000028\n",
      "Step 284, Loss Train: 7.2724, Loss Test: 7.1638, LR: 0.000028\n",
      "Step 285, Loss Train: 7.2776, Loss Test: 7.1638, LR: 0.000028\n",
      "Step 286, Loss Train: 7.2755, Loss Test: 7.1638, LR: 0.000029\n",
      "Step 287, Loss Train: 7.3262, Loss Test: 7.1638, LR: 0.000029\n",
      "Step 288, Loss Train: 7.3325, Loss Test: 7.1638, LR: 0.000029\n",
      "Step 289, Loss Train: 7.3500, Loss Test: 7.1638, LR: 0.000029\n",
      "Step 290, Loss Train: 7.3232, Loss Test: 7.1638, LR: 0.000029\n",
      "Step 291, Loss Train: 7.3224, Loss Test: 7.1638, LR: 0.000029\n",
      "Step 292, Loss Train: 7.2713, Loss Test: 7.1638, LR: 0.000029\n",
      "Step 293, Loss Train: 7.3194, Loss Test: 7.1638, LR: 0.000029\n",
      "Step 294, Loss Train: 7.2995, Loss Test: 7.1638, LR: 0.000029\n",
      "Step 295, Loss Train: 7.3289, Loss Test: 7.1638, LR: 0.000029\n",
      "Step 296, Loss Train: 7.3339, Loss Test: 7.1638, LR: 0.000030\n",
      "Step 297, Loss Train: 7.3465, Loss Test: 7.1638, LR: 0.000030\n",
      "Step 298, Loss Train: 7.3227, Loss Test: 7.1638, LR: 0.000030\n",
      "Step 299, Loss Train: 7.2588, Loss Test: 7.1638, LR: 0.000030\n",
      "Step 300, Loss Train: 7.2837, Loss Test: 7.1638, LR: 0.000030\n",
      "Step 301, Loss Train: 7.2900, Loss Test: 7.1809, LR: 0.000030\n",
      "Step 302, Loss Train: 7.2622, Loss Test: 7.1809, LR: 0.000030\n",
      "Step 303, Loss Train: 7.2679, Loss Test: 7.1809, LR: 0.000030\n",
      "Step 304, Loss Train: 7.2644, Loss Test: 7.1809, LR: 0.000030\n",
      "Step 305, Loss Train: 7.3339, Loss Test: 7.1809, LR: 0.000030\n",
      "Step 306, Loss Train: 7.2953, Loss Test: 7.1809, LR: 0.000031\n",
      "Step 307, Loss Train: 7.3343, Loss Test: 7.1809, LR: 0.000031\n",
      "Step 308, Loss Train: 7.2836, Loss Test: 7.1809, LR: 0.000031\n",
      "Step 309, Loss Train: 7.3047, Loss Test: 7.1809, LR: 0.000031\n",
      "Step 310, Loss Train: 7.2811, Loss Test: 7.1809, LR: 0.000031\n",
      "Step 311, Loss Train: 7.3136, Loss Test: 7.1809, LR: 0.000031\n",
      "Step 312, Loss Train: 7.3230, Loss Test: 7.1809, LR: 0.000031\n",
      "Step 313, Loss Train: 7.3304, Loss Test: 7.1809, LR: 0.000031\n",
      "Step 314, Loss Train: 7.2855, Loss Test: 7.1809, LR: 0.000031\n",
      "Step 315, Loss Train: 7.2791, Loss Test: 7.1809, LR: 0.000031\n",
      "Step 316, Loss Train: 7.3123, Loss Test: 7.1809, LR: 0.000032\n",
      "Step 317, Loss Train: 7.2967, Loss Test: 7.1809, LR: 0.000032\n",
      "Step 318, Loss Train: 7.2487, Loss Test: 7.1809, LR: 0.000032\n",
      "Step 319, Loss Train: 7.2956, Loss Test: 7.1809, LR: 0.000032\n",
      "Step 320, Loss Train: 7.2969, Loss Test: 7.1809, LR: 0.000032\n",
      "Step 321, Loss Train: 7.3067, Loss Test: 7.1809, LR: 0.000032\n",
      "Step 322, Loss Train: 7.2877, Loss Test: 7.1809, LR: 0.000032\n",
      "Step 323, Loss Train: 7.3429, Loss Test: 7.1809, LR: 0.000032\n",
      "Step 324, Loss Train: 7.2764, Loss Test: 7.1809, LR: 0.000032\n",
      "Step 325, Loss Train: 7.2873, Loss Test: 7.1809, LR: 0.000032\n",
      "Step 326, Loss Train: 7.2996, Loss Test: 7.1632, LR: 0.000033\n",
      "Step 327, Loss Train: 7.3256, Loss Test: 7.1632, LR: 0.000033\n",
      "Step 328, Loss Train: 7.2947, Loss Test: 7.1632, LR: 0.000033\n",
      "Step 329, Loss Train: 7.2956, Loss Test: 7.1632, LR: 0.000033\n",
      "Step 330, Loss Train: 7.2856, Loss Test: 7.1632, LR: 0.000033\n",
      "Step 331, Loss Train: 7.2660, Loss Test: 7.1632, LR: 0.000033\n",
      "Step 332, Loss Train: 7.2832, Loss Test: 7.1632, LR: 0.000033\n",
      "Step 333, Loss Train: 7.2626, Loss Test: 7.1632, LR: 0.000033\n",
      "Step 334, Loss Train: 7.3173, Loss Test: 7.1632, LR: 0.000033\n",
      "Step 335, Loss Train: 7.2857, Loss Test: 7.1632, LR: 0.000034\n",
      "Step 336, Loss Train: 7.2804, Loss Test: 7.1632, LR: 0.000034\n",
      "Step 337, Loss Train: 7.3059, Loss Test: 7.1632, LR: 0.000034\n",
      "Step 338, Loss Train: 7.3122, Loss Test: 7.1632, LR: 0.000034\n",
      "Step 339, Loss Train: 7.2740, Loss Test: 7.1632, LR: 0.000034\n",
      "Step 340, Loss Train: 7.3052, Loss Test: 7.1632, LR: 0.000034\n",
      "Step 341, Loss Train: 7.2646, Loss Test: 7.1632, LR: 0.000034\n",
      "Step 342, Loss Train: 7.2771, Loss Test: 7.1632, LR: 0.000034\n",
      "Step 343, Loss Train: 7.2770, Loss Test: 7.1632, LR: 0.000034\n",
      "Step 344, Loss Train: 7.3030, Loss Test: 7.1632, LR: 0.000034\n",
      "Step 345, Loss Train: 7.2988, Loss Test: 7.1632, LR: 0.000035\n",
      "Step 346, Loss Train: 7.2622, Loss Test: 7.1632, LR: 0.000035\n",
      "Step 347, Loss Train: 7.2790, Loss Test: 7.1632, LR: 0.000035\n",
      "Step 348, Loss Train: 7.2925, Loss Test: 7.1632, LR: 0.000035\n",
      "Step 349, Loss Train: 7.2909, Loss Test: 7.1632, LR: 0.000035\n",
      "Step 350, Loss Train: 7.2923, Loss Test: 7.1632, LR: 0.000035\n",
      "Step 351, Loss Train: 7.2637, Loss Test: 7.1413, LR: 0.000035\n",
      "Step 352, Loss Train: 7.2815, Loss Test: 7.1413, LR: 0.000035\n",
      "Step 353, Loss Train: 7.3297, Loss Test: 7.1413, LR: 0.000035\n",
      "Step 354, Loss Train: 7.2867, Loss Test: 7.1413, LR: 0.000035\n",
      "Step 355, Loss Train: 7.3163, Loss Test: 7.1413, LR: 0.000035\n",
      "Step 356, Loss Train: 7.2843, Loss Test: 7.1413, LR: 0.000036\n",
      "Step 357, Loss Train: 7.2904, Loss Test: 7.1413, LR: 0.000036\n",
      "Step 358, Loss Train: 7.2126, Loss Test: 7.1413, LR: 0.000036\n",
      "Step 359, Loss Train: 7.2690, Loss Test: 7.1413, LR: 0.000036\n",
      "Step 360, Loss Train: 7.2646, Loss Test: 7.1413, LR: 0.000036\n",
      "Step 361, Loss Train: 7.2870, Loss Test: 7.1413, LR: 0.000036\n",
      "Step 362, Loss Train: 7.2995, Loss Test: 7.1413, LR: 0.000036\n",
      "Step 363, Loss Train: 7.2955, Loss Test: 7.1413, LR: 0.000036\n",
      "Step 364, Loss Train: 7.2664, Loss Test: 7.1413, LR: 0.000036\n",
      "Step 365, Loss Train: 7.2518, Loss Test: 7.1413, LR: 0.000037\n",
      "Step 366, Loss Train: 7.2661, Loss Test: 7.1413, LR: 0.000037\n",
      "Step 367, Loss Train: 7.2400, Loss Test: 7.1413, LR: 0.000037\n",
      "Step 368, Loss Train: 7.2871, Loss Test: 7.1413, LR: 0.000037\n",
      "Step 369, Loss Train: 7.2693, Loss Test: 7.1413, LR: 0.000037\n",
      "Step 370, Loss Train: 7.2833, Loss Test: 7.1413, LR: 0.000037\n",
      "Step 371, Loss Train: 7.2570, Loss Test: 7.1413, LR: 0.000037\n",
      "Step 372, Loss Train: 7.2982, Loss Test: 7.1413, LR: 0.000037\n",
      "Step 373, Loss Train: 7.2962, Loss Test: 7.1413, LR: 0.000037\n",
      "Step 374, Loss Train: 7.2702, Loss Test: 7.1413, LR: 0.000037\n",
      "Step 375, Loss Train: 7.2959, Loss Test: 7.1413, LR: 0.000037\n",
      "Step 376, Loss Train: 7.2670, Loss Test: 7.1326, LR: 0.000038\n",
      "Step 377, Loss Train: 7.2919, Loss Test: 7.1326, LR: 0.000038\n",
      "Step 378, Loss Train: 7.2716, Loss Test: 7.1326, LR: 0.000038\n",
      "Step 379, Loss Train: 7.2836, Loss Test: 7.1326, LR: 0.000038\n",
      "Step 380, Loss Train: 7.3047, Loss Test: 7.1326, LR: 0.000038\n",
      "Step 381, Loss Train: 7.3126, Loss Test: 7.1326, LR: 0.000038\n",
      "Step 382, Loss Train: 7.2449, Loss Test: 7.1326, LR: 0.000038\n",
      "Step 383, Loss Train: 7.2690, Loss Test: 7.1326, LR: 0.000038\n",
      "Step 384, Loss Train: 7.3154, Loss Test: 7.1326, LR: 0.000038\n",
      "Step 385, Loss Train: 7.2697, Loss Test: 7.1326, LR: 0.000038\n",
      "Step 386, Loss Train: 7.2540, Loss Test: 7.1326, LR: 0.000039\n",
      "Step 387, Loss Train: 7.2818, Loss Test: 7.1326, LR: 0.000039\n",
      "Step 388, Loss Train: 7.3134, Loss Test: 7.1326, LR: 0.000039\n",
      "Step 389, Loss Train: 7.2942, Loss Test: 7.1326, LR: 0.000039\n",
      "Step 390, Loss Train: 7.2841, Loss Test: 7.1326, LR: 0.000039\n",
      "Step 391, Loss Train: 7.2744, Loss Test: 7.1326, LR: 0.000039\n",
      "Step 392, Loss Train: 7.2768, Loss Test: 7.1326, LR: 0.000039\n",
      "Step 393, Loss Train: 7.2403, Loss Test: 7.1326, LR: 0.000039\n",
      "Step 394, Loss Train: 7.2206, Loss Test: 7.1326, LR: 0.000039\n",
      "Step 395, Loss Train: 7.2148, Loss Test: 7.1326, LR: 0.000039\n",
      "Step 396, Loss Train: 7.3110, Loss Test: 7.1326, LR: 0.000040\n",
      "Step 397, Loss Train: 7.2689, Loss Test: 7.1326, LR: 0.000040\n",
      "Step 398, Loss Train: 7.3042, Loss Test: 7.1326, LR: 0.000040\n",
      "Step 399, Loss Train: 7.2652, Loss Test: 7.1326, LR: 0.000040\n",
      "Step 400, Loss Train: 7.2700, Loss Test: 7.1326, LR: 0.000040\n",
      "Step 401, Loss Train: 7.2389, Loss Test: 7.0958, LR: 0.000040\n",
      "Step 402, Loss Train: 7.2795, Loss Test: 7.0958, LR: 0.000040\n",
      "Step 403, Loss Train: 7.2626, Loss Test: 7.0958, LR: 0.000040\n",
      "Step 404, Loss Train: 7.2801, Loss Test: 7.0958, LR: 0.000040\n",
      "Step 405, Loss Train: 7.3240, Loss Test: 7.0958, LR: 0.000040\n",
      "Step 406, Loss Train: 7.2545, Loss Test: 7.0958, LR: 0.000041\n",
      "Step 407, Loss Train: 7.3290, Loss Test: 7.0958, LR: 0.000041\n",
      "Step 408, Loss Train: 7.2472, Loss Test: 7.0958, LR: 0.000041\n",
      "Step 409, Loss Train: 7.2286, Loss Test: 7.0958, LR: 0.000041\n",
      "Step 410, Loss Train: 7.2666, Loss Test: 7.0958, LR: 0.000041\n",
      "Step 411, Loss Train: 7.2509, Loss Test: 7.0958, LR: 0.000041\n",
      "Step 412, Loss Train: 7.2327, Loss Test: 7.0958, LR: 0.000041\n",
      "Step 413, Loss Train: 7.2109, Loss Test: 7.0958, LR: 0.000041\n",
      "Step 414, Loss Train: 7.2673, Loss Test: 7.0958, LR: 0.000041\n",
      "Step 415, Loss Train: 7.2478, Loss Test: 7.0958, LR: 0.000041\n",
      "Step 416, Loss Train: 7.2596, Loss Test: 7.0958, LR: 0.000042\n",
      "Step 417, Loss Train: 7.2726, Loss Test: 7.0958, LR: 0.000042\n",
      "Step 418, Loss Train: 7.2708, Loss Test: 7.0958, LR: 0.000042\n",
      "Step 419, Loss Train: 7.2948, Loss Test: 7.0958, LR: 0.000042\n",
      "Step 420, Loss Train: 7.3285, Loss Test: 7.0958, LR: 0.000042\n",
      "Step 421, Loss Train: 7.2271, Loss Test: 7.0958, LR: 0.000042\n",
      "Step 422, Loss Train: 7.2632, Loss Test: 7.0958, LR: 0.000042\n",
      "Step 423, Loss Train: 7.3001, Loss Test: 7.0958, LR: 0.000042\n",
      "Step 424, Loss Train: 7.2920, Loss Test: 7.0958, LR: 0.000042\n",
      "Step 425, Loss Train: 7.2454, Loss Test: 7.0958, LR: 0.000042\n",
      "Step 426, Loss Train: 7.2590, Loss Test: 7.1218, LR: 0.000043\n",
      "Step 427, Loss Train: 7.2184, Loss Test: 7.1218, LR: 0.000043\n",
      "Step 428, Loss Train: 7.2539, Loss Test: 7.1218, LR: 0.000043\n",
      "Step 429, Loss Train: 7.1970, Loss Test: 7.1218, LR: 0.000043\n",
      "Step 430, Loss Train: 7.2661, Loss Test: 7.1218, LR: 0.000043\n",
      "Step 431, Loss Train: 7.2544, Loss Test: 7.1218, LR: 0.000043\n",
      "Step 432, Loss Train: 7.2413, Loss Test: 7.1218, LR: 0.000043\n",
      "Step 433, Loss Train: 7.2266, Loss Test: 7.1218, LR: 0.000043\n",
      "Step 434, Loss Train: 7.2640, Loss Test: 7.1218, LR: 0.000043\n",
      "Step 435, Loss Train: 7.2838, Loss Test: 7.1218, LR: 0.000044\n",
      "Step 436, Loss Train: 7.2692, Loss Test: 7.1218, LR: 0.000044\n",
      "Step 437, Loss Train: 7.2367, Loss Test: 7.1218, LR: 0.000044\n",
      "Step 438, Loss Train: 7.2905, Loss Test: 7.1218, LR: 0.000044\n",
      "Step 439, Loss Train: 7.2473, Loss Test: 7.1218, LR: 0.000044\n",
      "Step 440, Loss Train: 7.2681, Loss Test: 7.1218, LR: 0.000044\n",
      "Step 441, Loss Train: 7.2186, Loss Test: 7.1218, LR: 0.000044\n",
      "Step 442, Loss Train: 7.2046, Loss Test: 7.1218, LR: 0.000044\n",
      "Step 443, Loss Train: 7.2478, Loss Test: 7.1218, LR: 0.000044\n",
      "Step 444, Loss Train: 7.2635, Loss Test: 7.1218, LR: 0.000044\n",
      "Step 445, Loss Train: 7.2523, Loss Test: 7.1218, LR: 0.000044\n",
      "Step 446, Loss Train: 7.2795, Loss Test: 7.1218, LR: 0.000045\n",
      "Step 447, Loss Train: 7.2495, Loss Test: 7.1218, LR: 0.000045\n",
      "Step 448, Loss Train: 7.2427, Loss Test: 7.1218, LR: 0.000045\n",
      "Step 449, Loss Train: 7.2481, Loss Test: 7.1218, LR: 0.000045\n",
      "Step 450, Loss Train: 7.2449, Loss Test: 7.1218, LR: 0.000045\n",
      "Step 451, Loss Train: 7.2592, Loss Test: 7.1389, LR: 0.000045\n",
      "Step 452, Loss Train: 7.2746, Loss Test: 7.1389, LR: 0.000045\n",
      "Step 453, Loss Train: 7.2665, Loss Test: 7.1389, LR: 0.000045\n",
      "Step 454, Loss Train: 7.2369, Loss Test: 7.1389, LR: 0.000045\n",
      "Step 455, Loss Train: 7.2799, Loss Test: 7.1389, LR: 0.000046\n",
      "Step 456, Loss Train: 7.2675, Loss Test: 7.1389, LR: 0.000046\n",
      "Step 457, Loss Train: 7.2553, Loss Test: 7.1389, LR: 0.000046\n",
      "Step 458, Loss Train: 7.2642, Loss Test: 7.1389, LR: 0.000046\n",
      "Step 459, Loss Train: 7.2650, Loss Test: 7.1389, LR: 0.000046\n",
      "Step 460, Loss Train: 7.2599, Loss Test: 7.1389, LR: 0.000046\n",
      "Step 461, Loss Train: 7.2726, Loss Test: 7.1389, LR: 0.000046\n",
      "Step 462, Loss Train: 7.2711, Loss Test: 7.1389, LR: 0.000046\n",
      "Step 463, Loss Train: 7.2568, Loss Test: 7.1389, LR: 0.000046\n",
      "Step 464, Loss Train: 7.2311, Loss Test: 7.1389, LR: 0.000046\n",
      "Step 465, Loss Train: 7.2397, Loss Test: 7.1389, LR: 0.000046\n",
      "Step 466, Loss Train: 7.2635, Loss Test: 7.1389, LR: 0.000047\n",
      "Step 467, Loss Train: 7.2223, Loss Test: 7.1389, LR: 0.000047\n",
      "Step 468, Loss Train: 7.2437, Loss Test: 7.1389, LR: 0.000047\n",
      "Step 469, Loss Train: 7.2675, Loss Test: 7.1389, LR: 0.000047\n",
      "Step 470, Loss Train: 7.2616, Loss Test: 7.1389, LR: 0.000047\n",
      "Step 471, Loss Train: 7.3225, Loss Test: 7.1389, LR: 0.000047\n",
      "Step 472, Loss Train: 7.2310, Loss Test: 7.1389, LR: 0.000047\n",
      "Step 473, Loss Train: 7.2807, Loss Test: 7.1389, LR: 0.000047\n",
      "Step 474, Loss Train: 7.2467, Loss Test: 7.1389, LR: 0.000047\n",
      "Step 475, Loss Train: 7.2268, Loss Test: 7.1389, LR: 0.000047\n",
      "Step 476, Loss Train: 7.1667, Loss Test: 7.1059, LR: 0.000048\n",
      "Step 477, Loss Train: 7.2058, Loss Test: 7.1059, LR: 0.000048\n",
      "Step 478, Loss Train: 7.2843, Loss Test: 7.1059, LR: 0.000048\n",
      "Step 479, Loss Train: 7.2651, Loss Test: 7.1059, LR: 0.000048\n",
      "Step 480, Loss Train: 7.2645, Loss Test: 7.1059, LR: 0.000048\n",
      "Step 481, Loss Train: 7.2688, Loss Test: 7.1059, LR: 0.000048\n",
      "Step 482, Loss Train: 7.2302, Loss Test: 7.1059, LR: 0.000048\n",
      "Step 483, Loss Train: 7.2767, Loss Test: 7.1059, LR: 0.000048\n",
      "Step 484, Loss Train: 7.2324, Loss Test: 7.1059, LR: 0.000048\n",
      "Step 485, Loss Train: 7.2109, Loss Test: 7.1059, LR: 0.000049\n",
      "Step 486, Loss Train: 7.2214, Loss Test: 7.1059, LR: 0.000049\n",
      "Step 487, Loss Train: 7.2507, Loss Test: 7.1059, LR: 0.000049\n",
      "Step 488, Loss Train: 7.2178, Loss Test: 7.1059, LR: 0.000049\n",
      "Step 489, Loss Train: 7.2454, Loss Test: 7.1059, LR: 0.000049\n",
      "Step 490, Loss Train: 7.2527, Loss Test: 7.1059, LR: 0.000049\n",
      "Step 491, Loss Train: 7.2463, Loss Test: 7.1059, LR: 0.000049\n",
      "Step 492, Loss Train: 7.2312, Loss Test: 7.1059, LR: 0.000049\n",
      "Step 493, Loss Train: 7.2571, Loss Test: 7.1059, LR: 0.000049\n",
      "Step 494, Loss Train: 7.2321, Loss Test: 7.1059, LR: 0.000049\n",
      "Step 495, Loss Train: 7.2621, Loss Test: 7.1059, LR: 0.000049\n",
      "Step 496, Loss Train: 7.2320, Loss Test: 7.1059, LR: 0.000050\n",
      "Step 497, Loss Train: 7.2033, Loss Test: 7.1059, LR: 0.000050\n",
      "Step 498, Loss Train: 7.2654, Loss Test: 7.1059, LR: 0.000050\n",
      "Step 499, Loss Train: 7.2447, Loss Test: 7.1059, LR: 0.000050\n",
      "Step 500, Loss Train: 7.1813, Loss Test: 7.1059, LR: 0.000050\n",
      "Step 501, Loss Train: 7.2316, Loss Test: 7.0966, LR: 0.000050\n",
      "Step 502, Loss Train: 7.2590, Loss Test: 7.0966, LR: 0.000050\n",
      "Step 503, Loss Train: 7.2322, Loss Test: 7.0966, LR: 0.000050\n",
      "Step 504, Loss Train: 7.2643, Loss Test: 7.0966, LR: 0.000050\n",
      "Step 505, Loss Train: 7.2367, Loss Test: 7.0966, LR: 0.000050\n",
      "Step 506, Loss Train: 7.2491, Loss Test: 7.0966, LR: 0.000051\n",
      "Step 507, Loss Train: 7.2217, Loss Test: 7.0966, LR: 0.000051\n",
      "Step 508, Loss Train: 7.2064, Loss Test: 7.0966, LR: 0.000051\n",
      "Step 509, Loss Train: 7.2095, Loss Test: 7.0966, LR: 0.000051\n",
      "Step 510, Loss Train: 7.2016, Loss Test: 7.0966, LR: 0.000051\n",
      "Step 511, Loss Train: 7.2244, Loss Test: 7.0966, LR: 0.000051\n",
      "Step 512, Loss Train: 7.2383, Loss Test: 7.0966, LR: 0.000051\n",
      "Step 513, Loss Train: 7.2284, Loss Test: 7.0966, LR: 0.000051\n",
      "Step 514, Loss Train: 7.2011, Loss Test: 7.0966, LR: 0.000051\n",
      "Step 515, Loss Train: 7.2339, Loss Test: 7.0966, LR: 0.000051\n",
      "Step 516, Loss Train: 7.2108, Loss Test: 7.0966, LR: 0.000052\n",
      "Step 517, Loss Train: 7.2461, Loss Test: 7.0966, LR: 0.000052\n",
      "Step 518, Loss Train: 7.1844, Loss Test: 7.0966, LR: 0.000052\n",
      "Step 519, Loss Train: 7.2022, Loss Test: 7.0966, LR: 0.000052\n",
      "Step 520, Loss Train: 7.2053, Loss Test: 7.0966, LR: 0.000052\n",
      "Step 521, Loss Train: 7.2080, Loss Test: 7.0966, LR: 0.000052\n",
      "Step 522, Loss Train: 7.2085, Loss Test: 7.0966, LR: 0.000052\n",
      "Step 523, Loss Train: 7.2271, Loss Test: 7.0966, LR: 0.000052\n",
      "Step 524, Loss Train: 7.2041, Loss Test: 7.0966, LR: 0.000052\n",
      "Step 525, Loss Train: 7.2350, Loss Test: 7.0966, LR: 0.000052\n",
      "Step 526, Loss Train: 7.2450, Loss Test: 7.0776, LR: 0.000053\n",
      "Step 527, Loss Train: 7.2115, Loss Test: 7.0776, LR: 0.000053\n",
      "Step 528, Loss Train: 7.2292, Loss Test: 7.0776, LR: 0.000053\n",
      "Step 529, Loss Train: 7.1767, Loss Test: 7.0776, LR: 0.000053\n",
      "Step 530, Loss Train: 7.2054, Loss Test: 7.0776, LR: 0.000053\n",
      "Step 531, Loss Train: 7.2320, Loss Test: 7.0776, LR: 0.000053\n",
      "Step 532, Loss Train: 7.2337, Loss Test: 7.0776, LR: 0.000053\n",
      "Step 533, Loss Train: 7.1944, Loss Test: 7.0776, LR: 0.000053\n",
      "Step 534, Loss Train: 7.2392, Loss Test: 7.0776, LR: 0.000053\n",
      "Step 535, Loss Train: 7.2043, Loss Test: 7.0776, LR: 0.000053\n",
      "Step 536, Loss Train: 7.2443, Loss Test: 7.0776, LR: 0.000054\n",
      "Step 537, Loss Train: 7.2311, Loss Test: 7.0776, LR: 0.000054\n",
      "Step 538, Loss Train: 7.2179, Loss Test: 7.0776, LR: 0.000054\n",
      "Step 539, Loss Train: 7.2141, Loss Test: 7.0776, LR: 0.000054\n",
      "Step 540, Loss Train: 7.2434, Loss Test: 7.0776, LR: 0.000054\n",
      "Step 541, Loss Train: 7.2323, Loss Test: 7.0776, LR: 0.000054\n",
      "Step 542, Loss Train: 7.1843, Loss Test: 7.0776, LR: 0.000054\n",
      "Step 543, Loss Train: 7.1657, Loss Test: 7.0776, LR: 0.000054\n",
      "Step 544, Loss Train: 7.2118, Loss Test: 7.0776, LR: 0.000054\n",
      "Step 545, Loss Train: 7.1841, Loss Test: 7.0776, LR: 0.000055\n",
      "Step 546, Loss Train: 7.1631, Loss Test: 7.0776, LR: 0.000055\n",
      "Step 547, Loss Train: 7.1881, Loss Test: 7.0776, LR: 0.000055\n",
      "Step 548, Loss Train: 7.2114, Loss Test: 7.0776, LR: 0.000055\n",
      "Step 549, Loss Train: 7.1334, Loss Test: 7.0776, LR: 0.000055\n",
      "Step 550, Loss Train: 7.1598, Loss Test: 7.0776, LR: 0.000055\n",
      "Step 551, Loss Train: 7.1442, Loss Test: 7.0360, LR: 0.000055\n",
      "Step 552, Loss Train: 7.1830, Loss Test: 7.0360, LR: 0.000055\n",
      "Step 553, Loss Train: 7.1635, Loss Test: 7.0360, LR: 0.000055\n",
      "Step 554, Loss Train: 7.1738, Loss Test: 7.0360, LR: 0.000055\n",
      "Step 555, Loss Train: 7.2110, Loss Test: 7.0360, LR: 0.000056\n",
      "Step 556, Loss Train: 7.1932, Loss Test: 7.0360, LR: 0.000056\n",
      "Step 557, Loss Train: 7.1379, Loss Test: 7.0360, LR: 0.000056\n",
      "Step 558, Loss Train: 7.1706, Loss Test: 7.0360, LR: 0.000056\n",
      "Step 559, Loss Train: 7.1561, Loss Test: 7.0360, LR: 0.000056\n",
      "Step 560, Loss Train: 7.1796, Loss Test: 7.0360, LR: 0.000056\n",
      "Step 561, Loss Train: 7.1615, Loss Test: 7.0360, LR: 0.000056\n",
      "Step 562, Loss Train: 7.2092, Loss Test: 7.0360, LR: 0.000056\n",
      "Step 563, Loss Train: 7.1379, Loss Test: 7.0360, LR: 0.000056\n",
      "Step 564, Loss Train: 7.2116, Loss Test: 7.0360, LR: 0.000056\n",
      "Step 565, Loss Train: 7.1362, Loss Test: 7.0360, LR: 0.000056\n",
      "Step 566, Loss Train: 7.1473, Loss Test: 7.0360, LR: 0.000057\n",
      "Step 567, Loss Train: 7.1528, Loss Test: 7.0360, LR: 0.000057\n",
      "Step 568, Loss Train: 7.1665, Loss Test: 7.0360, LR: 0.000057\n",
      "Step 569, Loss Train: 7.2451, Loss Test: 7.0360, LR: 0.000057\n",
      "Step 570, Loss Train: 7.1165, Loss Test: 7.0360, LR: 0.000057\n",
      "Step 571, Loss Train: 7.1508, Loss Test: 7.0360, LR: 0.000057\n",
      "Step 572, Loss Train: 7.1440, Loss Test: 7.0360, LR: 0.000057\n",
      "Step 573, Loss Train: 7.1440, Loss Test: 7.0360, LR: 0.000057\n",
      "Step 574, Loss Train: 7.1839, Loss Test: 7.0360, LR: 0.000057\n",
      "Step 575, Loss Train: 7.1672, Loss Test: 7.0360, LR: 0.000057\n",
      "Step 576, Loss Train: 7.1173, Loss Test: 7.0331, LR: 0.000058\n",
      "Step 577, Loss Train: 7.1497, Loss Test: 7.0331, LR: 0.000058\n",
      "Step 578, Loss Train: 7.1435, Loss Test: 7.0331, LR: 0.000058\n",
      "Step 579, Loss Train: 7.1604, Loss Test: 7.0331, LR: 0.000058\n",
      "Step 580, Loss Train: 7.1517, Loss Test: 7.0331, LR: 0.000058\n",
      "Step 581, Loss Train: 7.1823, Loss Test: 7.0331, LR: 0.000058\n",
      "Step 582, Loss Train: 7.1425, Loss Test: 7.0331, LR: 0.000058\n",
      "Step 583, Loss Train: 7.1875, Loss Test: 7.0331, LR: 0.000058\n",
      "Step 584, Loss Train: 7.1760, Loss Test: 7.0331, LR: 0.000058\n",
      "Step 585, Loss Train: 7.1496, Loss Test: 7.0331, LR: 0.000058\n",
      "Step 586, Loss Train: 7.1452, Loss Test: 7.0331, LR: 0.000059\n",
      "Step 587, Loss Train: 7.1564, Loss Test: 7.0331, LR: 0.000059\n",
      "Step 588, Loss Train: 7.1364, Loss Test: 7.0331, LR: 0.000059\n",
      "Step 589, Loss Train: 7.1520, Loss Test: 7.0331, LR: 0.000059\n",
      "Step 590, Loss Train: 7.1611, Loss Test: 7.0331, LR: 0.000059\n",
      "Step 591, Loss Train: 7.1583, Loss Test: 7.0331, LR: 0.000059\n",
      "Step 592, Loss Train: 7.1276, Loss Test: 7.0331, LR: 0.000059\n",
      "Step 593, Loss Train: 7.1702, Loss Test: 7.0331, LR: 0.000059\n",
      "Step 594, Loss Train: 7.1547, Loss Test: 7.0331, LR: 0.000059\n",
      "Step 595, Loss Train: 7.1804, Loss Test: 7.0331, LR: 0.000060\n",
      "Step 596, Loss Train: 7.1058, Loss Test: 7.0331, LR: 0.000060\n",
      "Step 597, Loss Train: 7.0617, Loss Test: 7.0331, LR: 0.000060\n",
      "Step 598, Loss Train: 7.1057, Loss Test: 7.0331, LR: 0.000060\n",
      "Step 599, Loss Train: 7.1577, Loss Test: 7.0331, LR: 0.000060\n",
      "Step 600, Loss Train: 7.1167, Loss Test: 7.0331, LR: 0.000060\n",
      "Step 601, Loss Train: 7.1122, Loss Test: 6.9706, LR: 0.000060\n",
      "Step 602, Loss Train: 7.0814, Loss Test: 6.9706, LR: 0.000060\n",
      "Step 603, Loss Train: 7.1195, Loss Test: 6.9706, LR: 0.000060\n",
      "Step 604, Loss Train: 7.0940, Loss Test: 6.9706, LR: 0.000060\n",
      "Step 605, Loss Train: 7.0853, Loss Test: 6.9706, LR: 0.000060\n",
      "Step 606, Loss Train: 7.0853, Loss Test: 6.9706, LR: 0.000061\n",
      "Step 607, Loss Train: 7.0733, Loss Test: 6.9706, LR: 0.000061\n",
      "Step 608, Loss Train: 7.1115, Loss Test: 6.9706, LR: 0.000061\n",
      "Step 609, Loss Train: 7.1474, Loss Test: 6.9706, LR: 0.000061\n",
      "Step 610, Loss Train: 7.0969, Loss Test: 6.9706, LR: 0.000061\n",
      "Step 611, Loss Train: 7.1152, Loss Test: 6.9706, LR: 0.000061\n",
      "Step 612, Loss Train: 7.1473, Loss Test: 6.9706, LR: 0.000061\n",
      "Step 613, Loss Train: 7.0699, Loss Test: 6.9706, LR: 0.000061\n",
      "Step 614, Loss Train: 7.0915, Loss Test: 6.9706, LR: 0.000061\n",
      "Step 615, Loss Train: 7.0949, Loss Test: 6.9706, LR: 0.000062\n",
      "Step 616, Loss Train: 7.1221, Loss Test: 6.9706, LR: 0.000062\n",
      "Step 617, Loss Train: 7.0898, Loss Test: 6.9706, LR: 0.000062\n",
      "Step 618, Loss Train: 7.1257, Loss Test: 6.9706, LR: 0.000062\n",
      "Step 619, Loss Train: 7.1069, Loss Test: 6.9706, LR: 0.000062\n",
      "Step 620, Loss Train: 7.1108, Loss Test: 6.9706, LR: 0.000062\n",
      "Step 621, Loss Train: 7.0785, Loss Test: 6.9706, LR: 0.000062\n",
      "Step 622, Loss Train: 7.0407, Loss Test: 6.9706, LR: 0.000062\n",
      "Step 623, Loss Train: 7.1079, Loss Test: 6.9706, LR: 0.000062\n",
      "Step 624, Loss Train: 7.0470, Loss Test: 6.9706, LR: 0.000062\n",
      "Step 625, Loss Train: 7.1210, Loss Test: 6.9706, LR: 0.000062\n",
      "Step 626, Loss Train: 7.0535, Loss Test: 6.9557, LR: 0.000063\n",
      "Step 627, Loss Train: 7.0689, Loss Test: 6.9557, LR: 0.000063\n",
      "Step 628, Loss Train: 7.1558, Loss Test: 6.9557, LR: 0.000063\n",
      "Step 629, Loss Train: 7.0914, Loss Test: 6.9557, LR: 0.000063\n",
      "Step 630, Loss Train: 7.1353, Loss Test: 6.9557, LR: 0.000063\n",
      "Step 631, Loss Train: 7.1327, Loss Test: 6.9557, LR: 0.000063\n",
      "Step 632, Loss Train: 7.0767, Loss Test: 6.9557, LR: 0.000063\n",
      "Step 633, Loss Train: 7.0612, Loss Test: 6.9557, LR: 0.000063\n",
      "Step 634, Loss Train: 7.1120, Loss Test: 6.9557, LR: 0.000063\n",
      "Step 635, Loss Train: 7.1003, Loss Test: 6.9557, LR: 0.000063\n",
      "Step 636, Loss Train: 7.0706, Loss Test: 6.9557, LR: 0.000064\n",
      "Step 637, Loss Train: 7.0716, Loss Test: 6.9557, LR: 0.000064\n",
      "Step 638, Loss Train: 7.1075, Loss Test: 6.9557, LR: 0.000064\n",
      "Step 639, Loss Train: 7.0646, Loss Test: 6.9557, LR: 0.000064\n",
      "Step 640, Loss Train: 7.1136, Loss Test: 6.9557, LR: 0.000064\n",
      "Step 641, Loss Train: 7.0959, Loss Test: 6.9557, LR: 0.000064\n",
      "Step 642, Loss Train: 7.0731, Loss Test: 6.9557, LR: 0.000064\n",
      "Step 643, Loss Train: 7.1015, Loss Test: 6.9557, LR: 0.000064\n",
      "Step 644, Loss Train: 7.0629, Loss Test: 6.9557, LR: 0.000064\n",
      "Step 645, Loss Train: 7.0803, Loss Test: 6.9557, LR: 0.000064\n",
      "Step 646, Loss Train: 7.0680, Loss Test: 6.9557, LR: 0.000065\n",
      "Step 647, Loss Train: 7.1177, Loss Test: 6.9557, LR: 0.000065\n",
      "Step 648, Loss Train: 7.1054, Loss Test: 6.9557, LR: 0.000065\n",
      "Step 649, Loss Train: 7.0963, Loss Test: 6.9557, LR: 0.000065\n",
      "Step 650, Loss Train: 7.0911, Loss Test: 6.9557, LR: 0.000065\n",
      "Step 651, Loss Train: 7.0773, Loss Test: 6.9209, LR: 0.000065\n",
      "Step 652, Loss Train: 7.0368, Loss Test: 6.9209, LR: 0.000065\n",
      "Step 653, Loss Train: 7.0017, Loss Test: 6.9209, LR: 0.000065\n",
      "Step 654, Loss Train: 7.0274, Loss Test: 6.9209, LR: 0.000065\n",
      "Step 655, Loss Train: 7.0382, Loss Test: 6.9209, LR: 0.000065\n",
      "Step 656, Loss Train: 7.0390, Loss Test: 6.9209, LR: 0.000066\n",
      "Step 657, Loss Train: 7.0827, Loss Test: 6.9209, LR: 0.000066\n",
      "Step 658, Loss Train: 7.0771, Loss Test: 6.9209, LR: 0.000066\n",
      "Step 659, Loss Train: 7.0597, Loss Test: 6.9209, LR: 0.000066\n",
      "Step 660, Loss Train: 7.0391, Loss Test: 6.9209, LR: 0.000066\n",
      "Step 661, Loss Train: 7.0309, Loss Test: 6.9209, LR: 0.000066\n",
      "Step 662, Loss Train: 7.0587, Loss Test: 6.9209, LR: 0.000066\n",
      "Step 663, Loss Train: 7.0222, Loss Test: 6.9209, LR: 0.000066\n",
      "Step 664, Loss Train: 7.0207, Loss Test: 6.9209, LR: 0.000066\n",
      "Step 665, Loss Train: 7.0338, Loss Test: 6.9209, LR: 0.000066\n",
      "Step 666, Loss Train: 7.0689, Loss Test: 6.9209, LR: 0.000067\n",
      "Step 667, Loss Train: 7.0844, Loss Test: 6.9209, LR: 0.000067\n",
      "Step 668, Loss Train: 7.0711, Loss Test: 6.9209, LR: 0.000067\n",
      "Step 669, Loss Train: 7.0419, Loss Test: 6.9209, LR: 0.000067\n",
      "Step 670, Loss Train: 7.0162, Loss Test: 6.9209, LR: 0.000067\n",
      "Step 671, Loss Train: 7.0861, Loss Test: 6.9209, LR: 0.000067\n",
      "Step 672, Loss Train: 7.0327, Loss Test: 6.9209, LR: 0.000067\n",
      "Step 673, Loss Train: 7.0327, Loss Test: 6.9209, LR: 0.000067\n",
      "Step 674, Loss Train: 7.0578, Loss Test: 6.9209, LR: 0.000067\n",
      "Step 675, Loss Train: 7.0878, Loss Test: 6.9209, LR: 0.000067\n",
      "Step 676, Loss Train: 7.0405, Loss Test: 6.8935, LR: 0.000068\n",
      "Step 677, Loss Train: 7.0478, Loss Test: 6.8935, LR: 0.000068\n",
      "Step 678, Loss Train: 7.0643, Loss Test: 6.8935, LR: 0.000068\n",
      "Step 679, Loss Train: 7.0559, Loss Test: 6.8935, LR: 0.000068\n",
      "Step 680, Loss Train: 7.0266, Loss Test: 6.8935, LR: 0.000068\n",
      "Step 681, Loss Train: 7.0032, Loss Test: 6.8935, LR: 0.000068\n",
      "Step 682, Loss Train: 7.0155, Loss Test: 6.8935, LR: 0.000068\n",
      "Step 683, Loss Train: 7.0478, Loss Test: 6.8935, LR: 0.000068\n",
      "Step 684, Loss Train: 7.0340, Loss Test: 6.8935, LR: 0.000068\n",
      "Step 685, Loss Train: 7.0267, Loss Test: 6.8935, LR: 0.000068\n",
      "Step 686, Loss Train: 6.9819, Loss Test: 6.8935, LR: 0.000069\n",
      "Step 687, Loss Train: 7.0415, Loss Test: 6.8935, LR: 0.000069\n",
      "Step 688, Loss Train: 6.9984, Loss Test: 6.8935, LR: 0.000069\n",
      "Step 689, Loss Train: 7.0177, Loss Test: 6.8935, LR: 0.000069\n",
      "Step 690, Loss Train: 7.0145, Loss Test: 6.8935, LR: 0.000069\n",
      "Step 691, Loss Train: 7.0340, Loss Test: 6.8935, LR: 0.000069\n",
      "Step 692, Loss Train: 6.9537, Loss Test: 6.8935, LR: 0.000069\n",
      "Step 693, Loss Train: 7.0010, Loss Test: 6.8935, LR: 0.000069\n",
      "Step 694, Loss Train: 6.9706, Loss Test: 6.8935, LR: 0.000069\n",
      "Step 695, Loss Train: 7.0334, Loss Test: 6.8935, LR: 0.000069\n",
      "Step 696, Loss Train: 7.0549, Loss Test: 6.8935, LR: 0.000070\n",
      "Step 697, Loss Train: 6.9859, Loss Test: 6.8935, LR: 0.000070\n",
      "Step 698, Loss Train: 7.0040, Loss Test: 6.8935, LR: 0.000070\n",
      "Step 699, Loss Train: 6.9818, Loss Test: 6.8935, LR: 0.000070\n",
      "Step 700, Loss Train: 6.9758, Loss Test: 6.8935, LR: 0.000070\n",
      "Step 701, Loss Train: 6.9866, Loss Test: 6.8294, LR: 0.000070\n",
      "Step 702, Loss Train: 6.9942, Loss Test: 6.8294, LR: 0.000070\n",
      "Step 703, Loss Train: 6.9724, Loss Test: 6.8294, LR: 0.000070\n",
      "Step 704, Loss Train: 6.9657, Loss Test: 6.8294, LR: 0.000070\n",
      "Step 705, Loss Train: 6.9981, Loss Test: 6.8294, LR: 0.000070\n",
      "Step 706, Loss Train: 7.0253, Loss Test: 6.8294, LR: 0.000071\n",
      "Step 707, Loss Train: 6.9769, Loss Test: 6.8294, LR: 0.000071\n",
      "Step 708, Loss Train: 7.0158, Loss Test: 6.8294, LR: 0.000071\n",
      "Step 709, Loss Train: 6.9657, Loss Test: 6.8294, LR: 0.000071\n",
      "Step 710, Loss Train: 6.9746, Loss Test: 6.8294, LR: 0.000071\n",
      "Step 711, Loss Train: 6.9886, Loss Test: 6.8294, LR: 0.000071\n",
      "Step 712, Loss Train: 7.0081, Loss Test: 6.8294, LR: 0.000071\n",
      "Step 713, Loss Train: 7.0125, Loss Test: 6.8294, LR: 0.000071\n",
      "Step 714, Loss Train: 6.9756, Loss Test: 6.8294, LR: 0.000071\n",
      "Step 715, Loss Train: 6.9701, Loss Test: 6.8294, LR: 0.000071\n",
      "Step 716, Loss Train: 6.9391, Loss Test: 6.8294, LR: 0.000072\n",
      "Step 717, Loss Train: 7.0428, Loss Test: 6.8294, LR: 0.000072\n",
      "Step 718, Loss Train: 7.0130, Loss Test: 6.8294, LR: 0.000072\n",
      "Step 719, Loss Train: 6.9722, Loss Test: 6.8294, LR: 0.000072\n",
      "Step 720, Loss Train: 6.9978, Loss Test: 6.8294, LR: 0.000072\n",
      "Step 721, Loss Train: 6.9999, Loss Test: 6.8294, LR: 0.000072\n",
      "Step 722, Loss Train: 6.9511, Loss Test: 6.8294, LR: 0.000072\n",
      "Step 723, Loss Train: 6.9370, Loss Test: 6.8294, LR: 0.000072\n",
      "Step 724, Loss Train: 7.0095, Loss Test: 6.8294, LR: 0.000072\n",
      "Step 725, Loss Train: 6.9062, Loss Test: 6.8294, LR: 0.000072\n",
      "Step 726, Loss Train: 6.9688, Loss Test: 6.8183, LR: 0.000073\n",
      "Step 727, Loss Train: 6.9219, Loss Test: 6.8183, LR: 0.000073\n",
      "Step 728, Loss Train: 6.9696, Loss Test: 6.8183, LR: 0.000073\n",
      "Step 729, Loss Train: 7.0359, Loss Test: 6.8183, LR: 0.000073\n",
      "Step 730, Loss Train: 6.9537, Loss Test: 6.8183, LR: 0.000073\n",
      "Step 731, Loss Train: 6.9321, Loss Test: 6.8183, LR: 0.000073\n",
      "Step 732, Loss Train: 6.9398, Loss Test: 6.8183, LR: 0.000073\n",
      "Step 733, Loss Train: 6.9774, Loss Test: 6.8183, LR: 0.000073\n",
      "Step 734, Loss Train: 6.9157, Loss Test: 6.8183, LR: 0.000073\n",
      "Step 735, Loss Train: 6.9644, Loss Test: 6.8183, LR: 0.000073\n",
      "Step 736, Loss Train: 7.0044, Loss Test: 6.8183, LR: 0.000074\n",
      "Step 737, Loss Train: 6.9187, Loss Test: 6.8183, LR: 0.000074\n",
      "Step 738, Loss Train: 6.9132, Loss Test: 6.8183, LR: 0.000074\n",
      "Step 739, Loss Train: 6.9225, Loss Test: 6.8183, LR: 0.000074\n",
      "Step 740, Loss Train: 6.8609, Loss Test: 6.8183, LR: 0.000074\n",
      "Step 741, Loss Train: 6.9870, Loss Test: 6.8183, LR: 0.000074\n",
      "Step 742, Loss Train: 6.9303, Loss Test: 6.8183, LR: 0.000074\n",
      "Step 743, Loss Train: 6.9566, Loss Test: 6.8183, LR: 0.000074\n",
      "Step 744, Loss Train: 6.9680, Loss Test: 6.8183, LR: 0.000074\n",
      "Step 745, Loss Train: 6.9289, Loss Test: 6.8183, LR: 0.000074\n",
      "Step 746, Loss Train: 6.9423, Loss Test: 6.8183, LR: 0.000075\n",
      "Step 747, Loss Train: 6.9317, Loss Test: 6.8183, LR: 0.000075\n",
      "Step 748, Loss Train: 6.9134, Loss Test: 6.8183, LR: 0.000075\n",
      "Step 749, Loss Train: 6.9589, Loss Test: 6.8183, LR: 0.000075\n",
      "Step 750, Loss Train: 6.9559, Loss Test: 6.8183, LR: 0.000075\n",
      "Step 751, Loss Train: 6.9633, Loss Test: 6.8377, LR: 0.000075\n",
      "Step 752, Loss Train: 6.9786, Loss Test: 6.8377, LR: 0.000075\n",
      "Step 753, Loss Train: 6.8885, Loss Test: 6.8377, LR: 0.000075\n",
      "Step 754, Loss Train: 7.0009, Loss Test: 6.8377, LR: 0.000075\n",
      "Step 755, Loss Train: 6.9417, Loss Test: 6.8377, LR: 0.000076\n",
      "Step 756, Loss Train: 6.9312, Loss Test: 6.8377, LR: 0.000076\n",
      "Step 757, Loss Train: 6.8916, Loss Test: 6.8377, LR: 0.000076\n",
      "Step 758, Loss Train: 6.9454, Loss Test: 6.8377, LR: 0.000076\n",
      "Step 759, Loss Train: 6.9218, Loss Test: 6.8377, LR: 0.000076\n",
      "Step 760, Loss Train: 6.9139, Loss Test: 6.8377, LR: 0.000076\n",
      "Step 761, Loss Train: 6.8849, Loss Test: 6.8377, LR: 0.000076\n",
      "Step 762, Loss Train: 6.8950, Loss Test: 6.8377, LR: 0.000076\n",
      "Step 763, Loss Train: 6.8981, Loss Test: 6.8377, LR: 0.000076\n",
      "Step 764, Loss Train: 6.9431, Loss Test: 6.8377, LR: 0.000076\n",
      "Step 765, Loss Train: 6.8990, Loss Test: 6.8377, LR: 0.000076\n",
      "Step 766, Loss Train: 6.8932, Loss Test: 6.8377, LR: 0.000077\n",
      "Step 767, Loss Train: 6.8499, Loss Test: 6.8377, LR: 0.000077\n",
      "Step 768, Loss Train: 6.9075, Loss Test: 6.8377, LR: 0.000077\n",
      "Step 769, Loss Train: 6.8990, Loss Test: 6.8377, LR: 0.000077\n",
      "Step 770, Loss Train: 6.8662, Loss Test: 6.8377, LR: 0.000077\n",
      "Step 771, Loss Train: 6.8721, Loss Test: 6.8377, LR: 0.000077\n",
      "Step 772, Loss Train: 6.8457, Loss Test: 6.8377, LR: 0.000077\n",
      "Step 773, Loss Train: 6.9224, Loss Test: 6.8377, LR: 0.000077\n",
      "Step 774, Loss Train: 6.9082, Loss Test: 6.8377, LR: 0.000077\n",
      "Step 775, Loss Train: 6.8586, Loss Test: 6.8377, LR: 0.000077\n",
      "Step 776, Loss Train: 6.9063, Loss Test: 6.7602, LR: 0.000078\n",
      "Step 777, Loss Train: 6.9015, Loss Test: 6.7602, LR: 0.000078\n",
      "Step 778, Loss Train: 6.8583, Loss Test: 6.7602, LR: 0.000078\n",
      "Step 779, Loss Train: 6.8392, Loss Test: 6.7602, LR: 0.000078\n",
      "Step 780, Loss Train: 6.9119, Loss Test: 6.7602, LR: 0.000078\n",
      "Step 781, Loss Train: 6.8974, Loss Test: 6.7602, LR: 0.000078\n",
      "Step 782, Loss Train: 6.8668, Loss Test: 6.7602, LR: 0.000078\n",
      "Step 783, Loss Train: 6.9270, Loss Test: 6.7602, LR: 0.000078\n",
      "Step 784, Loss Train: 6.8774, Loss Test: 6.7602, LR: 0.000078\n",
      "Step 785, Loss Train: 6.9134, Loss Test: 6.7602, LR: 0.000079\n",
      "Step 786, Loss Train: 6.8865, Loss Test: 6.7602, LR: 0.000079\n",
      "Step 787, Loss Train: 6.8329, Loss Test: 6.7602, LR: 0.000079\n",
      "Step 788, Loss Train: 6.8896, Loss Test: 6.7602, LR: 0.000079\n",
      "Step 789, Loss Train: 6.8529, Loss Test: 6.7602, LR: 0.000079\n",
      "Step 790, Loss Train: 6.8267, Loss Test: 6.7602, LR: 0.000079\n",
      "Step 791, Loss Train: 6.8959, Loss Test: 6.7602, LR: 0.000079\n",
      "Step 792, Loss Train: 6.8593, Loss Test: 6.7602, LR: 0.000079\n",
      "Step 793, Loss Train: 6.8762, Loss Test: 6.7602, LR: 0.000079\n",
      "Step 794, Loss Train: 6.7935, Loss Test: 6.7602, LR: 0.000079\n",
      "Step 795, Loss Train: 6.7890, Loss Test: 6.7602, LR: 0.000080\n",
      "Step 796, Loss Train: 6.8020, Loss Test: 6.7602, LR: 0.000080\n",
      "Step 797, Loss Train: 6.8152, Loss Test: 6.7602, LR: 0.000080\n",
      "Step 798, Loss Train: 6.8627, Loss Test: 6.7602, LR: 0.000080\n",
      "Step 799, Loss Train: 6.8488, Loss Test: 6.7602, LR: 0.000080\n",
      "Step 800, Loss Train: 6.7403, Loss Test: 6.7602, LR: 0.000080\n",
      "Step 801, Loss Train: 6.7999, Loss Test: 6.7105, LR: 0.000080\n",
      "Step 802, Loss Train: 6.8268, Loss Test: 6.7105, LR: 0.000080\n",
      "Step 803, Loss Train: 6.8202, Loss Test: 6.7105, LR: 0.000080\n",
      "Step 804, Loss Train: 6.9225, Loss Test: 6.7105, LR: 0.000080\n",
      "Step 805, Loss Train: 6.7962, Loss Test: 6.7105, LR: 0.000081\n",
      "Step 806, Loss Train: 6.8901, Loss Test: 6.7105, LR: 0.000081\n",
      "Step 807, Loss Train: 6.8442, Loss Test: 6.7105, LR: 0.000081\n",
      "Step 808, Loss Train: 6.8453, Loss Test: 6.7105, LR: 0.000081\n",
      "Step 809, Loss Train: 6.8019, Loss Test: 6.7105, LR: 0.000081\n",
      "Step 810, Loss Train: 6.8421, Loss Test: 6.7105, LR: 0.000081\n",
      "Step 811, Loss Train: 6.8498, Loss Test: 6.7105, LR: 0.000081\n",
      "Step 812, Loss Train: 6.8014, Loss Test: 6.7105, LR: 0.000081\n",
      "Step 813, Loss Train: 6.8378, Loss Test: 6.7105, LR: 0.000081\n",
      "Step 814, Loss Train: 6.7739, Loss Test: 6.7105, LR: 0.000081\n",
      "Step 815, Loss Train: 6.8124, Loss Test: 6.7105, LR: 0.000081\n",
      "Step 816, Loss Train: 6.7806, Loss Test: 6.7105, LR: 0.000082\n",
      "Step 817, Loss Train: 6.8347, Loss Test: 6.7105, LR: 0.000082\n",
      "Step 818, Loss Train: 6.8222, Loss Test: 6.7105, LR: 0.000082\n",
      "Step 819, Loss Train: 6.7921, Loss Test: 6.7105, LR: 0.000082\n",
      "Step 820, Loss Train: 6.7774, Loss Test: 6.7105, LR: 0.000082\n",
      "Step 821, Loss Train: 6.8395, Loss Test: 6.7105, LR: 0.000082\n",
      "Step 822, Loss Train: 6.7876, Loss Test: 6.7105, LR: 0.000082\n",
      "Step 823, Loss Train: 6.7584, Loss Test: 6.7105, LR: 0.000082\n",
      "Step 824, Loss Train: 6.8012, Loss Test: 6.7105, LR: 0.000082\n",
      "Step 825, Loss Train: 6.8500, Loss Test: 6.7105, LR: 0.000082\n",
      "Step 826, Loss Train: 6.8050, Loss Test: 6.6652, LR: 0.000083\n",
      "Step 827, Loss Train: 6.8030, Loss Test: 6.6652, LR: 0.000083\n",
      "Step 828, Loss Train: 6.7687, Loss Test: 6.6652, LR: 0.000083\n",
      "Step 829, Loss Train: 6.7613, Loss Test: 6.6652, LR: 0.000083\n",
      "Step 830, Loss Train: 6.7483, Loss Test: 6.6652, LR: 0.000083\n",
      "Step 831, Loss Train: 6.8129, Loss Test: 6.6652, LR: 0.000083\n",
      "Step 832, Loss Train: 6.7464, Loss Test: 6.6652, LR: 0.000083\n",
      "Step 833, Loss Train: 6.7788, Loss Test: 6.6652, LR: 0.000083\n",
      "Step 834, Loss Train: 6.7771, Loss Test: 6.6652, LR: 0.000083\n",
      "Step 835, Loss Train: 6.7508, Loss Test: 6.6652, LR: 0.000083\n",
      "Step 836, Loss Train: 6.8132, Loss Test: 6.6652, LR: 0.000084\n",
      "Step 837, Loss Train: 6.7771, Loss Test: 6.6652, LR: 0.000084\n",
      "Step 838, Loss Train: 6.7236, Loss Test: 6.6652, LR: 0.000084\n",
      "Step 839, Loss Train: 6.7502, Loss Test: 6.6652, LR: 0.000084\n",
      "Step 840, Loss Train: 6.7579, Loss Test: 6.6652, LR: 0.000084\n",
      "Step 841, Loss Train: 6.7791, Loss Test: 6.6652, LR: 0.000084\n",
      "Step 842, Loss Train: 6.7687, Loss Test: 6.6652, LR: 0.000084\n",
      "Step 843, Loss Train: 6.7286, Loss Test: 6.6652, LR: 0.000084\n",
      "Step 844, Loss Train: 6.6891, Loss Test: 6.6652, LR: 0.000084\n",
      "Step 845, Loss Train: 6.7384, Loss Test: 6.6652, LR: 0.000085\n",
      "Step 846, Loss Train: 6.7713, Loss Test: 6.6652, LR: 0.000085\n",
      "Step 847, Loss Train: 6.7280, Loss Test: 6.6652, LR: 0.000085\n",
      "Step 848, Loss Train: 6.6699, Loss Test: 6.6652, LR: 0.000085\n",
      "Step 849, Loss Train: 6.8050, Loss Test: 6.6652, LR: 0.000085\n",
      "Step 850, Loss Train: 6.7552, Loss Test: 6.6652, LR: 0.000085\n",
      "Step 851, Loss Train: 6.7247, Loss Test: 6.5682, LR: 0.000085\n",
      "Step 852, Loss Train: 6.7190, Loss Test: 6.5682, LR: 0.000085\n",
      "Step 853, Loss Train: 6.7891, Loss Test: 6.5682, LR: 0.000085\n",
      "Step 854, Loss Train: 6.7667, Loss Test: 6.5682, LR: 0.000085\n",
      "Step 855, Loss Train: 6.6964, Loss Test: 6.5682, LR: 0.000085\n",
      "Step 856, Loss Train: 6.7657, Loss Test: 6.5682, LR: 0.000086\n",
      "Step 857, Loss Train: 6.7476, Loss Test: 6.5682, LR: 0.000086\n",
      "Step 858, Loss Train: 6.7620, Loss Test: 6.5682, LR: 0.000086\n",
      "Step 859, Loss Train: 6.7099, Loss Test: 6.5682, LR: 0.000086\n",
      "Step 860, Loss Train: 6.7332, Loss Test: 6.5682, LR: 0.000086\n",
      "Step 861, Loss Train: 6.7131, Loss Test: 6.5682, LR: 0.000086\n",
      "Step 862, Loss Train: 6.7189, Loss Test: 6.5682, LR: 0.000086\n",
      "Step 863, Loss Train: 6.7388, Loss Test: 6.5682, LR: 0.000086\n",
      "Step 864, Loss Train: 6.6874, Loss Test: 6.5682, LR: 0.000086\n",
      "Step 865, Loss Train: 6.6900, Loss Test: 6.5682, LR: 0.000086\n",
      "Step 866, Loss Train: 6.7057, Loss Test: 6.5682, LR: 0.000087\n",
      "Step 867, Loss Train: 6.6619, Loss Test: 6.5682, LR: 0.000087\n",
      "Step 868, Loss Train: 6.7686, Loss Test: 6.5682, LR: 0.000087\n",
      "Step 869, Loss Train: 6.6607, Loss Test: 6.5682, LR: 0.000087\n",
      "Step 870, Loss Train: 6.7036, Loss Test: 6.5682, LR: 0.000087\n",
      "Step 871, Loss Train: 6.7078, Loss Test: 6.5682, LR: 0.000087\n",
      "Step 872, Loss Train: 6.6789, Loss Test: 6.5682, LR: 0.000087\n",
      "Step 873, Loss Train: 6.6904, Loss Test: 6.5682, LR: 0.000087\n",
      "Step 874, Loss Train: 6.7097, Loss Test: 6.5682, LR: 0.000087\n",
      "Step 875, Loss Train: 6.6467, Loss Test: 6.5682, LR: 0.000087\n",
      "Step 876, Loss Train: 6.7042, Loss Test: 6.5948, LR: 0.000088\n",
      "Step 877, Loss Train: 6.6630, Loss Test: 6.5948, LR: 0.000088\n",
      "Step 878, Loss Train: 6.6384, Loss Test: 6.5948, LR: 0.000088\n",
      "Step 879, Loss Train: 6.6464, Loss Test: 6.5948, LR: 0.000088\n",
      "Step 880, Loss Train: 6.6524, Loss Test: 6.5948, LR: 0.000088\n",
      "Step 881, Loss Train: 6.6079, Loss Test: 6.5948, LR: 0.000088\n",
      "Step 882, Loss Train: 6.7049, Loss Test: 6.5948, LR: 0.000088\n",
      "Step 883, Loss Train: 6.6110, Loss Test: 6.5948, LR: 0.000088\n",
      "Step 884, Loss Train: 6.6884, Loss Test: 6.5948, LR: 0.000088\n",
      "Step 885, Loss Train: 6.6384, Loss Test: 6.5948, LR: 0.000088\n",
      "Step 886, Loss Train: 6.6515, Loss Test: 6.5948, LR: 0.000089\n",
      "Step 887, Loss Train: 6.6656, Loss Test: 6.5948, LR: 0.000089\n",
      "Step 888, Loss Train: 6.6751, Loss Test: 6.5948, LR: 0.000089\n",
      "Step 889, Loss Train: 6.6115, Loss Test: 6.5948, LR: 0.000089\n",
      "Step 890, Loss Train: 6.6020, Loss Test: 6.5948, LR: 0.000089\n",
      "Step 891, Loss Train: 6.6305, Loss Test: 6.5948, LR: 0.000089\n",
      "Step 892, Loss Train: 6.6125, Loss Test: 6.5948, LR: 0.000089\n",
      "Step 893, Loss Train: 6.6386, Loss Test: 6.5948, LR: 0.000089\n",
      "Step 894, Loss Train: 6.6332, Loss Test: 6.5948, LR: 0.000089\n",
      "Step 895, Loss Train: 6.6152, Loss Test: 6.5948, LR: 0.000089\n",
      "Step 896, Loss Train: 6.6068, Loss Test: 6.5948, LR: 0.000090\n",
      "Step 897, Loss Train: 6.6037, Loss Test: 6.5948, LR: 0.000090\n",
      "Step 898, Loss Train: 6.6700, Loss Test: 6.5948, LR: 0.000090\n",
      "Step 899, Loss Train: 6.5882, Loss Test: 6.5948, LR: 0.000090\n",
      "Step 900, Loss Train: 6.6294, Loss Test: 6.5948, LR: 0.000090\n",
      "Step 901, Loss Train: 6.6052, Loss Test: 6.5309, LR: 0.000090\n",
      "Step 902, Loss Train: 6.5984, Loss Test: 6.5309, LR: 0.000090\n",
      "Step 903, Loss Train: 6.6027, Loss Test: 6.5309, LR: 0.000090\n",
      "Step 904, Loss Train: 6.6192, Loss Test: 6.5309, LR: 0.000090\n",
      "Step 905, Loss Train: 6.5807, Loss Test: 6.5309, LR: 0.000090\n",
      "Step 906, Loss Train: 6.6128, Loss Test: 6.5309, LR: 0.000091\n",
      "Step 907, Loss Train: 6.5685, Loss Test: 6.5309, LR: 0.000091\n",
      "Step 908, Loss Train: 6.6125, Loss Test: 6.5309, LR: 0.000091\n",
      "Step 909, Loss Train: 6.6050, Loss Test: 6.5309, LR: 0.000091\n",
      "Step 910, Loss Train: 6.5548, Loss Test: 6.5309, LR: 0.000091\n",
      "Step 911, Loss Train: 6.5605, Loss Test: 6.5309, LR: 0.000091\n",
      "Step 912, Loss Train: 6.5897, Loss Test: 6.5309, LR: 0.000091\n",
      "Step 913, Loss Train: 6.6042, Loss Test: 6.5309, LR: 0.000091\n",
      "Step 914, Loss Train: 6.5846, Loss Test: 6.5309, LR: 0.000091\n",
      "Step 915, Loss Train: 6.5974, Loss Test: 6.5309, LR: 0.000091\n",
      "Step 916, Loss Train: 6.5576, Loss Test: 6.5309, LR: 0.000092\n",
      "Step 917, Loss Train: 6.6190, Loss Test: 6.5309, LR: 0.000092\n",
      "Step 918, Loss Train: 6.6491, Loss Test: 6.5309, LR: 0.000092\n",
      "Step 919, Loss Train: 6.5859, Loss Test: 6.5309, LR: 0.000092\n",
      "Step 920, Loss Train: 6.5836, Loss Test: 6.5309, LR: 0.000092\n",
      "Step 921, Loss Train: 6.5469, Loss Test: 6.5309, LR: 0.000092\n",
      "Step 922, Loss Train: 6.5612, Loss Test: 6.5309, LR: 0.000092\n",
      "Step 923, Loss Train: 6.5505, Loss Test: 6.5309, LR: 0.000092\n",
      "Step 924, Loss Train: 6.5538, Loss Test: 6.5309, LR: 0.000092\n",
      "Step 925, Loss Train: 6.5356, Loss Test: 6.5309, LR: 0.000092\n",
      "Step 926, Loss Train: 6.5768, Loss Test: 6.4053, LR: 0.000093\n",
      "Step 927, Loss Train: 6.5169, Loss Test: 6.4053, LR: 0.000093\n",
      "Step 928, Loss Train: 6.6094, Loss Test: 6.4053, LR: 0.000093\n",
      "Step 929, Loss Train: 6.5589, Loss Test: 6.4053, LR: 0.000093\n",
      "Step 930, Loss Train: 6.5373, Loss Test: 6.4053, LR: 0.000093\n",
      "Step 931, Loss Train: 6.5250, Loss Test: 6.4053, LR: 0.000093\n",
      "Step 932, Loss Train: 6.5579, Loss Test: 6.4053, LR: 0.000093\n",
      "Step 933, Loss Train: 6.5537, Loss Test: 6.4053, LR: 0.000093\n",
      "Step 934, Loss Train: 6.5892, Loss Test: 6.4053, LR: 0.000093\n",
      "Step 935, Loss Train: 6.5691, Loss Test: 6.4053, LR: 0.000093\n",
      "Step 936, Loss Train: 6.5644, Loss Test: 6.4053, LR: 0.000094\n",
      "Step 937, Loss Train: 6.5353, Loss Test: 6.4053, LR: 0.000094\n",
      "Step 938, Loss Train: 6.4990, Loss Test: 6.4053, LR: 0.000094\n",
      "Step 939, Loss Train: 6.6061, Loss Test: 6.4053, LR: 0.000094\n",
      "Step 940, Loss Train: 6.5007, Loss Test: 6.4053, LR: 0.000094\n",
      "Step 941, Loss Train: 6.5761, Loss Test: 6.4053, LR: 0.000094\n",
      "Step 942, Loss Train: 6.6055, Loss Test: 6.4053, LR: 0.000094\n",
      "Step 943, Loss Train: 6.5032, Loss Test: 6.4053, LR: 0.000094\n",
      "Step 944, Loss Train: 6.5478, Loss Test: 6.4053, LR: 0.000094\n",
      "Step 945, Loss Train: 6.5529, Loss Test: 6.4053, LR: 0.000094\n",
      "Step 946, Loss Train: 6.5397, Loss Test: 6.4053, LR: 0.000095\n",
      "Step 947, Loss Train: 6.5353, Loss Test: 6.4053, LR: 0.000095\n",
      "Step 948, Loss Train: 6.5548, Loss Test: 6.4053, LR: 0.000095\n",
      "Step 949, Loss Train: 6.5177, Loss Test: 6.4053, LR: 0.000095\n",
      "Step 950, Loss Train: 6.5324, Loss Test: 6.4053, LR: 0.000095\n",
      "Step 951, Loss Train: 6.4651, Loss Test: 6.4561, LR: 0.000095\n",
      "Step 952, Loss Train: 6.5152, Loss Test: 6.4561, LR: 0.000095\n",
      "Step 953, Loss Train: 6.5365, Loss Test: 6.4561, LR: 0.000095\n",
      "Step 954, Loss Train: 6.5736, Loss Test: 6.4561, LR: 0.000095\n",
      "Step 955, Loss Train: 6.4782, Loss Test: 6.4561, LR: 0.000095\n",
      "Step 956, Loss Train: 6.5291, Loss Test: 6.4561, LR: 0.000096\n",
      "Step 957, Loss Train: 6.5373, Loss Test: 6.4561, LR: 0.000096\n",
      "Step 958, Loss Train: 6.5585, Loss Test: 6.4561, LR: 0.000096\n",
      "Step 959, Loss Train: 6.5341, Loss Test: 6.4561, LR: 0.000096\n",
      "Step 960, Loss Train: 6.4781, Loss Test: 6.4561, LR: 0.000096\n",
      "Step 961, Loss Train: 6.5444, Loss Test: 6.4561, LR: 0.000096\n",
      "Step 962, Loss Train: 6.5082, Loss Test: 6.4561, LR: 0.000096\n",
      "Step 963, Loss Train: 6.4933, Loss Test: 6.4561, LR: 0.000096\n",
      "Step 964, Loss Train: 6.4858, Loss Test: 6.4561, LR: 0.000096\n",
      "Step 965, Loss Train: 6.4772, Loss Test: 6.4561, LR: 0.000096\n",
      "Step 966, Loss Train: 6.4883, Loss Test: 6.4561, LR: 0.000097\n",
      "Step 967, Loss Train: 6.4148, Loss Test: 6.4561, LR: 0.000097\n",
      "Step 968, Loss Train: 6.4892, Loss Test: 6.4561, LR: 0.000097\n",
      "Step 969, Loss Train: 6.4803, Loss Test: 6.4561, LR: 0.000097\n",
      "Step 970, Loss Train: 6.4243, Loss Test: 6.4561, LR: 0.000097\n",
      "Step 971, Loss Train: 6.4673, Loss Test: 6.4561, LR: 0.000097\n",
      "Step 972, Loss Train: 6.4900, Loss Test: 6.4561, LR: 0.000097\n",
      "Step 973, Loss Train: 6.4325, Loss Test: 6.4561, LR: 0.000097\n",
      "Step 974, Loss Train: 6.4699, Loss Test: 6.4561, LR: 0.000097\n",
      "Step 975, Loss Train: 6.4945, Loss Test: 6.4561, LR: 0.000098\n",
      "Step 976, Loss Train: 6.4668, Loss Test: 6.4075, LR: 0.000098\n",
      "Step 977, Loss Train: 6.4368, Loss Test: 6.4075, LR: 0.000098\n",
      "Step 978, Loss Train: 6.4346, Loss Test: 6.4075, LR: 0.000098\n",
      "Step 979, Loss Train: 6.4809, Loss Test: 6.4075, LR: 0.000098\n",
      "Step 980, Loss Train: 6.4737, Loss Test: 6.4075, LR: 0.000098\n",
      "Step 981, Loss Train: 6.4386, Loss Test: 6.4075, LR: 0.000098\n",
      "Step 982, Loss Train: 6.4986, Loss Test: 6.4075, LR: 0.000098\n",
      "Step 983, Loss Train: 6.4311, Loss Test: 6.4075, LR: 0.000098\n",
      "Step 984, Loss Train: 6.4264, Loss Test: 6.4075, LR: 0.000098\n",
      "Step 985, Loss Train: 6.4308, Loss Test: 6.4075, LR: 0.000099\n",
      "Step 986, Loss Train: 6.4428, Loss Test: 6.4075, LR: 0.000099\n",
      "Step 987, Loss Train: 6.4144, Loss Test: 6.4075, LR: 0.000099\n",
      "Step 988, Loss Train: 6.3888, Loss Test: 6.4075, LR: 0.000099\n",
      "Step 989, Loss Train: 6.4018, Loss Test: 6.4075, LR: 0.000099\n",
      "Step 990, Loss Train: 6.3704, Loss Test: 6.4075, LR: 0.000099\n",
      "Step 991, Loss Train: 6.4280, Loss Test: 6.4075, LR: 0.000099\n",
      "Step 992, Loss Train: 6.3840, Loss Test: 6.4075, LR: 0.000099\n",
      "Step 993, Loss Train: 6.4192, Loss Test: 6.4075, LR: 0.000099\n",
      "Step 994, Loss Train: 6.4314, Loss Test: 6.4075, LR: 0.000099\n",
      "Step 995, Loss Train: 6.4041, Loss Test: 6.4075, LR: 0.000100\n",
      "Step 996, Loss Train: 6.4286, Loss Test: 6.4075, LR: 0.000100\n",
      "Step 997, Loss Train: 6.3713, Loss Test: 6.4075, LR: 0.000100\n",
      "Step 998, Loss Train: 6.4255, Loss Test: 6.4075, LR: 0.000100\n",
      "Step 999, Loss Train: 6.3729, Loss Test: 6.4075, LR: 0.000100\n",
      "Step 1000, Loss Train: 6.3731, Loss Test: 6.4075, LR: 0.000100\n",
      "Step 1001, Loss Train: 6.3998, Loss Test: 6.3123, LR: 0.000100\n",
      "Step 1002, Loss Train: 6.3880, Loss Test: 6.3123, LR: 0.000100\n",
      "Step 1003, Loss Train: 6.3898, Loss Test: 6.3123, LR: 0.000100\n",
      "Step 1004, Loss Train: 6.3912, Loss Test: 6.3123, LR: 0.000100\n",
      "Step 1005, Loss Train: 6.4098, Loss Test: 6.3123, LR: 0.000100\n",
      "Step 1006, Loss Train: 6.4260, Loss Test: 6.3123, LR: 0.000100\n",
      "Step 1007, Loss Train: 6.3653, Loss Test: 6.3123, LR: 0.000100\n",
      "Step 1008, Loss Train: 6.3585, Loss Test: 6.3123, LR: 0.000100\n",
      "Step 1009, Loss Train: 6.3496, Loss Test: 6.3123, LR: 0.000100\n",
      "Step 1010, Loss Train: 6.3660, Loss Test: 6.3123, LR: 0.000100\n",
      "Step 1011, Loss Train: 6.3249, Loss Test: 6.3123, LR: 0.000100\n",
      "Step 1012, Loss Train: 6.3159, Loss Test: 6.3123, LR: 0.000100\n",
      "Step 1013, Loss Train: 6.3440, Loss Test: 6.3123, LR: 0.000100\n",
      "Step 1014, Loss Train: 6.3752, Loss Test: 6.3123, LR: 0.000100\n",
      "Step 1015, Loss Train: 6.3398, Loss Test: 6.3123, LR: 0.000100\n",
      "Step 1016, Loss Train: 6.3376, Loss Test: 6.3123, LR: 0.000100\n",
      "Step 1017, Loss Train: 6.3122, Loss Test: 6.3123, LR: 0.000100\n",
      "Step 1018, Loss Train: 6.4050, Loss Test: 6.3123, LR: 0.000100\n",
      "Step 1019, Loss Train: 6.3482, Loss Test: 6.3123, LR: 0.000100\n",
      "Step 1020, Loss Train: 6.3300, Loss Test: 6.3123, LR: 0.000100\n",
      "Step 1021, Loss Train: 6.3444, Loss Test: 6.3123, LR: 0.000100\n",
      "Step 1022, Loss Train: 6.3446, Loss Test: 6.3123, LR: 0.000100\n",
      "Step 1023, Loss Train: 6.3562, Loss Test: 6.3123, LR: 0.000100\n",
      "Step 1024, Loss Train: 6.3256, Loss Test: 6.3123, LR: 0.000100\n",
      "Step 1025, Loss Train: 6.3092, Loss Test: 6.3123, LR: 0.000100\n",
      "Step 1026, Loss Train: 6.3207, Loss Test: 6.2299, LR: 0.000100\n",
      "Step 1027, Loss Train: 6.3476, Loss Test: 6.2299, LR: 0.000100\n",
      "Step 1028, Loss Train: 6.3204, Loss Test: 6.2299, LR: 0.000100\n",
      "Step 1029, Loss Train: 6.3325, Loss Test: 6.2299, LR: 0.000100\n",
      "Step 1030, Loss Train: 6.3282, Loss Test: 6.2299, LR: 0.000100\n",
      "Step 1031, Loss Train: 6.3371, Loss Test: 6.2299, LR: 0.000100\n",
      "Step 1032, Loss Train: 6.3037, Loss Test: 6.2299, LR: 0.000100\n",
      "Step 1033, Loss Train: 6.3217, Loss Test: 6.2299, LR: 0.000100\n",
      "Step 1034, Loss Train: 6.3305, Loss Test: 6.2299, LR: 0.000100\n",
      "Step 1035, Loss Train: 6.3426, Loss Test: 6.2299, LR: 0.000100\n",
      "Step 1036, Loss Train: 6.2791, Loss Test: 6.2299, LR: 0.000100\n",
      "Step 1037, Loss Train: 6.3073, Loss Test: 6.2299, LR: 0.000100\n",
      "Step 1038, Loss Train: 6.3050, Loss Test: 6.2299, LR: 0.000100\n",
      "Step 1039, Loss Train: 6.3084, Loss Test: 6.2299, LR: 0.000100\n",
      "Step 1040, Loss Train: 6.3166, Loss Test: 6.2299, LR: 0.000100\n",
      "Step 1041, Loss Train: 6.3194, Loss Test: 6.2299, LR: 0.000100\n",
      "Step 1042, Loss Train: 6.3038, Loss Test: 6.2299, LR: 0.000100\n",
      "Step 1043, Loss Train: 6.2810, Loss Test: 6.2299, LR: 0.000100\n",
      "Step 1044, Loss Train: 6.3143, Loss Test: 6.2299, LR: 0.000100\n",
      "Step 1045, Loss Train: 6.2669, Loss Test: 6.2299, LR: 0.000100\n",
      "Step 1046, Loss Train: 6.3037, Loss Test: 6.2299, LR: 0.000100\n",
      "Step 1047, Loss Train: 6.3097, Loss Test: 6.2299, LR: 0.000100\n",
      "Step 1048, Loss Train: 6.2567, Loss Test: 6.2299, LR: 0.000100\n",
      "Step 1049, Loss Train: 6.2906, Loss Test: 6.2299, LR: 0.000100\n",
      "Step 1050, Loss Train: 6.2690, Loss Test: 6.2299, LR: 0.000100\n",
      "Step 1051, Loss Train: 6.2454, Loss Test: 6.1701, LR: 0.000100\n",
      "Step 1052, Loss Train: 6.2570, Loss Test: 6.1701, LR: 0.000100\n",
      "Step 1053, Loss Train: 6.3033, Loss Test: 6.1701, LR: 0.000100\n",
      "Step 1054, Loss Train: 6.2760, Loss Test: 6.1701, LR: 0.000100\n",
      "Step 1055, Loss Train: 6.3090, Loss Test: 6.1701, LR: 0.000100\n",
      "Step 1056, Loss Train: 6.2740, Loss Test: 6.1701, LR: 0.000100\n",
      "Step 1057, Loss Train: 6.2320, Loss Test: 6.1701, LR: 0.000100\n",
      "Step 1058, Loss Train: 6.2674, Loss Test: 6.1701, LR: 0.000100\n",
      "Step 1059, Loss Train: 6.2883, Loss Test: 6.1701, LR: 0.000100\n",
      "Step 1060, Loss Train: 6.2715, Loss Test: 6.1701, LR: 0.000100\n",
      "Step 1061, Loss Train: 6.2952, Loss Test: 6.1701, LR: 0.000100\n",
      "Step 1062, Loss Train: 6.2614, Loss Test: 6.1701, LR: 0.000100\n",
      "Step 1063, Loss Train: 6.2646, Loss Test: 6.1701, LR: 0.000100\n",
      "Step 1064, Loss Train: 6.2768, Loss Test: 6.1701, LR: 0.000100\n",
      "Step 1065, Loss Train: 6.2306, Loss Test: 6.1701, LR: 0.000100\n",
      "Step 1066, Loss Train: 6.2587, Loss Test: 6.1701, LR: 0.000100\n",
      "Step 1067, Loss Train: 6.2309, Loss Test: 6.1701, LR: 0.000100\n",
      "Step 1068, Loss Train: 6.2507, Loss Test: 6.1701, LR: 0.000100\n",
      "Step 1069, Loss Train: 6.2661, Loss Test: 6.1701, LR: 0.000100\n",
      "Step 1070, Loss Train: 6.2507, Loss Test: 6.1701, LR: 0.000100\n",
      "Step 1071, Loss Train: 6.2799, Loss Test: 6.1701, LR: 0.000100\n",
      "Step 1072, Loss Train: 6.2541, Loss Test: 6.1701, LR: 0.000100\n",
      "Step 1073, Loss Train: 6.2523, Loss Test: 6.1701, LR: 0.000100\n",
      "Step 1074, Loss Train: 6.2817, Loss Test: 6.1701, LR: 0.000100\n",
      "Step 1075, Loss Train: 6.2290, Loss Test: 6.1701, LR: 0.000100\n",
      "Step 1076, Loss Train: 6.2496, Loss Test: 6.1955, LR: 0.000100\n",
      "Step 1077, Loss Train: 6.2278, Loss Test: 6.1955, LR: 0.000100\n",
      "Step 1078, Loss Train: 6.2243, Loss Test: 6.1955, LR: 0.000100\n",
      "Step 1079, Loss Train: 6.2695, Loss Test: 6.1955, LR: 0.000100\n",
      "Step 1080, Loss Train: 6.2186, Loss Test: 6.1955, LR: 0.000100\n",
      "Step 1081, Loss Train: 6.2612, Loss Test: 6.1955, LR: 0.000100\n",
      "Step 1082, Loss Train: 6.2376, Loss Test: 6.1955, LR: 0.000100\n",
      "Step 1083, Loss Train: 6.2376, Loss Test: 6.1955, LR: 0.000100\n",
      "Step 1084, Loss Train: 6.2638, Loss Test: 6.1955, LR: 0.000100\n",
      "Step 1085, Loss Train: 6.2162, Loss Test: 6.1955, LR: 0.000100\n",
      "Step 1086, Loss Train: 6.1863, Loss Test: 6.1955, LR: 0.000100\n",
      "Step 1087, Loss Train: 6.2576, Loss Test: 6.1955, LR: 0.000100\n",
      "Step 1088, Loss Train: 6.2318, Loss Test: 6.1955, LR: 0.000100\n",
      "Step 1089, Loss Train: 6.1969, Loss Test: 6.1955, LR: 0.000100\n",
      "Step 1090, Loss Train: 6.2984, Loss Test: 6.1955, LR: 0.000100\n",
      "Step 1091, Loss Train: 6.2103, Loss Test: 6.1955, LR: 0.000100\n",
      "Step 1092, Loss Train: 6.1976, Loss Test: 6.1955, LR: 0.000100\n",
      "Step 1093, Loss Train: 6.1612, Loss Test: 6.1955, LR: 0.000100\n",
      "Step 1094, Loss Train: 6.2302, Loss Test: 6.1955, LR: 0.000100\n",
      "Step 1095, Loss Train: 6.1715, Loss Test: 6.1955, LR: 0.000100\n",
      "Step 1096, Loss Train: 6.2311, Loss Test: 6.1955, LR: 0.000100\n",
      "Step 1097, Loss Train: 6.1879, Loss Test: 6.1955, LR: 0.000100\n",
      "Step 1098, Loss Train: 6.2043, Loss Test: 6.1955, LR: 0.000100\n",
      "Step 1099, Loss Train: 6.2379, Loss Test: 6.1955, LR: 0.000100\n",
      "Step 1100, Loss Train: 6.2199, Loss Test: 6.1955, LR: 0.000100\n",
      "Step 1101, Loss Train: 6.1965, Loss Test: 6.1063, LR: 0.000100\n",
      "Step 1102, Loss Train: 6.2773, Loss Test: 6.1063, LR: 0.000100\n",
      "Step 1103, Loss Train: 6.2075, Loss Test: 6.1063, LR: 0.000100\n",
      "Step 1104, Loss Train: 6.1593, Loss Test: 6.1063, LR: 0.000100\n",
      "Step 1105, Loss Train: 6.2006, Loss Test: 6.1063, LR: 0.000100\n",
      "Step 1106, Loss Train: 6.1714, Loss Test: 6.1063, LR: 0.000100\n",
      "Step 1107, Loss Train: 6.1848, Loss Test: 6.1063, LR: 0.000100\n",
      "Step 1108, Loss Train: 6.1648, Loss Test: 6.1063, LR: 0.000100\n",
      "Step 1109, Loss Train: 6.2068, Loss Test: 6.1063, LR: 0.000100\n",
      "Step 1110, Loss Train: 6.1603, Loss Test: 6.1063, LR: 0.000100\n",
      "Step 1111, Loss Train: 6.2528, Loss Test: 6.1063, LR: 0.000100\n",
      "Step 1112, Loss Train: 6.1790, Loss Test: 6.1063, LR: 0.000100\n",
      "Step 1113, Loss Train: 6.2063, Loss Test: 6.1063, LR: 0.000100\n",
      "Step 1114, Loss Train: 6.2033, Loss Test: 6.1063, LR: 0.000100\n",
      "Step 1115, Loss Train: 6.1626, Loss Test: 6.1063, LR: 0.000100\n",
      "Step 1116, Loss Train: 6.1185, Loss Test: 6.1063, LR: 0.000100\n",
      "Step 1117, Loss Train: 6.1587, Loss Test: 6.1063, LR: 0.000100\n",
      "Step 1118, Loss Train: 6.1653, Loss Test: 6.1063, LR: 0.000100\n",
      "Step 1119, Loss Train: 6.1365, Loss Test: 6.1063, LR: 0.000100\n",
      "Step 1120, Loss Train: 6.1330, Loss Test: 6.1063, LR: 0.000100\n",
      "Step 1121, Loss Train: 6.1383, Loss Test: 6.1063, LR: 0.000100\n",
      "Step 1122, Loss Train: 6.1443, Loss Test: 6.1063, LR: 0.000100\n",
      "Step 1123, Loss Train: 6.1789, Loss Test: 6.1063, LR: 0.000100\n",
      "Step 1124, Loss Train: 6.1792, Loss Test: 6.1063, LR: 0.000100\n",
      "Step 1125, Loss Train: 6.1637, Loss Test: 6.1063, LR: 0.000100\n",
      "Step 1126, Loss Train: 6.2225, Loss Test: 6.0995, LR: 0.000100\n",
      "Step 1127, Loss Train: 6.1705, Loss Test: 6.0995, LR: 0.000100\n",
      "Step 1128, Loss Train: 6.1590, Loss Test: 6.0995, LR: 0.000100\n",
      "Step 1129, Loss Train: 6.1609, Loss Test: 6.0995, LR: 0.000100\n",
      "Step 1130, Loss Train: 6.2110, Loss Test: 6.0995, LR: 0.000100\n",
      "Step 1131, Loss Train: 6.1809, Loss Test: 6.0995, LR: 0.000100\n",
      "Step 1132, Loss Train: 6.1434, Loss Test: 6.0995, LR: 0.000100\n",
      "Step 1133, Loss Train: 6.1375, Loss Test: 6.0995, LR: 0.000100\n",
      "Step 1134, Loss Train: 6.1474, Loss Test: 6.0995, LR: 0.000100\n",
      "Step 1135, Loss Train: 6.1498, Loss Test: 6.0995, LR: 0.000100\n",
      "Step 1136, Loss Train: 6.1015, Loss Test: 6.0995, LR: 0.000100\n",
      "Step 1137, Loss Train: 6.1097, Loss Test: 6.0995, LR: 0.000100\n",
      "Step 1138, Loss Train: 6.2308, Loss Test: 6.0995, LR: 0.000100\n",
      "Step 1139, Loss Train: 6.1536, Loss Test: 6.0995, LR: 0.000100\n",
      "Step 1140, Loss Train: 6.1178, Loss Test: 6.0995, LR: 0.000100\n",
      "Step 1141, Loss Train: 6.1636, Loss Test: 6.0995, LR: 0.000100\n",
      "Step 1142, Loss Train: 6.0813, Loss Test: 6.0995, LR: 0.000100\n",
      "Step 1143, Loss Train: 6.2296, Loss Test: 6.0995, LR: 0.000100\n",
      "Step 1144, Loss Train: 6.1581, Loss Test: 6.0995, LR: 0.000100\n",
      "Step 1145, Loss Train: 6.1519, Loss Test: 6.0995, LR: 0.000100\n",
      "Step 1146, Loss Train: 6.2167, Loss Test: 6.0995, LR: 0.000100\n",
      "Step 1147, Loss Train: 6.1735, Loss Test: 6.0995, LR: 0.000100\n",
      "Step 1148, Loss Train: 6.0829, Loss Test: 6.0995, LR: 0.000100\n",
      "Step 1149, Loss Train: 6.1295, Loss Test: 6.0995, LR: 0.000100\n",
      "Step 1150, Loss Train: 6.1574, Loss Test: 6.0995, LR: 0.000100\n",
      "Step 1151, Loss Train: 6.0885, Loss Test: 6.0245, LR: 0.000100\n",
      "Step 1152, Loss Train: 6.0966, Loss Test: 6.0245, LR: 0.000100\n",
      "Step 1153, Loss Train: 6.0640, Loss Test: 6.0245, LR: 0.000100\n",
      "Step 1154, Loss Train: 6.0727, Loss Test: 6.0245, LR: 0.000100\n",
      "Step 1155, Loss Train: 6.1498, Loss Test: 6.0245, LR: 0.000100\n",
      "Step 1156, Loss Train: 6.1348, Loss Test: 6.0245, LR: 0.000100\n",
      "Step 1157, Loss Train: 6.1022, Loss Test: 6.0245, LR: 0.000100\n",
      "Step 1158, Loss Train: 6.1131, Loss Test: 6.0245, LR: 0.000100\n",
      "Step 1159, Loss Train: 6.1395, Loss Test: 6.0245, LR: 0.000100\n",
      "Step 1160, Loss Train: 6.1073, Loss Test: 6.0245, LR: 0.000100\n",
      "Step 1161, Loss Train: 6.0687, Loss Test: 6.0245, LR: 0.000100\n",
      "Step 1162, Loss Train: 6.0758, Loss Test: 6.0245, LR: 0.000100\n",
      "Step 1163, Loss Train: 6.1031, Loss Test: 6.0245, LR: 0.000100\n",
      "Step 1164, Loss Train: 6.0927, Loss Test: 6.0245, LR: 0.000100\n",
      "Step 1165, Loss Train: 6.0797, Loss Test: 6.0245, LR: 0.000100\n",
      "Step 1166, Loss Train: 6.0910, Loss Test: 6.0245, LR: 0.000100\n",
      "Step 1167, Loss Train: 6.0848, Loss Test: 6.0245, LR: 0.000100\n",
      "Step 1168, Loss Train: 6.0961, Loss Test: 6.0245, LR: 0.000100\n",
      "Step 1169, Loss Train: 6.0620, Loss Test: 6.0245, LR: 0.000100\n",
      "Step 1170, Loss Train: 6.1021, Loss Test: 6.0245, LR: 0.000100\n",
      "Step 1171, Loss Train: 6.1002, Loss Test: 6.0245, LR: 0.000100\n",
      "Step 1172, Loss Train: 6.0726, Loss Test: 6.0245, LR: 0.000100\n",
      "Step 1173, Loss Train: 6.1257, Loss Test: 6.0245, LR: 0.000100\n",
      "Step 1174, Loss Train: 6.0584, Loss Test: 6.0245, LR: 0.000100\n",
      "Step 1175, Loss Train: 6.1401, Loss Test: 6.0245, LR: 0.000100\n",
      "Step 1176, Loss Train: 6.0622, Loss Test: 5.9553, LR: 0.000100\n",
      "Step 1177, Loss Train: 6.0917, Loss Test: 5.9553, LR: 0.000100\n",
      "Step 1178, Loss Train: 6.0605, Loss Test: 5.9553, LR: 0.000100\n",
      "Step 1179, Loss Train: 6.0688, Loss Test: 5.9553, LR: 0.000100\n",
      "Step 1180, Loss Train: 6.0798, Loss Test: 5.9553, LR: 0.000100\n",
      "Step 1181, Loss Train: 6.1144, Loss Test: 5.9553, LR: 0.000100\n",
      "Step 1182, Loss Train: 6.1044, Loss Test: 5.9553, LR: 0.000100\n",
      "Step 1183, Loss Train: 6.1218, Loss Test: 5.9553, LR: 0.000100\n",
      "Step 1184, Loss Train: 6.1552, Loss Test: 5.9553, LR: 0.000100\n",
      "Step 1185, Loss Train: 6.0775, Loss Test: 5.9553, LR: 0.000100\n",
      "Step 1186, Loss Train: 6.0699, Loss Test: 5.9553, LR: 0.000100\n",
      "Step 1187, Loss Train: 6.0528, Loss Test: 5.9553, LR: 0.000100\n",
      "Step 1188, Loss Train: 6.1410, Loss Test: 5.9553, LR: 0.000100\n",
      "Step 1189, Loss Train: 6.1237, Loss Test: 5.9553, LR: 0.000100\n",
      "Step 1190, Loss Train: 6.0866, Loss Test: 5.9553, LR: 0.000100\n",
      "Step 1191, Loss Train: 6.0037, Loss Test: 5.9553, LR: 0.000100\n",
      "Step 1192, Loss Train: 6.1415, Loss Test: 5.9553, LR: 0.000100\n",
      "Step 1193, Loss Train: 6.0718, Loss Test: 5.9553, LR: 0.000100\n",
      "Step 1194, Loss Train: 6.0832, Loss Test: 5.9553, LR: 0.000100\n",
      "Step 1195, Loss Train: 6.1165, Loss Test: 5.9553, LR: 0.000100\n",
      "Step 1196, Loss Train: 6.0265, Loss Test: 5.9553, LR: 0.000100\n",
      "Step 1197, Loss Train: 6.1014, Loss Test: 5.9553, LR: 0.000100\n",
      "Step 1198, Loss Train: 6.1088, Loss Test: 5.9553, LR: 0.000100\n",
      "Step 1199, Loss Train: 6.0749, Loss Test: 5.9553, LR: 0.000100\n",
      "Step 1200, Loss Train: 6.0642, Loss Test: 5.9553, LR: 0.000100\n",
      "Step 1201, Loss Train: 6.0964, Loss Test: 5.9606, LR: 0.000100\n",
      "Step 1202, Loss Train: 6.0385, Loss Test: 5.9606, LR: 0.000100\n",
      "Step 1203, Loss Train: 6.0555, Loss Test: 5.9606, LR: 0.000100\n",
      "Step 1204, Loss Train: 5.9969, Loss Test: 5.9606, LR: 0.000100\n",
      "Step 1205, Loss Train: 6.0360, Loss Test: 5.9606, LR: 0.000100\n",
      "Step 1206, Loss Train: 6.0707, Loss Test: 5.9606, LR: 0.000100\n",
      "Step 1207, Loss Train: 6.0902, Loss Test: 5.9606, LR: 0.000100\n",
      "Step 1208, Loss Train: 6.0902, Loss Test: 5.9606, LR: 0.000100\n",
      "Step 1209, Loss Train: 6.0827, Loss Test: 5.9606, LR: 0.000100\n",
      "Step 1210, Loss Train: 6.0752, Loss Test: 5.9606, LR: 0.000100\n",
      "Step 1211, Loss Train: 6.0180, Loss Test: 5.9606, LR: 0.000100\n",
      "Step 1212, Loss Train: 6.0585, Loss Test: 5.9606, LR: 0.000100\n",
      "Step 1213, Loss Train: 6.1033, Loss Test: 5.9606, LR: 0.000100\n",
      "Step 1214, Loss Train: 6.0689, Loss Test: 5.9606, LR: 0.000100\n",
      "Step 1215, Loss Train: 6.0927, Loss Test: 5.9606, LR: 0.000100\n",
      "Step 1216, Loss Train: 6.0564, Loss Test: 5.9606, LR: 0.000100\n",
      "Step 1217, Loss Train: 6.0839, Loss Test: 5.9606, LR: 0.000100\n",
      "Step 1218, Loss Train: 5.9818, Loss Test: 5.9606, LR: 0.000100\n",
      "Step 1219, Loss Train: 6.0553, Loss Test: 5.9606, LR: 0.000100\n",
      "Step 1220, Loss Train: 6.0183, Loss Test: 5.9606, LR: 0.000100\n",
      "Step 1221, Loss Train: 6.0644, Loss Test: 5.9606, LR: 0.000100\n",
      "Step 1222, Loss Train: 6.0035, Loss Test: 5.9606, LR: 0.000100\n",
      "Step 1223, Loss Train: 6.0007, Loss Test: 5.9606, LR: 0.000100\n",
      "Step 1224, Loss Train: 6.0825, Loss Test: 5.9606, LR: 0.000100\n",
      "Step 1225, Loss Train: 6.1033, Loss Test: 5.9606, LR: 0.000100\n",
      "Step 1226, Loss Train: 6.0437, Loss Test: 5.9007, LR: 0.000100\n",
      "Step 1227, Loss Train: 6.0246, Loss Test: 5.9007, LR: 0.000100\n",
      "Step 1228, Loss Train: 6.0535, Loss Test: 5.9007, LR: 0.000100\n",
      "Step 1229, Loss Train: 6.0378, Loss Test: 5.9007, LR: 0.000100\n",
      "Step 1230, Loss Train: 5.9969, Loss Test: 5.9007, LR: 0.000100\n",
      "Step 1231, Loss Train: 6.0147, Loss Test: 5.9007, LR: 0.000100\n",
      "Step 1232, Loss Train: 5.9934, Loss Test: 5.9007, LR: 0.000100\n",
      "Step 1233, Loss Train: 6.0094, Loss Test: 5.9007, LR: 0.000100\n",
      "Step 1234, Loss Train: 6.0302, Loss Test: 5.9007, LR: 0.000100\n",
      "Step 1235, Loss Train: 5.9821, Loss Test: 5.9007, LR: 0.000100\n",
      "Step 1236, Loss Train: 6.0880, Loss Test: 5.9007, LR: 0.000100\n",
      "Step 1237, Loss Train: 6.0271, Loss Test: 5.9007, LR: 0.000100\n",
      "Step 1238, Loss Train: 6.0641, Loss Test: 5.9007, LR: 0.000100\n",
      "Step 1239, Loss Train: 6.0764, Loss Test: 5.9007, LR: 0.000100\n",
      "Step 1240, Loss Train: 6.0684, Loss Test: 5.9007, LR: 0.000100\n",
      "Step 1241, Loss Train: 6.0716, Loss Test: 5.9007, LR: 0.000100\n",
      "Step 1242, Loss Train: 5.9984, Loss Test: 5.9007, LR: 0.000100\n",
      "Step 1243, Loss Train: 6.0061, Loss Test: 5.9007, LR: 0.000100\n",
      "Step 1244, Loss Train: 6.0389, Loss Test: 5.9007, LR: 0.000100\n",
      "Step 1245, Loss Train: 6.0084, Loss Test: 5.9007, LR: 0.000100\n",
      "Step 1246, Loss Train: 5.9902, Loss Test: 5.9007, LR: 0.000100\n",
      "Step 1247, Loss Train: 5.9855, Loss Test: 5.9007, LR: 0.000100\n",
      "Step 1248, Loss Train: 6.0183, Loss Test: 5.9007, LR: 0.000100\n",
      "Step 1249, Loss Train: 5.9916, Loss Test: 5.9007, LR: 0.000100\n",
      "Step 1250, Loss Train: 6.0534, Loss Test: 5.9007, LR: 0.000100\n",
      "Step 1251, Loss Train: 6.0439, Loss Test: 5.9556, LR: 0.000100\n",
      "Step 1252, Loss Train: 5.9884, Loss Test: 5.9556, LR: 0.000100\n",
      "Step 1253, Loss Train: 5.9540, Loss Test: 5.9556, LR: 0.000100\n",
      "Step 1254, Loss Train: 5.9994, Loss Test: 5.9556, LR: 0.000100\n",
      "Step 1255, Loss Train: 5.9815, Loss Test: 5.9556, LR: 0.000100\n",
      "Step 1256, Loss Train: 6.0187, Loss Test: 5.9556, LR: 0.000100\n",
      "Step 1257, Loss Train: 6.0226, Loss Test: 5.9556, LR: 0.000100\n",
      "Step 1258, Loss Train: 5.9797, Loss Test: 5.9556, LR: 0.000100\n",
      "Step 1259, Loss Train: 5.9805, Loss Test: 5.9556, LR: 0.000100\n",
      "Step 1260, Loss Train: 5.9992, Loss Test: 5.9556, LR: 0.000100\n",
      "Step 1261, Loss Train: 6.0017, Loss Test: 5.9556, LR: 0.000100\n",
      "Step 1262, Loss Train: 6.0090, Loss Test: 5.9556, LR: 0.000100\n",
      "Step 1263, Loss Train: 5.9793, Loss Test: 5.9556, LR: 0.000100\n",
      "Step 1264, Loss Train: 6.0003, Loss Test: 5.9556, LR: 0.000100\n",
      "Step 1265, Loss Train: 5.9636, Loss Test: 5.9556, LR: 0.000100\n",
      "Step 1266, Loss Train: 6.0415, Loss Test: 5.9556, LR: 0.000100\n",
      "Step 1267, Loss Train: 5.9585, Loss Test: 5.9556, LR: 0.000100\n",
      "Step 1268, Loss Train: 5.9952, Loss Test: 5.9556, LR: 0.000100\n",
      "Step 1269, Loss Train: 5.9708, Loss Test: 5.9556, LR: 0.000100\n",
      "Step 1270, Loss Train: 5.9450, Loss Test: 5.9556, LR: 0.000100\n",
      "Step 1271, Loss Train: 5.9317, Loss Test: 5.9556, LR: 0.000100\n",
      "Step 1272, Loss Train: 5.9611, Loss Test: 5.9556, LR: 0.000100\n",
      "Step 1273, Loss Train: 5.9861, Loss Test: 5.9556, LR: 0.000100\n",
      "Step 1274, Loss Train: 5.9769, Loss Test: 5.9556, LR: 0.000100\n",
      "Step 1275, Loss Train: 5.9948, Loss Test: 5.9556, LR: 0.000100\n",
      "Step 1276, Loss Train: 5.9627, Loss Test: 5.8883, LR: 0.000100\n",
      "Step 1277, Loss Train: 5.9981, Loss Test: 5.8883, LR: 0.000100\n",
      "Step 1278, Loss Train: 6.0137, Loss Test: 5.8883, LR: 0.000100\n",
      "Step 1279, Loss Train: 6.0227, Loss Test: 5.8883, LR: 0.000100\n",
      "Step 1280, Loss Train: 5.9811, Loss Test: 5.8883, LR: 0.000100\n",
      "Step 1281, Loss Train: 5.9401, Loss Test: 5.8883, LR: 0.000100\n",
      "Step 1282, Loss Train: 5.9971, Loss Test: 5.8883, LR: 0.000100\n",
      "Step 1283, Loss Train: 6.0066, Loss Test: 5.8883, LR: 0.000100\n",
      "Step 1284, Loss Train: 6.0312, Loss Test: 5.8883, LR: 0.000100\n",
      "Step 1285, Loss Train: 5.9122, Loss Test: 5.8883, LR: 0.000100\n",
      "Step 1286, Loss Train: 5.9639, Loss Test: 5.8883, LR: 0.000100\n",
      "Step 1287, Loss Train: 5.9408, Loss Test: 5.8883, LR: 0.000100\n",
      "Step 1288, Loss Train: 5.9564, Loss Test: 5.8883, LR: 0.000100\n",
      "Step 1289, Loss Train: 5.9248, Loss Test: 5.8883, LR: 0.000100\n",
      "Step 1290, Loss Train: 5.9616, Loss Test: 5.8883, LR: 0.000100\n",
      "Step 1291, Loss Train: 6.0265, Loss Test: 5.8883, LR: 0.000100\n",
      "Step 1292, Loss Train: 5.9829, Loss Test: 5.8883, LR: 0.000100\n",
      "Step 1293, Loss Train: 5.9478, Loss Test: 5.8883, LR: 0.000100\n",
      "Step 1294, Loss Train: 6.0172, Loss Test: 5.8883, LR: 0.000100\n",
      "Step 1295, Loss Train: 5.9482, Loss Test: 5.8883, LR: 0.000100\n",
      "Step 1296, Loss Train: 5.9414, Loss Test: 5.8883, LR: 0.000100\n",
      "Step 1297, Loss Train: 5.9356, Loss Test: 5.8883, LR: 0.000100\n",
      "Step 1298, Loss Train: 5.9515, Loss Test: 5.8883, LR: 0.000100\n",
      "Step 1299, Loss Train: 5.9659, Loss Test: 5.8883, LR: 0.000100\n",
      "Step 1300, Loss Train: 5.9402, Loss Test: 5.8883, LR: 0.000100\n",
      "Step 1301, Loss Train: 5.9380, Loss Test: 5.8331, LR: 0.000100\n",
      "Step 1302, Loss Train: 5.9275, Loss Test: 5.8331, LR: 0.000100\n",
      "Step 1303, Loss Train: 5.9835, Loss Test: 5.8331, LR: 0.000100\n",
      "Step 1304, Loss Train: 5.9072, Loss Test: 5.8331, LR: 0.000100\n",
      "Step 1305, Loss Train: 5.9786, Loss Test: 5.8331, LR: 0.000100\n",
      "Step 1306, Loss Train: 6.0052, Loss Test: 5.8331, LR: 0.000100\n",
      "Step 1307, Loss Train: 5.9523, Loss Test: 5.8331, LR: 0.000100\n",
      "Step 1308, Loss Train: 5.9208, Loss Test: 5.8331, LR: 0.000100\n",
      "Step 1309, Loss Train: 5.9397, Loss Test: 5.8331, LR: 0.000100\n",
      "Step 1310, Loss Train: 5.9816, Loss Test: 5.8331, LR: 0.000100\n",
      "Step 1311, Loss Train: 5.9466, Loss Test: 5.8331, LR: 0.000100\n",
      "Step 1312, Loss Train: 5.9564, Loss Test: 5.8331, LR: 0.000100\n",
      "Step 1313, Loss Train: 5.9407, Loss Test: 5.8331, LR: 0.000100\n",
      "Step 1314, Loss Train: 5.9560, Loss Test: 5.8331, LR: 0.000100\n",
      "Step 1315, Loss Train: 5.9806, Loss Test: 5.8331, LR: 0.000100\n",
      "Step 1316, Loss Train: 5.9684, Loss Test: 5.8331, LR: 0.000100\n",
      "Step 1317, Loss Train: 5.9232, Loss Test: 5.8331, LR: 0.000100\n",
      "Step 1318, Loss Train: 5.9573, Loss Test: 5.8331, LR: 0.000100\n",
      "Step 1319, Loss Train: 5.9500, Loss Test: 5.8331, LR: 0.000100\n",
      "Step 1320, Loss Train: 5.9373, Loss Test: 5.8331, LR: 0.000100\n",
      "Step 1321, Loss Train: 5.9234, Loss Test: 5.8331, LR: 0.000100\n",
      "Step 1322, Loss Train: 5.8856, Loss Test: 5.8331, LR: 0.000100\n",
      "Step 1323, Loss Train: 5.9229, Loss Test: 5.8331, LR: 0.000100\n",
      "Step 1324, Loss Train: 5.9392, Loss Test: 5.8331, LR: 0.000100\n",
      "Step 1325, Loss Train: 5.9606, Loss Test: 5.8331, LR: 0.000100\n",
      "Step 1326, Loss Train: 5.9262, Loss Test: 5.8382, LR: 0.000100\n",
      "Step 1327, Loss Train: 5.9789, Loss Test: 5.8382, LR: 0.000100\n",
      "Step 1328, Loss Train: 5.8803, Loss Test: 5.8382, LR: 0.000100\n",
      "Step 1329, Loss Train: 5.9156, Loss Test: 5.8382, LR: 0.000100\n",
      "Step 1330, Loss Train: 5.9419, Loss Test: 5.8382, LR: 0.000100\n",
      "Step 1331, Loss Train: 5.9178, Loss Test: 5.8382, LR: 0.000100\n",
      "Step 1332, Loss Train: 5.8858, Loss Test: 5.8382, LR: 0.000100\n",
      "Step 1333, Loss Train: 5.9389, Loss Test: 5.8382, LR: 0.000100\n",
      "Step 1334, Loss Train: 5.9196, Loss Test: 5.8382, LR: 0.000100\n",
      "Step 1335, Loss Train: 5.8560, Loss Test: 5.8382, LR: 0.000100\n",
      "Step 1336, Loss Train: 5.9394, Loss Test: 5.8382, LR: 0.000100\n",
      "Step 1337, Loss Train: 5.9582, Loss Test: 5.8382, LR: 0.000100\n",
      "Step 1338, Loss Train: 5.9132, Loss Test: 5.8382, LR: 0.000100\n",
      "Step 1339, Loss Train: 5.9486, Loss Test: 5.8382, LR: 0.000100\n",
      "Step 1340, Loss Train: 5.9058, Loss Test: 5.8382, LR: 0.000100\n",
      "Step 1341, Loss Train: 5.9413, Loss Test: 5.8382, LR: 0.000100\n",
      "Step 1342, Loss Train: 5.9129, Loss Test: 5.8382, LR: 0.000100\n",
      "Step 1343, Loss Train: 5.9235, Loss Test: 5.8382, LR: 0.000100\n",
      "Step 1344, Loss Train: 5.9279, Loss Test: 5.8382, LR: 0.000100\n",
      "Step 1345, Loss Train: 5.9061, Loss Test: 5.8382, LR: 0.000100\n",
      "Step 1346, Loss Train: 5.9574, Loss Test: 5.8382, LR: 0.000100\n",
      "Step 1347, Loss Train: 5.9899, Loss Test: 5.8382, LR: 0.000100\n",
      "Step 1348, Loss Train: 5.9273, Loss Test: 5.8382, LR: 0.000100\n",
      "Step 1349, Loss Train: 5.9533, Loss Test: 5.8382, LR: 0.000100\n",
      "Step 1350, Loss Train: 5.9262, Loss Test: 5.8382, LR: 0.000100\n",
      "Step 1351, Loss Train: 5.9463, Loss Test: 5.8901, LR: 0.000100\n",
      "Step 1352, Loss Train: 5.8982, Loss Test: 5.8901, LR: 0.000100\n",
      "Step 1353, Loss Train: 5.9215, Loss Test: 5.8901, LR: 0.000100\n",
      "Step 1354, Loss Train: 5.8744, Loss Test: 5.8901, LR: 0.000100\n",
      "Step 1355, Loss Train: 5.9963, Loss Test: 5.8901, LR: 0.000100\n",
      "Step 1356, Loss Train: 5.9356, Loss Test: 5.8901, LR: 0.000100\n",
      "Step 1357, Loss Train: 5.9640, Loss Test: 5.8901, LR: 0.000100\n",
      "Step 1358, Loss Train: 5.9225, Loss Test: 5.8901, LR: 0.000100\n",
      "Step 1359, Loss Train: 5.9054, Loss Test: 5.8901, LR: 0.000100\n",
      "Step 1360, Loss Train: 5.9734, Loss Test: 5.8901, LR: 0.000100\n",
      "Step 1361, Loss Train: 5.9174, Loss Test: 5.8901, LR: 0.000100\n",
      "Step 1362, Loss Train: 5.9910, Loss Test: 5.8901, LR: 0.000100\n",
      "Step 1363, Loss Train: 5.9028, Loss Test: 5.8901, LR: 0.000100\n",
      "Step 1364, Loss Train: 5.9213, Loss Test: 5.8901, LR: 0.000100\n",
      "Step 1365, Loss Train: 5.9113, Loss Test: 5.8901, LR: 0.000100\n",
      "Step 1366, Loss Train: 5.8788, Loss Test: 5.8901, LR: 0.000100\n",
      "Step 1367, Loss Train: 5.8795, Loss Test: 5.8901, LR: 0.000100\n",
      "Step 1368, Loss Train: 5.9425, Loss Test: 5.8901, LR: 0.000100\n",
      "Step 1369, Loss Train: 5.9167, Loss Test: 5.8901, LR: 0.000100\n",
      "Step 1370, Loss Train: 5.9166, Loss Test: 5.8901, LR: 0.000100\n",
      "Step 1371, Loss Train: 5.9330, Loss Test: 5.8901, LR: 0.000100\n",
      "Step 1372, Loss Train: 5.9563, Loss Test: 5.8901, LR: 0.000100\n",
      "Step 1373, Loss Train: 5.9115, Loss Test: 5.8901, LR: 0.000100\n",
      "Step 1374, Loss Train: 5.9108, Loss Test: 5.8901, LR: 0.000100\n",
      "Step 1375, Loss Train: 5.9117, Loss Test: 5.8901, LR: 0.000100\n",
      "Step 1376, Loss Train: 5.9275, Loss Test: 5.8609, LR: 0.000100\n",
      "Step 1377, Loss Train: 5.9150, Loss Test: 5.8609, LR: 0.000100\n",
      "Step 1378, Loss Train: 5.9441, Loss Test: 5.8609, LR: 0.000100\n",
      "Step 1379, Loss Train: 5.8757, Loss Test: 5.8609, LR: 0.000100\n",
      "Step 1380, Loss Train: 5.9024, Loss Test: 5.8609, LR: 0.000100\n",
      "Step 1381, Loss Train: 5.8836, Loss Test: 5.8609, LR: 0.000100\n",
      "Step 1382, Loss Train: 5.9444, Loss Test: 5.8609, LR: 0.000100\n",
      "Step 1383, Loss Train: 5.8879, Loss Test: 5.8609, LR: 0.000100\n",
      "Step 1384, Loss Train: 5.8765, Loss Test: 5.8609, LR: 0.000100\n",
      "Step 1385, Loss Train: 5.8933, Loss Test: 5.8609, LR: 0.000100\n",
      "Step 1386, Loss Train: 5.8380, Loss Test: 5.8609, LR: 0.000100\n",
      "Step 1387, Loss Train: 5.9295, Loss Test: 5.8609, LR: 0.000100\n",
      "Step 1388, Loss Train: 5.9152, Loss Test: 5.8609, LR: 0.000100\n",
      "Step 1389, Loss Train: 5.8680, Loss Test: 5.8609, LR: 0.000100\n",
      "Step 1390, Loss Train: 5.9553, Loss Test: 5.8609, LR: 0.000100\n",
      "Step 1391, Loss Train: 5.9067, Loss Test: 5.8609, LR: 0.000100\n",
      "Step 1392, Loss Train: 5.9855, Loss Test: 5.8609, LR: 0.000100\n",
      "Step 1393, Loss Train: 5.9056, Loss Test: 5.8609, LR: 0.000100\n",
      "Step 1394, Loss Train: 5.8714, Loss Test: 5.8609, LR: 0.000100\n",
      "Step 1395, Loss Train: 5.8731, Loss Test: 5.8609, LR: 0.000100\n",
      "Step 1396, Loss Train: 5.9208, Loss Test: 5.8609, LR: 0.000100\n",
      "Step 1397, Loss Train: 5.9446, Loss Test: 5.8609, LR: 0.000100\n",
      "Step 1398, Loss Train: 5.8616, Loss Test: 5.8609, LR: 0.000100\n",
      "Step 1399, Loss Train: 5.8875, Loss Test: 5.8609, LR: 0.000100\n",
      "Step 1400, Loss Train: 5.8982, Loss Test: 5.8609, LR: 0.000100\n",
      "Step 1401, Loss Train: 5.8967, Loss Test: 5.7691, LR: 0.000100\n",
      "Step 1402, Loss Train: 5.9172, Loss Test: 5.7691, LR: 0.000100\n",
      "Step 1403, Loss Train: 5.8790, Loss Test: 5.7691, LR: 0.000100\n",
      "Step 1404, Loss Train: 5.8657, Loss Test: 5.7691, LR: 0.000100\n",
      "Step 1405, Loss Train: 5.8355, Loss Test: 5.7691, LR: 0.000100\n",
      "Step 1406, Loss Train: 5.9119, Loss Test: 5.7691, LR: 0.000100\n",
      "Step 1407, Loss Train: 5.8754, Loss Test: 5.7691, LR: 0.000100\n",
      "Step 1408, Loss Train: 5.8749, Loss Test: 5.7691, LR: 0.000100\n",
      "Step 1409, Loss Train: 5.9571, Loss Test: 5.7691, LR: 0.000100\n",
      "Step 1410, Loss Train: 5.8658, Loss Test: 5.7691, LR: 0.000100\n",
      "Step 1411, Loss Train: 5.8487, Loss Test: 5.7691, LR: 0.000100\n",
      "Step 1412, Loss Train: 5.8152, Loss Test: 5.7691, LR: 0.000100\n",
      "Step 1413, Loss Train: 5.9271, Loss Test: 5.7691, LR: 0.000100\n",
      "Step 1414, Loss Train: 5.8732, Loss Test: 5.7691, LR: 0.000100\n",
      "Step 1415, Loss Train: 5.9158, Loss Test: 5.7691, LR: 0.000100\n",
      "Step 1416, Loss Train: 5.9318, Loss Test: 5.7691, LR: 0.000100\n",
      "Step 1417, Loss Train: 5.9155, Loss Test: 5.7691, LR: 0.000100\n",
      "Step 1418, Loss Train: 5.8351, Loss Test: 5.7691, LR: 0.000100\n",
      "Step 1419, Loss Train: 5.9338, Loss Test: 5.7691, LR: 0.000100\n",
      "Step 1420, Loss Train: 5.8066, Loss Test: 5.7691, LR: 0.000100\n",
      "Step 1421, Loss Train: 5.8014, Loss Test: 5.7691, LR: 0.000100\n",
      "Step 1422, Loss Train: 5.8193, Loss Test: 5.7691, LR: 0.000100\n",
      "Step 1423, Loss Train: 5.8113, Loss Test: 5.7691, LR: 0.000100\n",
      "Step 1424, Loss Train: 5.8622, Loss Test: 5.7691, LR: 0.000100\n",
      "Step 1425, Loss Train: 5.9077, Loss Test: 5.7691, LR: 0.000100\n",
      "Step 1426, Loss Train: 5.8699, Loss Test: 5.7402, LR: 0.000100\n",
      "Step 1427, Loss Train: 5.8983, Loss Test: 5.7402, LR: 0.000100\n",
      "Step 1428, Loss Train: 5.8085, Loss Test: 5.7402, LR: 0.000100\n",
      "Step 1429, Loss Train: 5.8842, Loss Test: 5.7402, LR: 0.000100\n",
      "Step 1430, Loss Train: 5.9422, Loss Test: 5.7402, LR: 0.000100\n",
      "Step 1431, Loss Train: 5.8174, Loss Test: 5.7402, LR: 0.000100\n",
      "Step 1432, Loss Train: 5.8345, Loss Test: 5.7402, LR: 0.000100\n",
      "Step 1433, Loss Train: 5.8036, Loss Test: 5.7402, LR: 0.000100\n",
      "Step 1434, Loss Train: 5.8459, Loss Test: 5.7402, LR: 0.000100\n",
      "Step 1435, Loss Train: 5.8249, Loss Test: 5.7402, LR: 0.000100\n",
      "Step 1436, Loss Train: 5.8676, Loss Test: 5.7402, LR: 0.000100\n",
      "Step 1437, Loss Train: 5.8185, Loss Test: 5.7402, LR: 0.000100\n",
      "Step 1438, Loss Train: 5.8762, Loss Test: 5.7402, LR: 0.000100\n",
      "Step 1439, Loss Train: 5.8495, Loss Test: 5.7402, LR: 0.000100\n",
      "Step 1440, Loss Train: 5.8527, Loss Test: 5.7402, LR: 0.000100\n",
      "Step 1441, Loss Train: 5.8266, Loss Test: 5.7402, LR: 0.000100\n",
      "Step 1442, Loss Train: 5.8415, Loss Test: 5.7402, LR: 0.000100\n",
      "Step 1443, Loss Train: 5.8380, Loss Test: 5.7402, LR: 0.000100\n",
      "Step 1444, Loss Train: 5.9042, Loss Test: 5.7402, LR: 0.000100\n",
      "Step 1445, Loss Train: 5.8868, Loss Test: 5.7402, LR: 0.000100\n",
      "Step 1446, Loss Train: 5.8506, Loss Test: 5.7402, LR: 0.000100\n",
      "Step 1447, Loss Train: 5.8806, Loss Test: 5.7402, LR: 0.000100\n",
      "Step 1448, Loss Train: 5.8868, Loss Test: 5.7402, LR: 0.000100\n",
      "Step 1449, Loss Train: 5.8619, Loss Test: 5.7402, LR: 0.000100\n",
      "Step 1450, Loss Train: 5.8188, Loss Test: 5.7402, LR: 0.000100\n",
      "Step 1451, Loss Train: 5.8526, Loss Test: 5.7598, LR: 0.000100\n",
      "Step 1452, Loss Train: 5.8001, Loss Test: 5.7598, LR: 0.000100\n",
      "Step 1453, Loss Train: 5.8612, Loss Test: 5.7598, LR: 0.000100\n",
      "Step 1454, Loss Train: 5.8755, Loss Test: 5.7598, LR: 0.000100\n",
      "Step 1455, Loss Train: 5.8445, Loss Test: 5.7598, LR: 0.000100\n",
      "Step 1456, Loss Train: 5.8129, Loss Test: 5.7598, LR: 0.000100\n",
      "Step 1457, Loss Train: 5.7827, Loss Test: 5.7598, LR: 0.000100\n",
      "Step 1458, Loss Train: 5.8220, Loss Test: 5.7598, LR: 0.000100\n",
      "Step 1459, Loss Train: 5.8475, Loss Test: 5.7598, LR: 0.000100\n",
      "Step 1460, Loss Train: 5.8227, Loss Test: 5.7598, LR: 0.000100\n",
      "Step 1461, Loss Train: 5.9015, Loss Test: 5.7598, LR: 0.000100\n",
      "Step 1462, Loss Train: 5.8532, Loss Test: 5.7598, LR: 0.000100\n",
      "Step 1463, Loss Train: 5.8039, Loss Test: 5.7598, LR: 0.000100\n",
      "Step 1464, Loss Train: 5.8180, Loss Test: 5.7598, LR: 0.000100\n",
      "Step 1465, Loss Train: 5.8423, Loss Test: 5.7598, LR: 0.000100\n",
      "Step 1466, Loss Train: 5.8781, Loss Test: 5.7598, LR: 0.000100\n",
      "Step 1467, Loss Train: 5.8391, Loss Test: 5.7598, LR: 0.000100\n",
      "Step 1468, Loss Train: 5.8263, Loss Test: 5.7598, LR: 0.000100\n",
      "Step 1469, Loss Train: 5.8527, Loss Test: 5.7598, LR: 0.000100\n",
      "Step 1470, Loss Train: 5.8320, Loss Test: 5.7598, LR: 0.000100\n",
      "Step 1471, Loss Train: 5.8351, Loss Test: 5.7598, LR: 0.000100\n",
      "Step 1472, Loss Train: 5.8542, Loss Test: 5.7598, LR: 0.000100\n",
      "Step 1473, Loss Train: 5.8355, Loss Test: 5.7598, LR: 0.000100\n",
      "Step 1474, Loss Train: 5.8262, Loss Test: 5.7598, LR: 0.000100\n",
      "Step 1475, Loss Train: 5.8751, Loss Test: 5.7598, LR: 0.000100\n",
      "Step 1476, Loss Train: 5.8338, Loss Test: 5.7013, LR: 0.000100\n",
      "Step 1477, Loss Train: 5.8172, Loss Test: 5.7013, LR: 0.000100\n",
      "Step 1478, Loss Train: 5.7611, Loss Test: 5.7013, LR: 0.000100\n",
      "Step 1479, Loss Train: 5.8204, Loss Test: 5.7013, LR: 0.000100\n",
      "Step 1480, Loss Train: 5.8174, Loss Test: 5.7013, LR: 0.000100\n",
      "Step 1481, Loss Train: 5.8346, Loss Test: 5.7013, LR: 0.000100\n",
      "Step 1482, Loss Train: 5.8469, Loss Test: 5.7013, LR: 0.000100\n",
      "Step 1483, Loss Train: 5.8232, Loss Test: 5.7013, LR: 0.000100\n",
      "Step 1484, Loss Train: 5.8223, Loss Test: 5.7013, LR: 0.000100\n",
      "Step 1485, Loss Train: 5.7712, Loss Test: 5.7013, LR: 0.000100\n",
      "Step 1486, Loss Train: 5.7678, Loss Test: 5.7013, LR: 0.000100\n",
      "Step 1487, Loss Train: 5.8375, Loss Test: 5.7013, LR: 0.000100\n",
      "Step 1488, Loss Train: 5.8440, Loss Test: 5.7013, LR: 0.000100\n",
      "Step 1489, Loss Train: 5.8198, Loss Test: 5.7013, LR: 0.000100\n",
      "Step 1490, Loss Train: 5.8734, Loss Test: 5.7013, LR: 0.000100\n",
      "Step 1491, Loss Train: 5.7870, Loss Test: 5.7013, LR: 0.000100\n",
      "Step 1492, Loss Train: 5.8327, Loss Test: 5.7013, LR: 0.000100\n",
      "Step 1493, Loss Train: 5.8121, Loss Test: 5.7013, LR: 0.000100\n",
      "Step 1494, Loss Train: 5.8442, Loss Test: 5.7013, LR: 0.000100\n",
      "Step 1495, Loss Train: 5.8053, Loss Test: 5.7013, LR: 0.000100\n",
      "Step 1496, Loss Train: 5.7908, Loss Test: 5.7013, LR: 0.000100\n",
      "Step 1497, Loss Train: 5.8365, Loss Test: 5.7013, LR: 0.000100\n",
      "Step 1498, Loss Train: 5.7903, Loss Test: 5.7013, LR: 0.000100\n",
      "Step 1499, Loss Train: 5.8782, Loss Test: 5.7013, LR: 0.000100\n",
      "Step 1500, Loss Train: 5.7995, Loss Test: 5.7013, LR: 0.000100\n",
      "Step 1501, Loss Train: 5.8026, Loss Test: 5.7481, LR: 0.000100\n",
      "Step 1502, Loss Train: 5.8275, Loss Test: 5.7481, LR: 0.000100\n",
      "Step 1503, Loss Train: 5.8380, Loss Test: 5.7481, LR: 0.000100\n",
      "Step 1504, Loss Train: 5.7805, Loss Test: 5.7481, LR: 0.000100\n",
      "Step 1505, Loss Train: 5.8261, Loss Test: 5.7481, LR: 0.000100\n",
      "Step 1506, Loss Train: 5.8453, Loss Test: 5.7481, LR: 0.000100\n",
      "Step 1507, Loss Train: 5.8399, Loss Test: 5.7481, LR: 0.000100\n",
      "Step 1508, Loss Train: 5.8672, Loss Test: 5.7481, LR: 0.000100\n",
      "Step 1509, Loss Train: 5.7579, Loss Test: 5.7481, LR: 0.000100\n",
      "Step 1510, Loss Train: 5.7884, Loss Test: 5.7481, LR: 0.000100\n",
      "Step 1511, Loss Train: 5.8118, Loss Test: 5.7481, LR: 0.000100\n",
      "Step 1512, Loss Train: 5.7601, Loss Test: 5.7481, LR: 0.000100\n",
      "Step 1513, Loss Train: 5.7863, Loss Test: 5.7481, LR: 0.000100\n",
      "Step 1514, Loss Train: 5.8084, Loss Test: 5.7481, LR: 0.000100\n",
      "Step 1515, Loss Train: 5.7481, Loss Test: 5.7481, LR: 0.000100\n",
      "Step 1516, Loss Train: 5.8082, Loss Test: 5.7481, LR: 0.000100\n",
      "Step 1517, Loss Train: 5.8456, Loss Test: 5.7481, LR: 0.000100\n",
      "Step 1518, Loss Train: 5.7543, Loss Test: 5.7481, LR: 0.000100\n",
      "Step 1519, Loss Train: 5.7533, Loss Test: 5.7481, LR: 0.000100\n",
      "Step 1520, Loss Train: 5.7418, Loss Test: 5.7481, LR: 0.000100\n",
      "Step 1521, Loss Train: 5.7983, Loss Test: 5.7481, LR: 0.000100\n",
      "Step 1522, Loss Train: 5.8356, Loss Test: 5.7481, LR: 0.000100\n",
      "Step 1523, Loss Train: 5.8391, Loss Test: 5.7481, LR: 0.000100\n",
      "Step 1524, Loss Train: 5.7500, Loss Test: 5.7481, LR: 0.000100\n",
      "Step 1525, Loss Train: 5.7803, Loss Test: 5.7481, LR: 0.000100\n",
      "Step 1526, Loss Train: 5.8324, Loss Test: 5.7006, LR: 0.000100\n",
      "Step 1527, Loss Train: 5.7946, Loss Test: 5.7006, LR: 0.000100\n",
      "Step 1528, Loss Train: 5.8237, Loss Test: 5.7006, LR: 0.000100\n",
      "Step 1529, Loss Train: 5.7951, Loss Test: 5.7006, LR: 0.000100\n",
      "Step 1530, Loss Train: 5.8059, Loss Test: 5.7006, LR: 0.000100\n",
      "Step 1531, Loss Train: 5.7762, Loss Test: 5.7006, LR: 0.000100\n",
      "Step 1532, Loss Train: 5.8056, Loss Test: 5.7006, LR: 0.000100\n",
      "Step 1533, Loss Train: 5.8067, Loss Test: 5.7006, LR: 0.000100\n",
      "Step 1534, Loss Train: 5.7770, Loss Test: 5.7006, LR: 0.000100\n",
      "Step 1535, Loss Train: 5.7872, Loss Test: 5.7006, LR: 0.000100\n",
      "Step 1536, Loss Train: 5.8415, Loss Test: 5.7006, LR: 0.000100\n",
      "Step 1537, Loss Train: 5.7283, Loss Test: 5.7006, LR: 0.000100\n",
      "Step 1538, Loss Train: 5.7608, Loss Test: 5.7006, LR: 0.000100\n",
      "Step 1539, Loss Train: 5.8054, Loss Test: 5.7006, LR: 0.000100\n",
      "Step 1540, Loss Train: 5.7762, Loss Test: 5.7006, LR: 0.000100\n",
      "Step 1541, Loss Train: 5.7484, Loss Test: 5.7006, LR: 0.000100\n",
      "Step 1542, Loss Train: 5.7828, Loss Test: 5.7006, LR: 0.000100\n",
      "Step 1543, Loss Train: 5.7684, Loss Test: 5.7006, LR: 0.000100\n",
      "Step 1544, Loss Train: 5.7592, Loss Test: 5.7006, LR: 0.000100\n",
      "Step 1545, Loss Train: 5.8139, Loss Test: 5.7006, LR: 0.000100\n",
      "Step 1546, Loss Train: 5.7500, Loss Test: 5.7006, LR: 0.000100\n",
      "Step 1547, Loss Train: 5.7992, Loss Test: 5.7006, LR: 0.000100\n",
      "Step 1548, Loss Train: 5.7107, Loss Test: 5.7006, LR: 0.000100\n",
      "Step 1549, Loss Train: 5.8617, Loss Test: 5.7006, LR: 0.000100\n",
      "Step 1550, Loss Train: 5.8399, Loss Test: 5.7006, LR: 0.000100\n",
      "Step 1551, Loss Train: 5.7736, Loss Test: 5.7503, LR: 0.000100\n",
      "Step 1552, Loss Train: 5.7592, Loss Test: 5.7503, LR: 0.000100\n",
      "Step 1553, Loss Train: 5.7702, Loss Test: 5.7503, LR: 0.000100\n",
      "Step 1554, Loss Train: 5.7468, Loss Test: 5.7503, LR: 0.000100\n",
      "Step 1555, Loss Train: 5.7739, Loss Test: 5.7503, LR: 0.000100\n",
      "Step 1556, Loss Train: 5.7419, Loss Test: 5.7503, LR: 0.000100\n",
      "Step 1557, Loss Train: 5.7203, Loss Test: 5.7503, LR: 0.000100\n",
      "Step 1558, Loss Train: 5.8014, Loss Test: 5.7503, LR: 0.000100\n",
      "Step 1559, Loss Train: 5.7641, Loss Test: 5.7503, LR: 0.000100\n",
      "Step 1560, Loss Train: 5.7921, Loss Test: 5.7503, LR: 0.000100\n",
      "Step 1561, Loss Train: 5.7619, Loss Test: 5.7503, LR: 0.000100\n",
      "Step 1562, Loss Train: 5.7849, Loss Test: 5.7503, LR: 0.000100\n",
      "Step 1563, Loss Train: 5.7767, Loss Test: 5.7503, LR: 0.000100\n",
      "Step 1564, Loss Train: 5.8275, Loss Test: 5.7503, LR: 0.000100\n",
      "Step 1565, Loss Train: 5.7901, Loss Test: 5.7503, LR: 0.000100\n",
      "Step 1566, Loss Train: 5.7912, Loss Test: 5.7503, LR: 0.000100\n",
      "Step 1567, Loss Train: 5.8024, Loss Test: 5.7503, LR: 0.000100\n",
      "Step 1568, Loss Train: 5.7283, Loss Test: 5.7503, LR: 0.000100\n",
      "Step 1569, Loss Train: 5.7498, Loss Test: 5.7503, LR: 0.000100\n",
      "Step 1570, Loss Train: 5.7454, Loss Test: 5.7503, LR: 0.000100\n",
      "Step 1571, Loss Train: 5.7483, Loss Test: 5.7503, LR: 0.000100\n",
      "Step 1572, Loss Train: 5.7767, Loss Test: 5.7503, LR: 0.000100\n",
      "Step 1573, Loss Train: 5.7062, Loss Test: 5.7503, LR: 0.000100\n",
      "Step 1574, Loss Train: 5.7866, Loss Test: 5.7503, LR: 0.000100\n",
      "Step 1575, Loss Train: 5.7327, Loss Test: 5.7503, LR: 0.000100\n",
      "Step 1576, Loss Train: 5.7409, Loss Test: 5.6508, LR: 0.000100\n",
      "Step 1577, Loss Train: 5.7861, Loss Test: 5.6508, LR: 0.000100\n",
      "Step 1578, Loss Train: 5.7575, Loss Test: 5.6508, LR: 0.000100\n",
      "Step 1579, Loss Train: 5.7789, Loss Test: 5.6508, LR: 0.000100\n",
      "Step 1580, Loss Train: 5.7228, Loss Test: 5.6508, LR: 0.000100\n",
      "Step 1581, Loss Train: 5.7460, Loss Test: 5.6508, LR: 0.000100\n",
      "Step 1582, Loss Train: 5.8383, Loss Test: 5.6508, LR: 0.000100\n",
      "Step 1583, Loss Train: 5.7685, Loss Test: 5.6508, LR: 0.000100\n",
      "Step 1584, Loss Train: 5.6923, Loss Test: 5.6508, LR: 0.000100\n",
      "Step 1585, Loss Train: 5.7810, Loss Test: 5.6508, LR: 0.000100\n",
      "Step 1586, Loss Train: 5.7637, Loss Test: 5.6508, LR: 0.000100\n",
      "Step 1587, Loss Train: 5.6867, Loss Test: 5.6508, LR: 0.000100\n",
      "Step 1588, Loss Train: 5.7963, Loss Test: 5.6508, LR: 0.000100\n",
      "Step 1589, Loss Train: 5.7227, Loss Test: 5.6508, LR: 0.000100\n",
      "Step 1590, Loss Train: 5.7294, Loss Test: 5.6508, LR: 0.000100\n",
      "Step 1591, Loss Train: 5.7128, Loss Test: 5.6508, LR: 0.000100\n",
      "Step 1592, Loss Train: 5.6325, Loss Test: 5.6508, LR: 0.000100\n",
      "Step 1593, Loss Train: 5.8056, Loss Test: 5.6508, LR: 0.000100\n",
      "Step 1594, Loss Train: 5.7388, Loss Test: 5.6508, LR: 0.000100\n",
      "Step 1595, Loss Train: 5.7510, Loss Test: 5.6508, LR: 0.000100\n",
      "Step 1596, Loss Train: 5.7661, Loss Test: 5.6508, LR: 0.000100\n",
      "Step 1597, Loss Train: 5.7101, Loss Test: 5.6508, LR: 0.000100\n",
      "Step 1598, Loss Train: 5.7603, Loss Test: 5.6508, LR: 0.000100\n",
      "Step 1599, Loss Train: 5.7778, Loss Test: 5.6508, LR: 0.000100\n",
      "Step 1600, Loss Train: 5.7707, Loss Test: 5.6508, LR: 0.000100\n",
      "Step 1601, Loss Train: 5.7842, Loss Test: 5.6398, LR: 0.000100\n",
      "Step 1602, Loss Train: 5.7727, Loss Test: 5.6398, LR: 0.000100\n",
      "Step 1603, Loss Train: 5.7497, Loss Test: 5.6398, LR: 0.000100\n",
      "Step 1604, Loss Train: 5.7213, Loss Test: 5.6398, LR: 0.000100\n",
      "Step 1605, Loss Train: 5.7767, Loss Test: 5.6398, LR: 0.000100\n",
      "Step 1606, Loss Train: 5.7446, Loss Test: 5.6398, LR: 0.000100\n",
      "Step 1607, Loss Train: 5.7478, Loss Test: 5.6398, LR: 0.000100\n",
      "Step 1608, Loss Train: 5.7578, Loss Test: 5.6398, LR: 0.000100\n",
      "Step 1609, Loss Train: 5.7272, Loss Test: 5.6398, LR: 0.000100\n",
      "Step 1610, Loss Train: 5.7396, Loss Test: 5.6398, LR: 0.000100\n",
      "Step 1611, Loss Train: 5.7719, Loss Test: 5.6398, LR: 0.000100\n",
      "Step 1612, Loss Train: 5.7789, Loss Test: 5.6398, LR: 0.000100\n",
      "Step 1613, Loss Train: 5.7324, Loss Test: 5.6398, LR: 0.000100\n",
      "Step 1614, Loss Train: 5.7586, Loss Test: 5.6398, LR: 0.000100\n",
      "Step 1615, Loss Train: 5.7153, Loss Test: 5.6398, LR: 0.000100\n",
      "Step 1616, Loss Train: 5.7125, Loss Test: 5.6398, LR: 0.000100\n",
      "Step 1617, Loss Train: 5.7063, Loss Test: 5.6398, LR: 0.000100\n",
      "Step 1618, Loss Train: 5.7423, Loss Test: 5.6398, LR: 0.000100\n",
      "Step 1619, Loss Train: 5.7871, Loss Test: 5.6398, LR: 0.000100\n",
      "Step 1620, Loss Train: 5.7588, Loss Test: 5.6398, LR: 0.000100\n",
      "Step 1621, Loss Train: 5.7491, Loss Test: 5.6398, LR: 0.000100\n",
      "Step 1622, Loss Train: 5.6835, Loss Test: 5.6398, LR: 0.000100\n",
      "Step 1623, Loss Train: 5.7337, Loss Test: 5.6398, LR: 0.000100\n",
      "Step 1624, Loss Train: 5.6968, Loss Test: 5.6398, LR: 0.000100\n",
      "Step 1625, Loss Train: 5.7136, Loss Test: 5.6398, LR: 0.000100\n",
      "Step 1626, Loss Train: 5.7251, Loss Test: 5.6540, LR: 0.000100\n",
      "Step 1627, Loss Train: 5.6732, Loss Test: 5.6540, LR: 0.000100\n",
      "Step 1628, Loss Train: 5.7355, Loss Test: 5.6540, LR: 0.000100\n",
      "Step 1629, Loss Train: 5.6783, Loss Test: 5.6540, LR: 0.000100\n",
      "Step 1630, Loss Train: 5.7515, Loss Test: 5.6540, LR: 0.000100\n",
      "Step 1631, Loss Train: 5.6588, Loss Test: 5.6540, LR: 0.000100\n",
      "Step 1632, Loss Train: 5.7340, Loss Test: 5.6540, LR: 0.000100\n",
      "Step 1633, Loss Train: 5.7830, Loss Test: 5.6540, LR: 0.000100\n",
      "Step 1634, Loss Train: 5.8020, Loss Test: 5.6540, LR: 0.000100\n",
      "Step 1635, Loss Train: 5.6799, Loss Test: 5.6540, LR: 0.000100\n",
      "Step 1636, Loss Train: 5.7486, Loss Test: 5.6540, LR: 0.000100\n",
      "Step 1637, Loss Train: 5.6862, Loss Test: 5.6540, LR: 0.000100\n",
      "Step 1638, Loss Train: 5.6621, Loss Test: 5.6540, LR: 0.000100\n",
      "Step 1639, Loss Train: 5.6594, Loss Test: 5.6540, LR: 0.000100\n",
      "Step 1640, Loss Train: 5.6828, Loss Test: 5.6540, LR: 0.000100\n",
      "Step 1641, Loss Train: 5.7054, Loss Test: 5.6540, LR: 0.000100\n",
      "Step 1642, Loss Train: 5.6832, Loss Test: 5.6540, LR: 0.000100\n",
      "Step 1643, Loss Train: 5.6910, Loss Test: 5.6540, LR: 0.000100\n",
      "Step 1644, Loss Train: 5.7118, Loss Test: 5.6540, LR: 0.000100\n",
      "Step 1645, Loss Train: 5.7189, Loss Test: 5.6540, LR: 0.000100\n",
      "Step 1646, Loss Train: 5.7388, Loss Test: 5.6540, LR: 0.000100\n",
      "Step 1647, Loss Train: 5.6975, Loss Test: 5.6540, LR: 0.000100\n",
      "Step 1648, Loss Train: 5.6856, Loss Test: 5.6540, LR: 0.000100\n",
      "Step 1649, Loss Train: 5.6857, Loss Test: 5.6540, LR: 0.000100\n",
      "Step 1650, Loss Train: 5.7172, Loss Test: 5.6540, LR: 0.000100\n",
      "Step 1651, Loss Train: 5.7121, Loss Test: 5.5510, LR: 0.000100\n",
      "Step 1652, Loss Train: 5.6981, Loss Test: 5.5510, LR: 0.000100\n",
      "Step 1653, Loss Train: 5.7603, Loss Test: 5.5510, LR: 0.000100\n",
      "Step 1654, Loss Train: 5.6922, Loss Test: 5.5510, LR: 0.000100\n",
      "Step 1655, Loss Train: 5.7842, Loss Test: 5.5510, LR: 0.000100\n",
      "Step 1656, Loss Train: 5.7269, Loss Test: 5.5510, LR: 0.000100\n",
      "Step 1657, Loss Train: 5.7048, Loss Test: 5.5510, LR: 0.000100\n",
      "Step 1658, Loss Train: 5.7459, Loss Test: 5.5510, LR: 0.000100\n",
      "Step 1659, Loss Train: 5.7090, Loss Test: 5.5510, LR: 0.000100\n",
      "Step 1660, Loss Train: 5.7048, Loss Test: 5.5510, LR: 0.000100\n",
      "Step 1661, Loss Train: 5.6483, Loss Test: 5.5510, LR: 0.000100\n",
      "Step 1662, Loss Train: 5.6749, Loss Test: 5.5510, LR: 0.000100\n",
      "Step 1663, Loss Train: 5.6484, Loss Test: 5.5510, LR: 0.000100\n",
      "Step 1664, Loss Train: 5.6919, Loss Test: 5.5510, LR: 0.000100\n",
      "Step 1665, Loss Train: 5.7057, Loss Test: 5.5510, LR: 0.000100\n",
      "Step 1666, Loss Train: 5.6917, Loss Test: 5.5510, LR: 0.000100\n",
      "Step 1667, Loss Train: 5.6856, Loss Test: 5.5510, LR: 0.000100\n",
      "Step 1668, Loss Train: 5.7154, Loss Test: 5.5510, LR: 0.000100\n",
      "Step 1669, Loss Train: 5.7169, Loss Test: 5.5510, LR: 0.000100\n",
      "Step 1670, Loss Train: 5.7064, Loss Test: 5.5510, LR: 0.000100\n",
      "Step 1671, Loss Train: 5.7423, Loss Test: 5.5510, LR: 0.000100\n",
      "Step 1672, Loss Train: 5.7728, Loss Test: 5.5510, LR: 0.000100\n",
      "Step 1673, Loss Train: 5.6449, Loss Test: 5.5510, LR: 0.000100\n",
      "Step 1674, Loss Train: 5.6874, Loss Test: 5.5510, LR: 0.000100\n",
      "Step 1675, Loss Train: 5.7015, Loss Test: 5.5510, LR: 0.000100\n",
      "Step 1676, Loss Train: 5.6999, Loss Test: 5.5916, LR: 0.000100\n",
      "Step 1677, Loss Train: 5.6493, Loss Test: 5.5916, LR: 0.000100\n",
      "Step 1678, Loss Train: 5.7893, Loss Test: 5.5916, LR: 0.000100\n",
      "Step 1679, Loss Train: 5.7235, Loss Test: 5.5916, LR: 0.000100\n",
      "Step 1680, Loss Train: 5.7071, Loss Test: 5.5916, LR: 0.000100\n",
      "Step 1681, Loss Train: 5.6775, Loss Test: 5.5916, LR: 0.000100\n",
      "Step 1682, Loss Train: 5.7019, Loss Test: 5.5916, LR: 0.000100\n",
      "Step 1683, Loss Train: 5.7582, Loss Test: 5.5916, LR: 0.000100\n",
      "Step 1684, Loss Train: 5.6919, Loss Test: 5.5916, LR: 0.000100\n",
      "Step 1685, Loss Train: 5.6801, Loss Test: 5.5916, LR: 0.000100\n",
      "Step 1686, Loss Train: 5.7492, Loss Test: 5.5916, LR: 0.000100\n",
      "Step 1687, Loss Train: 5.6877, Loss Test: 5.5916, LR: 0.000100\n",
      "Step 1688, Loss Train: 5.7642, Loss Test: 5.5916, LR: 0.000100\n",
      "Step 1689, Loss Train: 5.7096, Loss Test: 5.5916, LR: 0.000100\n",
      "Step 1690, Loss Train: 5.7561, Loss Test: 5.5916, LR: 0.000100\n",
      "Step 1691, Loss Train: 5.7344, Loss Test: 5.5916, LR: 0.000100\n",
      "Step 1692, Loss Train: 5.7045, Loss Test: 5.5916, LR: 0.000100\n",
      "Step 1693, Loss Train: 5.7115, Loss Test: 5.5916, LR: 0.000100\n",
      "Step 1694, Loss Train: 5.7015, Loss Test: 5.5916, LR: 0.000100\n",
      "Step 1695, Loss Train: 5.6939, Loss Test: 5.5916, LR: 0.000100\n",
      "Step 1696, Loss Train: 5.6517, Loss Test: 5.5916, LR: 0.000100\n",
      "Step 1697, Loss Train: 5.7043, Loss Test: 5.5916, LR: 0.000100\n",
      "Step 1698, Loss Train: 5.6906, Loss Test: 5.5916, LR: 0.000100\n",
      "Step 1699, Loss Train: 5.6523, Loss Test: 5.5916, LR: 0.000100\n",
      "Step 1700, Loss Train: 5.6959, Loss Test: 5.5916, LR: 0.000100\n",
      "Step 1701, Loss Train: 5.7230, Loss Test: 5.6045, LR: 0.000100\n",
      "Step 1702, Loss Train: 5.6324, Loss Test: 5.6045, LR: 0.000100\n",
      "Step 1703, Loss Train: 5.6722, Loss Test: 5.6045, LR: 0.000100\n",
      "Step 1704, Loss Train: 5.6720, Loss Test: 5.6045, LR: 0.000100\n",
      "Step 1705, Loss Train: 5.6832, Loss Test: 5.6045, LR: 0.000100\n",
      "Step 1706, Loss Train: 5.6202, Loss Test: 5.6045, LR: 0.000100\n",
      "Step 1707, Loss Train: 5.6297, Loss Test: 5.6045, LR: 0.000100\n",
      "Step 1708, Loss Train: 5.7272, Loss Test: 5.6045, LR: 0.000100\n",
      "Step 1709, Loss Train: 5.6883, Loss Test: 5.6045, LR: 0.000100\n",
      "Step 1710, Loss Train: 5.6564, Loss Test: 5.6045, LR: 0.000100\n",
      "Step 1711, Loss Train: 5.6524, Loss Test: 5.6045, LR: 0.000100\n",
      "Step 1712, Loss Train: 5.6516, Loss Test: 5.6045, LR: 0.000100\n",
      "Step 1713, Loss Train: 5.6269, Loss Test: 5.6045, LR: 0.000100\n",
      "Step 1714, Loss Train: 5.6537, Loss Test: 5.6045, LR: 0.000100\n",
      "Step 1715, Loss Train: 5.6885, Loss Test: 5.6045, LR: 0.000100\n",
      "Step 1716, Loss Train: 5.6965, Loss Test: 5.6045, LR: 0.000100\n",
      "Step 1717, Loss Train: 5.6781, Loss Test: 5.6045, LR: 0.000100\n",
      "Step 1718, Loss Train: 5.6995, Loss Test: 5.6045, LR: 0.000100\n",
      "Step 1719, Loss Train: 5.6920, Loss Test: 5.6045, LR: 0.000100\n",
      "Step 1720, Loss Train: 5.7101, Loss Test: 5.6045, LR: 0.000100\n",
      "Step 1721, Loss Train: 5.6462, Loss Test: 5.6045, LR: 0.000100\n",
      "Step 1722, Loss Train: 5.6834, Loss Test: 5.6045, LR: 0.000100\n",
      "Step 1723, Loss Train: 5.6723, Loss Test: 5.6045, LR: 0.000100\n",
      "Step 1724, Loss Train: 5.6187, Loss Test: 5.6045, LR: 0.000100\n",
      "Step 1725, Loss Train: 5.6172, Loss Test: 5.6045, LR: 0.000100\n",
      "Step 1726, Loss Train: 5.6529, Loss Test: 5.5989, LR: 0.000100\n",
      "Step 1727, Loss Train: 5.6495, Loss Test: 5.5989, LR: 0.000100\n",
      "Step 1728, Loss Train: 5.6945, Loss Test: 5.5989, LR: 0.000100\n",
      "Step 1729, Loss Train: 5.6670, Loss Test: 5.5989, LR: 0.000100\n",
      "Step 1730, Loss Train: 5.6190, Loss Test: 5.5989, LR: 0.000100\n",
      "Step 1731, Loss Train: 5.6614, Loss Test: 5.5989, LR: 0.000100\n",
      "Step 1732, Loss Train: 5.6663, Loss Test: 5.5989, LR: 0.000100\n",
      "Step 1733, Loss Train: 5.7079, Loss Test: 5.5989, LR: 0.000100\n",
      "Step 1734, Loss Train: 5.7489, Loss Test: 5.5989, LR: 0.000100\n",
      "Step 1735, Loss Train: 5.6851, Loss Test: 5.5989, LR: 0.000100\n",
      "Step 1736, Loss Train: 5.6373, Loss Test: 5.5989, LR: 0.000100\n",
      "Step 1737, Loss Train: 5.6775, Loss Test: 5.5989, LR: 0.000100\n",
      "Step 1738, Loss Train: 5.6511, Loss Test: 5.5989, LR: 0.000100\n",
      "Step 1739, Loss Train: 5.6770, Loss Test: 5.5989, LR: 0.000100\n",
      "Step 1740, Loss Train: 5.6472, Loss Test: 5.5989, LR: 0.000100\n",
      "Step 1741, Loss Train: 5.6939, Loss Test: 5.5989, LR: 0.000100\n",
      "Step 1742, Loss Train: 5.6480, Loss Test: 5.5989, LR: 0.000100\n",
      "Step 1743, Loss Train: 5.5631, Loss Test: 5.5989, LR: 0.000100\n",
      "Step 1744, Loss Train: 5.5654, Loss Test: 5.5989, LR: 0.000100\n",
      "Step 1745, Loss Train: 5.6977, Loss Test: 5.5989, LR: 0.000100\n",
      "Step 1746, Loss Train: 5.6197, Loss Test: 5.5989, LR: 0.000100\n",
      "Step 1747, Loss Train: 5.6993, Loss Test: 5.5989, LR: 0.000100\n",
      "Step 1748, Loss Train: 5.6498, Loss Test: 5.5989, LR: 0.000100\n",
      "Step 1749, Loss Train: 5.6429, Loss Test: 5.5989, LR: 0.000100\n",
      "Step 1750, Loss Train: 5.6547, Loss Test: 5.5989, LR: 0.000100\n",
      "Step 1751, Loss Train: 5.6149, Loss Test: 5.5424, LR: 0.000100\n",
      "Step 1752, Loss Train: 5.6625, Loss Test: 5.5424, LR: 0.000100\n",
      "Step 1753, Loss Train: 5.5872, Loss Test: 5.5424, LR: 0.000100\n",
      "Step 1754, Loss Train: 5.6539, Loss Test: 5.5424, LR: 0.000100\n",
      "Step 1755, Loss Train: 5.6546, Loss Test: 5.5424, LR: 0.000100\n",
      "Step 1756, Loss Train: 5.6273, Loss Test: 5.5424, LR: 0.000100\n",
      "Step 1757, Loss Train: 5.6277, Loss Test: 5.5424, LR: 0.000100\n",
      "Step 1758, Loss Train: 5.6759, Loss Test: 5.5424, LR: 0.000100\n",
      "Step 1759, Loss Train: 5.5847, Loss Test: 5.5424, LR: 0.000100\n",
      "Step 1760, Loss Train: 5.6631, Loss Test: 5.5424, LR: 0.000100\n",
      "Step 1761, Loss Train: 5.6005, Loss Test: 5.5424, LR: 0.000100\n",
      "Step 1762, Loss Train: 5.6197, Loss Test: 5.5424, LR: 0.000100\n",
      "Step 1763, Loss Train: 5.6631, Loss Test: 5.5424, LR: 0.000100\n",
      "Step 1764, Loss Train: 5.6435, Loss Test: 5.5424, LR: 0.000100\n",
      "Step 1765, Loss Train: 5.6084, Loss Test: 5.5424, LR: 0.000100\n",
      "Step 1766, Loss Train: 5.6664, Loss Test: 5.5424, LR: 0.000100\n",
      "Step 1767, Loss Train: 5.6535, Loss Test: 5.5424, LR: 0.000100\n",
      "Step 1768, Loss Train: 5.6838, Loss Test: 5.5424, LR: 0.000100\n",
      "Step 1769, Loss Train: 5.6262, Loss Test: 5.5424, LR: 0.000100\n",
      "Step 1770, Loss Train: 5.6370, Loss Test: 5.5424, LR: 0.000100\n",
      "Step 1771, Loss Train: 5.6289, Loss Test: 5.5424, LR: 0.000100\n",
      "Step 1772, Loss Train: 5.6679, Loss Test: 5.5424, LR: 0.000100\n",
      "Step 1773, Loss Train: 5.6543, Loss Test: 5.5424, LR: 0.000100\n",
      "Step 1774, Loss Train: 5.6965, Loss Test: 5.5424, LR: 0.000100\n",
      "Step 1775, Loss Train: 5.5875, Loss Test: 5.5424, LR: 0.000100\n",
      "Step 1776, Loss Train: 5.5930, Loss Test: 5.5432, LR: 0.000100\n",
      "Step 1777, Loss Train: 5.6500, Loss Test: 5.5432, LR: 0.000100\n",
      "Step 1778, Loss Train: 5.6742, Loss Test: 5.5432, LR: 0.000100\n",
      "Step 1779, Loss Train: 5.5577, Loss Test: 5.5432, LR: 0.000100\n",
      "Step 1780, Loss Train: 5.5989, Loss Test: 5.5432, LR: 0.000100\n",
      "Step 1781, Loss Train: 5.6524, Loss Test: 5.5432, LR: 0.000100\n",
      "Step 1782, Loss Train: 5.6479, Loss Test: 5.5432, LR: 0.000100\n",
      "Step 1783, Loss Train: 5.6024, Loss Test: 5.5432, LR: 0.000100\n",
      "Step 1784, Loss Train: 5.6157, Loss Test: 5.5432, LR: 0.000100\n",
      "Step 1785, Loss Train: 5.7009, Loss Test: 5.5432, LR: 0.000100\n",
      "Step 1786, Loss Train: 5.5606, Loss Test: 5.5432, LR: 0.000100\n",
      "Step 1787, Loss Train: 5.6501, Loss Test: 5.5432, LR: 0.000100\n",
      "Step 1788, Loss Train: 5.5848, Loss Test: 5.5432, LR: 0.000100\n",
      "Step 1789, Loss Train: 5.6849, Loss Test: 5.5432, LR: 0.000100\n",
      "Step 1790, Loss Train: 5.6927, Loss Test: 5.5432, LR: 0.000100\n",
      "Step 1791, Loss Train: 5.6046, Loss Test: 5.5432, LR: 0.000100\n",
      "Step 1792, Loss Train: 5.6173, Loss Test: 5.5432, LR: 0.000100\n",
      "Step 1793, Loss Train: 5.5969, Loss Test: 5.5432, LR: 0.000100\n",
      "Step 1794, Loss Train: 5.6404, Loss Test: 5.5432, LR: 0.000100\n",
      "Step 1795, Loss Train: 5.6831, Loss Test: 5.5432, LR: 0.000100\n",
      "Step 1796, Loss Train: 5.5574, Loss Test: 5.5432, LR: 0.000100\n",
      "Step 1797, Loss Train: 5.5884, Loss Test: 5.5432, LR: 0.000100\n",
      "Step 1798, Loss Train: 5.5833, Loss Test: 5.5432, LR: 0.000100\n",
      "Step 1799, Loss Train: 5.6016, Loss Test: 5.5432, LR: 0.000100\n",
      "Step 1800, Loss Train: 5.5888, Loss Test: 5.5432, LR: 0.000100\n",
      "Step 1801, Loss Train: 5.6297, Loss Test: 5.5031, LR: 0.000100\n",
      "Step 1802, Loss Train: 5.5701, Loss Test: 5.5031, LR: 0.000100\n",
      "Step 1803, Loss Train: 5.6255, Loss Test: 5.5031, LR: 0.000100\n",
      "Step 1804, Loss Train: 5.6388, Loss Test: 5.5031, LR: 0.000100\n",
      "Step 1805, Loss Train: 5.6506, Loss Test: 5.5031, LR: 0.000100\n",
      "Step 1806, Loss Train: 5.6136, Loss Test: 5.5031, LR: 0.000100\n",
      "Step 1807, Loss Train: 5.6291, Loss Test: 5.5031, LR: 0.000100\n",
      "Step 1808, Loss Train: 5.6470, Loss Test: 5.5031, LR: 0.000100\n",
      "Step 1809, Loss Train: 5.5793, Loss Test: 5.5031, LR: 0.000100\n",
      "Step 1810, Loss Train: 5.6160, Loss Test: 5.5031, LR: 0.000100\n",
      "Step 1811, Loss Train: 5.6233, Loss Test: 5.5031, LR: 0.000100\n",
      "Step 1812, Loss Train: 5.6205, Loss Test: 5.5031, LR: 0.000100\n",
      "Step 1813, Loss Train: 5.5839, Loss Test: 5.5031, LR: 0.000100\n",
      "Step 1814, Loss Train: 5.6906, Loss Test: 5.5031, LR: 0.000100\n",
      "Step 1815, Loss Train: 5.6140, Loss Test: 5.5031, LR: 0.000100\n",
      "Step 1816, Loss Train: 5.5789, Loss Test: 5.5031, LR: 0.000100\n",
      "Step 1817, Loss Train: 5.5972, Loss Test: 5.5031, LR: 0.000100\n",
      "Step 1818, Loss Train: 5.6662, Loss Test: 5.5031, LR: 0.000100\n",
      "Step 1819, Loss Train: 5.5926, Loss Test: 5.5031, LR: 0.000100\n",
      "Step 1820, Loss Train: 5.5724, Loss Test: 5.5031, LR: 0.000100\n",
      "Step 1821, Loss Train: 5.6399, Loss Test: 5.5031, LR: 0.000100\n",
      "Step 1822, Loss Train: 5.6671, Loss Test: 5.5031, LR: 0.000100\n",
      "Step 1823, Loss Train: 5.5933, Loss Test: 5.5031, LR: 0.000100\n",
      "Step 1824, Loss Train: 5.6314, Loss Test: 5.5031, LR: 0.000100\n",
      "Step 1825, Loss Train: 5.6512, Loss Test: 5.5031, LR: 0.000100\n",
      "Step 1826, Loss Train: 5.6440, Loss Test: 5.5299, LR: 0.000100\n",
      "Step 1827, Loss Train: 5.5906, Loss Test: 5.5299, LR: 0.000100\n",
      "Step 1828, Loss Train: 5.6644, Loss Test: 5.5299, LR: 0.000100\n",
      "Step 1829, Loss Train: 5.5687, Loss Test: 5.5299, LR: 0.000100\n",
      "Step 1830, Loss Train: 5.6005, Loss Test: 5.5299, LR: 0.000100\n",
      "Step 1831, Loss Train: 5.5868, Loss Test: 5.5299, LR: 0.000100\n",
      "Step 1832, Loss Train: 5.5545, Loss Test: 5.5299, LR: 0.000100\n",
      "Step 1833, Loss Train: 5.6624, Loss Test: 5.5299, LR: 0.000100\n",
      "Step 1834, Loss Train: 5.6164, Loss Test: 5.5299, LR: 0.000100\n",
      "Step 1835, Loss Train: 5.6512, Loss Test: 5.5299, LR: 0.000100\n",
      "Step 1836, Loss Train: 5.6220, Loss Test: 5.5299, LR: 0.000100\n",
      "Step 1837, Loss Train: 5.6246, Loss Test: 5.5299, LR: 0.000100\n",
      "Step 1838, Loss Train: 5.5741, Loss Test: 5.5299, LR: 0.000100\n",
      "Step 1839, Loss Train: 5.5876, Loss Test: 5.5299, LR: 0.000100\n",
      "Step 1840, Loss Train: 5.5828, Loss Test: 5.5299, LR: 0.000100\n",
      "Step 1841, Loss Train: 5.6384, Loss Test: 5.5299, LR: 0.000100\n",
      "Step 1842, Loss Train: 5.6242, Loss Test: 5.5299, LR: 0.000100\n",
      "Step 1843, Loss Train: 5.5714, Loss Test: 5.5299, LR: 0.000100\n",
      "Step 1844, Loss Train: 5.6348, Loss Test: 5.5299, LR: 0.000100\n",
      "Step 1845, Loss Train: 5.6258, Loss Test: 5.5299, LR: 0.000100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43miter_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miter_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 9\u001b[0m, in \u001b[0;36mtrain_loop\u001b[1;34m(iter_train, iter_test, steps_per_epoch, epochs)\u001b[0m\n\u001b[0;32m      6\u001b[0m batch_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iter_train)\n\u001b[0;32m      7\u001b[0m batch_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iter_test)\n\u001b[1;32m----> 9\u001b[0m loss_train_temp \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m25\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     12\u001b[0m     loss_test_temp \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(batch_test)\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\krist\\anaconda3\\envs\\keras-gpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loop(iter_train, iter_test, steps_per_epoch, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0eeef46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x219262cf0d0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses_test[200:], label=\"Test Loss\")\n",
    "plt.plot(losses_train[200:], label=\"Train Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e216ce01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7ce2d9702e142c2824a3d5404e8ac52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[16070]], shape=(1, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\n",
    "text = text.lower()\n",
    "SOS = tf.convert_to_tensor([[tokenizer.token_to_idx[\"<s>\"]]])\n",
    "tokens = tf.cast(tokenizer.encode(text), tf.int32)\n",
    "tokens = tf.concat([SOS, tokens], axis=1)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74a017b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae15f3ea93b429f8a4f0cf5f5e0f94e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='', disabled=True, layout=Layout(height='20em', width='80ch'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import textwrap\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "tf.random.set_seed(43)\n",
    "wrapper = textwrap.TextWrapper(width=80)\n",
    "\n",
    "# create a read-only text area\n",
    "ta = widgets.Textarea(\n",
    "    value=\"\",\n",
    "    layout=widgets.Layout(width='80ch', height='20em'),\n",
    "    disabled=True\n",
    ")\n",
    "display(ta)\n",
    "\n",
    "\n",
    "T = 1\n",
    "k = 50\n",
    "\n",
    "for i in range(1024):\n",
    "    logits = model.call(tokens)[0, -1:]\n",
    "    topk_vals, _      = tf.math.top_k(logits, k=k)\n",
    "    kth_value         = topk_vals[:,-1]\n",
    "\n",
    "    logits = tf.where(logits >= kth_value, logits, tf.constant(-np.inf, logits.dtype))\n",
    "\n",
    "    idx = tf.cast(\n",
    "        tf.random.categorical(logits / T, num_samples=1),\n",
    "        tf.int32\n",
    "    ) \n",
    "    tokens = tf.concat([tokens, idx], axis=1)\n",
    "\n",
    "    text_pred = (\n",
    "        tokenizer\n",
    "        .decode(tokens)\n",
    "        .numpy()[0]\n",
    "        .decode('utf-8')\n",
    "        .replace(\"\\n\", \" \")\n",
    "    )\n",
    "    ta.value = wrapper.fill(text_pred)  # this updates in-place\n",
    "\n",
    "    if idx[0, 0] == tokenizer.token_to_idx[\"</s>\"]:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a5f20b",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5d873d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def cosine_similarity(embed_a, embed_b):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two vectors.\n",
    "    \"\"\"\n",
    "    embed_b_T = tf.transpose(embed_b)\n",
    "    dot_product = embed_a@embed_b_T\n",
    "    \n",
    "    norm_a = tf.linalg.norm(embed_a, axis=1, keepdims=True)\n",
    "    norm_b = tf.linalg.norm(embed_b_T, axis=0, keepdims=True)\n",
    "\n",
    "    return dot_product / (norm_a * norm_b)\n",
    "\n",
    "\n",
    "def cluster(X, n_clusters, normalize=True):\n",
    "    if normalize:\n",
    "        X = X/np.linalg.norm(X, axis=1, keepdims=True)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(X)\n",
    "    inertia = kmeans.inertia_\n",
    "    labels = kmeans.labels_\n",
    "    clusters = kmeans.cluster_centers_\n",
    "\n",
    "    return inertia, labels, clusters\n",
    "\n",
    "\n",
    "class EmbeddingClustering:\n",
    "    def __init__(self, tokenizer, n_clusters=10):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.n_clusters = n_clusters\n",
    "\n",
    "    def fit(self, word_embed, normalize=True):\n",
    "        inertia, labels, clusters = cluster(word_embed, self.n_clusters, normalize)\n",
    "        self.word_embed = word_embed\n",
    "        self.inertia = inertia\n",
    "        self.labels = labels\n",
    "        self.clusters = tf.convert_to_tensor(clusters, dtype=tf.float32)\n",
    "\n",
    "        cos_sim = cosine_similarity(self.clusters, word_embed, normalize)\n",
    "        self.idx_list =  tf.argsort(cos_sim, axis=-1, direction='DESCENDING', stable=False, name=None)\n",
    "\n",
    "    def print_clusters(self, n_words=10):\n",
    "        for idx in self.idx_list:\n",
    "            for i in idx[:n_words]:\n",
    "                word = self.tokenizer.decode(tf.expand_dims(tf.cast(i, tf.int32), axis=0))\n",
    "                word = word.numpy().decode('utf-8')\n",
    "                print(word)\n",
    "            print(\"\\n\")\n",
    "\n",
    "\n",
    "def cosine_similarity(embed_a, embed_b, normalize=True):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two vectors.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        embed_a = tf.nn.l2_normalize(embed_a, axis=1)\n",
    "        embed_b = tf.nn.l2_normalize(embed_b, axis=1)\n",
    "    dot_product = embed_a@tf.transpose(embed_b)\n",
    "\n",
    "\n",
    "    return dot_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f2f72aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steven\n",
      "robert\n",
      "john\n",
      "jason\n",
      "kevin\n",
      "stephen\n",
      "lionel\n",
      "brian\n",
      "david\n",
      "sergio\n",
      "\n",
      "\n",
      "sadness\n",
      "belief\n",
      "excitement\n",
      "attitude\n",
      "frustration\n",
      "sympathy\n",
      "reminder\n",
      "skepticism\n",
      "outrage\n",
      "impression\n",
      "\n",
      "\n",
      "croatia\n",
      "sweden\n",
      "denmark\n",
      "belgium\n",
      "portugal\n",
      "hungary\n",
      "romania\n",
      "switzerland\n",
      "austria\n",
      "kazakhstan\n",
      "\n",
      "\n",
      "treats\n",
      "covers\n",
      "moves\n",
      "hits\n",
      "fights\n",
      "handles\n",
      "feeds\n",
      "addresses\n",
      "encourages\n",
      "aims\n",
      "\n",
      "\n",
      "eclip\n",
      "plumme\n",
      "accum\n",
      "incre\n",
      "collap\n",
      "stif\n",
      "destab\n",
      "redu\n",
      "glimp\n",
      "unve\n",
      "\n",
      "\n",
      "bought\n",
      "produced\n",
      "enjoyed\n",
      "invented\n",
      "attended\n",
      "received\n",
      "embraced\n",
      "commissioned\n",
      "earned\n",
      "recruited\n",
      "\n",
      "\n",
      "urging\n",
      "asking\n",
      "expressing\n",
      "helping\n",
      "describing\n",
      "inviting\n",
      "letting\n",
      "prompting\n",
      "allowing\n",
      "highlighting\n",
      "\n",
      "\n",
      "illum\n",
      "acknowled\n",
      "scrut\n",
      "discrim\n",
      "fasc\n",
      "detr\n",
      "refle\n",
      "inst\n",
      "stair\n",
      "reper\n",
      "\n",
      "\n",
      "cities\n",
      "colleges\n",
      "sectors\n",
      "departments\n",
      "regions\n",
      "companies\n",
      "organizations\n",
      "agencies\n",
      "institutions\n",
      "universities\n",
      "\n",
      "\n",
      "taking\n",
      "turning\n",
      "getting\n",
      "pulling\n",
      "putting\n",
      "stepping\n",
      "moving\n",
      "returning\n",
      "trying\n",
      "sending\n",
      "\n",
      "\n",
      "ob\n",
      "con\n",
      "sub\n",
      "recon\n",
      "dis\n",
      "trans\n",
      "cir\n",
      "rein\n",
      "dist\n",
      "circum\n",
      "\n",
      "\n",
      "territor\n",
      "secutive\n",
      "portra\n",
      "attem\n",
      "mingham\n",
      "occas\n",
      "consid\n",
      "enthus\n",
      "fahren\n",
      "theast\n",
      "\n",
      "\n",
      "certainly\n",
      "hardly\n",
      "never\n",
      "rarely\n",
      "actually\n",
      "probably\n",
      "perhaps\n",
      "usually\n",
      "definitely\n",
      "obviously\n",
      "\n",
      "\n",
      "republicans\n",
      "democrats\n",
      "republican\n",
      "senator\n",
      "gop\n",
      "conservatives\n",
      "senators\n",
      "independents\n",
      "senate\n",
      "democrat\n",
      "\n",
      "\n",
      "secutive\n",
      "mingham\n",
      "moil\n",
      "enthusia\n",
      "satell\n",
      "delaw\n",
      "onsored\n",
      "nandez\n",
      "leban\n",
      "attem\n",
      "\n",
      "\n",
      "decrease\n",
      "proximity\n",
      "severity\n",
      "effectiveness\n",
      "abundance\n",
      "reduction\n",
      "contribution\n",
      "effic\n",
      "volume\n",
      "exposure\n",
      "\n",
      "\n",
      "big\n",
      "high\n",
      "small\n",
      "long\n",
      "hot\n",
      "deep\n",
      "hard\n",
      "low\n",
      "great\n",
      "large\n",
      "\n",
      "\n",
      "someone\n",
      "a\n",
      "their\n",
      "anyone\n",
      "everyone\n",
      "your\n",
      "it\n",
      "an\n",
      "our\n",
      "any\n",
      "\n",
      "\n",
      "attem\n",
      "experi\n",
      "occas\n",
      "strugg\n",
      "avez\n",
      "cigare\n",
      "secutive\n",
      "abulary\n",
      "prede\n",
      "includ\n",
      "\n",
      "\n",
      "takes\n",
      "gets\n",
      "brings\n",
      "puts\n",
      "pulls\n",
      "sends\n",
      "spends\n",
      "goes\n",
      "becomes\n",
      "makes\n",
      "\n",
      "\n",
      "preventing\n",
      "reducing\n",
      "providing\n",
      "attracting\n",
      "forcing\n",
      "letting\n",
      "allowing\n",
      "causing\n",
      "gaining\n",
      "removing\n",
      "\n",
      "\n",
      "neuro\n",
      "mathem\n",
      "proto\n",
      "stair\n",
      "nutr\n",
      "incar\n",
      "scand\n",
      "lef\n",
      "reper\n",
      "sophist\n",
      "\n",
      "\n",
      "commitments\n",
      "revenues\n",
      "investments\n",
      "agreements\n",
      "contributions\n",
      "contracts\n",
      "initiatives\n",
      "proposals\n",
      "debts\n",
      "budgets\n",
      "\n",
      "\n",
      "felony\n",
      "convicted\n",
      "misdemeanor\n",
      "criminal\n",
      "indicted\n",
      "acquitted\n",
      "arrested\n",
      "jailed\n",
      "prosecuted\n",
      "aggravated\n",
      "\n",
      "\n",
      "kh\n",
      "naj\n",
      "zawah\n",
      "zah\n",
      "ouatt\n",
      "laim\n",
      "mah\n",
      "sark\n",
      "oth\n",
      "moroc\n",
      "\n",
      "\n",
      "colorful\n",
      "distinctive\n",
      "ancient\n",
      "gorgeous\n",
      "fictional\n",
      "stunning\n",
      "vibrant\n",
      "elegant\n",
      "iconic\n",
      "glamorous\n",
      "\n",
      "\n",
      "encourages\n",
      "provides\n",
      "gives\n",
      "requires\n",
      "allows\n",
      "reflects\n",
      "helps\n",
      "creates\n",
      "promotes\n",
      "relies\n",
      "\n",
      "\n",
      "helicopters\n",
      "planes\n",
      "trucks\n",
      "tunnels\n",
      "containers\n",
      "satellites\n",
      "missiles\n",
      "vessels\n",
      "boats\n",
      "vehicles\n",
      "\n",
      "\n",
      "take\n",
      "make\n",
      "go\n",
      "bring\n",
      "get\n",
      "do\n",
      "pull\n",
      "keep\n",
      "move\n",
      "come\n",
      "\n",
      "\n",
      "tens\n",
      "hundreds\n",
      "billions\n",
      "months\n",
      "weeks\n",
      "years\n",
      "kinds\n",
      "decades\n",
      "thousands\n",
      "shortly\n",
      "\n",
      "\n",
      "mu\n",
      "sha\n",
      "mar\n",
      "bu\n",
      "mc\n",
      "nu\n",
      "lo\n",
      "lu\n",
      "tus\n",
      "dar\n",
      "\n",
      "\n",
      "vettel\n",
      "barca\n",
      "rosberg\n",
      "psg\n",
      "federer\n",
      "juventus\n",
      "neymar\n",
      "balotelli\n",
      "guardiola\n",
      "mcilroy\n",
      "\n",
      "\n",
      "ansar\n",
      "abdel\n",
      "khalid\n",
      "ayatollah\n",
      "fallu\n",
      "saeed\n",
      "houthi\n",
      "aqap\n",
      "nusra\n",
      "umar\n",
      "\n",
      "\n",
      "ukraine's\n",
      "egypt's\n",
      "syria's\n",
      "mexico's\n",
      "russia's\n",
      "europe's\n",
      "spain's\n",
      "somalia's\n",
      "iraq's\n",
      "venezuela's\n",
      "\n",
      "\n",
      "refused\n",
      "apologized\n",
      "yelled\n",
      "reacted\n",
      "laughed\n",
      "shouted\n",
      "compelled\n",
      "screamed\n",
      "cried\n",
      "confessed\n",
      "\n",
      "\n",
      "ela\n",
      "lene\n",
      "aco\n",
      "ien\n",
      "ola\n",
      "iel\n",
      "ino\n",
      "iano\n",
      "ury\n",
      "ika\n",
      "\n",
      "\n",
      "says\n",
      "argues\n",
      "insisted\n",
      "cautions\n",
      "said\n",
      "warned\n",
      "explained\n",
      "explains\n",
      "warns\n",
      "argued\n",
      "\n",
      "\n",
      "jiang\n",
      "guang\n",
      "jin\n",
      "zhang\n",
      "zhou\n",
      "qing\n",
      "iao\n",
      "shang\n",
      "yang\n",
      "iang\n",
      "\n",
      "\n",
      "greater\n",
      "bigger\n",
      "nearly\n",
      "roughly\n",
      "fewer\n",
      "higher\n",
      "more\n",
      "larger\n",
      "almost\n",
      "lower\n",
      "\n",
      "\n",
      "unbelievable\n",
      "interesting\n",
      "amazing\n",
      "incredibly\n",
      "incredible\n",
      "unfortunate\n",
      "exciting\n",
      "horrible\n",
      "wonderful\n",
      "heartbreaking\n",
      "\n",
      "\n",
      "scram\n",
      "dwar\n",
      "chry\n",
      "stum\n",
      "resem\n",
      "scu\n",
      "pum\n",
      "sli\n",
      "crum\n",
      "plun\n",
      "\n",
      "\n",
      "until\n",
      "by\n",
      "during\n",
      "before\n",
      "from\n",
      "when\n",
      "despite\n",
      "since\n",
      "after\n",
      "between\n",
      "\n",
      "\n",
      "oking\n",
      "ucing\n",
      "icting\n",
      "inated\n",
      "sured\n",
      "enthusia\n",
      "oked\n",
      "experi\n",
      "iated\n",
      "ipped\n",
      "\n",
      "\n",
      "took\n",
      "went\n",
      "pulled\n",
      "jumped\n",
      "walked\n",
      "dragged\n",
      "moved\n",
      "turned\n",
      "drove\n",
      "brought\n",
      "\n",
      "\n",
      "northern\n",
      "southern\n",
      "british\n",
      "saudi\n",
      "european\n",
      "syrian\n",
      "turkish\n",
      "ukrainian\n",
      "italian\n",
      "sri\n",
      "\n",
      "\n",
      "allegations\n",
      "accusations\n",
      "investigations\n",
      "lawsuits\n",
      "charges\n",
      "revelations\n",
      "attorneys\n",
      "statements\n",
      "testimony\n",
      "documents\n",
      "\n",
      "\n",
      "novels\n",
      "musical\n",
      "literary\n",
      "genre\n",
      "albums\n",
      "cinema\n",
      "films\n",
      "comic\n",
      "comedy\n",
      "comics\n",
      "\n",
      "\n",
      "cautious\n",
      "thoughtful\n",
      "divisive\n",
      "passionate\n",
      "resilient\n",
      "talented\n",
      "charismatic\n",
      "outspoken\n",
      "generous\n",
      "meaningful\n",
      "\n",
      "\n",
      "hacker\n",
      "laptop\n",
      "keyboard\n",
      "browser\n",
      "commenter\n",
      "computer\n",
      "tablet\n",
      "iphone\n",
      "smartphone\n",
      "ipod\n",
      "\n",
      "\n",
      "neth\n",
      "annah\n",
      "arin\n",
      "anmen\n",
      "ismatic\n",
      "pene\n",
      "tov\n",
      "kvito\n",
      "ansas\n",
      "eous\n",
      "\n",
      "\n",
      "depri\n",
      "indul\n",
      "glimp\n",
      "remind\n",
      "reim\n",
      "rehear\n",
      "overwhel\n",
      "clut\n",
      "dele\n",
      "eclip\n",
      "\n",
      "\n",
      "popu\n",
      "nove\n",
      "yester\n",
      "proto\n",
      "twel\n",
      "supre\n",
      "engul\n",
      "discre\n",
      "hydro\n",
      "teles\n",
      "\n",
      "\n",
      "bachmann\n",
      "blagojevich\n",
      "huckabee\n",
      "limbaugh\n",
      "toobin\n",
      "sebelius\n",
      "schumer\n",
      "zelizer\n",
      "kagan\n",
      "feinstein\n",
      "\n",
      "\n",
      "indis\n",
      "disgr\n",
      "spear\n",
      "incar\n",
      "resur\n",
      "contr\n",
      "juris\n",
      "stub\n",
      "stair\n",
      "distr\n",
      "\n",
      "\n",
      "department\n",
      "ministry\n",
      "government\n",
      "state\n",
      "bureau\n",
      "center\n",
      "city\n",
      "university\n",
      "agency\n",
      "district\n",
      "\n",
      "\n",
      "beliefs\n",
      "ideals\n",
      "empathy\n",
      "attitudes\n",
      "oppression\n",
      "freedoms\n",
      "honesty\n",
      "stances\n",
      "injustice\n",
      "creativity\n",
      "\n",
      "\n",
      "sent\n",
      "brought\n",
      "issued\n",
      "caused\n",
      "taken\n",
      "carried\n",
      "ordered\n",
      "received\n",
      "prompted\n",
      "transported\n",
      "\n",
      "\n",
      "fbi's\n",
      "department's\n",
      "nation's\n",
      "hotel's\n",
      "couple's\n",
      "tour's\n",
      "church's\n",
      "club's\n",
      "military's\n",
      "school's\n",
      "\n",
      "\n",
      "containers\n",
      "bags\n",
      "bottles\n",
      "shades\n",
      "layers\n",
      "floors\n",
      "particles\n",
      "costumes\n",
      "hats\n",
      "iphones\n",
      "\n",
      "\n",
      "erad\n",
      "intox\n",
      "refle\n",
      "supp\n",
      "ubiqu\n",
      "facilit\n",
      "juris\n",
      "stair\n",
      "evalu\n",
      "irrit\n",
      "\n",
      "\n",
      "jennifer\n",
      "jessica\n",
      "stephanie\n",
      "michelle\n",
      "melissa\n",
      "laura\n",
      "nicole\n",
      "christina\n",
      "debbie\n",
      "tina\n",
      "\n",
      "\n",
      "legislative\n",
      "electoral\n",
      "political\n",
      "constitutional\n",
      "civil\n",
      "republican\n",
      "parliamentary\n",
      "supreme\n",
      "democratic\n",
      "governmental\n",
      "\n",
      "\n",
      "spacecraft\n",
      "dreamliner\n",
      "comet\n",
      "submarine\n",
      "tanker\n",
      "yacht\n",
      "volcanic\n",
      "airliner\n",
      "shuttle\n",
      "vessel\n",
      "\n",
      "\n",
      "think\n",
      "believe\n",
      "wondering\n",
      "wondered\n",
      "warn\n",
      "remind\n",
      "know\n",
      "realize\n",
      "tell\n",
      "worry\n",
      "\n",
      "\n",
      "kansas\n",
      "oklahoma\n",
      "maryland\n",
      "colorado\n",
      "wisconsin\n",
      "illinois\n",
      "indiana\n",
      "tennessee\n",
      "philadelphia\n",
      "pennsylvania\n",
      "\n",
      "\n",
      "finale\n",
      "semifinals\n",
      "ceremony\n",
      "quarterfinals\n",
      "tournament\n",
      "festiv\n",
      "semifinal\n",
      "championship\n",
      "quarterfinal\n",
      "fixture\n",
      "\n",
      "\n",
      "trunk\n",
      "bottle\n",
      "layer\n",
      "waist\n",
      "bedroom\n",
      "roof\n",
      "closet\n",
      "header\n",
      "ledge\n",
      "door\n",
      "\n",
      "\n",
      "fairly\n",
      "widely\n",
      "relatively\n",
      "wildly\n",
      "highly\n",
      "rarely\n",
      "thoroughly\n",
      "effectively\n",
      "routinely\n",
      "unfairly\n",
      "\n",
      "\n",
      "mother\n",
      "daughter\n",
      "grandmother\n",
      "father\n",
      "wife\n",
      "boyfriend\n",
      "aunt\n",
      "girlfriend\n",
      "husband\n",
      "sister\n",
      "\n",
      "\n",
      "pay\n",
      "buy\n",
      "give\n",
      "make\n",
      "send\n",
      "purchase\n",
      "share\n",
      "offer\n",
      "sell\n",
      "bring\n",
      "\n",
      "\n",
      "cor\n",
      "gar\n",
      "por\n",
      "bun\n",
      "bur\n",
      "tor\n",
      "cas\n",
      "po\n",
      "ur\n",
      "hor\n",
      "\n",
      "\n",
      "algeria\n",
      "niger\n",
      "mali\n",
      "ethiopia\n",
      "tunisia\n",
      "zambia\n",
      "mogadishu\n",
      "rwanda\n",
      "cameroon\n",
      "senegal\n",
      "\n",
      "\n",
      "hughes\n",
      "myers\n",
      "sutton\n",
      "jenkins\n",
      "patterson\n",
      "johnston\n",
      "nichols\n",
      "wallace\n",
      "robinson\n",
      "griffin\n",
      "\n",
      "\n",
      "nintendo\n",
      "microsoft's\n",
      "android\n",
      "yahoo\n",
      "xbox\n",
      "microsoft\n",
      "playstation\n",
      "itunes\n",
      "samsung\n",
      "mashable\n",
      "\n",
      "\n",
      "2005\n",
      "2006\n",
      "2002\n",
      "1997\n",
      "1995\n",
      "2004\n",
      "2003\n",
      "1980\n",
      "2007\n",
      "1992\n",
      "\n",
      "\n",
      "meningitis\n",
      "chemotherapy\n",
      "infection\n",
      "asthma\n",
      "infections\n",
      "medication\n",
      "diarrhea\n",
      "cancers\n",
      "bacter\n",
      "bacteria\n",
      "\n",
      "\n",
      "proposal\n",
      "directive\n",
      "decree\n",
      "injunction\n",
      "lawsuit\n",
      "decision\n",
      "agreement\n",
      "initiative\n",
      "recommendation\n",
      "provision\n",
      "\n",
      "\n",
      "syl\n",
      "bren\n",
      "joh\n",
      "oliv\n",
      "alv\n",
      "alexand\n",
      "gonz\n",
      "jacqu\n",
      "krist\n",
      "gior\n",
      "\n",
      "\n",
      "secular\n",
      "islamist\n",
      "religious\n",
      "sectarian\n",
      "ethnic\n",
      "authoritarian\n",
      "buddhist\n",
      "islamic\n",
      "extremist\n",
      "muslim\n",
      "\n",
      "\n",
      "strategies\n",
      "methods\n",
      "policies\n",
      "challenges\n",
      "factors\n",
      "regulations\n",
      "improvements\n",
      "measures\n",
      "decisions\n",
      "restrictions\n",
      "\n",
      "\n",
      "encourage\n",
      "persuade\n",
      "convince\n",
      "minimize\n",
      "violate\n",
      "impose\n",
      "promote\n",
      "eliminate\n",
      "prohibit\n",
      "facilitate\n",
      "\n",
      "\n",
      "urged\n",
      "described\n",
      "denied\n",
      "criticized\n",
      "acknowledged\n",
      "instructed\n",
      "prompted\n",
      "characterized\n",
      "reiterated\n",
      "encouraged\n",
      "\n",
      "\n",
      "gian\n",
      "scand\n",
      "christop\n",
      "gior\n",
      "leng\n",
      "fitz\n",
      "kyr\n",
      "moroc\n",
      "debor\n",
      "karad\n",
      "\n",
      "\n",
      "editor\n",
      "historian\n",
      "manager\n",
      "consultant\n",
      "strategist\n",
      "ceo\n",
      "adviser\n",
      "commentator\n",
      "lawyer\n",
      "researcher\n",
      "\n",
      "\n",
      "militants\n",
      "jihadists\n",
      "insurgents\n",
      "iraqis\n",
      "sunnis\n",
      "libyans\n",
      "syrians\n",
      "extremists\n",
      "terrorists\n",
      "egyptians\n",
      "\n",
      "\n",
      "marred\n",
      "raided\n",
      "invaded\n",
      "destroyed\n",
      "prevented\n",
      "escorted\n",
      "chased\n",
      "ravaged\n",
      "hampered\n",
      "blocked\n",
      "\n",
      "\n",
      "peanut\n",
      "vitamin\n",
      "chicken\n",
      "soda\n",
      "dairy\n",
      "potato\n",
      "chocolate\n",
      "cheese\n",
      "protein\n",
      "tomato\n",
      "\n",
      "\n",
      "investment\n",
      "financial\n",
      "securities\n",
      "stimulus\n",
      "bailout\n",
      "austerity\n",
      "unemployment\n",
      "economic\n",
      "investments\n",
      "savings\n",
      "\n",
      "\n",
      "designers\n",
      "patrons\n",
      "researchers\n",
      "managers\n",
      "students\n",
      "guests\n",
      "teenagers\n",
      "residents\n",
      "participants\n",
      "staffers\n",
      "\n",
      "\n",
      "ske\n",
      "bru\n",
      "resur\n",
      "cau\n",
      "spe\n",
      "pav\n",
      "vul\n",
      "compo\n",
      "conne\n",
      "stair\n",
      "\n",
      "\n",
      "l\n",
      "tr\n",
      "p\n",
      "sh\n",
      "sp\n",
      "st\n",
      "ch\n",
      "gr\n",
      "j\n",
      "g\n",
      "\n",
      "\n",
      "yanukovych\n",
      "maduro\n",
      "morsy's\n",
      "gbagbo\n",
      "musharraf\n",
      "ahmadinejad\n",
      "berluscon\n",
      "yingluck\n",
      "poroshenko\n",
      "morsy\n",
      "\n",
      "\n",
      "geor\n",
      "fitz\n",
      "bou\n",
      "gior\n",
      "chand\n",
      "mol\n",
      "mccul\n",
      "nich\n",
      "bren\n",
      "cau\n",
      "\n",
      "\n",
      "enthusia\n",
      "experi\n",
      "nandez\n",
      "delaw\n",
      "attem\n",
      "mingham\n",
      "strugg\n",
      "portra\n",
      "occas\n",
      "includ\n",
      "\n",
      "\n",
      "won't\n",
      "i'll\n",
      "wouldn't\n",
      "hasn't\n",
      "can't\n",
      "haven't\n",
      "didn't\n",
      "they'll\n",
      "i've\n",
      "we'll\n",
      "\n",
      "\n",
      "shootings\n",
      "bombings\n",
      "massacre\n",
      "killings\n",
      "slayings\n",
      "ambush\n",
      "airstrike\n",
      "bloodshed\n",
      "kidnappings\n",
      "rampage\n",
      "\n",
      "\n",
      "notion\n",
      "suggestion\n",
      "possibility\n",
      "desire\n",
      "obligation\n",
      "willingness\n",
      "assumption\n",
      "refusal\n",
      "inability\n",
      "intention\n",
      "\n",
      "\n",
      "disap\n",
      "redist\n",
      "refle\n",
      "redu\n",
      "eclip\n",
      "reper\n",
      "assum\n",
      "frust\n",
      "resol\n",
      "introdu\n",
      "\n",
      "\n",
      "15\n",
      "26\n",
      "28\n",
      "25\n",
      "35\n",
      "40\n",
      "14\n",
      "27\n",
      "23\n",
      "13\n",
      "\n",
      "\n",
      "significant\n",
      "massive\n",
      "extensive\n",
      "substantial\n",
      "widespread\n",
      "catastrophic\n",
      "tremendous\n",
      "unspecified\n",
      "highly\n",
      "enormous\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "word_embed = model.word_embed\n",
    "embedding_clustering = EmbeddingClustering(tokenizer, n_clusters=100)\n",
    "embedding_clustering.fit(word_embed, normalize=True)\n",
    "embedding_clustering.print_clusters(n_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1ace1256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5101becde22c4b7796f6cdb1714c4aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[103]], shape=(1, 1), dtype=int32)\n",
      "he\n",
      "she\n",
      "he's\n",
      "they\n",
      "his\n",
      "i\n",
      "he'd\n",
      "it\n",
      "\"\n",
      "we\n",
      "an\n",
      "who\n",
      "him\n",
      "a\n",
      "she's\n",
      "himself\n",
      "there\n",
      "that\n",
      "'i\n",
      "they've\n",
      "you\n",
      "he'll\n",
      "'\n",
      "i've\n",
      "which\n",
      "it's\n",
      "president\n",
      "but\n",
      "being\n",
      "man\n",
      "i'm\n",
      "her\n",
      "obama\n",
      "ever\n",
      "the\n",
      "we've\n",
      "gu\n",
      "what\n",
      "be\n",
      "no\n",
      "nobody\n",
      "vic\n",
      "they're\n",
      "she'd\n",
      "this\n",
      "then\n",
      "ger\n",
      "father\n",
      "someone\n",
      "never\n",
      "'we\n",
      "chief\n",
      "one\n",
      "nelson\n",
      "leader\n",
      "people\n",
      "son\n",
      "whose\n",
      "lau\n",
      "smith\n",
      "and\n",
      "men\n",
      "you're\n",
      "che\n",
      "so\n",
      "was\n",
      "we're\n",
      "sch\n",
      "as\n",
      "you've\n",
      "prosecutors\n",
      "bush\n",
      "ke\n",
      "''\n",
      "while\n",
      "not\n",
      "er\n",
      "too\n",
      "jur\n",
      "boy\n",
      " \n",
      "authorities\n",
      "ch\n",
      "once\n",
      "they'd\n",
      "director\n",
      "even\n",
      "bin\n",
      "anyone\n",
      "still\n",
      "professor\n",
      "doctors\n",
      "i'd\n",
      "me\n",
      "members\n",
      "wa\n",
      "\n",
      "\n",
      "officials\n",
      "whoever\n",
      "ali\n"
     ]
    }
   ],
   "source": [
    "word_embed = model.word_embed\n",
    "\n",
    "text = \"he\"\n",
    "text = text.lower()\n",
    "\n",
    "idx = tf.cast(tokenizer.encode(text), tf.int32)\n",
    "print(idx)\n",
    "embed = tf.expand_dims(word_embed[idx[0][0]], axis=0)\n",
    "\n",
    "cosine_sim = embed@tf.transpose(word_embed)\n",
    "idx = tf.argsort(cosine_sim, axis=-1, \n",
    "                 direction='DESCENDING',\n",
    "                 #direction='ASCENDING', \n",
    "                 stable=False, name=None)[0]\n",
    "\n",
    "for i in idx[:100]:\n",
    "    i = tf.expand_dims(i, axis=0)\n",
    "    print(tokenizer.decode(i).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fb3d21e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86224c3e1d04f069968945dfb509d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b446e9b60004d5fbe93acadc9dabab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3086f839ea146bfa747d6b8f545bb3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8da14e5d463f496790c49c868eacd214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.83709925]], shape=(1, 1), dtype=float32)\n",
      "tf.Tensor([[0.6219947]], shape=(1, 1), dtype=float32)\n",
      "tf.Tensor([[0.67831075]], shape=(1, 1), dtype=float32)\n",
      "tf.Tensor([[0.6626094]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "word_embed = model.word_embed\n",
    "\n",
    "text = \"the terrorists killed the civilians\"\n",
    "text = text.lower()\n",
    "\n",
    "indices = tf.cast(tokenizer.encode(text), tf.int32)\n",
    "embeds = model.call(indices, logits=False)\n",
    "embed_mean1 = tf.cast(tf.reduce_mean(embeds, axis=1), dtype=tf.float32)\n",
    "\n",
    "text = \"the criminals were violent towards the people\"\n",
    "text = text.lower()\n",
    "indices = tf.cast(tokenizer.encode(text), tf.int32)\n",
    "embeds = model.call(indices, logits=False)\n",
    "embed_mean2 = tf.cast(tf.reduce_mean(embeds, axis=1), dtype=tf.float32)\n",
    "\n",
    "text = \"The cat was cute\"\n",
    "text = text.lower()\n",
    "indices = tf.cast(tokenizer.encode(text), tf.int32)\n",
    "embeds = model.call(indices, logits=False)\n",
    "embed_mean3 = tf.cast(tf.reduce_mean(embeds, axis=1), dtype=tf.float32)\n",
    "\n",
    "text = \"Dogs are so adorable\"\n",
    "text = text.lower()\n",
    "indices = tf.cast(tokenizer.encode(text), tf.int32)\n",
    "embeds = model.call(indices, logits=False)\n",
    "embed_mean4 = tf.cast(tf.reduce_mean(embeds, axis=1), dtype=tf.float32)\n",
    "\n",
    "\n",
    "cosine_sim1 = cosine_similarity(embed_mean1, embed_mean2, normalize=True)\n",
    "\n",
    "cosine_sim2 = cosine_similarity(embed_mean3, embed_mean4, normalize=True)\n",
    "cosine_sim3 = cosine_similarity(embed_mean1, embed_mean3, normalize=True)\n",
    "cosine_sim4 = cosine_similarity(embed_mean2, embed_mean4, normalize=True)\n",
    "print(cosine_sim1)\n",
    "print(cosine_sim2)\n",
    "print(cosine_sim3)\n",
    "print(cosine_sim4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
